package org.apache.spark.sql.execution.bootstrap


import org.apache.spark.sql.sources.MapColumnToWeight

import scala.collection.mutable


import org.apache.spark.sql.catalyst.{expressions, InternalRow}
import org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion
import org.apache.spark.sql.catalyst.errors.TreeNodeException
import org.apache.spark.sql.catalyst.expressions.Count
import org.apache.spark.sql.catalyst.expressions.Sum
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate._
import org.apache.spark.sql.catalyst.expressions.codegen.{CodeGenContext, GeneratedExpressionCode}
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.columnar.{InMemoryRelation, InMemoryColumnarTableScan}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.aggregate.SortBasedAggregate
import org.apache.spark.sql.execution.joins._
import org.apache.spark.sql.hive.SampledRelation
import org.apache.spark.sql.hive.execution.HiveTableScan
import org.apache.spark.sql.types.{IntegerType, Metadata, BooleanType, DataType}
import org.apache.spark.util.collection.BitSet

/**
 * Remark:
 * 1. Online planning should be applied before inserting exchange
 * (and thus we exclude exchange here),
 * because:
 * (1) ImplementJoin will change join types, which will affect exchange;
 * (2) Pushing resampling above exchange is always subsumed by pushing it above join.
 * Note that pushing above exchange/join will change the order of tuples,
 * and thus cannot be applied to multi-instance relations.
 * 2. Turn off LeftSemiJoinHash for now, using Join instead.
 */
object SafetyCheck extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transform {
    case leaf: LeafNode => leaf
    case filter: Filter => filter
    case project: Project => project
    case aggregate: Aggregate => aggregate
    case join: BroadcastHashJoin => join
    case join: ShuffledHashJoin => join
    // case join: LeftSemiJoinHash => join
    case sort: Sort => sort
    case _ => plan
  }.transformAllExpressions {
    case alias: Alias =>
      alias.transformDown {
        case Alias(child, _) => child
      }
  }
}


case class ResamplePlaceholder(child: SparkPlan) extends UnaryNode {

  override def output = child.output

  override def doExecute() =
    throw new TreeNodeException(this, s"No function to execute plan. type: ${this.nodeName}")
}

/**
 * Insert resampling operator at the optimal place.
 * Remark:
 * 1. Optimize by pushing ResamplePlaceholder above Filter, Project.
 * 2. This optimization is valid because ResamplePlaceholder is only used
 * for tuples from base relations. Uncertain attributes are generated by aggregates
 * which have to go through join in order to be combined,
 * and we will merge the bootstrap-counts columns into one immediately after joins.
 * 3. We cannot push ResamplePlaceholder above Join.
 * (TODO: Unless we know this is a foreign key join.)
 */
object PushUpResample extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case sampledRelation: SampledRelation => ResamplePlaceholder(sampledRelation)
    case Filter(condition, ResamplePlaceholder(child)) =>
      ResamplePlaceholder(Filter(condition, child))
    case Project(projectList, ResamplePlaceholder(child)) =>
      ResamplePlaceholder(Project(projectList ++ BootStrapUtils.getSeeds(child.output), child))
    case BroadcastHashJoin(leftKeys, rightKeys, BuildLeft,
    left: ResamplePlaceholder, ResamplePlaceholder(right)) =>
      ResamplePlaceholder(BroadcastHashJoin(leftKeys, rightKeys, BuildLeft, left, right))
    case BroadcastHashJoin(leftKeys, rightKeys, BuildRight,
    ResamplePlaceholder(left), right: ResamplePlaceholder) =>
      ResamplePlaceholder(BroadcastHashJoin(leftKeys, rightKeys, BuildLeft, left, right))
    case BroadcastHashJoin(leftKeys, rightKeys, buildSide, ResamplePlaceholder(left), right) =>
      ResamplePlaceholder(BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, ResamplePlaceholder(right)) =>
      ResamplePlaceholder(BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case ShuffledHashJoin(leftKeys, rightKeys, BuildLeft,
    left: ResamplePlaceholder, ResamplePlaceholder(right)) =>
      ResamplePlaceholder(ShuffledHashJoin(leftKeys, rightKeys, BuildLeft, left, right))
    case ShuffledHashJoin(leftKeys, rightKeys, BuildRight,
    ResamplePlaceholder(left), right: ResamplePlaceholder) =>
      ResamplePlaceholder(ShuffledHashJoin(leftKeys, rightKeys, BuildLeft, left, right))
    case ShuffledHashJoin(leftKeys, rightKeys, buildSide, ResamplePlaceholder(left), right) =>
      ResamplePlaceholder(ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, ResamplePlaceholder(right)) =>
      ResamplePlaceholder(ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case LeftSemiJoinHash(leftKeys, rightKeys, ResamplePlaceholder(left), right, expressions) =>
      ResamplePlaceholder(LeftSemiJoinHash(leftKeys, rightKeys, left, right, expressions))
    case Sort(sortOrder, global, ResamplePlaceholder(child)) =>
      ResamplePlaceholder(Sort(sortOrder, global, child))
  }
}

/**
 * Postpone materializing seed.
 */
object PushUpSeed extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    /* case stream@StreamedRelation(Project(projectList, child)) =>
       val (seeds, others) = projectList.partition {
         case TaggedAlias(Seed(ref), _, _) if !ref.multiRef => true
         case TaggedAttribute(Seed(ref), _, _, _, _) if !ref.multiRef => true
         case _ => false
       }
       val stream2 = StreamedRelation(Project(others, child))(
         stream.reference, stream.controller, stream.trace)
       Project(stream2.output ++ seeds, stream2)*/

    case Project(projectList1, Project(projectList2, child)) =>
      // This should be efficient,
      // as we don't manage bootstrap-counts in Project except after joins.
      val map = projectList2.map(named => (named.exprId, named)).toMap
      val projectList = projectList1.map {
        case attr: Attribute => map(attr.exprId)
        case named => named.transformUp {
          case attr: Attribute => map(attr.exprId)
        }.transformUp {
          case alias: Alias => alias.child
          case alias: TaggedAlias => alias.child
        }.asInstanceOf[NamedExpression]
      }
      Project(projectList, child)

    /* case filter@Filter(condition, Project(projectList, child)) =>
       val (seeds, others) = projectList.partition {
         case TaggedAlias(Seed(ref), _, _) if !ref.multiRef => true
         case TaggedAttribute(Seed(ref), _, _, _, _) if !ref.multiRef => true
         case _ => false
       }
       if (AttributeSet(seeds.map(_.toAttribute)).intersect(condition.references).isEmpty) {
         val filter2 = Filter(condition, Project(others, child))
         Project(filter2.output ++ seeds, filter2)
       } else {
         filter
       }*/
  }
}

/**
 * Partial-Aggregate
 * Join
 * Stem -- Computes all partial aggregates references
 * Filter-Branch -- (1) Only computes the join keys, serving as a filter
 * (2) Tuples may be uncertain
 * (i.e., has uncertain flag after IdentifyUncertainTuples)
 *
 * is optimized into:
 *
 * Join
 * Partial-Aggregate
 * Stem
 * Branch
 */
object PushDownPartialAggregate extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case aggregate@Aggregate(true, groupings, aggrs,
    BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right)) =>
      val references = (groupings ++ aggrs).map(_.references).reduceOption(_ ++ _)
          .getOrElse(AttributeSet(Nil))

      if (references.subsetOf(right.outputSet) && isFilterBranch(left)) {
        val groupKeys = (groupings ++ rightKeys.flatMap(_.references)).distinct
        val aggregates = (aggrs ++ rightKeys.flatMap(_.references)).distinct
        BroadcastHashJoin(leftKeys, rightKeys, buildSide,
          left, Aggregate(partial = true, groupKeys, aggregates, right))
      } else if (references.subsetOf(left.outputSet) && isFilterBranch(right)) {
        val groupKeys = (groupings ++ leftKeys.flatMap(_.references)).distinct
        val aggregates = (aggrs ++ leftKeys.flatMap(_.references)).distinct
        BroadcastHashJoin(leftKeys, rightKeys, buildSide,
          Aggregate(partial = true, groupKeys, aggregates, left), right)
      } else {
        aggregate
      }

    case aggregate@Aggregate(true, groupings, aggrs,
    ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right)) =>
      val references = (groupings ++ aggrs).map(_.references).reduce(_ ++ _)

      if (references.subsetOf(right.outputSet) && isFilterBranch(left)) {
        val groupKeys = (groupings ++ rightKeys.flatMap(_.references)).distinct
        val aggregates = (aggrs ++ rightKeys.flatMap(_.references)).distinct
        ShuffledHashJoin(leftKeys, rightKeys, buildSide,
          left, Aggregate(partial = true, groupKeys, aggregates, right))
      } else if (references.subsetOf(left.outputSet) && isFilterBranch(right)) {
        val groupKeys = (groupings ++ leftKeys.flatMap(_.references)).distinct
        val aggregates = (aggrs ++ leftKeys.flatMap(_.references)).distinct
        ShuffledHashJoin(leftKeys, rightKeys, buildSide,
          Aggregate(partial = true, groupKeys, aggregates, left), right)
      } else {
        aggregate
      }
  }

  def intersect(expr: Expression, attrs: AttributeSet): Boolean =
    expr.references.intersect(attrs).nonEmpty

  def intersect(exprs: Seq[Expression], attrs: AttributeSet*): Boolean =
    exprs.exists(e => attrs.exists(a => intersect(e, a)))

  def aggregations(expr: Expression) = expr.collect { case a: AggregateExpression => a }

  def isFilterBranch(plan: SparkPlan): Boolean = {
    var smplAttr = AttributeSet(Seq())
    var uncAttr = AttributeSet(Seq())
    var flag = false

    plan.transformUp {
      case resample: ResamplePlaceholder =>
        smplAttr = smplAttr ++ AttributeSet(resample.output)
        resample
      case project@Project(projectList, child) =>
        smplAttr = smplAttr ++
            AttributeSet(projectList.collect { case e if intersect(e, smplAttr) => e.toAttribute })
        uncAttr = uncAttr ++
            AttributeSet(projectList.collect { case e if intersect(e, uncAttr) => e.toAttribute })
        project
      case aggregate@Aggregate(_, _, aggrs, _) =>
        smplAttr = smplAttr ++
            AttributeSet(aggrs.collect {
              case e if !intersect(aggregations(e), smplAttr, uncAttr) => e.toAttribute
            })
        uncAttr = uncAttr ++
            AttributeSet(aggrs.collect {
              case e if intersect(aggregations(e), smplAttr, uncAttr) => e.toAttribute
            })
        aggregate
      case filter@Filter(condition, _) if intersect(condition, uncAttr) =>
        flag = true
        filter
    }

    flag
  }
}


/**
 * Replace the resampling operator placeholder with its actual implementation.
 */
object ImplementResample extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transform {
    case rs@ResamplePlaceholder(child) =>
      val (others, seeds) = child.output.partition {
        case TaggedAttribute(_: Seed, _, _, _, _) => false
        case _ => true
      }
      val btCnt = TaggedAlias(Bootstrap(),
        seeds.map(s => SetSeedAndPoisson(s, Poisson())).reduce(Multiply), "_btcnt")()
      Project(others :+ btCnt, child)
  }
}

case class LowerPlaceholder(child: Expression) extends UnaryExpression {
  type EvaluatedType = Any

  override def foldable = child.foldable

  override def nullable: Boolean = child.nullable

  override def dataType: DataType = child.dataType

  override def toString = s"Lower($child)"

  override def eval(input: InternalRow): Any =
    throw new TreeNodeException(this, s"No function to evaluate expression. type: ${this.nodeName}")

  override def genCode(ctx: CodeGenContext, ev: GeneratedExpressionCode): String =
    defineCodeGen(ctx, ev, c => c)
}

case class UpperPlaceholder(child: Expression) extends UnaryExpression {
  type EvaluatedType = Any

  override def foldable = child.foldable

  override def nullable: Boolean = child.nullable

  override def dataType: DataType = child.dataType

  override def toString = s"Upper($child)"

  override def eval(input: InternalRow): Any =
    throw new TreeNodeException(this, s"No function to evaluate expression. type: ${this.nodeName}")

  override def genCode(ctx: CodeGenContext, ev: GeneratedExpressionCode): String =
    defineCodeGen(ctx, ev, c => c)
}

/**
 * Propagate reampling through the plan, and insert uncertain attributes, e.g.
 * 1. Propagate bootstrap-counts
 * 2. Aggregates should take in account of bootstrap-counts
 * 3. Join should multiply bootstrap-counts columns
 * 4. Filter should update bootstrap-counts
 * NOTE: handled after uncertain attributes are identified
 * 5. Uncertain attributes are inferred based on the assumption of
 * no partitioning in the input relations.
 */
object PropagateBootstrap extends Rule[SparkPlan] {

  import BoundType._

  private[this] val JOIN_KEY_ERROR = "Join keys cannot be uncertain columns."
  private[this] val GROUP_BY_KEY_ERROR = "Group-by keys cannot be uncertain columns."
  private[this] val SORT_KEY_ERROR = "Sort keys cannot be uncertain columns."
  val boolTrue = Literal(true)
  val boolFalse = Literal(false)
  val byte0 = Literal(0.toByte)
  val byte1 = Literal(1.toByte)

  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case q: SparkPlan =>
      val withNewLeaves: SparkPlan = q.transformExpressionsUp {
        case attr: Attribute => q.inputSet.find(_.exprId == attr.exprId).getOrElse(attr)
      }


      val transformed = withNewLeaves match {
        case project@Project(projectList, child) =>
          BootStrapUtils.getBtCnt(child.output) match {
            case Some(btCnt@TaggedAttribute(Bootstrap(), _, _, _, _)) =>
              var seenBtCnt = false
              val toBounds = BootStrapUtils.extractBounds(child.output)
              val updated = projectList.flatMap {
                case alias@TaggedAlias(_, expr, name) =>
                  seenBtCnt = true
                  TaggedAlias(Bootstrap(), ByteMultiply(btCnt, expr), name)(
                    alias.exprId, alias.qualifiers) :: Nil
                case alias@Alias(expr, name) if BootStrapUtils.containsUncertain(expr.references) =>
                  TaggedAlias(Uncertain, expr, name)(alias.exprId, alias.qualifiers) +:
                      BootStrapUtils.explode(expr, toBounds).map { e =>
                        TaggedAlias(Bound(Mixed, alias.exprId), e, name)(
                          qualifiers = alias.qualifiers)
                      }
                case attr@TaggedAttribute(Uncertain, _, _, _, _) =>
                  attr +: toBounds(attr.exprId)
                case expr => expr :: Nil
              }
              val newProjectList = if (seenBtCnt) updated else updated :+ btCnt
              Project(newProjectList, child)
            case _ => project
          }

        /*case aggregate@Aggregate(partial, groupings, aggrs, child) =>
          require(deterministicKeys(groupings), GROUP_BY_KEY_ERROR)
          BootStrapUtils.getBtCnt(child.output) match {
            case Some(btCnt@TaggedAttribute(Bootstrap(), _, _, _, _)) =>
              val params = aggrs.flatMap {
                _.collect { case a: AggregateExpression => a.children }.flatten
              }.distinct

              val (optimized, newChild) =
                if (params.forall { case _: Attribute | _: Literal => true case _ => false }) {
                  (aggrs, child)
                } else {
                  val map = params.flatMap {
                    case n: NamedExpression => Some((n, n))
                    case _: Literal => None
                    case e => Some((e, Alias(e, "_param")()))
                  }.toMap
                  val projectList =
                    (groupings.flatMap(_.references) ++ map.values :+ btCnt).distinct
                  val consolidated = aggrs.map {
                    _.transformUp { case a: AggregateExpression =>
                      a.mapChildren {
                        case e if map.contains(e) => map(e).toAttribute
                        case e => e
                      }
                    }
                  }
                  (consolidated, Project(projectList, child))
                }

              val aBtCnt = TaggedAlias(Bootstrap(),
                Delegate(btCnt, Count01(boolTrue)), "_btcnt")()
              val newAggrs = optimized.flatMap {
                case alias@Alias(expr, name) if isAggregate(expr) =>
                  val newExpr = expr.transformUp {
                    case sum: org.apache.spark.sql.catalyst.expressions.Sum =>
                      val dSum = if (partial) {
                        Delegate(btCnt, sum)
                      } else {
                        Cast(scale(Delegate(btCnt, sum), ScaleFactor()), sum.dataType)
                      }

                      If(ByteNonZero(aBtCnt.child), dSum, Literal.create(null, dSum.dataType))

                    /*case CombineSum(e) =>
                      val sum = Sum(e) // TODO: fix this hack
                    val dSum = if (partial || b.isEmpty) {
                        Delegate(btCnt, sum)
                      } else {
                        Cast(scale(Delegate(btCnt, sum), ScaleFactor(b)), sum.dataType)
                      }
                      If(ByteNonZero(aBtCnt.child), dSum, Literal.create(null, dSum.dataType))
*/
                    case count: org.apache.spark.sql.catalyst.expressions.Count =>
                      if (partial) {
                        Delegate(btCnt, count)
                      } else {
                        Cast(scale(Delegate(btCnt, count), ScaleFactor()), count.dataType)
                      }

                    case _: AggregateExpression => ??? // There should not be any Average
                  }
                  if (!partial) {
                    TaggedAlias(Uncertain, newExpr, name)(alias.exprId, alias.qualifiers) ::
                        TaggedAlias(Bound(Lower, alias.exprId), LowerPlaceholder(newExpr), name)(
                          qualifiers = alias.qualifiers) ::
                        TaggedAlias(Bound(Upper, alias.exprId), UpperPlaceholder(newExpr), name)(
                          qualifiers = alias.qualifiers) ::
                        Nil
                  } else if (BootStrapUtils.containsUncertain(expr.references)) {
                    TaggedAlias(Uncertain, newExpr, name)(alias.exprId, alias.qualifiers) :: Nil
                  } else {
                    Alias(newExpr, name)(alias.exprId, alias.qualifiers) :: Nil
                  }

                case expr: NamedExpression => expr :: Nil
              } :+ aBtCnt

              BootstrapAggregate(partial, groupings, newAggrs, newChild)

            case _ => aggregate
          }*/

        case aggregate@SortBasedAggregate(_,
        groupings: Seq[NamedExpression],
        nonCompleteAggregates: Seq[AggregateExpression2],
        nonCompleteAggregateAttributes: Seq[Attribute],
        _,
        _,
        _,
        resultExpressions: Seq[NamedExpression],
        child: SparkPlan) =>

          val partial = nonCompleteAggregates.exists(_.mode match {
            case Partial => true
            case PartialMerge => true
            case Final => false
            case Complete => false
          })
          require(deterministicKeys(groupings), GROUP_BY_KEY_ERROR)
          BootStrapUtils.getBtCnt(child.output) match {
            case Some(btCnt@TaggedAttribute(Bootstrap(), _, _, _, _)) =>
              val params = nonCompleteAggregates.flatMap {
                _.collect {
                  case a: AggregateExpression1 => a.children
                  case agg@AggregateExpression2(aggFunc: AlgebraicAggregate, _, _) => if (partial) {
                    aggFunc.children.flatMap { y => y match {
                      case x: BinaryExpression => x.left.children ++ x.right.children
                      case _ => y.children

                    }
                    }
                  } else {
                    aggFunc.mergeExpressions.flatMap {
                      _.collect {
                        case a: Attribute => a
                      }
                    }
                  }
                }.flatten
              }.distinct

              val (optimized, newChild) =
                if (params.forall { case _: Attribute | _: Literal => true case _ => false }) {
                  (nonCompleteAggregates.zipWithIndex, child)
                } else {
                  //TODO. Identify the right aggregate attributes for the expressions
                //  throw new UnsupportedOperationException("not yet implemented")
                  val map = params.flatMap {
                    case n: NamedExpression => Some((n, n))
                    case _: Literal => None
                    case Cast(n: NamedExpression, _) => Some((n, n))
                    case x => Some((x, Alias(x, "_param")()))
                  }.toMap
                  val projectList =
                    (groupings.flatMap(_.references) ++ map.values :+ btCnt).distinct
                  val consolidated = nonCompleteAggregates.zipWithIndex.map { case (exprsn, index) => {
                    (exprsn.transformUp {
                      case a: AggregateExpression =>
                        a.mapChildren {
                          case e if map.contains(e) => map(e).toAttribute
                          case e => e
                        }
                    }
                      , index)
                  }
                  }
                  (consolidated, Project(projectList, child))
                }



              val aBtCnt = TaggedAlias(Bootstrap(),
                Delegate(btCnt, Count01(boolTrue)), "_btcnt")()
              val exchangeOption = aggregate.find  {
                case Exchange(_,_) => true
                case _ => false
              }

              var i = 0

              val newAggrs = optimized.flatMap {
                case (a:AggregateExpression2, index: Int) =>
                  val newExpr = a.transformUp {

                    case avg@org.apache.spark.sql.catalyst.expressions.aggregate.Average(e) =>
                      val dAvg = if (partial) {
                        var weightCol: MapColumnToWeight = null
                        //extract out weightage column as it is showing as Coalesce with the averaged quantity
                         val modified = avg.transformUp {
                           case Coalesce(seq) => {
                             val newSeq = seq.map {
                               case weight: MapColumnToWeight => {
                                 weightCol = weight
                                 Literal(null)
                               }
                               case x => x
                             }
                             Coalesce(newSeq)
                           }
                         }.asInstanceOf[org.apache.spark.sql.catalyst.expressions.aggregate.Average]
                         val sumExpression = if (weightCol != null) {
                           Multiply(weightCol, modified.child)
                         } else {
                           e
                         }

                        DelegateFunction(btCnt, org.apache.spark.sql.catalyst.expressions.aggregate.Sum(sumExpression))
                      } else {
                        //remap the parameter
                        // find exchange operator

                        remapExchange(btCnt, exchangeOption, avg)
                      }

                      dAvg

                    case sum: org.apache.spark.sql.catalyst.expressions.aggregate.Sum =>
                      val dSum = if (partial) {
                        DelegateFunction(btCnt, sum)
                      } else {
                        //remap the parameter
                        // find exchange operator

                        remapExchange(btCnt, exchangeOption, sum)
                      }

                      dSum
                    //If(ByteNonZero(aBtCnt.child), dSum, Literal.create(null, dSum.dataType))

                    /*case CombineSum(e) =>
                      val sum = Sum(e) // TODO: fix this hack
                    val dSum = if (partial || b.isEmpty) {
                        Delegate(btCnt, sum)
                      } else {
                        Cast(scale(Delegate(btCnt, sum), ScaleFactor(b)), sum.dataType)
                      }
                      If(ByteNonZero(aBtCnt.child), dSum, Literal.create(null, dSum.dataType))
                   */
                    case count: org.apache.spark.sql.catalyst.expressions.aggregate.Count =>
                      if (partial) {
                        DelegateFunction(btCnt, count)
                      } else {
                        //Cast(scale(DelegateFunction(btCnt, count), ScaleFactor()), count.dataType)
                        DelegateFunction(btCnt, count)
                      }


                    //case _: AggregateExpression => ??? // There should not be any Average
                  }.asInstanceOf[AggregateExpression2]


                  if (!partial) {
                    val name = aggregate.resultExpressions.find(_.children.exists {
                      case x: NamedExpression => x.exprId == nonCompleteAggregateAttributes(index).exprId
                      case _ => false
                    }) match {
                      case Some(x) => x.name
                      case None => nonCompleteAggregateAttributes(index).name
                    }
                    val ne1 = TaggedAggregateExpression2(Uncertain, newExpr.aggregateFunction, a.mode, a.isDistinct, name)()
                    val ne = TaggedAlias(Uncertain, ne1, name)(nonCompleteAggregateAttributes(index).exprId)

                    ne ::
                        TaggedAlias(Bound(Lower, ne.exprId), LowerPlaceholder(newExpr), ne.name)(
                          qualifiers = ne.qualifiers) ::
                        TaggedAlias(Bound(Upper, ne.exprId), UpperPlaceholder(newExpr), ne.name)(
                          qualifiers = ne.qualifiers) ::
                        Nil

                  } /*else if (BootStrapUtils.containsUncertain(expr.references)) {
                 TaggedAlias(Uncertain, newExpr, name)(alias.exprId, alias.qualifiers) :: Nil
                }*/
                  else {
                    val exprID = aggregate.nonCompleteAggregateAttributes(i).exprId
                    UnTaggedAggregateExpression2(newExpr.aggregateFunction, a.mode, a.isDistinct, "dummy")() :: Nil
                  }


                case (expr:NamedExpression, _) => expr :: Nil
              } :+ aBtCnt
              val groupByPosInResulst = new scala.collection.mutable.ArrayBuffer[Int]()
              val bs = if(!(partial || groupings.isEmpty)) {
                 val bsTemp = new BitSet(groupings.length)

                 val resultExpWithIndex = resultExpressions.zipWithIndex
                 0 until groupings.length foreach { i =>
                   resultExpWithIndex.find{ case(p,_) => p.toAttribute.exprId == groupings(i).exprId } match {
                     case Some((_, index)) => {
                       bsTemp.set(i)
                       groupByPosInResulst +=(index)
                     }
                     case None =>

                   }
                   /*if(resultExpWithIndex.exists(_.toAttribute.exprId == groupings(i).exprId)) {
                     bsTemp.set(i)
                   }*/
                 }
                 Some(bsTemp)

              }else {
                None
              }

              i = i + 1

              BootstrapSortedAggregate(partial, groupings, newAggrs.asInstanceOf[Seq[NamedExpression]], bs,
                groupByPosInResulst, newChild)

            case _ => aggregate

          }
        case join@BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
          // TODO: move uncertain join keys to a filter
          require(deterministicKeys(leftKeys) && deterministicKeys(rightKeys), JOIN_KEY_ERROR)
          postprocess(join)

        case join@ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
          require(deterministicKeys(leftKeys) && deterministicKeys(rightKeys), JOIN_KEY_ERROR)
          postprocess(join)

       /* case join@LeftSemiJoinHash(leftKeys, rightKeys, left, right, None) =>
          require(deterministicKeys(leftKeys) && deterministicKeys(rightKeys), JOIN_KEY_ERROR)
          // Distinct the right branch
          val newJoin = BootStrapUtils.getBtCnt(right.output) match {
            case Some(btCnt@TaggedAttribute(Bootstrap(), _, _, _, _)) =>
              val newBtCnt = TaggedAlias(Bootstrap(),
                Delegate(btCnt, Count01(boolTrue)), "_btcnt")()
              val newRight = BootstrapAggregate(partial = false, rightKeys,
                BootStrapUtils.toNamed(rightKeys) :+ newBtCnt, right)
              LeftSemiJoinHash(leftKeys, rightKeys, left, newRight, None)
            case None => join
          }
          postprocess(newJoin)*/

        case sort@Sort(sortOrder, global, child) =>
          require(deterministicKeys(sortOrder), SORT_KEY_ERROR)
          sort

        case other => other
      }

      // Simplify cast
      transformed.transformExpressionsUp(BootStrapUtils.simplifyCast)


  }

  def  remapExchange[T <: AggregateFunction2](btCnt: TaggedAttribute, exchangeOption: Option[SparkPlan], aggFunc2: T): DelegateFunction = {
    val z = exchangeOption match {
      case Some(Exchange(_, child)) =>


        val newAggChildOption = child match {
          case bsa@BootstrapSortedAggregate(_, groupings, aggExp, _, _, _) => {


            val exp = aggExp.zipWithIndex.filter { case (x, idx) => {
              aggFunc2.children.flatMap(_.references).forall {
                attrib => x.flatMap(_.references).exists {
                  _.exprId == attrib.exprId
                }

              }

            }

            }

            if (!exp.isEmpty) {
              val (temp, idx) = exp(0)
              Some(AttributeReference("dummy_replace", temp.dataType,
                temp.nullable,
                temp.metadata)(
                bsa.output(groupings.length + idx).exprId,
                temp.qualifiers))

            } else {
              None
            }
          }

          case _ => None
        }

        newAggChildOption match {

          case Some(newAggChild) => aggFunc2.withNewChildren(newAggChild :: Nil)
          case None => aggFunc2

        }

      case None => throw new scala.UnsupportedOperationException("no exchange operator found")
    }

    //Cast(scale(DelegateFunction(btCnt, sum), ScaleFactor()), sum.dataType)
    DelegateFunction(btCnt, z.asInstanceOf[T])
  }

  def deterministicKeys(keys: Seq[Expression]): Boolean =
    keys.forall(key => !BootStrapUtils.containsUncertain(key.references))

  def isAggregate(expr: Expression): Boolean = expr match {
    case _: AggregateExpression | AggregateExpression2(_, _, _) => true
    case _ => expr.children.exists(isAggregate)
  }

  def scale(expr: Expression, factor: Expression) = {
    val (left, right) = BootStrapUtils.widenTypes(expr, factor)
    Multiply(left, right)
  }

  def postprocess(plan: SparkPlan): SparkPlan =
    plan.output.partition {
      case TaggedAttribute(_: Bootstrap, _, _, _, _) => true
      case _ => false
    } match {
      case (Seq(a@TaggedAttribute(Bootstrap(), _, _, _, _),
      b@TaggedAttribute(Bootstrap(), _, _, _, _)), others) =>
        Project(others :+ TaggedAlias(Bootstrap(), ByteMultiply(a, b), "_btcnt")(), plan)
      case _ => plan
    }
}

case class ExistsPlaceholder(child: Expression) extends UnaryExpression with Predicate {
  override def foldable = child.foldable

  override def nullable: Boolean = child.nullable

  override def toString = s"Exists($child)"

  override def eval(input: InternalRow): Any =
    throw new TreeNodeException(this, s"No function to evaluate expression. type: ${this.nodeName}")

  override def genCode(ctx: CodeGenContext, ev: GeneratedExpressionCode): String =
    defineCodeGen(ctx, ev, c => c)
}

case class ForAllPlaceholder(child: Expression) extends UnaryExpression with Predicate {
  override def foldable = child.foldable

  override def nullable: Boolean = child.nullable

  override def toString = s"ForAll($child)"

  override def eval(input: InternalRow): Any =
    throw new TreeNodeException(this, s"No function to evaluate expression. type: ${this.nodeName}")

  override def genCode(ctx: CodeGenContext, ev: GeneratedExpressionCode): String =
    defineCodeGen(ctx, ev, c => c)
}

/**
 * Insert uncertain flag.
 * Remark:
 * 1. Uncertainty flags are inserted to optimize caching.
 * 2. Any case where the bootstrap-counts may change is flagged uncertain.
 */
object IdentifyUncertainTuples extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case Filter(condition, child) if BootStrapUtils.containsUncertain(condition.references) =>
      val checks = BootStrapUtils.explode(condition, BootStrapUtils.extractBounds(child.output))
      val flag = checks.tail.map { check =>
        Coalesce(EqualTo(check, checks.head) :: BootStrapUtils.boolFalse :: Nil)
      }.reduce(And)
      val projectList = child.output.collect {
        case a@TaggedAttribute(tag: Bootstrap, _, _, _, _) =>
          TaggedAlias(tag, If(condition, a, BootStrapUtils.byte0), "_btcnt")(a.exprId, a.qualifiers)
        case a@TaggedAttribute(Flag, _, _, _, _) =>
          TaggedAlias(Flag, And(a, flag), "_flag")(a.exprId, a.qualifiers)
        case a => a
      } ++ {
        BootStrapUtils.getFlag(child.output) match {
          case Some(_) => None
          case None => Some(TaggedAlias(Flag, flag, "_flag")())
        }
      }
      val filter = ExistsPlaceholder(ByteNonZero(BootStrapUtils.getBtCnt(projectList.map(_.toAttribute)).get))
      Filter(filter, Project(projectList, child))

    case project@Project(projectList, child) =>
      Project(projectList ++ BootStrapUtils.getFlag(child.output), child)

    case aggregate@BootstrapAggregate(partial, groupings, aggrs, child) =>
      val btCnt = BootStrapUtils.getBtCnt(child.output).get
      if (partial) {
        BootStrapUtils.getFlag(child.output) match {
          case Some(flag) => BootstrapAggregate(partial, groupings :+ flag, aggrs :+ flag, child)
          case _ => aggregate
        }
      } else {
        val flag = BootStrapUtils.getFlag(child.output) match {
          case Some(f) =>
            TaggedAlias(Flag,
              ForAllPlaceholder(ByteNonZero(Delegate(btCnt, Count01(f)))), "_flag")()
          case _ =>
            TaggedAlias(Flag,
              ForAllPlaceholder(ByteNonZero(Delegate(btCnt, Count01(BootStrapUtils.boolTrue)))),
              "_flag")()
        }
        BootstrapAggregate(partial, groupings, aggrs :+ flag, child)
      }

    case aggregate@BootstrapSortedAggregate(partial, groupings, aggrs, bs, groupByPosInRs, child) =>
      val btCnt = BootStrapUtils.getBtCnt(child.output).get
      if (partial) {
        BootStrapUtils.getFlag(child.output) match {
          case Some(flag) => BootstrapSortedAggregate(partial, groupings :+ flag, aggrs :+ flag, bs, groupByPosInRs, child)
          case _ => aggregate
        }
      } else {
        val flag = BootStrapUtils.getFlag(child.output) match {
          case Some(f) =>
            TaggedAlias(Flag,
              ForAllPlaceholder(ByteNonZero(Delegate(btCnt, Count01(f)))), "_flag")()
          case _ =>
            TaggedAlias(Flag,
              ForAllPlaceholder(ByteNonZero(Delegate(btCnt, Count01(BootStrapUtils.boolTrue)))),
              "_flag")()
        }
        BootstrapSortedAggregate(partial, groupings, aggrs :+ flag, bs, groupByPosInRs, child)
      }
    case join@BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
      postprocess(join)

    case join@ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
      postprocess(join)

    case join@LeftSemiJoinHash(leftKeys, rightKeys, left, right, None) =>
      postprocess(join)

    case sort@Sort(sortOrder, global, child) =>
      BootStrapUtils.getFlag(child.output) match {
        case None => sort
        case Some(flag) =>
          val projectList = child.output.map {
            case a@TaggedAttribute(Flag, _, _, _, _) =>
              TaggedAlias(Flag, And(BootStrapUtils.boolFalse, a), "_flag")(a.exprId, a.qualifiers)
            case a => a
          }
          Sort(sortOrder, global, Project(projectList, child))
      }
  }

  def postprocess(plan: SparkPlan): SparkPlan =
    plan.output.partition {
      case TaggedAttribute(Flag, _, _, _, _) => true
      case _ => false
    } match {
      case (Seq(f1@TaggedAttribute(Flag, _, _, _, _),
      f2@TaggedAttribute(Flag, _, _, _, _)), others) =>
        Project(others :+ TaggedAlias(Flag, And(f1, f2), "_flag")(), plan)
      case _ => plan
    }
}

case class ImplementProject extends Rule[SparkPlan] {

  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case project@Project(projectList, child) if flagModified(projectList) =>
      val mode = BootStrapUtils.getGrowthMode(project)

      OnlineProject(

        mode.numPartitions == Growth.Fixed, projectList, child)()

  }

  def flagModified(projectList: Seq[Expression]): Boolean = projectList.exists {
    case TaggedAlias(Flag, _, _) => true
    case _ => false
  }
}

object CleanupOutputTuples extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = BootStrapUtils.getBtCnt(plan.output) match {
    case Some(b) => Filter(ExistsPlaceholder(ByteNonZero(b)), plan)
    case None => plan
  }
}

case class CollectPlaceholder(projectList: Seq[NamedExpression], child: SparkPlan)
    extends UnaryNode {

  override def references: AttributeSet = super.references ++ AttributeSet(BootStrapUtils.getFlag(child.output))

  override def output = projectList.map(_.toAttribute)

  override def doExecute() =
    throw new TreeNodeException(this, s"No function to execute plan. type: ${this.nodeName}")
}

case class InsertCollect(isDebug: Boolean, confidence: Double) extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.collect {
    case s: SampledRelation => s
  }.headOption match {
    case Some(_) =>
      val projectList = BootStrapUtils.getBtCnt(plan.output) match {
        case Some(b) =>
          plan.output.filter {
            case TaggedAttribute(Flag, _, _, _, _) => false
            case TaggedAttribute(_: Bound, _, _, _, _) => false
            case TaggedAttribute(_: Bootstrap, _, _, _, _) => false
            case _ => true
          }.map {
            case a: TaggedAttribute if !isDebug =>
              Alias(ApproxColumn(confidence, a :: Nil, b :: Nil), a.name)(a.exprId, a.qualifiers)
            case a: Attribute => a
          }
        case None => plan.output
      }
      CollectPlaceholder(projectList, plan)
    case None => plan
  }
}

object PruneColumns extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = {
    val used = new mutable.HashSet[ExprId]()
    plan.output.foreach {
      case TaggedAttribute(_: Bound, _, _, _, _) =>
      case a => used += a.exprId
    }

    plan.transformDown {
      case Project(projectList, child) =>
        val project = Project(projectList.filter(e => used.contains(e.exprId)), child)
        used ++= project.references.map(_.exprId)
        project

      case BootstrapAggregate(partial, groupings, aggregates, child) =>
        val aggregate = BootstrapAggregate(partial, groupings,
          aggregates.filter(e => used.contains(e.exprId)), child)
        used ++= aggregate.references.map(_.exprId)
        aggregate
      case bsa@BootstrapSortedAggregate(partial, groupings, aggregates, bs, groupByPosInRS, child) =>
        //Identify those from output result expressions which are not used by the outer parent,
        //which will then be used to filter the aggregate expression
        val unusedIndices = bsa.output.zipWithIndex.filter{case (e,indx) => !used.contains(e.exprId)}.map{ case(_, index) => index }
        //the index also contains the group by cols attended

        val newAggs = aggregates.zipWithIndex.filter{case (e, index) => !unusedIndices.contains(index + groupings.length)}.map{case(e,_) => e}
        val aggregate = BootstrapSortedAggregate(partial, groupings,
         newAggs, bs, groupByPosInRS, child)
        used ++= aggregate.references.map(_.exprId)
        aggregate

      case Aggregate(partial, groupings, aggregates, child) =>
        val aggregate = Aggregate(partial, groupings,
          aggregates.filter(e => used.contains(e.exprId)), child)
        used ++= aggregate.references.map(_.exprId)
        aggregate

      case op =>
        used ++= op.references.map(_.exprId)
        op
    }
  }
}

object PushDownFilter extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transform {
    case filter@Filter(condition, Project(projectList, child))
      if condition.references.subsetOf(child.outputSet) =>
      val exprIds = condition.references.map(_.exprId).toSet
      val accessed = projectList.filter(ne => exprIds.contains(ne.exprId))
      if (!accessed.forall(_.isInstanceOf[Attribute])) filter
      else Project(projectList, Filter(condition, child))
  }
}

object PruneProjects extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case Project(projectList1, Project(projectList2, child)) =>
      // This should be efficient,
      // as we don't manage bootstrap-counts in Project except after joins.
      val map = projectList2.map(named => (named.exprId, named)).toMap
      val projectList = projectList1.map {
        case attr: Attribute => map(attr.exprId)
        case named => named.transformUp {
          case attr: Attribute => map(attr.exprId)
        }.transformUp {
          case alias: Alias => alias.child
          case alias: TaggedAlias => alias.child
        }.asInstanceOf[NamedExpression]
      }
      Project(projectList, child)

    case Project(projectList, child) if projectList.toSet == child.output.toSet => child

    case Aggregate(partial, groupings, aggregates, Project(projectList, child))
      if simpleCopy(projectList) =>
      Aggregate(partial, groupings, aggregates, child)

    case BootstrapAggregate(partial, groupings, aggregates, Project(projectList, child))
      if simpleCopy(projectList) =>
      BootstrapAggregate(partial, groupings, aggregates, child)

    case BootstrapSortedAggregate(partial, groupings, aggregates, bs, groupByPosInRS, Project(projectList, child))
      if simpleCopy(projectList) =>
      BootstrapSortedAggregate(partial, groupings, aggregates, bs, groupByPosInRS, child)

    case CollectPlaceholder(projectList1, Project(projectList2, child))
      if projectList2.forall {
        case _: Attribute => true
        case TaggedAlias(Flag, Literal(false, BooleanType), _) => true
        case _ => false
      } =>
      CollectPlaceholder(projectList1, child)


  }

  def simpleCopy(projectList: Seq[NamedExpression]): Boolean = projectList.forall {
    case _: Attribute => true
    case _ => false
  }
}

object OptimizeOperatorOrder extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case CollectPlaceholder(projectList, Sort(sortOrder, true, child)) =>
      Sort(sortOrder, global = true, CollectPlaceholder(projectList, child))

    case Filter(condition, Sort(sortOrder, global, child)) =>
      Sort(sortOrder, global, Filter(condition, child))
  }
}

object PruneFilters extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case Filter(condition1, Filter(condition2, child)) =>
      val condition =
        if (condition1 == condition2) condition1
        else And(condition1, condition2)
      Filter(condition, child)
  }
}

/**
 * Flatten the whole plan so that all bootstraps are done in one pass.
 * NOTE: we need to propagate all the attributes again
 * because we changed the output attributes of operators.
 */
case class ConsolidateBootstrap(numTrials: Int, debug: Boolean) extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = {
    val toFlattened = new mutable.HashMap[ExprId, Seq[Attribute]]()


    def getName(name: String, idx: Int) = if(debug) {
      name + idx
    }else {
      name
    }

    def flatten(expression: Expression): Seq[Expression] = expression match {
      case ApproxColumn(conf, columns, mults, _) => {
        ApproxColumn(conf, columns.flatMap(flatten), mults.flatMap(flatten)) :: Nil
      }
      case ExistsPlaceholder(child) => flatten(child).reduceRight(Or) :: Nil
      case ForAllPlaceholder(child) => flatten(child).reduceRight(And) :: Nil
      case LowerPlaceholder(child) => flatten(child).reduceRight(MinOf) :: Nil
      case UpperPlaceholder(child) => flatten(child).reduceRight(MaxOf) :: Nil
      case attribute: Attribute if toFlattened.contains(attribute.exprId) =>
        toFlattened(attribute.exprId)
      case alias@TaggedAlias(tag: Bootstrap, child, name)
        if !alias.references.exists(ref => toFlattened.contains(ref.exprId)) =>
        val flattened = flatten(child).zipWithIndex.map { case (expr, idx) =>
          if (idx == 0) TaggedAlias(tag, expr, getName(name, idx))(alias.exprId, alias.qualifiers)
          else TaggedAlias(tag, expr, getName(name, idx))(qualifiers = alias.qualifiers)
        }
        toFlattened(alias.exprId) = flattened.map(_.toAttribute)
        flattened
      case alias@TaggedAlias(tag, child, name) => {
        val flattened = flatten(child).zipWithIndex.map { case (expr, idx) =>
          if (idx == 0) {
            // child match {
            // case x: TaggedAggregateExpression2 => x
            // case _ =>
            TaggedAlias(tag, expr, getName(name, idx))(alias.exprId, alias.qualifiers)
            // }
          }
          else {
            // child match {
            // case x: TaggedAggregateExpression2 => TaggedAggregateExpression2(x.tag, x.aggregateFunction, x.mode,
            //   x.isDistinct, x.name)(qualifiers = x.qualifiers, explicitMetadata = x.explicitMetadata)
            // case _ =>
            TaggedAlias(tag, expr, getName(name, idx))(qualifiers = alias.qualifiers)
          }


        }


        toFlattened(alias.exprId) = flattened.map(_.toAttribute)
        flattened

      }
      case alias@Alias(child, name) =>
        val flattened = flatten(child).zipWithIndex.map { case (expr, idx) =>
          if (idx == 0) Alias(expr, getName(name, idx))(alias.exprId, alias.qualifiers)
          else Alias(expr, getName(name, idx))(qualifiers = alias.qualifiers)
        }
        toFlattened(alias.exprId) = flattened.map(_.toAttribute)
        flattened

      case uta@UnTaggedAggregateExpression2(aggFunc: DelegateFunction, mode,
      isDistinct, name) => {
        val flattened = flatten(aggFunc).zipWithIndex.map { case (expr, idx) =>
          if (idx == 0) UnTaggedAggregateExpression2(expr.asInstanceOf[AggregateFunction2], mode, isDistinct, getName(name, idx))(uta.exprId, uta.qualifiers,
            uta.explicitMetadata)
          else UnTaggedAggregateExpression2(expr.asInstanceOf[AggregateFunction2], mode, isDistinct, getName(name, idx))(qualifiers = uta.qualifiers,
            explicitMetadata = uta.explicitMetadata)
        }
        toFlattened(uta.exprId) = flattened.map(_.toAttribute)
        flattened
      }
      case poisson: SetSeedAndPoisson =>
        poisson +: Seq.tabulate(numTrials - 1)(_ => poisson.poisson)
      case expr if expr.children.nonEmpty =>
        val flattened: Seq[Seq[Expression]] = expr.children.map(flatten)
        val num = flattened.map(_.length).max
        val newChildren = flattened.map {
          seq => if (seq.size != num) seq.padTo(num, seq.head) else seq
        }.transpose
        newChildren.map { c => expr.withNewChildren(c) }
      case other => other :: Nil
    }

    def flattenNamed(namedExpressions: Seq[NamedExpression]): Seq[NamedExpression] =
      namedExpressions.flatMap { case named =>
        flatten(named).asInstanceOf[Seq[NamedExpression]]
      }.map {
        case alias@TaggedAlias(bound: Bound, _, _) =>
          bound.boundAttributes = toFlattened(bound.child)
          alias
        case other => other
      }

    plan.transformUp {
      case BootstrapAggregate(partial, groupings, aggrs, child) =>
        BootstrapAggregate(partial, groupings, flattenNamed(aggrs), child)

      case BootstrapSortedAggregate(partial, groupings, aggrs, bs, groupByPosInRS,  child) =>
        BootstrapSortedAggregate(partial, groupings, flattenNamed(aggrs), bs, groupByPosInRS, child)

      case Filter(condition, child) =>
        Filter(flatten(condition).head, child)

      case Project(projectList, child) =>
        Project(flattenNamed(projectList), child)

      case CollectPlaceholder(projectList, child) => {
        val x = flattenNamed(projectList)
        CollectPlaceholder(x, child)
      }
    }
  }
}

/**
 * Insert lazy evaluates, and prepare for embedding lineage into plan.
 * REMARK:
 * 1. lazy evaluates are only necessary for attributes
 * whose value can change and will be accessed later in the plan, including be output to users.
 * 2. An attribute may change because it is affected by a running value.
 * A *running* value is flagged by bootstrap-counts.
 * Uncertain tuples are always subsumed by tuples with bootstrap-counts,
 * and thus do not need to be considered for now.
 * 3. we need to propagate all the attributes again
 * because we changed the output attributes of operators.
 */
object IdentifyLazyEvaluates extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case q: SparkPlan =>
      val withNewLeaves: SparkPlan = q.transformExpressionsUp {
        case attr: Attribute =>  q.inputSet.find(_.exprId == attr.exprId).getOrElse(attr)
      }

      System.out.print(withNewLeaves)

      withNewLeaves match {
        case aggregate@BootstrapSortedAggregate(false, groupings, aggrs, bs, groupByPosInRS, child) =>
          // An partial aggregate does not need to be converted to lazy evaluates,
          // even if its input has lazy evaluates,
          // because its output won't be reused
          val opId = OpId.newOpId
          val numShuffleParts = aggregate.sqlContext.conf.numShufflePartitions
          val keys = BootStrapUtils.toNamed(groupings)

          val toNewAttr = mutable.HashMap[ExprId, Attribute]()
          val schema = aggrs.collect {
            case alias@TaggedAlias(Uncertain, _, _) =>
              // To distinguish the schema for lazy evals from the output
              val attribute = alias.toAlias.toAttribute.newInstance()
              toNewAttr(alias.exprId) = attribute
              attribute
            case alias@TaggedAlias(_: Bound, _, _) =>
              val attribute = alias.toAlias.toAttribute.newInstance()
              toNewAttr(alias.exprId) = attribute
              attribute
            case taggedExpr@TaggedAggregateExpression2(Uncertain, _, _, _, _) =>
              // To distinguish the schema for lazy evals from the output
              val attribute = taggedExpr.toAttribute.newInstance()
              toNewAttr(taggedExpr.exprId) = attribute
              attribute
          }

          val newAggrs = aggrs.map {
            case alias@TaggedAlias(Uncertain, expr, name) =>
              TaggedAlias(
                LazyAggregate(opId, numShuffleParts, keys, schema, toNewAttr(alias.exprId)),
                expr, name)(alias.exprId, alias.qualifiers)
            case alias@TaggedAlias(bound: Bound, expr, name) =>
              TaggedAlias(
                LazyAggrBound(opId, numShuffleParts, keys, schema, toNewAttr(alias.exprId), bound),
                expr, name)(alias.exprId, alias.qualifiers)
            case taggedExpr@TaggedAggregateExpression2(Uncertain, aggregateFunction, mode, isDistinct, name) =>
              TaggedAlias(
                LazyAggregate(opId, numShuffleParts, keys, schema, toNewAttr(taggedExpr.exprId)),
                aggregateFunction, name)(taggedExpr.exprId, taggedExpr.qualifiers)
            case expr => expr
          }
          BootstrapSortedAggregate(partial = false, groupings, newAggrs, bs, groupByPosInRS, child)

        case aggregate@BootstrapAggregate(false, groupings, aggrs, child) =>
          // An partial aggregate does not need to be converted to lazy evaluates,
          // even if its input has lazy evaluates,
          // because its output won't be reused
          val opId = OpId.newOpId
          val numShuffleParts = aggregate.sqlContext.conf.numShufflePartitions
          val keys = BootStrapUtils.toNamed(groupings)

          val toNewAttr = mutable.HashMap[ExprId, Attribute]()
          val schema = aggrs.collect {
            case alias@TaggedAlias(Uncertain, _, _) =>
              // To distinguish the schema for lazy evals from the output
              val attribute = alias.toAlias.toAttribute.newInstance()
              toNewAttr(alias.exprId) = attribute
              attribute
            case alias@TaggedAlias(_: Bound, _, _) =>
              val attribute = alias.toAlias.toAttribute.newInstance()
              toNewAttr(alias.exprId) = attribute
              attribute
          }

          val newAggrs = aggrs.map {
            case alias@TaggedAlias(Uncertain, expr, name) =>
              TaggedAlias(
                LazyAggregate(opId, numShuffleParts, keys, schema, toNewAttr(alias.exprId)),
                expr, name)(alias.exprId, alias.qualifiers)
            case alias@TaggedAlias(bound: Bound, expr, name) =>
              TaggedAlias(
                LazyAggrBound(opId, numShuffleParts, keys, schema, toNewAttr(alias.exprId), bound),
                expr, name)(alias.exprId, alias.qualifiers)
            case expr => expr
          }
          BootstrapAggregate(partial = false, groupings, newAggrs, child)
        case project: Project =>
          project.transformExpressionsUp {
            case alias@TaggedAlias(Uncertain, child, name)
              if BootStrapUtils.containsLazyEvaluates(alias.references) =>
              TaggedAlias(LazyAlias(child), child, name)(alias.exprId, alias.qualifiers)
          }

        case other => other
      }
  }
}

/**
 * Embed lineage into operators.
 */
object EmbedLineage extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case BootstrapAggregate(partial, grouping, aggrs, child) =>
      BootstrapAggregate(partial, grouping, aggrs ++ lineageNeedToPropagate(aggrs), child)

    case BootstrapSortedAggregate(partial, grouping, aggrs, bs, groupByPosInRS, child) =>
      BootstrapSortedAggregate(partial, grouping, aggrs ++ lineageNeedToPropagate(aggrs), bs,  groupByPosInRS, child)
    case Project(projectList, child) =>
      Project(projectList ++ lineageNeedToPropagate(projectList), child)
  }

  // Filter out bootstrap-counts, it will be replaced with the bootstrap-counts in the current row.
  // This is a safe optimization to avoid propagating bootstrap-counts,
  // as the bootstrap-counts column is always monotonic in the AND lattice.
  def lineageNeedToPropagate(exprs: Seq[NamedExpression]): Seq[NamedExpression] =
    exprs.collect {
      case TaggedAlias(gen: GenerateLazyEvaluate, _, _) => gen.lineage
      case TaggedAttribute(lazyEval: LazyEvaluate, _, _, _, _) => lazyEval.lineage
    }.flatten.distinct.diff(exprs)
}

object ImplementSort extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case sort@Sort(_, _, _: CollectPlaceholder) => sort

    case sort@Sort(sortOrder, global, child) =>
      BootStrapUtils.getFlag(child.output) match {
        case None => sort
        case Some(flag) =>
          val projectList = child.output.map {
            case a@TaggedAttribute(Flag, _, _, _, _) =>
              TaggedAlias(Flag, BootStrapUtils.boolFalse, "_flag")(a.exprId, a.qualifiers)
            case a => a
          }
          Sort(sortOrder, global, Project(projectList, child))
      }
  }
}

/*
case class ImplementProject extends Rule[SparkPlan] {

  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case project@Project(projectList, child) if flagModified(projectList) =>
      val mode = BootStrapUtils.getGrowthMode(project)

      OnlineProject(BootStrapUtils.collectRefreshInfo(child.output),
        BootStrapUtils.collectCacheInfo(child.output, project.output),
        projectList, child)()
  }

  def flagModified(projectList: Seq[Expression]): Boolean = projectList.exists {
    case TaggedAlias(Flag, _, _) => true
    case _ => false
  }
}*/

/**
 * Remark:
 * 1. Take broadcast implementation out of aggregate,
 * that can be implemented as a separate operator,
 * and optimized by considering the plan as a whole (e.g., later broadcast join).
 * 2. Only aggregates that output lazy evaluates need to be broadcast.
 * 3. We have already assumed no partitioning of input relations.
 * 4. Caching can be categorized into 6 classes according to
 * (1) whether the input tuples are uncertain, lazy or normal attributes
 * (2) whether there is a flag column
 *
 * uncertain --> no caching
 * lazy --> per tuple caching
 * normal --> partial result caching (TODO: currently use per tuple caching)
 *
 * if flag exists, only those tuples with flag=true
 * will be cached or merged into the cached partial result
 */
case class ImplementAggregate(slackParam: Double)
    extends Rule[SparkPlan] {


  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case aggregate@BootstrapAggregate(partial, groupings, aggrs, child) =>
      val opId = aggrs.collectFirst {
        case TaggedAlias(tag: LazyAggregate, _, _) => tag.opId
      }.getOrElse(OpId.newOpId)
      val references = aggrs.flatMap(_.references)

      if (partial) {
        if (BootStrapUtils.containsLazyEvaluates(references)) {

          AggregateWith2Inputs(BootStrapUtils.getFlag(child.output),

            partial, groupings, aggrs, child)(opId = opId)
        } else {
          aggregate
        }
      } else {

        AggregateWith2Inputs2Outputs(BootStrapUtils.getFlag(child.output),
          BootStrapUtils.collectLineageRelay(aggregate.output), BootStrapUtils.collectIntegrityInfo(aggrs, slackParam),
          groupings, aggrs, child)(-1 :: Nil, opId = opId)
      }

    case aggregate@BootstrapSortedAggregate(partial, groupings, aggrs, bs, groupByPosInRS, child) =>
      val opId = aggrs.collectFirst {
        case TaggedAlias(tag: LazyAggregate, _, _) => tag.opId
      }.getOrElse(OpId.newOpId)
      val references = aggrs.flatMap(_.references)

      if (partial) {
        if (BootStrapUtils.containsLazyEvaluates(references)) {
          throw new UnsupportedOperationException("Not implemented")
          /*AggregateWith2Inputs(BootStrapUtils.getFlag(child.output),

            partial, groupings, aggrs, child)( opId = opId)*/
        } else {
          aggregate
        }
      } else {

        SortedAggregateWith2Inputs2Outputs(BootStrapUtils.getFlag(child.output),
          BootStrapUtils.collectLineageRelay(aggregate.output), BootStrapUtils.collectIntegrityInfo(aggrs, slackParam),
          groupings, aggrs, bs, groupByPosInRS, child)(-1 :: Nil, opId = opId)
      }
  }


}

/**
 * Remark:
 * 1. We use broadcast join for the side which is fixed or almost-fixed per partition.
 * 2. We should build both sides if neither side is fixed/almost-fixed per partition.
 * (TODO: currently not implemented)
 */
/*
case class ImplementJoin extends Rule[SparkPlan] {
  import org.apache.spark.sql.hive.online.Growth._

  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case join@BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
      val (buildMode, streamMode, build, stream) = buildSide match {
        case BuildLeft => (BootStrapUtils.getGrowthMode(left),
            BootStrapUtils.getGrowthMode(right), left, right)
        case BuildRight => (BootStrapUtils.getGrowthMode(right),
            BootStrapUtils.getGrowthMode(left), right, left)
      }
      (buildMode, streamMode) match {
        case (GrowthMode(_, Grow), GrowthMode(_, Grow)) => ???
        case (GrowthMode(Growth.Fixed, Growth.Fixed), GrowthMode(Growth.Fixed, Growth.Fixed)) => join
        case (GrowthMode(Growth.Fixed, Growth.Fixed), _) =>
          require(!BootStrapUtils.containsLazyEvaluates(build.output),
            s"!!! Warning: incorrect growth mode in ${join.simpleString}")
          OTBroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right)
        case (_, GrowthMode(Fixed, Fixed)) =>
          require(!containsLazyEvaluates(stream.output),
            s"!!! Warning: incorrect growth mode in ${join.simpleString}")
          OTBroadcastHashJoin(leftKeys, rightKeys, swap(buildSide), left, right)(
            controller)
        case (GrowthMode(AlmostFixed, Fixed), _) =>
          MTBroadcastHashJoin(
            getFlag(left.output), getFlag(right.output),
            collectRefreshInfo(stream.output),
            collectRefreshInfo(build.output),
            leftKeys, rightKeys, buildSide, left, right)(
            controller)
        case (_, GrowthMode(AlmostFixed, Fixed)) =>
          MTBroadcastHashJoin(
            getFlag(left.output), getFlag(right.output),
            collectRefreshInfo(build.output),
            collectRefreshInfo(stream.output),
            leftKeys, rightKeys, swap(buildSide), left, right)(
            controller)
        case _ => ???
      }

    case join@ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
      val (buildMode, streamMode, build, stream) = buildSide match {
        case BuildLeft =>
          (exchange(getGrowthMode(left)), exchange(getGrowthMode(right)), left, right)
        case BuildRight =>
          (exchange(getGrowthMode(right)), exchange(getGrowthMode(left)), right, left)
      }
      require(buildMode.numPartitions == Fixed && streamMode.numPartitions == Fixed,
        s"!!! Warning: unexpected growth mode ($buildMode, $streamMode) in ${join.simpleString}")
      (buildMode, streamMode) match {
        case (GrowthMode(Fixed, _), GrowthMode(Fixed, _)) => join
        case (GrowthMode(Fixed, _), _) =>
          require(!containsLazyEvaluates(build.output),
            s"!!! Warning: incorrect growth mode in ${join.simpleString}")
          OTShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right)(
            controller)
        case (_, GrowthMode(Fixed, _)) =>
          require(!containsLazyEvaluates(stream.output),
            s"!!! Warning: incorrect growth mode in ${join.simpleString}")
          OTShuffledHashJoin(leftKeys, rightKeys, swap(buildSide), left, right)(
            controller)
        case (GrowthMode(AlmostFixed, _), _) =>
          MTBroadcastHashJoin(
            getFlag(left.output), getFlag(right.output),
            collectRefreshInfo(stream.output),
            collectRefreshInfo(build.output),
            leftKeys, rightKeys, buildSide, left, right)(
            controller)
        case (_, GrowthMode(AlmostFixed, _)) =>
          MTBroadcastHashJoin(
            getFlag(left.output), getFlag(right.output),
            collectRefreshInfo(build.output),
            collectRefreshInfo(stream.output),
            leftKeys, rightKeys, swap(buildSide), left, right)(
            controller)
        case _ => ???
      }

    case join@LeftSemiJoinHash(leftKeys, rightKeys, left, right) =>
      val build = right
      val buildMode = exchange(getGrowthMode(right))
      val streamMode = exchange(getGrowthMode(left))
      require(buildMode.numPartitions == Fixed && streamMode.numPartitions == Fixed,
        s"!!! Warning: unexpected growth mode ($buildMode, $streamMode) in ${join.simpleString}")
      (buildMode, streamMode) match {
        case (GrowthMode(Fixed, _), GrowthMode(Fixed, _)) => join
        case (GrowthMode(Fixed, _), _) =>
          require(!containsLazyEvaluates(build.output),
            s"!!! Warning: incorrect growth mode in ${join.simpleString}")
          OTBLeftSemiHashJoin(leftKeys, rightKeys, left, right)(controller)
        case (GrowthMode(AlmostFixed, _), _) =>
          MTBLeftSemiHashJoin(leftKeys, rightKeys, left, right)(controller)
        case _ => ???
      }
  }

  def swap(buildSide: BuildSide): BuildSide = buildSide match {
    case BuildLeft => BuildRight
    case BuildRight => BuildLeft
  }
}
*/
/*case class ImplementCollect extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case CollectPlaceholder(projectList, child) =>
      Collect(BootStrapUtils.getFlag(child.output), BootStrapUtils.collectRefreshInfo(child.output),
        projectList, child)()
  }
}*/

case class ImplementCollect extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case CollectPlaceholder(projectList, child) =>
      Collect(BootStrapUtils.getFlag(child.output),
        projectList, child)()
  }
}

object CleanupAnalysisExpressions extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case q: SparkPlan => q.transformExpressionsUp {
      case attr: TaggedAttribute => {

        attr.toAttributeReference
      }
      case alias: TaggedAlias => {
        alias.toAlias
      }

      case taggedAggExp: TaggedAggregateExpression2 => {
        taggedAggExp.toUntaggedAggregateExpression2
      }

    }
  }
}

object Growth extends Enumeration {
  type Growth = Value
  val Fixed, AlmostFixed, Grow = Value

  def min(x: Growth, y: Growth) = if (x < y) x else y

  def max(x: Growth, y: Growth) = if (x < y) y else x
}

case class GrowthMode(perPartition: Growth.Growth, numPartitions: Growth.Growth) {
  require(numPartitions != Growth.AlmostFixed)
}

object OnlinePlannerUtil {

  val boolTrue = Literal(true)
  val boolFalse = Literal(false)
  val byte0 = Literal(0.toByte)
  val byte1 = Literal(1.toByte)

  def getSeeds(attributes: Seq[Attribute]): Seq[Attribute] =
    attributes.collect { case seed@TaggedAttribute(_: Seed, _, _, _, _) => seed }

  /**
   * Get the first BootstrapCounts if any of this plan.
   */
  def getBtCnt(attributes: Seq[Attribute]): Option[TaggedAttribute] =
    attributes.collectFirst { case btCnt@TaggedAttribute(_: Bootstrap, _, _, _, _) => btCnt }

  /**
   * Get the first flag if any of this plan
   */
  def getFlag(attributes: Seq[Attribute]): Option[TaggedAttribute] =
    attributes.collectFirst { case flag@TaggedAttribute(Flag, _, _, _, _) => flag }

  def extractBounds(attributes: Seq[Attribute]): Map[ExprId, Seq[Attribute]] =
    attributes.collect { case a@TaggedAttribute(_: Bound, _, _, _, _) => a }
        .groupBy { case a@TaggedAttribute(b: Bound, _, _, _, _) => b.child }

  def explode(expr: Expression, map: Map[ExprId, Seq[Attribute]]): Seq[Expression] = expr match {
    case a: Attribute => if (map.contains(a.exprId)) map(a.exprId) else Seq(a)
    case e => cartesian(e.children.map(c => explode(c, map))).map(e.withNewChildren)
  }

  private[this] def cartesian(exprs: Seq[Seq[Expression]]): Seq[Seq[Expression]] = exprs match {
    case Seq() => Seq(Nil)
    case _ => for (x <- exprs.head; y <- cartesian(exprs.tail)) yield x +: y
  }

  /**
   * Whether attributes contain uncertain ones.
   */
  def containsUncertain(attributes: Traversable[Attribute]) = attributes.exists {
    case TaggedAttribute(Uncertain, _, _, _, _) => true
    case _ => false
  }

  def widenTypes(left: Expression, right: Expression): (Expression, Expression) =
    if (left.dataType != right.dataType) {
      HiveTypeCoercion.findTightestCommonTypeOfTwo(left.dataType, right.dataType)
          .map { widestType =>
            val l = if (left.dataType != widestType) {
              Cast(left, widestType)
            } else {
              left
            }
            val r = if (right.dataType != widestType) {
              Cast(right, widestType)
            } else {
              right
            }
            (l, r)
          }
          .getOrElse((left, right))
    } else {
      (left, right)
    }

  val simplifyCast: PartialFunction[Expression, Expression] = {
    case expr@Cast(_: Literal, dataType) => Literal.create(expr.eval(EmptyRow), dataType)
    case Cast(e, dataType) if e.dataType == dataType => e
    case Cast(Cast(e, _), dataType) => if (e.dataType == dataType) e else Cast(e, dataType)
  }

  /**
   * Whether attributes contain lazy evalutes.
   */
  def containsLazyEvaluates(attributes: Traversable[Attribute]) = attributes.exists {
    case TaggedAttribute(_: LazyAttribute, _, _, _, _) => true
    case _ => false
  }

  def toNamed(exprs: Seq[Expression], name: String = "_key"): Seq[NamedExpression] =
    exprs.map {
      case named: NamedExpression => named
      case expr => Alias(expr, name)()
    }


  def collectLineageRelay(output: Seq[Attribute]) = {
    val filterExpression = getFlag(output).get.toAttributeReference
    val broadcastExpressions = output.collect {
      case a@TaggedAttribute(_: LazyAttribute, _, _, _, _) => a.toAttributeReference
    }
    LineageRelay(filterExpression, broadcastExpressions)
  }

  def collectCacheInfo(input: Seq[Attribute], output: Seq[Attribute]): (Int, Int) = {
    def getFlagIndex(exprs: Seq[NamedExpression]) =
      exprs.indexWhere {
        case TaggedAttribute(Flag, _, _, _, _) => true
        case _ => false
      }
    (getFlagIndex(input), getFlagIndex(output))
  }

  def collectIntegrityInfo(
      exprs: Seq[NamedExpression], slackParam: Double): Option[IntegrityInfo] = {
    import BoundType._

    def cleanupAttrs(attrs: Seq[Attribute]): Seq[Attribute] = attrs.map {
      case a: TaggedAttribute => a.toAttributeReference
      case o => o
    }

    def stdev(xs: Seq[Attribute]): Expression = {
      val (sum, n) = widenTypes(xs.reduce(Add), Literal(xs.length))
      val mean = Divide(sum, n)
      val (sum2, _) = widenTypes(xs.map(a => Multiply(a, a)).reduce(Add), n)
      val mean2 = Divide(sum2, n)

      val dataType = xs.head.dataType
      Cast(Multiply(Literal(slackParam), Sqrt(Subtract(mean2, Multiply(mean, mean)))), dataType)
          .transformUp(simplifyCast)
    }

    exprs.zipWithIndex.collect {
      case (alias@TaggedAlias(LazyAggrBound(_, _, _, schema, attribute, bound), _, _), idx) =>
        val boundAttrs = cleanupAttrs(bound.boundAttributes)
        val tight = alias.toAlias.toAttribute
        val slack = stdev(boundAttrs)
        bound.boundType match {
          case Lower =>
            (Coalesce(GreaterThanOrEqual(tight, attribute) :: boolTrue :: Nil),
                (idx, MaxOf(Subtract(tight, slack), attribute)),
                schema)
          case Upper =>
            (Coalesce(LessThanOrEqual(tight, attribute) :: boolTrue :: Nil),
                (idx, MinOf(Add(tight, slack), attribute)),
                schema)
        }
    } match {
      case Seq() => None
      case all =>
        val (checks, updates, schemas) = all.unzip3
        val (updateIndexes, updateExpressions) = updates.unzip
        Some(IntegrityInfo(
          checks.reduce(And), updateIndexes.toArray, updateExpressions, schemas.head))
    }
  }

  def exchange(mode: GrowthMode): GrowthMode = mode match {
    case GrowthMode(p, n) => GrowthMode(Growth.max(p, n), Growth.Fixed)
  }


  def getGrowthMode(plan: SparkPlan): GrowthMode = {
    def fullAggregate(child: SparkPlan) = {
      exchange(getGrowthMode(child)) match {
        case GrowthMode(p, Growth.Fixed) => GrowthMode(Growth.min(p, Growth.AlmostFixed), Growth.Fixed)
      }
    }

    def broadcastJoin(buildSide: BuildSide, left: SparkPlan, right: SparkPlan) = {
      val (buildMode, streamMode) = buildSide match {
        case BuildLeft => (getGrowthMode(left), getGrowthMode(right))
        case BuildRight => (getGrowthMode(right), getGrowthMode(left))
      }
      (buildMode, streamMode) match {
        case (GrowthMode(bp, bn), GrowthMode(sp, sn)) =>
          GrowthMode(Seq(sp, bp, bn).reduce(Growth.max), sn)
      }
    }

    plan match {
      case _: LeafNode => GrowthMode(Growth.Fixed, Growth.Fixed)
      /* case sample: StreamedRelation =>
         require(getGrowthMode(sample.child) == GrowthMode(Growth.Fixed, Growth.Fixed),
           s"!!! Warning: unexpected growth mode ${getGrowthMode(sample.child)} " +
               s"in ${sample.simpleString}.")
         GrowthMode(Growth.Fixed, Growth.Grow)*/
      case aggregate: Aggregate =>
        if (aggregate.partial) getGrowthMode(aggregate.child) else fullAggregate(aggregate.child)
      case aggregate: BootstrapAggregate =>
        if (aggregate.partial) getGrowthMode(aggregate.child) else fullAggregate(aggregate.child)
      case aggregate: BootstrapSortedAggregate =>
        if (aggregate.partial) getGrowthMode(aggregate.child) else fullAggregate(aggregate.child)
      case aggregate: AggregateWith2Inputs2Outputs =>
        fullAggregate(aggregate.child)
      case aggregate: AggregateWith2Inputs =>
        getGrowthMode(aggregate.child)
      /*case _: ShuffledHashJoin | _: LeftSemiJoinHash | _: OTShuffledHashJoin =>
        val modes = plan.children.map(getGrowthMode).map(exchange)
        GrowthMode(modes.map(_.perPartition).reduce(Growth.max), Fixed)
      case join: BroadcastHashJoin =>
        broadcastJoin(join.buildSide, join.left, join.right)
      case join: OTBroadcastHashJoin =>
        broadcastJoin(join.buildSide, join.left, join.right)
      case join: MTBroadcastHashJoin =>
        broadcastJoin(join.buildSide, join.left, join.right)
      case join: OTBLeftSemiHashJoin =>
        broadcastJoin(join.buildSide, join.left, join.right)
      case join: MTBLeftSemiHashJoin =>
        broadcastJoin(join.buildSide, join.left, join.right)*/
      case unary: UnaryNode => getGrowthMode(unary.child)
      case _ => ???
    }
  }
}
