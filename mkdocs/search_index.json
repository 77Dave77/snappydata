{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nSnappyData fuses Apache Spark with an in-memory database to deliver a compute+data engine capable of stream processing, transactions, interactive analytics and prediction in a single cluster.\n\n\n\n\nAttention\n\n\nThis document assumes that you have familiarity with Apache Spark and its concepts. If you are new to Spark, refer to the \nSpark documentation\n to learn more about using Spark.\n\n\n\n\nThe current release of SnappyData is fully compatible with Spark 2.1.1\n\n\nThe Challenge with Spark and Remote Data Sources\n\n\nApache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and can access disparate data sources in a highly parallelized manner for its distributed computations. Typically, data is fetched lazily as a result of SQL query or a Dataset (RDD) getting materialized. This can be quite inefficient and expensive since most workloads require the data set to be repeatedly processed. \n\n\nAnalytic processing requires large datasets to be repeatedly copied from an external data source like HDFS, into Spark. Copying data, reformatting it (into a columnar format, depending on where the data is being copied from) and moving it across process and machine boundaries can be very expensive. As a result, we see that in several cases, applications using Spark with an external data source fail to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Spark to do the aggregation. The alternative to working with a stateful store is to cache the data in Spark. This, of course, suffers from the problems associated with stale data.\n\n\nThe SnappyData Approach\n\n\nSnappyData fuses a low latency, highly available in-memory transactional database (GemFire) into Spark with shared memory management and several optimizations that deliver performance and concurrency for mixed workloads. Data in the highly available in-memory store is laid out using a custom columnar format somewhat similar to the layout used by Spark caching. Query engine operators are optimized through better vectorization and code generation. The net effect of these changes is, an order of magnitude performance improvement when compared to native Spark caching, and more than two orders of magnitude better Spark performance when working with external data sources.\n\n\nEssentially, Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with streams and running analytic SQL queries.\n\n\n\n\nSnappyData is an in-memory database that runs Spark\u2019s compute engine directly in the database, and offers \nSpark's API and SQL as its interface and computational engine\n. The fusion with Spark allows SnappyData to work with a large number of data sources like HDFS, NoSQL etc. through bespoke Spark connectors. \n\nWhile the SnappyData engine (that builds on Spark Catalyst SQL engine) is primarily designed for SQL processing, applications can also work with Objects through Spark RDDs and the Spark Datasets API.\n\n\nAny Spark DataFrame can be easily managed as a SnappyData table or conversely any table can be accessed as a DataFrame.\n\n\nBy default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted, Spark executors are automatically launched within the SnappyData process space (JVMs). There is no need to connect and manage external data store clusters. The SnappyData store can synchronously replicate for high availability (HA) with strong consistency and store/recover from disk for additional reliability.\n\n\nKey Features\n\n\n\n\n\n\n100% compatible with Spark\n- Use SnappyData as a database, and additionally use any of the Spark APIs - ML, Graph, etc.\n\n\n\n\n\n\nIn-memory row and column stores\n: Run the store colocated in Spark executors or in its own process space (that is, a computational cluster and a data cluster)\n\n\n\n\n\n\nSQL standard compliance\n: Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.\n\n\n\n\n\n\nSQL based extensions for streaming processing\n: Use native Spark streaming, DataFrame APIs or declaratively specify your streams and how you want it processed. You do not need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.\n\n\n\n\n\n\nNot-Only SQL\n: Use either as a SQL database or work with JSON or even arbitrary Application Objects. Essentially, any Spark RDD/DataSet can also be persisted into SnappyData tables (type system same as Spark Datasets). \n\n\n\n\n\n\nMutate, transact on data in Spark\n: You can use SQL to insert, update, delete data in tables as one would expect. Extensions to Spark\u2019s context are also provided so you can mutate data in your Spark programs. Tables defined in SnappyData are automatically visible as DataFrames. By eliminating the need to store data separately in a data store and then cache it in Spark for computing, SnappyData simplifies system architecture and reduces the overall cost of ownership while simultaneously offering much better performance.\n\n\n\n\n\n\nOptimizations - Indexing\n: From version 1.0, you can add indexes to your row format tables and the query optimizer automatically uses in-memory indexes when available, to provide better performance.\n\n\n\n\n\n\nOptimizations - colocation\n: SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be colocated using declarative custom partitioning strategies (for example, common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent.\n\n\n\n\n\n\nHigh availability not just Fault tolerance\n: Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster. It is deeply integrated with a membership-based distributed system to detect and handle failures, instantaneously providing applications continuous HA.\n\n\n\n\n\n\nDurability and recovery:\n Tables can be configured to be persisted to disk (the default) and recovered upon startup. Utilities for backup, restore and import/export are provided with the system.\n\n\n\n\n\n\nInteractive analytics using Synopsis Data Engine (SDE)\n: Multiple synopses techniques are introduced through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and are completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions or SparkSession API extensions. \n\n\n\n\n\n\n\n\nExtensions to the Spark Runtime\n\n\nSnappyData makes the following contributions to deliver a unified and optimized runtime.\n\n\n\n\n\n\nIntegrating an operational in-memory data store with Spark\u2019s computational model\n: A number of extensions are introduced to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy. \n This design is extended by allowing low latency and fine-grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, the runtime is extended with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby the state is shared across many clients and applications. \n\n\n\n\n\n\nUnified API for OLAP, OLTP, and Streaming\n: Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL.\nSnappyData extends Spark\u2019s unified API: \n\n\n\n\n\n\nAllow for OLTP operations, for example, transactions and mutations (inserts/updates/deletions) on tables \n\n\n\n\n\n\nConform with SQL standards, for example, allowing tables alterations, constraints, and indexes   \n\n\n\n\n\n\nSupport declarative stream processing in SQL\n\n\n\n\n\n\nA unique addition of SnappyData is ability to mutate (all of inserts/updates/deletes) even column format tables efficiently without much change in query performance profile.\n\n\n\n\n\n\n\n\n\n\nOptimizing Spark application execution times\n: Our goal is to eliminate the need for yet another external store (for example, a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting colocated schema designs (tables and streams) where related data is colocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios.\n\n\n\n\n\n\nSynopsis Data Engine support built into Spark\n: The SnappyData \nSynopsis Data Engine (SDE)\n offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time. \nThe SDE engine enables you to:\n\n\n\n\n\n\nIntelligently sample the data set on frequently accessed dimensions to have a good representation across the entire data set (stratified sampling). Queries can execute on samples and return answers instantly.\n\n\n\n\n\n\nCompute estimates for any ad hoc query from the sample(s). It can also provide error estimates for arbitrarily complex queries on streams.\n\n\n\n\n\n\nProvide simple knobs for the user to trade off speed for accuracy, that is, simple SQL extensions so the user can specify the error tolerance for all queries. When query error is higher than tolerance level, the system automatically delegates the query to the source.\n\n\n\n\n\n\nExpress their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts.\n\n\n\n\n\n\n\n\n\n\nMorphing Spark to support mixed workloads (OLTP, OLAP)\n\n\nSpark is designed as a computational engine for processing batch jobs. Each Spark application (for example, a Map-reduce job) runs as an independent set of processes (that is, executor JVMs) in the cluster. These JVMs are reused for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients require an external storage tier, such as HDFS. SnappyData, on the other hand, targets a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database in the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and decoupled from individual applications.\n\n\nA second but related challenge is Spark\u2019s design for how user requests (jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces:\n\n\n\n\n\n\nA single point of contention for all requests, and \n\n\n\n\n\n\nA barrier for achieving high availability (HA). Executors are shut down if the driver fails, requiring a full refresh of any cached state.\n\n\n\n\n\n\nSpark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, more complex data structure needs to be managed (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mutability characteristics and conformance to SQL.\n\n\nFinally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "SnappyData fuses Apache Spark with an in-memory database to deliver a compute+data engine capable of stream processing, transactions, interactive analytics and prediction in a single cluster.   Attention  This document assumes that you have familiarity with Apache Spark and its concepts. If you are new to Spark, refer to the  Spark documentation  to learn more about using Spark.   The current release of SnappyData is fully compatible with Spark 2.1.1", 
            "title": "Introduction"
        }, 
        {
            "location": "/#the-challenge-with-spark-and-remote-data-sources", 
            "text": "Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and can access disparate data sources in a highly parallelized manner for its distributed computations. Typically, data is fetched lazily as a result of SQL query or a Dataset (RDD) getting materialized. This can be quite inefficient and expensive since most workloads require the data set to be repeatedly processed.   Analytic processing requires large datasets to be repeatedly copied from an external data source like HDFS, into Spark. Copying data, reformatting it (into a columnar format, depending on where the data is being copied from) and moving it across process and machine boundaries can be very expensive. As a result, we see that in several cases, applications using Spark with an external data source fail to deliver the promise of interactive analytic performance. For instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Spark to do the aggregation. The alternative to working with a stateful store is to cache the data in Spark. This, of course, suffers from the problems associated with stale data.", 
            "title": "The Challenge with Spark and Remote Data Sources"
        }, 
        {
            "location": "/#the-snappydata-approach", 
            "text": "SnappyData fuses a low latency, highly available in-memory transactional database (GemFire) into Spark with shared memory management and several optimizations that deliver performance and concurrency for mixed workloads. Data in the highly available in-memory store is laid out using a custom columnar format somewhat similar to the layout used by Spark caching. Query engine operators are optimized through better vectorization and code generation. The net effect of these changes is, an order of magnitude performance improvement when compared to native Spark caching, and more than two orders of magnitude better Spark performance when working with external data sources.  Essentially, Spark is turned into an in-memory operational database capable of transactions, point reads, writes, working with streams and running analytic SQL queries.   SnappyData is an in-memory database that runs Spark\u2019s compute engine directly in the database, and offers  Spark's API and SQL as its interface and computational engine . The fusion with Spark allows SnappyData to work with a large number of data sources like HDFS, NoSQL etc. through bespoke Spark connectors.  \nWhile the SnappyData engine (that builds on Spark Catalyst SQL engine) is primarily designed for SQL processing, applications can also work with Objects through Spark RDDs and the Spark Datasets API.  Any Spark DataFrame can be easily managed as a SnappyData table or conversely any table can be accessed as a DataFrame.  By default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted, Spark executors are automatically launched within the SnappyData process space (JVMs). There is no need to connect and manage external data store clusters. The SnappyData store can synchronously replicate for high availability (HA) with strong consistency and store/recover from disk for additional reliability.", 
            "title": "The SnappyData Approach"
        }, 
        {
            "location": "/#key-features", 
            "text": "100% compatible with Spark - Use SnappyData as a database, and additionally use any of the Spark APIs - ML, Graph, etc.    In-memory row and column stores : Run the store colocated in Spark executors or in its own process space (that is, a computational cluster and a data cluster)    SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.    SQL based extensions for streaming processing : Use native Spark streaming, DataFrame APIs or declaratively specify your streams and how you want it processed. You do not need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.    Not-Only SQL : Use either as a SQL database or work with JSON or even arbitrary Application Objects. Essentially, any Spark RDD/DataSet can also be persisted into SnappyData tables (type system same as Spark Datasets).     Mutate, transact on data in Spark : You can use SQL to insert, update, delete data in tables as one would expect. Extensions to Spark\u2019s context are also provided so you can mutate data in your Spark programs. Tables defined in SnappyData are automatically visible as DataFrames. By eliminating the need to store data separately in a data store and then cache it in Spark for computing, SnappyData simplifies system architecture and reduces the overall cost of ownership while simultaneously offering much better performance.    Optimizations - Indexing : From version 1.0, you can add indexes to your row format tables and the query optimizer automatically uses in-memory indexes when available, to provide better performance.    Optimizations - colocation : SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be colocated using declarative custom partitioning strategies (for example, common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent.    High availability not just Fault tolerance : Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster. It is deeply integrated with a membership-based distributed system to detect and handle failures, instantaneously providing applications continuous HA.    Durability and recovery:  Tables can be configured to be persisted to disk (the default) and recovered upon startup. Utilities for backup, restore and import/export are provided with the system.    Interactive analytics using Synopsis Data Engine (SDE) : Multiple synopses techniques are introduced through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and are completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions or SparkSession API extensions.", 
            "title": "Key Features"
        }, 
        {
            "location": "/#extensions-to-the-spark-runtime", 
            "text": "SnappyData makes the following contributions to deliver a unified and optimized runtime.    Integrating an operational in-memory data store with Spark\u2019s computational model : A number of extensions are introduced to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy.   This design is extended by allowing low latency and fine-grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, the runtime is extended with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby the state is shared across many clients and applications.     Unified API for OLAP, OLTP, and Streaming : Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL.\nSnappyData extends Spark\u2019s unified API:     Allow for OLTP operations, for example, transactions and mutations (inserts/updates/deletions) on tables     Conform with SQL standards, for example, allowing tables alterations, constraints, and indexes       Support declarative stream processing in SQL    A unique addition of SnappyData is ability to mutate (all of inserts/updates/deletes) even column format tables efficiently without much change in query performance profile.      Optimizing Spark application execution times : Our goal is to eliminate the need for yet another external store (for example, a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting colocated schema designs (tables and streams) where related data is colocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios.    Synopsis Data Engine support built into Spark : The SnappyData  Synopsis Data Engine (SDE)  offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time.  The SDE engine enables you to:    Intelligently sample the data set on frequently accessed dimensions to have a good representation across the entire data set (stratified sampling). Queries can execute on samples and return answers instantly.    Compute estimates for any ad hoc query from the sample(s). It can also provide error estimates for arbitrarily complex queries on streams.    Provide simple knobs for the user to trade off speed for accuracy, that is, simple SQL extensions so the user can specify the error tolerance for all queries. When query error is higher than tolerance level, the system automatically delegates the query to the source.    Express their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts.", 
            "title": "Extensions to the Spark Runtime"
        }, 
        {
            "location": "/#morphing-spark-to-support-mixed-workloads-oltp-olap", 
            "text": "Spark is designed as a computational engine for processing batch jobs. Each Spark application (for example, a Map-reduce job) runs as an independent set of processes (that is, executor JVMs) in the cluster. These JVMs are reused for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients require an external storage tier, such as HDFS. SnappyData, on the other hand, targets a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database in the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and decoupled from individual applications.  A second but related challenge is Spark\u2019s design for how user requests (jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces:    A single point of contention for all requests, and     A barrier for achieving high availability (HA). Executors are shut down if the driver fails, requiring a full refresh of any cached state.    Spark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, more complex data structure needs to be managed (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mutability characteristics and conformance to SQL.  Finally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Morphing Spark to support mixed workloads (OLTP, OLAP)"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Getting Started in 5 Minutes or less\n\n\nWelcome to the Getting Started section! \n\nIn this section, multiple options are provided for getting started with SnappyData.\n\n\nBefore you begin\n\n\nYou must first \ndownload and extract\n the SnappyData product distribution, navigate to the SnappyData product root directory and \nstart the cluster\n.\n\n\nDepending on your preference, you can try any of the following options:\n\n\n\n\n\n\nSnappyData with Spark distribution\n\n\n\n\n\n\nSnappyData On-Premise\n\n\n\n\n\n\nSnappyData with Docker image\n\n\n\n\n\n\nSnappyData on Kubernetes\n\n\n\n\n\n\nYou can use the following instructions and examples to try out SnappyData in 5 minutes or less:\n\n\n\n\n\n\nUsing Spark Scala APIs\n\n\n\n\n\n\nSnappyData performance: 16x-20x faster than Apache Spark\n\n\n\n\n\n\nUsing SQL", 
            "title": "Getting Started in 5 minutes or Less"
        }, 
        {
            "location": "/quickstart/#getting-started-in-5-minutes-or-less", 
            "text": "Welcome to the Getting Started section!  \nIn this section, multiple options are provided for getting started with SnappyData.  Before you begin  You must first  download and extract  the SnappyData product distribution, navigate to the SnappyData product root directory and  start the cluster .  Depending on your preference, you can try any of the following options:    SnappyData with Spark distribution    SnappyData On-Premise    SnappyData with Docker image    SnappyData on Kubernetes    You can use the following instructions and examples to try out SnappyData in 5 minutes or less:    Using Spark Scala APIs    SnappyData performance: 16x-20x faster than Apache Spark    Using SQL", 
            "title": "Getting Started in 5 Minutes or less"
        }, 
        {
            "location": "/quickstart/getting_started_with_your_spark_distribution/", 
            "text": "Getting Started with your Spark Distribution\n\n\nIf you are a Spark developer and already using Spark 2.1.1 the fastest way to work with SnappyData is to add SnappyData as a dependency. For instance, using the \npackage\n option in the Spark shell.\n\n\nOpen a command terminal, go to the location of the Spark installation directory, and enter the following:\n\n\n$ cd \nSpark_Install_dir\n\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \nSnappyDataInc:snappydata:1.0.2.1-s_2.11\n\n\n\n\n\nThis opens the Spark shell and downloads the relevant SnappyData files to your local machine. Depending on your network connection speed, it may take some time to download the files.\n\nAll SnappyData metadata, as well as persistent data, is stored in the directory \nquickstartdatadir\n. The spark-shell can now be used to work with SnappyData using \nScala APIs\n and \nSQL\n.\n\n\n\nFor this exercise, it is assumed that you are either familiar with Spark or SQL (not necessarily both). Basic database capabilities like working with Columnar and Row-oriented tables, querying and updating these tables is showcased.\n\n\nTables in SnappyData exhibit many operational capabilities like disk persistence, redundancy for HA, eviction, etc. For more information, you can refer to the \ndetailed documentation\n. \n\n\nNext, you can try using the \nScala APIs\n or \nSQL\n. We will add Java/Python examples in the future.", 
            "title": "Getting Started with your Spark Distribution"
        }, 
        {
            "location": "/quickstart/getting_started_with_your_spark_distribution/#getting-started-with-your-spark-distribution", 
            "text": "If you are a Spark developer and already using Spark 2.1.1 the fastest way to work with SnappyData is to add SnappyData as a dependency. For instance, using the  package  option in the Spark shell.  Open a command terminal, go to the location of the Spark installation directory, and enter the following:  $ cd  Spark_Install_dir \n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$ ./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages  SnappyDataInc:snappydata:1.0.2.1-s_2.11   This opens the Spark shell and downloads the relevant SnappyData files to your local machine. Depending on your network connection speed, it may take some time to download the files. \nAll SnappyData metadata, as well as persistent data, is stored in the directory  quickstartdatadir . The spark-shell can now be used to work with SnappyData using  Scala APIs  and  SQL .  \nFor this exercise, it is assumed that you are either familiar with Spark or SQL (not necessarily both). Basic database capabilities like working with Columnar and Row-oriented tables, querying and updating these tables is showcased.  Tables in SnappyData exhibit many operational capabilities like disk persistence, redundancy for HA, eviction, etc. For more information, you can refer to the  detailed documentation .   Next, you can try using the  Scala APIs  or  SQL . We will add Java/Python examples in the future.", 
            "title": "Getting Started with your Spark Distribution"
        }, 
        {
            "location": "/quickstart/getting_started_by_installing_snappydata_on-premise/", 
            "text": "Getting Started by Installing SnappyData On-Premise\n\n\nDownload the latest version of SnappyData from the \nSnappyData Release Page\n, which lists the latest and previous releases of SnappyData.\n\n\n$ tar -xzf snappydata-1.0.2.1-bin.tar.gz\n$ cd snappydata-1.0.2.1-bin/\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log\n\n\n\n\nIt opens the Spark shell. All SnappyData metadata, as well as persistent data, is stored in the directory \nquickstartdatadir\n.\n\n\nThe spark-shell can now be used to work with SnappyData using \nScala APIs\n and \nSQL\n.", 
            "title": "Getting Started with SnappyData On-Premise"
        }, 
        {
            "location": "/quickstart/getting_started_by_installing_snappydata_on-premise/#getting-started-by-installing-snappydata-on-premise", 
            "text": "Download the latest version of SnappyData from the  SnappyData Release Page , which lists the latest and previous releases of SnappyData.  $ tar -xzf snappydata-1.0.2.1-bin.tar.gz\n$ cd snappydata-1.0.2.1-bin/\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir\n$./bin/spark-shell --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log  It opens the Spark shell. All SnappyData metadata, as well as persistent data, is stored in the directory  quickstartdatadir .  The spark-shell can now be used to work with SnappyData using  Scala APIs  and  SQL .", 
            "title": "Getting Started by Installing SnappyData On-Premise"
        }, 
        {
            "location": "/quickstart/getting_started_on_kubernetes/", 
            "text": "Getting Started with SnappyData on Kubernetes\n\n\nKubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes.\n\n\nThis following sections are included in this topic:\n\n\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nDeploying SnappyData Chart in Kubernetes\n\n\n\n\n\n\n \n\n\nPrerequisites\n\n\nThe following prerequisites must be met to deploy SnappyData on Kubernetes:\n\n\n\n\n\n\nKubernetes cluster\n A running Kubernetes cluster of version 1.9 or higher.\n\n\n\n\n\n\nHelm tool\n Helm tool must be deployed in the Kubernetes environment. Helm comprises of two parts, that is a client and a Tiller (Server portion of Helm) inside the kube-system namespace. Tiller runs inside the Kubernetes cluster and manages the deployment of charts or packages. You can follow the instructions \nhere\n to deploy Helm in your Kubernetes enviroment.\n\n\n\n\n\n\n \n\n\nDeploying SnappyData on Kubernetes\n\n\nSnappyData Helm chart is used to deploy SnappyData on Kubernetes.  It uses Kubernetes \nstatefulsets\n to launch the locator, lead, and server members. \n\n\nTo deploy SnappyData on Kubernetes:\n\n\n\n\n\n\nClone the \nspark-on-k8s\n repository and change to \ncharts\n directory.\n\n\ngit clone https://github.com/SnappyDataInc/spark-on-k8s\n\n\ncd spark-on-k8s/charts\n\n\n\n\n\n\nOptionally, go to \nsnappydata\n \n \nvalues.yaml\n  file to edit the default configurations in SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer \nList of Configuration Parameters for SnappyData Chart\n\n\n\n\n\n\nInstall the \nsnappydata\n chart using the following command:\n\n\nhelm install --name snappydata --namespace snappy ./snappydata/\n\n\nThe above command installs the SnappyData chart in a namespace called \nsnappy\n and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console.\n\nBy default, SnappyData helm chart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints.\n\n\n\n\n\n\nYou can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions \nhere\n. \n\n\nSnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted.\n\n\nFor more details on accessing and interacting with SnappyData cluster on Kubernetes refer to \nSnappyData Cluster on Kubernetes", 
            "title": "Getting Started with SnappyData on Kubernetes"
        }, 
        {
            "location": "/quickstart/getting_started_on_kubernetes/#getting-started-with-snappydata-on-kubernetes", 
            "text": "Kubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes.  This following sections are included in this topic:    Prerequisites    Deploying SnappyData Chart in Kubernetes", 
            "title": "Getting Started with SnappyData on Kubernetes"
        }, 
        {
            "location": "/quickstart/getting_started_on_kubernetes/#prerequisites", 
            "text": "The following prerequisites must be met to deploy SnappyData on Kubernetes:    Kubernetes cluster  A running Kubernetes cluster of version 1.9 or higher.    Helm tool  Helm tool must be deployed in the Kubernetes environment. Helm comprises of two parts, that is a client and a Tiller (Server portion of Helm) inside the kube-system namespace. Tiller runs inside the Kubernetes cluster and manages the deployment of charts or packages. You can follow the instructions  here  to deploy Helm in your Kubernetes enviroment.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/quickstart/getting_started_on_kubernetes/#deploying-snappydata-on-kubernetes", 
            "text": "SnappyData Helm chart is used to deploy SnappyData on Kubernetes.  It uses Kubernetes  statefulsets  to launch the locator, lead, and server members.   To deploy SnappyData on Kubernetes:    Clone the  spark-on-k8s  repository and change to  charts  directory.  git clone https://github.com/SnappyDataInc/spark-on-k8s  cd spark-on-k8s/charts    Optionally, go to  snappydata     values.yaml   file to edit the default configurations in SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer  List of Configuration Parameters for SnappyData Chart    Install the  snappydata  chart using the following command:  helm install --name snappydata --namespace snappy ./snappydata/  The above command installs the SnappyData chart in a namespace called  snappy  and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console. \nBy default, SnappyData helm chart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints.    You can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions  here .   SnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted.  For more details on accessing and interacting with SnappyData cluster on Kubernetes refer to  SnappyData Cluster on Kubernetes", 
            "title": "Deploying SnappyData on Kubernetes"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/", 
            "text": "Getting Started with Docker Image\n\n\nSnappyData comes with a pre-configured Docker image. It has binaries for SnappyData, which enables you to try the quick start program and more, with SnappyData.\n\n\nThis section assumes you have already installed and configured Docker properly. Refer to \nDocker documentation\n for more details.\n\n\nVerify that Docker is Installed\n\n\nIn the command prompt run the command:\n\n\n$ docker run hello-world\n\n\n\n\n\n\n\nNote\n\n\nEnsure that the Docker containers have access to at least 4GB of RAM on your machine.\n\n\n\n\nGet the Docker Image\n\n\nIn the command prompt, type the following command to get the Docker image. This starts the container and takes you to the Spark shell.\n\n\n$  docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell\n\n\n\n\nThe latest image files start downloading to your local machine. Depending on your network connection, it may take some time. \n\n\nOnce you have launched the Spark shell, in the \n$ scala\n prompt, follow the steps explained \nhere\n.\n\n\nFor more details about SnappyData Docker image see \nSnappy Cloud Tools\n.", 
            "title": "Getting Started with SnappyData on Docker Image"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/#getting-started-with-docker-image", 
            "text": "SnappyData comes with a pre-configured Docker image. It has binaries for SnappyData, which enables you to try the quick start program and more, with SnappyData.  This section assumes you have already installed and configured Docker properly. Refer to  Docker documentation  for more details.", 
            "title": "Getting Started with Docker Image"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/#verify-that-docker-is-installed", 
            "text": "In the command prompt run the command:  $ docker run hello-world   Note  Ensure that the Docker containers have access to at least 4GB of RAM on your machine.", 
            "title": "Verify that Docker is Installed"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/#get-the-docker-image", 
            "text": "In the command prompt, type the following command to get the Docker image. This starts the container and takes you to the Spark shell.  $  docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell  The latest image files start downloading to your local machine. Depending on your network connection, it may take some time.   Once you have launched the Spark shell, in the  $ scala  prompt, follow the steps explained  here .  For more details about SnappyData Docker image see  Snappy Cloud Tools .", 
            "title": "Get the Docker Image"
        }, 
        {
            "location": "/quickstart/using_spark_scala_apis/", 
            "text": "Using Spark Scala APIs\n\n\n\n\n\n\nCreate a SnappySession\n\n    SnappySession extends the SparkSession so you can mutate data, get much higher performance, etc.\n\n\nscala\n val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\n// Import Snappy extensions\nscala\n import snappy.implicits._\n\n\n\n\n\n\n\nCreate a dataset using the Spark APIs\n\n\nscala\n val ds = Seq((1,\"a\"), (2, \"b\"), (3, \"c\")).toDS()\n\n\n\n\n\n\n\nDefine a schema for the table\n\n\nscala\n  import org.apache.spark.sql.types._\nscala\n  val tableSchema = StructType(Array(StructField(\"CustKey\", IntegerType, false),\n          StructField(\"CustName\", StringType, false)))\n\n\n\n\n\n\n\nCreate a \"column\" table with a simple schema [String, Int] and default options\n\n    For detailed option refer to the \nRow and Column Tables\n section.\n\n\n// Column tables manage data is columnar form and offer superior performance for analytic class queries.\nscala\n  snappy.createTable(tableName = \"colTable\",\n          provider = \"column\", // Create a SnappyData Column table\n          schema = tableSchema,\n          options = Map.empty[String, String], // Map for options\n          allowExisting = false)\n\n\n\nSnappyData (SnappySession) extends SparkSession, so you can simply use all the Spark's APIs.\n\n\n\n\n\n\nInsert the created DataSet to the column table \"colTable\"\n\n\nscala\n  ds.write.insertInto(\"colTable\")\n// Check the total row count.\nscala\n  snappy.table(\"colTable\").count\n\n\n\n\n\n\n\nCreate a row object using Spark's API and insert the row into the table\n\n    Unlike Spark DataFrames SnappyData column tables are mutable. You can insert new rows to a column table.\n\n\n// Insert a new record\nscala\n  import org.apache.spark.sql.Row\nscala\n  snappy.insert(\"colTable\", Row(10, \"f\"))\n// Check the total row count after inserting the row\nscala\n  snappy.table(\"colTable\").count\n\n\n\n\n\n\n\nCreate a \"row\" table with a simple schema [String, Int] and default options\n \n\n    For detailed option refer to the \nRow and Column Tables\n section.\n\n\n// Row formatted tables are better when datasets constantly change or access is selective (like based on a key)\nscala\n  snappy.createTable(tableName = \"rowTable\",\nprovider = \"row\",\nschema = tableSchema,\noptions = Map.empty[String, String],\nallowExisting = false)\n\n\n\n\n\n\n\nInsert the created DataSet to the row table \"rowTable\"\n\n\nscala\n  ds.write.insertInto(\"rowTable\")\n// Check the row count\nscala\n  snappy.table(\"rowTable\").count\n\n\n\n\n\n\n\nInsert a new record\n\n\nscala\n  snappy.insert(\"rowTable\", Row(4, \"d\"))\n//Check the row count now\nscala\n  snappy.table(\"rowTable\").count\n\n\n\n\n\n\n\nChange some data in the row table\n\n\n// Updating a row for customer with custKey = 1\nscala\n  snappy.update(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\",\n                newColumnValues = Row(\"d\"), updateColumns = \"CUSTNAME\")\n\nscala\n  snappy.table(\"rowTable\").orderBy(\"CUSTKEY\").show\n\n// Delete the row for customer with custKey = 1\nscala\n  snappy.delete(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\")\n\n// Drop the existing tables\nscala\n  snappy.dropTable(\"rowTable\", ifExists = true)\nscala\n  snappy.dropTable(\"colTable\", ifExists = true)", 
            "title": "Using Spark Scala APIs"
        }, 
        {
            "location": "/quickstart/using_spark_scala_apis/#using-spark-scala-apis", 
            "text": "Create a SnappySession \n    SnappySession extends the SparkSession so you can mutate data, get much higher performance, etc.  scala  val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\n// Import Snappy extensions\nscala  import snappy.implicits._    Create a dataset using the Spark APIs  scala  val ds = Seq((1,\"a\"), (2, \"b\"), (3, \"c\")).toDS()    Define a schema for the table  scala   import org.apache.spark.sql.types._\nscala   val tableSchema = StructType(Array(StructField(\"CustKey\", IntegerType, false),\n          StructField(\"CustName\", StringType, false)))    Create a \"column\" table with a simple schema [String, Int] and default options \n    For detailed option refer to the  Row and Column Tables  section.  // Column tables manage data is columnar form and offer superior performance for analytic class queries.\nscala   snappy.createTable(tableName = \"colTable\",\n          provider = \"column\", // Create a SnappyData Column table\n          schema = tableSchema,\n          options = Map.empty[String, String], // Map for options\n          allowExisting = false)  SnappyData (SnappySession) extends SparkSession, so you can simply use all the Spark's APIs.    Insert the created DataSet to the column table \"colTable\"  scala   ds.write.insertInto(\"colTable\")\n// Check the total row count.\nscala   snappy.table(\"colTable\").count    Create a row object using Spark's API and insert the row into the table \n    Unlike Spark DataFrames SnappyData column tables are mutable. You can insert new rows to a column table.  // Insert a new record\nscala   import org.apache.spark.sql.Row\nscala   snappy.insert(\"colTable\", Row(10, \"f\"))\n// Check the total row count after inserting the row\nscala   snappy.table(\"colTable\").count    Create a \"row\" table with a simple schema [String, Int] and default options   \n    For detailed option refer to the  Row and Column Tables  section.  // Row formatted tables are better when datasets constantly change or access is selective (like based on a key)\nscala   snappy.createTable(tableName = \"rowTable\",\nprovider = \"row\",\nschema = tableSchema,\noptions = Map.empty[String, String],\nallowExisting = false)    Insert the created DataSet to the row table \"rowTable\"  scala   ds.write.insertInto(\"rowTable\")\n// Check the row count\nscala   snappy.table(\"rowTable\").count    Insert a new record  scala   snappy.insert(\"rowTable\", Row(4, \"d\"))\n//Check the row count now\nscala   snappy.table(\"rowTable\").count    Change some data in the row table  // Updating a row for customer with custKey = 1\nscala   snappy.update(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\",\n                newColumnValues = Row(\"d\"), updateColumns = \"CUSTNAME\")\n\nscala   snappy.table(\"rowTable\").orderBy(\"CUSTKEY\").show\n\n// Delete the row for customer with custKey = 1\nscala   snappy.delete(tableName = \"rowTable\", filterExpr = \"CUSTKEY=1\")\n\n// Drop the existing tables\nscala   snappy.dropTable(\"rowTable\", ifExists = true)\nscala   snappy.dropTable(\"colTable\", ifExists = true)", 
            "title": "Using Spark Scala APIs"
        }, 
        {
            "location": "/quickstart/performance_apache_spark/", 
            "text": "SnappyData Performance: 16x-20x faster than Apache Spark\n\n\nIn this section, you are walked through a simple benchmark to compare SnappyData's performance to Spark 2.1.1.\n\nMillions of rows are loaded into a cached Spark DataFrame, some analytic queries measuring its performance are run, and then, the same using SnappyData's column table is repeated.\n\n\nA simple analytic query that scans a 100 million-row column table shows SnappyData outperforming Apache Spark by 16-20X when both products have all the data in memory.\n\n\n\n\nNote\n\n\nIt is recommended that you should have at least 4GB of RAM reserved for this test. \n\n\n\n\nStart the Spark Shell\n\n\nUse any of the options mentioned below to start the Spark shell:\n\n\n\n\n\n\nIf you are using your own Spark distribution that is compatible with version 2.1.1:\n\n\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir \n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \"SnappyDataInc:snappydata:1.0.2.1-s_2.11\" --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\"\n\n\n\n\n\n\n\nIf you have downloaded SnappyData\n:\n\n\n# Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir \n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\"\n\n\n\n\n\n\n\nIf you are using Docker\n:\n\n\n$ docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell --driver-memory=4g --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\"\n\n\n\n\n\n\n\nTo get the Performance Numbers\n\n\nEnsure that you are in a Spark shell, and then follow the instructions below to get the performance numbers.\n\n\n\n\n\n\nDefine a function \"benchmark\"\n, which tells us the average time to run queries after doing the initial warm-ups.\n\n\nscala\n  def benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: =\n Unit) {\n          for (i \n- 1 to warmups) {\n            f\n          }\n          val startTime = System.nanoTime\n          for (i \n- 1 to times) {\n            f\n          }\n          val endTime = System.nanoTime\n          println(s\"Average time taken in $name for $times runs: \" +\n            (endTime - startTime).toDouble / (times * 1000000.0) + \" millis\")\n        }\n\n\n\n\n\n\n\nCreate a DataFrame and temporary table using Spark's range method\n:\n\n\nCache it in Spark to get optimal performance. This creates a DataFrame of 100 million records.You can change the number of rows, based on your memory availability.\n\n\nscala\n  var testDF = spark.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as STRING)) as sym\")\nscala\n  testDF.cache\nscala\n  testDF.createOrReplaceTempView(\"sparkCacheTable\")\n\n\n\n\n\n\n\nRun a query and to check the performance\n:\n\n\nThe queries use an average of a field, without any \"where\" clause. This ensures that it touches all records while scanning.\n\n\nscala\n  benchmark(\"Spark perf\") {spark.sql(\"select sym, avg(id) from sparkCacheTable group by sym\").collect()}\n\n\n\n\n\n\n\nClean up the JVM\n:\n\n\nThis ensures that all in-memory artifacts for Spark are cleaned up.\n\n\nscala\n  testDF.unpersist()\nscala\n  System.gc()\nscala\n  System.runFinalization()\n\n\n\n\n\n\n\nCreate a SnappySession\n:\n\n\nscala\n  val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\n\n\n\n\n\n\n\nCreate similar 100 million record DataFrame\n:\n\n\nscala\n  testDF = snappy.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as varchar(10))) as sym\")\n\n\n\n\n\n\n\nCreate the table\n:\n\n\nscala\n  snappy.sql(\"drop table if exists snappyTable\")\nscala\n  snappy.sql(\"create table snappyTable (id bigint not null, sym varchar(10) not null) using column\")\n\n\n\n\n\n\n\nInsert the created DataFrame into the table and measure its performance\n:\n\n\nscala\n  benchmark(\"Snappy insert perf\", 1, 0) {testDF.write.insertInto(\"snappyTable\") }\n\n\n\n\n\n\n\nNow, let us run the same benchmark against Spark DataFrame\n:\n\n\nscala\n  benchmark(\"Snappy perf\") {snappy.sql(\"select sym, avg(id) from snappyTable group by sym\").collect()}\n\nscala\n :q // Quit the Spark Shell\n\n\n\n\n\n\n\n\n\nNote\n\n\nThis benchmark code is tested on a system with  4 CPUs (Intel(R) Core(TM) i7-5600U CPU @ 2.60GHz) and 16GiB System Memory. Also, in an AWS t2.xlarge (Variable ECUs, 4 vCPUs, 2.4 GHz, Intel Xeon Family, 16 GiB memory, EBS only) instance SnappyData is approximately 16 to 18 times faster than Spark 2.1.1.", 
            "title": "SnappyData Performance:16x-20x faster than Apache Spark"
        }, 
        {
            "location": "/quickstart/performance_apache_spark/#snappydata-performance-16x-20x-faster-than-apache-spark", 
            "text": "In this section, you are walked through a simple benchmark to compare SnappyData's performance to Spark 2.1.1. \nMillions of rows are loaded into a cached Spark DataFrame, some analytic queries measuring its performance are run, and then, the same using SnappyData's column table is repeated.  A simple analytic query that scans a 100 million-row column table shows SnappyData outperforming Apache Spark by 16-20X when both products have all the data in memory.   Note  It is recommended that you should have at least 4GB of RAM reserved for this test.", 
            "title": "SnappyData Performance: 16x-20x faster than Apache Spark"
        }, 
        {
            "location": "/quickstart/performance_apache_spark/#start-the-spark-shell", 
            "text": "Use any of the options mentioned below to start the Spark shell:    If you are using your own Spark distribution that is compatible with version 2.1.1:  # Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir \n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --packages \"SnappyDataInc:snappydata:1.0.2.1-s_2.11\" --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\"    If you have downloaded SnappyData :  # Create a directory for SnappyData artifacts\n$ mkdir quickstartdatadir \n$ ./bin/spark-shell --driver-memory=4g --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\"    If you are using Docker :  $ docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell --driver-memory=4g --driver-java-options=\"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g\"", 
            "title": "Start the Spark Shell"
        }, 
        {
            "location": "/quickstart/performance_apache_spark/#to-get-the-performance-numbers", 
            "text": "Ensure that you are in a Spark shell, and then follow the instructions below to get the performance numbers.    Define a function \"benchmark\" , which tells us the average time to run queries after doing the initial warm-ups.  scala   def benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: =  Unit) {\n          for (i  - 1 to warmups) {\n            f\n          }\n          val startTime = System.nanoTime\n          for (i  - 1 to times) {\n            f\n          }\n          val endTime = System.nanoTime\n          println(s\"Average time taken in $name for $times runs: \" +\n            (endTime - startTime).toDouble / (times * 1000000.0) + \" millis\")\n        }    Create a DataFrame and temporary table using Spark's range method :  Cache it in Spark to get optimal performance. This creates a DataFrame of 100 million records.You can change the number of rows, based on your memory availability.  scala   var testDF = spark.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as STRING)) as sym\")\nscala   testDF.cache\nscala   testDF.createOrReplaceTempView(\"sparkCacheTable\")    Run a query and to check the performance :  The queries use an average of a field, without any \"where\" clause. This ensures that it touches all records while scanning.  scala   benchmark(\"Spark perf\") {spark.sql(\"select sym, avg(id) from sparkCacheTable group by sym\").collect()}    Clean up the JVM :  This ensures that all in-memory artifacts for Spark are cleaned up.  scala   testDF.unpersist()\nscala   System.gc()\nscala   System.runFinalization()    Create a SnappySession :  scala   val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)    Create similar 100 million record DataFrame :  scala   testDF = snappy.range(100000000).selectExpr(\"id\", \"concat('sym', cast((id % 100) as varchar(10))) as sym\")    Create the table :  scala   snappy.sql(\"drop table if exists snappyTable\")\nscala   snappy.sql(\"create table snappyTable (id bigint not null, sym varchar(10) not null) using column\")    Insert the created DataFrame into the table and measure its performance :  scala   benchmark(\"Snappy insert perf\", 1, 0) {testDF.write.insertInto(\"snappyTable\") }    Now, let us run the same benchmark against Spark DataFrame :  scala   benchmark(\"Snappy perf\") {snappy.sql(\"select sym, avg(id) from snappyTable group by sym\").collect()}\n\nscala  :q // Quit the Spark Shell     Note  This benchmark code is tested on a system with  4 CPUs (Intel(R) Core(TM) i7-5600U CPU @ 2.60GHz) and 16GiB System Memory. Also, in an AWS t2.xlarge (Variable ECUs, 4 vCPUs, 2.4 GHz, Intel Xeon Family, 16 GiB memory, EBS only) instance SnappyData is approximately 16 to 18 times faster than Spark 2.1.1.", 
            "title": "To get the Performance Numbers"
        }, 
        {
            "location": "/quickstart/using_sql/", 
            "text": "Using SQL\n\n\nIn this section, you can also connect to SQL using Snappy Session API. \n\nYou can use any SQL client tool (for example, Snappy shell). For an example, refer to the \nHow-to\n section.\n\n\nCreate a column table with a simple schema [Int, String] and default options\n\n\nFor more information on the available options, refer to the \nRow and Column Tables\n section.\n\n\nscala\n  snappy.sql(\ncreate table colTable(CustKey Integer, CustName String) using column options()\n)\n\n\n\n\n//Insert couple of records to the column table\nscala\n  snappy.sql(\ninsert into colTable values(1, 'a')\n)\nscala\n  snappy.sql(\ninsert into colTable values(2, 'b')\n)\nscala\n  snappy.sql(\ninsert into colTable values(3, '3')\n)\n\n\n\n\n// Check the total row count now\nscala\n  snappy.sql(\nselect count(*) from colTable\n).show\n\n\n\n\nCreate a row table with a primary key\n:\n\n\n// Row formatted tables are better when data sets constantly change or access is selective (like based on a key).\nscala\n  snappy.sql(\ncreate table rowTable(CustKey Integer NOT NULL PRIMARY KEY, \n +\n            \nCustName String) using row options()\n)\n\n\n\n\nIf you create a table using standard SQL (that is, no 'row options' clause) it creates a replicated row table.\n\n\n//Insert couple of records to the row table\nscala\n  snappy.sql(\ninsert into rowTable values(1, 'a')\n)\nscala\n  snappy.sql(\ninsert into rowTable values(2, 'b')\n)\nscala\n  snappy.sql(\ninsert into rowTable values(3, '3')\n)\n\n\n\n\n//Update some rows\nscala\n  snappy.sql(\nupdate rowTable set CustName='d' where custkey = 1\n)\nscala\n  snappy.sql(\nselect * from rowTable order by custkey\n).show\n\n\n\n\n//Drop the existing tables\nscala\n  snappy.sql(\ndrop table if exists rowTable \n)\nscala\n  snappy.sql(\ndrop table if exists colTable \n)\n\n\n\n\nscala\n :q //Quit the Spark Shell\n\n\n\n\nNow that you have seen the basic working of SnappyData tables, let us run the \nbenchmark\n code to see the performance of SnappyData and compare it to Spark's native cache performance.\n\n\nMore Information\n\n\nFor more examples of the common operations, you can refer to the \nHow-tos\n section. \n\n\nIf you have questions or queries you can contact us through our \ncommunity channels\n.", 
            "title": "Using SQL"
        }, 
        {
            "location": "/quickstart/using_sql/#using-sql", 
            "text": "In this section, you can also connect to SQL using Snappy Session API.  \nYou can use any SQL client tool (for example, Snappy shell). For an example, refer to the  How-to  section.  Create a column table with a simple schema [Int, String] and default options  For more information on the available options, refer to the  Row and Column Tables  section.  scala   snappy.sql( create table colTable(CustKey Integer, CustName String) using column options() )  //Insert couple of records to the column table\nscala   snappy.sql( insert into colTable values(1, 'a') )\nscala   snappy.sql( insert into colTable values(2, 'b') )\nscala   snappy.sql( insert into colTable values(3, '3') )  // Check the total row count now\nscala   snappy.sql( select count(*) from colTable ).show  Create a row table with a primary key :  // Row formatted tables are better when data sets constantly change or access is selective (like based on a key).\nscala   snappy.sql( create table rowTable(CustKey Integer NOT NULL PRIMARY KEY,   +\n             CustName String) using row options() )  If you create a table using standard SQL (that is, no 'row options' clause) it creates a replicated row table.  //Insert couple of records to the row table\nscala   snappy.sql( insert into rowTable values(1, 'a') )\nscala   snappy.sql( insert into rowTable values(2, 'b') )\nscala   snappy.sql( insert into rowTable values(3, '3') )  //Update some rows\nscala   snappy.sql( update rowTable set CustName='d' where custkey = 1 )\nscala   snappy.sql( select * from rowTable order by custkey ).show  //Drop the existing tables\nscala   snappy.sql( drop table if exists rowTable  )\nscala   snappy.sql( drop table if exists colTable  )  scala  :q //Quit the Spark Shell  Now that you have seen the basic working of SnappyData tables, let us run the  benchmark  code to see the performance of SnappyData and compare it to Spark's native cache performance.", 
            "title": "Using SQL"
        }, 
        {
            "location": "/quickstart/using_sql/#more-information", 
            "text": "For more examples of the common operations, you can refer to the  How-tos  section.   If you have questions or queries you can contact us through our  community channels .", 
            "title": "More Information"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/", 
            "text": "SnappyData Cluster SQL Tutorial\n\n\nIn this section, you will get a quick tour to start a SnappyData cluster and try out the basic features and functionalities. The following items are covered in this guide:\n\n\n\n\n\n\nStart SnappyData Cluster\n\n\n\n\n\n\nCheck SnappyData Cluster Status\n\n\n\n\n\n\nConnect/Disconnect SnappyData Shell\n\n\n\n\n\n\nCreate Tables\n\n\n\n\n\n\nCreate Tables and Import Data Using quickstart Scripts\n\n\n\n\n\n\nCreate a Column Table using an External Table\n\n\n\n\n\n\nRun Queries\n\n\n\n\n\n\nAdd Servers to Cluster\n\n\n\n\n\n\nRebalance Data on Servers\n\n\n\n\n\n\nStop Cluster\n\n\n\n\n\n\n \n\n\nStart SnappyData Cluster\n\n\nNavigate to the SnappyData product root directory to start the cluster. Run the \n./sbin/snappy-start-all.sh\n script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server. \n\n\n$./sbin/snappy-start-all.sh\n\nLogs generated in /home/xyz/\nsnappydata_install_dir\n/work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 9086 status: running\n  Distributed system now has 1 members.\n  Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527]\nLogs generated in /home/xyz/\nsnappydata_install_dir\n/work/localhost-server-1/snappyserver.log\nSnappyData Server pid: 9220 status: running\n  Distributed system now has 2 members.\n  Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528]\nLogs generated in /home/xyz/\nsnappydata_install_dir\n/snappy/work/localhost-lead-1/snappyleader.log\nSnappyData Leader pid: 9370 status: running\n  Distributed system now has 3 members.\n  Starting job server on: 0.0.0.0[8090]\n\n\n\n\n\nYou can connect to \nSnappy SQL shell\n and run the \nselect id, kind, netservers from sys.members;\n query to view the cluster members.\n\n\n    ./bin/snappy\n    connect client '127.0.0.1:1527';\n    select id, kind, netservers from sys.members;\n\n    ID                                                                                                         |KIND                  |NETSERVERS\n    -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    127.0.0.1(3688)\nv1\n:32604                                                                                  |datastore(normal)     |localhost/127.0.0.1[1528]\n    127.0.0.1(3842)\nv2\n:25366                                                                                  |accessor(normal)      |                         \n    127.0.0.1(3431:locator)\nec\nv0\n:41406                                                                      |locator(normal)       |localhost/127.0.0.1[1527]\n\n\n\nIn the output, the details of the cluster members are displayed. Here in the KIND column, the member corresponding to \naccessor\n is the lead node.  In a cluster, you can connect as a client to any member by specifying localhost with the unique port number of the member (the one specified in the NETSERVERS column corresponding to each member). However, connecting to the locator provides basic load balancing by routing the connection request to an available server member.\n\n\n \n\n\nCheck SnappyData Cluster Status\n\n\nYou can check the status of a running cluster using the following command:\n\n\n$ ./sbin/snappy-status-all.sh\nSnappyData Locator pid: 9748 status: running\nSnappyData Server pid: 9887 status: running\nSnappyData Leader pid: 10468 status: running\n\n\n\n\nConnect to  \nSnappy SQL shell\n to perform various SQL operations.\n\n\nAlternatively , you can access the \nSnappyData Pulse\n monitoring tool by entering  \nhttp:// \nleadhost\n:5050/dashboard/\n in the web browser. For example,  http://localhost:5050/dashboard/. \nleadhost\n is the hostname or IP of the lead node in your cluster which is provided in the conf/leads file. On the SnappyData Pulse dashboards, after starting a cluster, you can check the status of each of the cluster member.\n\n\n \n\n\nConnect/Disconnect to SnappyData Shell\n\n\nAfter starting the SnappyData cluster, run these commands together to start the SnappyData shell:\n\n\n./bin/snappy\nconnect client '127.0.0.1:1527';\n\n\n\n\nType \nexit;\n or press \nCTRL + C\n to disconnect the SnappyData Shell.\n\n\n \n\n\nCreate Tables\n\n\nCreate a simple table and insert a few rows. By default, if no options are provided, row replicated table is formed. However, you can create tables using the row or column option.  \n\n\n# Create a table named quicktable. A replicated row table is formed without/empty options by default.\nCREATE TABLE quicktable (id int generated always as identity, item char(25));\n\n# Create column table. A column table is created which is partitioned by default.\nCREATE TABLE  quicktable_col (id int, item varchar(25)) using column options();\n\n# Create partitioned Row table using the partition by options. A partitioned row table is created with partitioning scheme on the 'id' column. \nCREATE TABLE quicktable_row (id int generated always as identity, item char(25)) using row options(partition_by 'id');\n\n# Insert one row into the table.\nINSERT into quicktable values (default, 'widget');\n\n# Insert one more row into the table.\nINSERT into quicktable values (default, 'gadget');\n\n# View the contents of the table.\nselect * from quicktable;\nID         |ITEM                     \n-------------------------------------\n2          |gadget                \n1          |widget\n\n2 rows selected\n\n# SnappyData replicates the row tables that are created by default onto the data store members. \n# You can validate this using the following query:\n\nselect tablename, datapolicy from sys.systables where tablename='QUICKTABLE';\n\n#The following output is displayed:\n\nTABLENAME                                                                                                                       |DATAPOLICY              \n---------------------------------------------------------------------------------------------------------------------------------------------------------\nQUICKTABLE                                                                                                                      |PERSISTENT_REPLICATE\n\n\n\n \n\n\nCreate Tables and Import Data Using quickstart Scripts\n\n\nSnappyData contains various quickstart scripts that can be used to run some basic functionalities. \nFor example, you can run the \ncreate_and_load_column_table.sql\n script. This script first drops the table if it exists and then  creates an external table named STAGING AIRLINE to load the formatted data from a parquet file. Then a column table is created with only the specified columns from this external table. \nConnect to SnappyData shell before running these scripts. These script files must be run from within the \nquickstart\n directory if you are providing the relative path as shown:\n\n\n        ./bin/snappy\n        connect client '127.0.0.1:1527';\n        RUN 'quickstart/scripts/create_and_load_column_table.sql';\n\n        # Use the following command to view the details of the external table. \n        describe staging_airline;\n\n        # The following output is displayed:\n        COLUMN_NAME         |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE\n        -----------------------------------------------------------------------------------------------------------------\n        Year                |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        Month               |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DayOfMonth          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DayOfWeek           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DepTime             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CRSDepTime          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        ArrTime             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CRSArrTime          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        UniqueCarrier       |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        FlightNum           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        TailNum             |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        ActualElapsedTime   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CRSElapsedTime      |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        AirTime             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        ArrDelay            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DepDelay            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        Origin              |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        Dest                |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        Distance            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        TaxiIn              |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        TaxiOut             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        Cancelled           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CancellationCode    |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        Diverted            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CarrierDelay        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        WeatherDelay        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        NASDelay            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        SecurityDelay       |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        LateAircraftDelay   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        ArrDelaySlot        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES\n\n        # Use the following command to check the number of records in the staging_airline external table:\n\n        select count(*) from staging_airline;\n        count(1)            \n        --------------------\n        1000000\n\n        1 row selected\n\n\n\nYou can also try the following:\n\n\n\n\n\n\nCreate and load a row table:\n\n\nRUN './quickstart/scripts/create_and_load_row_table.sql';\n\n\n\n\n\n\n\nView the status of the system:\n\n\nRUN './quickstart/scripts/status_queries.sql';\n\n\n\n\n\n\n\n \n\n\nCreate a Column Table Using an External Table\n\n\nSimilarly as the quickstart scripts, you can try to create an external table named staging_airline to load the formatted data from a airlineParquetData file with inferSchema option as true. Later, you can create a column table named airline and pull data from the external table into this table. After pulling in the data, you can check the number of records in the table.\n\n\n    CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData', inferSchema 'true');\n\n        CREATE TABLE AIRLINE2 USING column AS (SELECT * FROM STAGING_AIRLINE);\n\n        describe airline2;\n    COLUMN_NAME         |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE\n    -----------------------------------------------------------------------------------------------------------------\n    YEAR                |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    MONTH               |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DAYOFMONTH          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DAYOFWEEK           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DEPTIME             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CRSDEPTIME          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ARRTIME             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CRSARRTIME          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    UNIQUECARRIER       |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    FLIGHTNUM           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    TAILNUM             |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    ACTUALELAPSEDTIME   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CRSELAPSEDTIME      |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    AIRTIME             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ARRDELAY            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DEPDELAY            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ORIGIN              |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    DEST                |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    DISTANCE            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    TAXIIN              |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    TAXIOUT             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CANCELLED           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CANCELLATIONCODE    |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    DIVERTED            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CARRIERDELAY        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    WEATHERDELAY        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    NASDELAY            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    SECURITYDELAY       |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    LATEAIRCRAFTDELAY   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ARRDELAYSLOT        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES\n\n    30 rows selected\n\n    select count(*)\n    \n ;\n    count(1)            \n    --------------------\n    1\n\n    1 row selected\n\n\n\nAfter running these queries, you can check the table details on the SnappyData Pulse Dashboards. The details of the newly created tables are displayed in the \nTables\n section.\n\n\n\n\n \n\n\nRun Queries\n\n\nYou can try a couple of analytical queries as shown:\n\n\n\n\nQuery to find the average arrival delay.\n\n\n\n\nselect avg(arrdelay) from airline;\n\navg(ARRDELAY)         \n----------------------\n6.735443              \n\n1 row selected\n\n\n\n\n\n\n\nQuery for avg arrival delay of a specific airline.\n\n\n\n\nselect max(arrdelay) from airline where DEST = '';\n\nmax(ARRDELAY)\n-------------\nNULL         \n\n1 row selected\n\n\n\n\n\n \n\n\nAdd Servers into Cluster\n\n\nYou can add more than one server to a cluster. To add a new server, do the following:\n\n\n\n\nGo to SnappyData home directory.\ncd \nsnappydata_install_dir\n\n\nCreate a configuration file named \nservers\n in the conf folder in the the SnappyData home directory. To do so, you can copy the existing template files \nservers.template\n and rename it to \nservers\n as shown:\ncp -f conf/servers.template conf/servers\n\n\nOpen this file using a vi editor and add a hostname entry of the additional server, after the entry of the primary server, and save the file. \nFor example, suppose there is an entry \nlocalhost\n in this file for the primary server. You can add an entry \nlocalhost\n below this entry for the additional server. The \nservers\n file should contain the hostnames of the nodes (one per line) where you intend to start the member.\n\n\nFrom the SnappyData home directory, start the cluster again using the \n./sbin/snappy-start-all.sh\n command. The new server gets started. Ignore the error messages of the other nodes that are already running. You can check  details of the newly added member from the SnappyData Pulse UI. \n\n\n\n\n \n\n\nRebalancing Data on Servers\n\n\nFurther, you can distribute the data among the servers in the cluster. This ensures that each server carries almost equal data.\nTo balance the data equally on the servers, do the following:\n\n\n\n\nGo to SnappyData home directory.\ncd \nsnappydata_install_dir\n\n\nConnect to snappy shell\n and obtain the jdbc client connection.\n\n\nRun the rebalance command.\n \ncall sys.rebalance_all_buckets();\n\n\n\n\nOn SnappyData Pulse UI, check the Heap Memory Used/Total column for the servers. You will notice that before rebalancing the data, there was an unequal distribution of the memory usage and after running the rebalance command, the data is distributed equally among both the servers.\n\n\nBefore Rebalance\n\n\n\n\nAfter Rebalance\n\n\n\n\n\n\n\n\n \n\n\nStop the Cluster\n\n\nYou can stop the cluster using the \n./sbin/snappy-stop-all.sh\n command:\n\n\n./sbin/snappy-stop-all.sh\nThe SnappyData Leader has stopped.\nThe SnappyData Server has stopped.\nThe SnappyData Locator has stopped.\n\n\n\n\nFor more details, refer to \nStopping the Cluster", 
            "title": "SnappyData Cluster SQL Tutorials"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#snappydata-cluster-sql-tutorial", 
            "text": "In this section, you will get a quick tour to start a SnappyData cluster and try out the basic features and functionalities. The following items are covered in this guide:    Start SnappyData Cluster    Check SnappyData Cluster Status    Connect/Disconnect SnappyData Shell    Create Tables    Create Tables and Import Data Using quickstart Scripts    Create a Column Table using an External Table    Run Queries    Add Servers to Cluster    Rebalance Data on Servers    Stop Cluster", 
            "title": "SnappyData Cluster SQL Tutorial"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#start-snappydata-cluster", 
            "text": "Navigate to the SnappyData product root directory to start the cluster. Run the  ./sbin/snappy-start-all.sh  script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server.   $./sbin/snappy-start-all.sh\n\nLogs generated in /home/xyz/ snappydata_install_dir /work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 9086 status: running\n  Distributed system now has 1 members.\n  Started Thrift locator (Compact Protocol) on: localhost/127.0.0.1[1527]\nLogs generated in /home/xyz/ snappydata_install_dir /work/localhost-server-1/snappyserver.log\nSnappyData Server pid: 9220 status: running\n  Distributed system now has 2 members.\n  Started Thrift server (Compact Protocol) on: localhost/127.0.0.1[1528]\nLogs generated in /home/xyz/ snappydata_install_dir /snappy/work/localhost-lead-1/snappyleader.log\nSnappyData Leader pid: 9370 status: running\n  Distributed system now has 3 members.\n  Starting job server on: 0.0.0.0[8090]  You can connect to  Snappy SQL shell  and run the  select id, kind, netservers from sys.members;  query to view the cluster members.      ./bin/snappy\n    connect client '127.0.0.1:1527';\n    select id, kind, netservers from sys.members;\n\n    ID                                                                                                         |KIND                  |NETSERVERS\n    -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    127.0.0.1(3688) v1 :32604                                                                                  |datastore(normal)     |localhost/127.0.0.1[1528]\n    127.0.0.1(3842) v2 :25366                                                                                  |accessor(normal)      |                         \n    127.0.0.1(3431:locator) ec v0 :41406                                                                      |locator(normal)       |localhost/127.0.0.1[1527]  In the output, the details of the cluster members are displayed. Here in the KIND column, the member corresponding to  accessor  is the lead node.  In a cluster, you can connect as a client to any member by specifying localhost with the unique port number of the member (the one specified in the NETSERVERS column corresponding to each member). However, connecting to the locator provides basic load balancing by routing the connection request to an available server member.", 
            "title": "Start SnappyData Cluster"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#check-snappydata-cluster-status", 
            "text": "You can check the status of a running cluster using the following command:  $ ./sbin/snappy-status-all.sh\nSnappyData Locator pid: 9748 status: running\nSnappyData Server pid: 9887 status: running\nSnappyData Leader pid: 10468 status: running  Connect to   Snappy SQL shell  to perform various SQL operations.  Alternatively , you can access the  SnappyData Pulse  monitoring tool by entering   http://  leadhost :5050/dashboard/  in the web browser. For example,  http://localhost:5050/dashboard/.  leadhost  is the hostname or IP of the lead node in your cluster which is provided in the conf/leads file. On the SnappyData Pulse dashboards, after starting a cluster, you can check the status of each of the cluster member.", 
            "title": "Check SnappyData Cluster Status"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#connectdisconnect-to-snappydata-shell", 
            "text": "After starting the SnappyData cluster, run these commands together to start the SnappyData shell:  ./bin/snappy\nconnect client '127.0.0.1:1527';  Type  exit;  or press  CTRL + C  to disconnect the SnappyData Shell.", 
            "title": "Connect/Disconnect to SnappyData Shell"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#create-tables", 
            "text": "Create a simple table and insert a few rows. By default, if no options are provided, row replicated table is formed. However, you can create tables using the row or column option.    # Create a table named quicktable. A replicated row table is formed without/empty options by default.\nCREATE TABLE quicktable (id int generated always as identity, item char(25));\n\n# Create column table. A column table is created which is partitioned by default.\nCREATE TABLE  quicktable_col (id int, item varchar(25)) using column options();\n\n# Create partitioned Row table using the partition by options. A partitioned row table is created with partitioning scheme on the 'id' column. \nCREATE TABLE quicktable_row (id int generated always as identity, item char(25)) using row options(partition_by 'id');\n\n# Insert one row into the table.\nINSERT into quicktable values (default, 'widget');\n\n# Insert one more row into the table.\nINSERT into quicktable values (default, 'gadget');\n\n# View the contents of the table.\nselect * from quicktable;\nID         |ITEM                     \n-------------------------------------\n2          |gadget                \n1          |widget\n\n2 rows selected\n\n# SnappyData replicates the row tables that are created by default onto the data store members. \n# You can validate this using the following query:\n\nselect tablename, datapolicy from sys.systables where tablename='QUICKTABLE';\n\n#The following output is displayed:\n\nTABLENAME                                                                                                                       |DATAPOLICY              \n---------------------------------------------------------------------------------------------------------------------------------------------------------\nQUICKTABLE                                                                                                                      |PERSISTENT_REPLICATE", 
            "title": "Create Tables"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#create-tables-and-import-data-using-quickstart-scripts", 
            "text": "SnappyData contains various quickstart scripts that can be used to run some basic functionalities.  For example, you can run the  create_and_load_column_table.sql  script. This script first drops the table if it exists and then  creates an external table named STAGING AIRLINE to load the formatted data from a parquet file. Then a column table is created with only the specified columns from this external table.  Connect to SnappyData shell before running these scripts. These script files must be run from within the  quickstart  directory if you are providing the relative path as shown:          ./bin/snappy\n        connect client '127.0.0.1:1527';\n        RUN 'quickstart/scripts/create_and_load_column_table.sql';\n\n        # Use the following command to view the details of the external table. \n        describe staging_airline;\n\n        # The following output is displayed:\n        COLUMN_NAME         |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE\n        -----------------------------------------------------------------------------------------------------------------\n        Year                |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        Month               |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DayOfMonth          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DayOfWeek           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DepTime             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CRSDepTime          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        ArrTime             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CRSArrTime          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        UniqueCarrier       |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        FlightNum           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        TailNum             |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        ActualElapsedTime   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CRSElapsedTime      |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        AirTime             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        ArrDelay            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        DepDelay            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        Origin              |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        Dest                |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        Distance            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        TaxiIn              |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        TaxiOut             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        Cancelled           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CancellationCode    |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n        Diverted            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        CarrierDelay        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        WeatherDelay        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        NASDelay            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        SecurityDelay       |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        LateAircraftDelay   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n        ArrDelaySlot        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES\n\n        # Use the following command to check the number of records in the staging_airline external table:\n\n        select count(*) from staging_airline;\n        count(1)            \n        --------------------\n        1000000\n\n        1 row selected  You can also try the following:    Create and load a row table:  RUN './quickstart/scripts/create_and_load_row_table.sql';    View the status of the system:  RUN './quickstart/scripts/status_queries.sql';", 
            "title": "Create Tables and Import Data Using quickstart Scripts"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#create-a-column-table-using-an-external-table", 
            "text": "Similarly as the quickstart scripts, you can try to create an external table named staging_airline to load the formatted data from a airlineParquetData file with inferSchema option as true. Later, you can create a column table named airline and pull data from the external table into this table. After pulling in the data, you can check the number of records in the table.      CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData', inferSchema 'true');\n\n        CREATE TABLE AIRLINE2 USING column AS (SELECT * FROM STAGING_AIRLINE);\n\n        describe airline2;\n    COLUMN_NAME         |TYPE_NAME|DECIMAL_DIGITS|NUM_PREC_RADIX|COLUMN_SIZE|COLUMN_DEF|CHAR_OCTET_LENGTH|IS_NULLABLE\n    -----------------------------------------------------------------------------------------------------------------\n    YEAR                |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    MONTH               |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DAYOFMONTH          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DAYOFWEEK           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DEPTIME             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CRSDEPTIME          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ARRTIME             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CRSARRTIME          |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    UNIQUECARRIER       |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    FLIGHTNUM           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    TAILNUM             |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    ACTUALELAPSEDTIME   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CRSELAPSEDTIME      |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    AIRTIME             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ARRDELAY            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    DEPDELAY            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ORIGIN              |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    DEST                |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    DISTANCE            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    TAXIIN              |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    TAXIOUT             |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CANCELLED           |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CANCELLATIONCODE    |VARCHAR  |NULL          |NULL          |32672      |NULL      |65344            |YES        \n    DIVERTED            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    CARRIERDELAY        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    WEATHERDELAY        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    NASDELAY            |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    SECURITYDELAY       |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    LATEAIRCRAFTDELAY   |INTEGER  |0             |10            |10         |NULL      |NULL             |YES        \n    ARRDELAYSLOT        |INTEGER  |0             |10            |10         |NULL      |NULL             |YES\n\n    30 rows selected\n\n    select count(*)\n      ;\n    count(1)            \n    --------------------\n    1\n\n    1 row selected  After running these queries, you can check the table details on the SnappyData Pulse Dashboards. The details of the newly created tables are displayed in the  Tables  section.", 
            "title": "Create a Column Table Using an External Table"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#run-queries", 
            "text": "You can try a couple of analytical queries as shown:   Query to find the average arrival delay.   select avg(arrdelay) from airline;\n\navg(ARRDELAY)         \n----------------------\n6.735443              \n\n1 row selected   Query for avg arrival delay of a specific airline.   select max(arrdelay) from airline where DEST = '';\n\nmax(ARRDELAY)\n-------------\nNULL         \n\n1 row selected", 
            "title": "Run Queries"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#add-servers-into-cluster", 
            "text": "You can add more than one server to a cluster. To add a new server, do the following:   Go to SnappyData home directory. cd  snappydata_install_dir  Create a configuration file named  servers  in the conf folder in the the SnappyData home directory. To do so, you can copy the existing template files  servers.template  and rename it to  servers  as shown: cp -f conf/servers.template conf/servers  Open this file using a vi editor and add a hostname entry of the additional server, after the entry of the primary server, and save the file.  For example, suppose there is an entry  localhost  in this file for the primary server. You can add an entry  localhost  below this entry for the additional server. The  servers  file should contain the hostnames of the nodes (one per line) where you intend to start the member.  From the SnappyData home directory, start the cluster again using the  ./sbin/snappy-start-all.sh  command. The new server gets started. Ignore the error messages of the other nodes that are already running. You can check  details of the newly added member from the SnappyData Pulse UI.", 
            "title": "Add Servers into Cluster"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#rebalancing-data-on-servers", 
            "text": "Further, you can distribute the data among the servers in the cluster. This ensures that each server carries almost equal data.\nTo balance the data equally on the servers, do the following:   Go to SnappyData home directory. cd  snappydata_install_dir  Connect to snappy shell  and obtain the jdbc client connection.  Run the rebalance command.   call sys.rebalance_all_buckets();   On SnappyData Pulse UI, check the Heap Memory Used/Total column for the servers. You will notice that before rebalancing the data, there was an unequal distribution of the memory usage and after running the rebalance command, the data is distributed equally among both the servers.  Before Rebalance   After Rebalance", 
            "title": "Rebalancing Data on Servers"
        }, 
        {
            "location": "/quickstart/snappydataquick_start/#stop-the-cluster", 
            "text": "You can stop the cluster using the  ./sbin/snappy-stop-all.sh  command:  ./sbin/snappy-stop-all.sh\nThe SnappyData Leader has stopped.\nThe SnappyData Server has stopped.\nThe SnappyData Locator has stopped.  For more details, refer to  Stopping the Cluster", 
            "title": "Stop the Cluster"
        }, 
        {
            "location": "/install/", 
            "text": "Provisioning SnappyData\n\n\nSnappyData offers two editions of the product, SnappyData Community Edition, and SnappyData Enterprise Edition.\n\n\nThe SnappyData Community Edition is Apache 2.0 licensed. It is a free, open-source version of the product that can be downloaded by anyone.\nThe Enterprise Edition includes several additional capabilities that are closed source and only available as part of a licensed subscription.\n\n\nFor more information on the capabilities of the Community and Enterprise editions see \nCommunity Edition (Open Source)/Enterprise Edition Components\n.\n\n\n \n\n\n Download SnappyData Community Edition\n\n\nDownload the SnappyData 1.0.2.1 Community Edition (Open Source)\n from the release page, which lists the latest and previous releases of SnappyData. The packages are available in compressed files (.tar format).\n\n\n\n\n\n\nSnappyData 1.0.2.1 Release download link\n\n\n\n\n\n\nSnappyData 1.0.2.1 Release (user-provided Hadoop) download link\n \n\n\n\n\n\n\n Download SnappyData Enterprise Edition\n \n\n\n\n\n\n\nGo to the \nSnappyData website\n.\n\n\n\n\n\n\nOn this page, enter your email address.\n\n\n\n\n\n\nRead the END USER LICENSE AGREEMENT and click the \nAgree to terms of service\n option to accept it.\n\n\n\n\n\n\nClick \nDownload\n to download the installer (\nsnappydata-1.0.2.1-bin.tar.gz\n).\n\n\n\n\n\n\nYou can also download the following additional files by clicking on the links:\n\n\n\n\n\n\nDEBIAN INSTALLER\n\n\n\n\n\n\nREDHAT INSTALLER\n\n\n\n\n\n\nJDBC JAR FILE\n\n\n\n\n\n\nODBC INSTALLERS\n\n\n\n\n\n\n\n\n\n\n \n\n\nSnappyData Provisioning Options\n\n\nPrerequisites\n\n\nBefore you start the installation, make sure that Java SE Development Kit 8 is installed, and the \nJAVA_HOME\n environment variable is set on each computer.\n\n\nThe following options are available for provisioning SnappyData:\n\n\n\n\n\n\nOn-Premise\n \n\n\n\n\n\n\nAmazon Web Services (AWS)\n \n\n\n\n\n\n\nKubernetes\n\n\n\n\n\n\nDocker\n\n\n\n\n\n\nBuilding from Source\n\n\n\n\n\n\nConfiguring the Limit for Open Files and Threads/Processes\n\n\nOn a Linux system, you can set the limit of open files and thread processes in the \n/etc/security/limits.conf\n file. \n\nA minimum of \n8192\n is recommended for open file descriptors limit and \n128K\n is recommended for the number of active threads. \n\nA typical configuration used for SnappyData servers and leads can appear as follows:\n\n\nsnappydata          hard    nofile      32768\nsnappydata          soft    nofile      32768\nsnappydata          hard    nproc       unlimited\nsnappydata          soft    nproc       524288\nsnappydata          hard    sigpending  unlimited\nsnappydata          soft    sigpending  524288\n\n\n\n\n\n\nsnappydata\n is the user running SnappyData.\n\n\n\n\nRecent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) require the \nNOFILE\n limit to be increased in systemd configuration too. Edit \n/etc/systemd/system.conf \n as root, search for \n#DefaultLimitNOFILE\n under the \n[Manager] \nsection. Uncomment and change it to \nDefaultLimitNOFILE=32768\n. \nReboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with \n\"ulimit -a -S\"\n (soft limits) and \n\"ulimit -a -H\"\n (hard limits).", 
            "title": "Provisioning SnappyData"
        }, 
        {
            "location": "/install/#provisioning-snappydata", 
            "text": "SnappyData offers two editions of the product, SnappyData Community Edition, and SnappyData Enterprise Edition.  The SnappyData Community Edition is Apache 2.0 licensed. It is a free, open-source version of the product that can be downloaded by anyone.\nThe Enterprise Edition includes several additional capabilities that are closed source and only available as part of a licensed subscription.  For more information on the capabilities of the Community and Enterprise editions see  Community Edition (Open Source)/Enterprise Edition Components .      Download SnappyData Community Edition  Download the SnappyData 1.0.2.1 Community Edition (Open Source)  from the release page, which lists the latest and previous releases of SnappyData. The packages are available in compressed files (.tar format).    SnappyData 1.0.2.1 Release download link    SnappyData 1.0.2.1 Release (user-provided Hadoop) download link       Download SnappyData Enterprise Edition      Go to the  SnappyData website .    On this page, enter your email address.    Read the END USER LICENSE AGREEMENT and click the  Agree to terms of service  option to accept it.    Click  Download  to download the installer ( snappydata-1.0.2.1-bin.tar.gz ).    You can also download the following additional files by clicking on the links:    DEBIAN INSTALLER    REDHAT INSTALLER    JDBC JAR FILE    ODBC INSTALLERS         SnappyData Provisioning Options  Prerequisites  Before you start the installation, make sure that Java SE Development Kit 8 is installed, and the  JAVA_HOME  environment variable is set on each computer.  The following options are available for provisioning SnappyData:    On-Premise      Amazon Web Services (AWS)      Kubernetes    Docker    Building from Source    Configuring the Limit for Open Files and Threads/Processes  On a Linux system, you can set the limit of open files and thread processes in the  /etc/security/limits.conf  file.  A minimum of  8192  is recommended for open file descriptors limit and  128K  is recommended for the number of active threads.  A typical configuration used for SnappyData servers and leads can appear as follows:  snappydata          hard    nofile      32768\nsnappydata          soft    nofile      32768\nsnappydata          hard    nproc       unlimited\nsnappydata          soft    nproc       524288\nsnappydata          hard    sigpending  unlimited\nsnappydata          soft    sigpending  524288   snappydata  is the user running SnappyData.   Recent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) require the  NOFILE  limit to be increased in systemd configuration too. Edit  /etc/systemd/system.conf   as root, search for  #DefaultLimitNOFILE  under the  [Manager]  section. Uncomment and change it to  DefaultLimitNOFILE=32768 . \nReboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with  \"ulimit -a -S\"  (soft limits) and  \"ulimit -a -H\"  (hard limits).", 
            "title": "Provisioning SnappyData"
        }, 
        {
            "location": "/install/system_requirements/", 
            "text": "System Requirements\n\n\nIn this section, we discuss the hardware, software, and network requirements for SnappyData.\n\n\nHardware  Requirements\n\n\nSnappyData turns Apache Spark into a mission-critical, elastic scalable in-memory data store. This allows users to run Spark workloads and classic database workloads on SnappyData.\n\n\nMemory\n: SnappyData works well with anywhere from 8GB of memory to hundreds of GB of memory. While exact memory requirements depend on the end user application, we recommend allocating no more than 75% of the memory to SnappyData. We recommend using a machine with at least 8GB of RAM when working with SnappyData.\n\n\n\n\nNote\n\n\nIt is recommended to have a minimum of 8GB memory for server-grade machines.\n\n\n\n\nCPU Cores\n: SnappyData is a highly multi-threaded system and can take advantage of CPU cores to deliver higher throughput. It has been tested with multi-core multi-CPU machines. We recommend using machines with at least 16 cores when working with SnappyData. The degree of parallelism you can achieve with SnappyData directly depends on the number of cores, as higher core machines perform better than lower core machines.\n\n\nNetwork\n: SnappyData is a clustered scale-out in-memory data store and both jobs and queries use the network extensively to complete their job. Since data is mostly available in-memory, queries and jobs typically get CPU and/or network bound. We recommend running SnappyData on at least a 1GB network for testing and use a 10GB network for production scenarios.\n\n\nDisk\n: SnappyData overflows data to local disk files and tables can be configured for persistence. We recommend using flash storage for optimal performance for SnappyData shared nothing persistence. Data can be saved out to stores like HDFS and S3 using SnappyData DataFrame APIs.\n\n\nOperating Systems Supported\n\n\n\n\n\n\n\n\nOperating System\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nRed Hat Enterprise Linux\n\n\n- RHEL 6.0 \n - RHEL 7.0 (Mininum recommended kernel version: 3.10.0-693.2.2.el7.x86_64)\n\n\n\n\n\n\nUbuntu\n\n\nUbuntu Server 14.04 and later\n\n\n\n\n\n\nCentOS\n\n\nCentOS 6, 7 (Minimum recommended kernel version: 3.10.0-693.2.2.el7.x86_64)\n\n\n\n\n\n\n\n\nHost Machine Requirements\n\n\nRequirements for each host:\n\n\n\n\n\n\nA supported \nOracle Java SE 8\n installation. We recommend minimum version: 1.8.0_144 (see \nSNAP-2017\n, \nSNAP-1999\n, \nSNAP-1911\n, \nSNAP-1375\n for crashes reported with earlier versions).\n\n\n\n\n\n\nThe latest version of Bash shell.\n\n\n\n\n\n\nA file system that supports long file names.\n\n\n\n\n\n\nTCP/IP.\n\n\n\n\n\n\nSystem clock set to the correct time.\n\n\n\n\n\n\nFor each Linux host, the hostname and host files must be properly configured. See the system manual pages for hostname and host settings.\n\n\n\n\n\n\nFor each Linux host, configure the swap to be in the range of 32-64GB to allow for swapping out of unused pages.\n\n\n\n\n\n\nTime synchronization service such as Network Time Protocol (NTP).\n\n\n\n\n\n\ncURL must be installed on lead nodes for snappy scripts to work. On Red Hat based systems it can be installed using \nsudo yum install curl\n while on Debian/Ubuntu based systems, you can install using \nsudo apt-get install curl\ncommand.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nFor troubleshooting, you must run a time synchronization service on all hosts. Synchronized time stamps allow you to merge log messages from different hosts, for an accurate chronological history of a distributed run.\n\n\n\n\n\n\nIf you deploy SnappyData on a virtualized host, consult the documentation provided with the platform, for system requirements and recommended best practices, for running Java and latency-sensitive workloads.\n\n\n\n\n\n\n\n\nVSD Requirements\n\n\n\n\n\n\nInstall 32-bit libraries on 64-bit Linux:\n\n    \nyum install glibc.i686 libX11.i686\n on RHEL/CentOS\n\n    \napt-get install libc6:i386 libx11-6:i386\n on Ubuntu/Debian like systems\n\n\n\n\n\n\nLocally running X server. For example, an X server implementation like, XQuartz for Mac OS, Xming for Windows OS, and Xorg which is installed by default for Linux systems.\n\n\n\n\n\n\nPython Integration using pyspark\n\n\n\n\n\n\nThe Python pyspark module has the same requirements as in Apache Spark. The numpy package is required by many modules of pyspark including the examples shipped with SnappyData. On recent Red Hat based systems, it can be installed using \nsudo yum install numpy\n or \nsudo yum install python2-numpy\n commands. Whereas, on Debian/Ubuntu based systems, you can install using the \nsudo apt-get install python-numpy\n command.\n\n\n\n\n\n\nSome of the python APIs can use SciPy to optimize some algorithms (in linalg package), and some others need Pandas. On recent Red Hat based systems SciPy can be installed using \nsudo yum install scipy\n command. Whereas,  on Debian/Ubuntu based systems you can install using the \nsudo apt-get install python-scipy\n command. Likewise, Pandas on recent Red Hat based systems can be installed using \nsudo yum installed python-pandas\n command, while on Debian/Ubuntu based systems it can be installed using the \nsudo apt-get install python-pandas\n command.\n\n\n\n\n\n\nOn Red Hat based systems, some of the above Python packages may be available only after enabling the \nEPEL\n repository. If these are not available in the repositories for your OS version or if using \nEPEL\n is not an option, then you can use \npip\n. Refer to the respective project documentation for details and alternative options such as Anaconda.\n\n\n\n\n\n\nFilesystem Type for Linux Platforms\n\n\nFor optimum disk-store performance, we recommend the use of local filesystem for disk data storage and not over NFS.", 
            "title": "System Requirements"
        }, 
        {
            "location": "/install/system_requirements/#system-requirements", 
            "text": "In this section, we discuss the hardware, software, and network requirements for SnappyData.", 
            "title": "System Requirements"
        }, 
        {
            "location": "/install/system_requirements/#hardware-requirements", 
            "text": "SnappyData turns Apache Spark into a mission-critical, elastic scalable in-memory data store. This allows users to run Spark workloads and classic database workloads on SnappyData.  Memory : SnappyData works well with anywhere from 8GB of memory to hundreds of GB of memory. While exact memory requirements depend on the end user application, we recommend allocating no more than 75% of the memory to SnappyData. We recommend using a machine with at least 8GB of RAM when working with SnappyData.   Note  It is recommended to have a minimum of 8GB memory for server-grade machines.   CPU Cores : SnappyData is a highly multi-threaded system and can take advantage of CPU cores to deliver higher throughput. It has been tested with multi-core multi-CPU machines. We recommend using machines with at least 16 cores when working with SnappyData. The degree of parallelism you can achieve with SnappyData directly depends on the number of cores, as higher core machines perform better than lower core machines.  Network : SnappyData is a clustered scale-out in-memory data store and both jobs and queries use the network extensively to complete their job. Since data is mostly available in-memory, queries and jobs typically get CPU and/or network bound. We recommend running SnappyData on at least a 1GB network for testing and use a 10GB network for production scenarios.  Disk : SnappyData overflows data to local disk files and tables can be configured for persistence. We recommend using flash storage for optimal performance for SnappyData shared nothing persistence. Data can be saved out to stores like HDFS and S3 using SnappyData DataFrame APIs.", 
            "title": "Hardware  Requirements"
        }, 
        {
            "location": "/install/system_requirements/#operating-systems-supported", 
            "text": "Operating System  Version      Red Hat Enterprise Linux  - RHEL 6.0   - RHEL 7.0 (Mininum recommended kernel version: 3.10.0-693.2.2.el7.x86_64)    Ubuntu  Ubuntu Server 14.04 and later    CentOS  CentOS 6, 7 (Minimum recommended kernel version: 3.10.0-693.2.2.el7.x86_64)", 
            "title": "Operating Systems Supported"
        }, 
        {
            "location": "/install/system_requirements/#host-machine-requirements", 
            "text": "Requirements for each host:    A supported  Oracle Java SE 8  installation. We recommend minimum version: 1.8.0_144 (see  SNAP-2017 ,  SNAP-1999 ,  SNAP-1911 ,  SNAP-1375  for crashes reported with earlier versions).    The latest version of Bash shell.    A file system that supports long file names.    TCP/IP.    System clock set to the correct time.    For each Linux host, the hostname and host files must be properly configured. See the system manual pages for hostname and host settings.    For each Linux host, configure the swap to be in the range of 32-64GB to allow for swapping out of unused pages.    Time synchronization service such as Network Time Protocol (NTP).    cURL must be installed on lead nodes for snappy scripts to work. On Red Hat based systems it can be installed using  sudo yum install curl  while on Debian/Ubuntu based systems, you can install using  sudo apt-get install curl command.     Note    For troubleshooting, you must run a time synchronization service on all hosts. Synchronized time stamps allow you to merge log messages from different hosts, for an accurate chronological history of a distributed run.    If you deploy SnappyData on a virtualized host, consult the documentation provided with the platform, for system requirements and recommended best practices, for running Java and latency-sensitive workloads.", 
            "title": "Host Machine Requirements"
        }, 
        {
            "location": "/install/system_requirements/#vsd-requirements", 
            "text": "Install 32-bit libraries on 64-bit Linux: \n     yum install glibc.i686 libX11.i686  on RHEL/CentOS \n     apt-get install libc6:i386 libx11-6:i386  on Ubuntu/Debian like systems    Locally running X server. For example, an X server implementation like, XQuartz for Mac OS, Xming for Windows OS, and Xorg which is installed by default for Linux systems.", 
            "title": "VSD Requirements"
        }, 
        {
            "location": "/install/system_requirements/#python-integration-using-pyspark", 
            "text": "The Python pyspark module has the same requirements as in Apache Spark. The numpy package is required by many modules of pyspark including the examples shipped with SnappyData. On recent Red Hat based systems, it can be installed using  sudo yum install numpy  or  sudo yum install python2-numpy  commands. Whereas, on Debian/Ubuntu based systems, you can install using the  sudo apt-get install python-numpy  command.    Some of the python APIs can use SciPy to optimize some algorithms (in linalg package), and some others need Pandas. On recent Red Hat based systems SciPy can be installed using  sudo yum install scipy  command. Whereas,  on Debian/Ubuntu based systems you can install using the  sudo apt-get install python-scipy  command. Likewise, Pandas on recent Red Hat based systems can be installed using  sudo yum installed python-pandas  command, while on Debian/Ubuntu based systems it can be installed using the  sudo apt-get install python-pandas  command.    On Red Hat based systems, some of the above Python packages may be available only after enabling the  EPEL  repository. If these are not available in the repositories for your OS version or if using  EPEL  is not an option, then you can use  pip . Refer to the respective project documentation for details and alternative options such as Anaconda.", 
            "title": "Python Integration using pyspark"
        }, 
        {
            "location": "/install/system_requirements/#filesystem-type-for-linux-platforms", 
            "text": "For optimum disk-store performance, we recommend the use of local filesystem for disk data storage and not over NFS.", 
            "title": "Filesystem Type for Linux Platforms"
        }, 
        {
            "location": "/install/install_on_premise/", 
            "text": "Install On-Premise\n\n\nSnappyData runs on UNIX-like systems (for example, Linux, Mac OS). With on-premises installation, SnappyData is installed and operated from your in-house computing infrastructure.\n\n\n\n\nSingle-Host Installation\n\n\nThis is the simplest form of deployment and can be used for testing and POCs.\n\n\nOpen the command prompt, go the location of the downloaded SnappyData file, and run the following command to extract the archive file.\n\n\n$ tar -xzf snappydata-\nversion-number\nbin.tar.gz\n$ cd snappydata-\nversion-number\n-bin/\n\n\n\n\nStart a basic cluster with one data node, one lead, and one locator:\n\n\n./sbin/snappy-start-all.sh\n\n\n\n\nFor custom configuration and to start more nodes, refer \nconfiguring the SnappyData cluster\n.\n\n\n\n\nMulti-Host Installation\n\n\nFor real-life use cases, you require multiple machines on which SnappyData must be deployed. You can start one or more SnappyData node on a single machine based on your machine size.\n\n\nWhere there are multiple machines involved, you can deploy SnappyData on:\n\n\n\n\n\n\nMachines With Shared Path\n\n\n\n\n\n\nMachines Without a Shared Path\n\n\n\n\n\n\nMachines Without Passwordless SSH\n\n\n\n\n\n\n\n\nMachines With a Shared Path\n\n\nIf all the machines in your cluster can share a path over an NFS or similar protocol, then use the following instructions:\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that the \n/etc/hosts\n correctly configures the host and IP address of each SnappyData member machine.\n\n\n\n\n\n\nEnsure that SSH is supported and you have configured all the machines to be accessed by \npasswordless SSH\n. If SSH is not supported then follow the instructions in the \nMachines Without Passwordless SSH\n section.\n\n\n\n\n\n\nTo set up the cluster for machines with a shared path:\n\n\n\n\n\n\nCopy the downloaded binaries to the shared folder.\n\n\n\n\n\n\nExtract the downloaded archive file and go to SnappyData home directory.\n\n\n$ tar -xzf snappydata-\nversion-number\n-bin.tar.gz\n$ cd snappydata-\nversion-number\n.-bin/\n\n\n\n\n\n\n\nConfigure the cluster as described in \nConfiguring the Cluster\n.\n\n\n\n\n\n\nAfter configuring each of the members in the cluster, run the \nsnappy-start-all.sh\n script:\n\n\n./sbin/snappy-start-all.sh\n\n\n\nThis creates a default folder named \nwork\n and stores all SnappyData member's artifacts separately. The folder is identified by the name of the node.\n\n\n\n\n\n\n\n\nTip\n\n\nFor optimum performance, configure the \n-dir\n to a local directory and not to a network directory. When \n-dir\n property is configured for each member in the cluster, the artifacts of the respective members get created in the  \n-dir\n folder.\n\n\n\n\n\n\nMachines Without a Shared Path\n\n\nIn case all the machines in your cluster do not share a path over an NFS or similar protocol, then use the following instructions:\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that \n/etc/hosts\n correctly configures the host and IP Address of each SnappyData member machine.\n\n\n\n\n\n\nEnsure that SSH is supported and you have configured all the machines to be accessed by \npasswordless SSH\n. If SSH is not supported then follow the instructions in the \nMachines without passwordless SSH\n section.\n\n\n\n\n\n\nTo set up the cluster for machines without a shared path:\n\n\n\n\n\n\nCopy and extract the downloaded binaries into each machine. Ensure to maintain the same directory structure on all the machines. For example, if you copy the binaries in \n/opt/snappydata/\n on the first machine, then you must ensure to copy the binaries to \n/opt/snappydata/\n on rest of the machines.\n\n\n\n\n\n\nConfigure the cluster as described in \nConfiguring the Cluster\n. Maintain one node as the controller node, where you can configure your cluster. Usually this is done in the lead node. On that machine, you can edit files such as servers, locators, and leads which are in the \n$SNAPPY_HOME/conf/ directory\n.\n\n\n\n\n\n\nCreate a working directory on every machine, for each of the SnappyData member that you want to run. \n The member's working directory provides a default location for the logs, persistence, and status files of that member. \nFor example, if you want to run both a locator and server member on the local machine, create separate directories for each member.\n\n\n\n\n\n\nRun the \nsnappy-start-all.sh\n script:\n\n\n./sbin/snappy-start-all.sh\n\n\n\n\n\n\n\n\n\nMachines Without Passwordless SSH\n\n\nIn case the machines in your cluster do not share a common path as well as cannot be accessed by \npasswordless SSH\n, then you can use the following instructions to deploy SnappyData:\n\n\nTo set up the cluster for machines without passwordless SSH:\n\n\n\n\n\n\nCopy and extract the downloaded binaries into each machine. The binaries can be placed in different directory structures. \n\n\n\n\n\n\nConfigure each member separately.\n\n\n\n\nNote\nThe scripts used for starting individual members in the cluster do not read from the \nconf\n file of each member, hence there is no need to edit the \nconf\n files for starting the members. These scripts will start the member with the default configuration properties. To override the default configuration, you can pass the properties as arguments to the above scripts.\n\n\n\n\n\n\n\n\n\n\nStart the members in the cluster one at a time. Start the locator first, then the servers, and finally the leads. Use the following scripts to start the members:\n\n\n\n\n\n\n$SNAPPY_HOME/sbin/snappy-locator.sh\n\n\n\n\n\n\n$SNAPPY_HOME/sbin/snappy-server.sh\n\n\n\n\n\n\n$SNAPPY_HOME/sbin/snappy-lead.sh", 
            "title": "On-Premise"
        }, 
        {
            "location": "/install/install_on_premise/#install-on-premise", 
            "text": "SnappyData runs on UNIX-like systems (for example, Linux, Mac OS). With on-premises installation, SnappyData is installed and operated from your in-house computing infrastructure.", 
            "title": "Install On-Premise"
        }, 
        {
            "location": "/install/install_on_premise/#single-host-installation", 
            "text": "This is the simplest form of deployment and can be used for testing and POCs.  Open the command prompt, go the location of the downloaded SnappyData file, and run the following command to extract the archive file.  $ tar -xzf snappydata- version-number bin.tar.gz\n$ cd snappydata- version-number -bin/  Start a basic cluster with one data node, one lead, and one locator:  ./sbin/snappy-start-all.sh  For custom configuration and to start more nodes, refer  configuring the SnappyData cluster .", 
            "title": "Single-Host Installation"
        }, 
        {
            "location": "/install/install_on_premise/#multi-host-installation", 
            "text": "For real-life use cases, you require multiple machines on which SnappyData must be deployed. You can start one or more SnappyData node on a single machine based on your machine size.  Where there are multiple machines involved, you can deploy SnappyData on:    Machines With Shared Path    Machines Without a Shared Path    Machines Without Passwordless SSH", 
            "title": "Multi-Host Installation"
        }, 
        {
            "location": "/install/install_on_premise/#machines-with-a-shared-path", 
            "text": "If all the machines in your cluster can share a path over an NFS or similar protocol, then use the following instructions:  Prerequisites    Ensure that the  /etc/hosts  correctly configures the host and IP address of each SnappyData member machine.    Ensure that SSH is supported and you have configured all the machines to be accessed by  passwordless SSH . If SSH is not supported then follow the instructions in the  Machines Without Passwordless SSH  section.    To set up the cluster for machines with a shared path:    Copy the downloaded binaries to the shared folder.    Extract the downloaded archive file and go to SnappyData home directory.  $ tar -xzf snappydata- version-number -bin.tar.gz\n$ cd snappydata- version-number .-bin/    Configure the cluster as described in  Configuring the Cluster .    After configuring each of the members in the cluster, run the  snappy-start-all.sh  script:  ./sbin/snappy-start-all.sh  This creates a default folder named  work  and stores all SnappyData member's artifacts separately. The folder is identified by the name of the node.     Tip  For optimum performance, configure the  -dir  to a local directory and not to a network directory. When  -dir  property is configured for each member in the cluster, the artifacts of the respective members get created in the   -dir  folder.", 
            "title": "Machines With a Shared Path"
        }, 
        {
            "location": "/install/install_on_premise/#machines-without-a-shared-path", 
            "text": "In case all the machines in your cluster do not share a path over an NFS or similar protocol, then use the following instructions:  Prerequisites    Ensure that  /etc/hosts  correctly configures the host and IP Address of each SnappyData member machine.    Ensure that SSH is supported and you have configured all the machines to be accessed by  passwordless SSH . If SSH is not supported then follow the instructions in the  Machines without passwordless SSH  section.    To set up the cluster for machines without a shared path:    Copy and extract the downloaded binaries into each machine. Ensure to maintain the same directory structure on all the machines. For example, if you copy the binaries in  /opt/snappydata/  on the first machine, then you must ensure to copy the binaries to  /opt/snappydata/  on rest of the machines.    Configure the cluster as described in  Configuring the Cluster . Maintain one node as the controller node, where you can configure your cluster. Usually this is done in the lead node. On that machine, you can edit files such as servers, locators, and leads which are in the  $SNAPPY_HOME/conf/ directory .    Create a working directory on every machine, for each of the SnappyData member that you want to run.   The member's working directory provides a default location for the logs, persistence, and status files of that member.  For example, if you want to run both a locator and server member on the local machine, create separate directories for each member.    Run the  snappy-start-all.sh  script:  ./sbin/snappy-start-all.sh", 
            "title": "Machines Without a Shared Path"
        }, 
        {
            "location": "/install/install_on_premise/#machines-without-passwordless-ssh", 
            "text": "In case the machines in your cluster do not share a common path as well as cannot be accessed by  passwordless SSH , then you can use the following instructions to deploy SnappyData:  To set up the cluster for machines without passwordless SSH:    Copy and extract the downloaded binaries into each machine. The binaries can be placed in different directory structures.     Configure each member separately.   Note The scripts used for starting individual members in the cluster do not read from the  conf  file of each member, hence there is no need to edit the  conf  files for starting the members. These scripts will start the member with the default configuration properties. To override the default configuration, you can pass the properties as arguments to the above scripts.      Start the members in the cluster one at a time. Start the locator first, then the servers, and finally the leads. Use the following scripts to start the members:    $SNAPPY_HOME/sbin/snappy-locator.sh    $SNAPPY_HOME/sbin/snappy-server.sh    $SNAPPY_HOME/sbin/snappy-lead.sh", 
            "title": "Machines Without Passwordless SSH"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/", 
            "text": "Setting up Cluster on Amazon Web Services (AWS)\n\n\nAmazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3).\nYou can set up SnappyData cluster on Amazon Web Services using one of the following options:\n\n\n\n\nSnappyData CloudBuilder\n\n\nEC2 Scripts\n \n\n\nAWS Management Console\n\n\n\n\n\n\nSnappyData CloudBuilder\n\n\nSnappyData CloudBuilder\n is a web based utility that allows you to quickly launch SnappyData cluster on AWS instances. It also launches Apache Zeppelin server that allows you to build notebooks that visually represent key elements of your business.\n\n\nThis method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started.\n\n\n\n\nPrerequisites\n\n\nDeploying SnappyData Cloud Cluster Using SnappyData CloudBuilder\n\n\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources with AWS CloudFormation.\n\n\n\n\n\n\nSign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.\n\n\n\n\n\n\nCreate an EC2 Key Pair in the region where you want to launch the SnappyData cluster.\n\n\n\n\n\n\n\n\nDeploying SnappyData Cluster with SnappyData CloudBuilder\n\n\nSnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData cluster.\n\n\nIt is recommended that you select an instance type with higher processing power and more memory for the leads and servers  of the cluster.\n\n\nStep 1: Go to \nSnappyData CloudBuilder\n page\n\n\nStep 2: Define your Cluster\n\n\nFor Community Edition\n\n\n\n\n\n\nPick your version\n Select the \nCommunity\n option. \n\n    \n\n\n\n\n\n\nPick your Instance\nSelect an instance based on the capacity that you require.\n\n    \n\n\n\n\n\n\nEnter your email\n: Provide your email address. \n\n    \n\n\n\n\n\n\nClick \nGenerate\n.\n\n\n\n\n\n\nThe next page is displayed where you can \nSelect the Region and Launch your Cluster\n.\n\n\n\n\n\n\nFor Enterprise Edition\n\n\n\n\n\n\nPick your version\n Select the \nEnterprise\n option.\n\n\n\n\n\n\n\n\nMake Locators \n Leads Highly Available?\n\n\n\n\n\n\nSelect HA/Non-HA for the \nLocators\n\n\n\n\n\n\nSelect HA/Non-HA for the \nLeads\n \n\n\n\n\n\n\n\n\n\n\n\n\nPick total Memory \n Disk\n (GB): \n\n\n\n\n\n\nMemory\n: Click and drag the bar to select the required memory.\n\n\n\n\n\n\nDisk (3x Memory Recommended)\n: Click and drag the bar to select the required disk size. Currently, Amazon Elastic Block Storage (EBS) is provided.\n\n\n\n\n\n\n\n\n\n\n\n\nRecommended Cluster\n: Select an instance based on the required capacity.\n\n\n\n\n\n\nAdd servers to support high availability?\n: Select this option to add servers to support high availability.\n\n\n\n\n\n\nDo your workloads have high query volumes?\n: Select this option if your workloads have high query volumes.\n\n\n\n\n\n\nClick the \nEdit Nodes\n option to modify the number of nodes.\n\n\n\n\n\n\n\n\nEnter your email address and select the \nAgree to terms of service\n check-box. \n\n\n\n\n\n\n\n\nClick \nGenerate\n. The next page is displayed where you can \nSelect the Region and Launch your Cluster\n.\n\n\n\n\n\n\n\n\nStep 3: Select the Region and Launch your Cluster\n\n\n\n\n\n\nOn this page, select the AWS region, and then click \nLaunch Cluster\n to launch your single-node cluster.\n\n\n\n\nNote\n\n\n\n\n\n\nThe region you select must match the EC2 Key Pair you created.\n\n\n\n\n\n\nIf you are not already logged into AWS, you are redirected to the AWS sign-in page.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSelect Template\n page, the URL for the Amazon S3 template is pre-populated. Click \nNext\n to continue.   \n\n\n\n\n\n\n\n\nOn the \nSpecify Details\n page:\n\n\n\n\nStack name\n: You can change the stack name.\n\n\n\n\n\n\nNote\n\n\nThe stack name must contain only letters, numbers, dashes, and should start with an alpha character.\n\n\n\n\n\n\n\n\nKeyPairName\n: Enter a name of an existing EC2 KeyPair. This enables SSH access to the cluster. Refer to the Amazon documentation for more information on \ngenerating your own EC2 Key Pair\n.\n\n\n\n\n\n\nVPCID\n: From the drop-down list, select \ndefault\n Virtual Private Cloud (VPC) ID of your selected region. Your instances are launched within this VPC.\n Click \nNext\n to continue.\n\n\n\n\n\n\n\n\n\n\n\n\nSpecify the tags (key-value pairs) for resources in your stack or leave the field empty and click \nNext\n.\n\n\n\n\n\n\nOn the \nReview\n page, verify the details and click \nCreate\n to create a stack.\n\n\n\n\nNote\n\n\nThis operation may take a few minutes to complete.\n\n\n\n\n\n\n\n\nThe next page lists the existing stacks. Click \nRefresh\n to view the updated list and the status of the stack creation.\nWhen the cluster has started, the status of the stack changes to \nCREATE_COMPLETE\n. \n\n\n\n\n\n\n\n\n\n\nClick on the \nOutputs\n tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration. \n\n    \n\n\n\n\nNote\n\n\nIf the status of the stack displays \nROLLBACK_IN_PROGRESS\n or \nDELETE_COMPLETE\n, the stack creation may have failed. Some common problems that might have caused the failure are:\n\n\n\n\n\n\nInsufficient Permissions\n: Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.\n\n\n\n\n\n\nInvalid Keypair\n: Verify that the EC2 key pair exists in the region you selected in the SnappyData CloudBuilder creation steps.\n\n\n\n\n\n\nLimit Exceeded\n: Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nTo stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it.\n\n\n\n\nFor more information, refer to the \nApache Zeppelin\n section or refer to the \nApache Zeppelin documentation\n.\n\n\n\n\nSnappyData EC2 Scripts\n\n\nThe SnappyData EC2 scripts enable you to quickly launch and manage SnappyData clusters on Amazon EC2 instances. They also allow you to provide custom configuration for the cluster via SnappyData configuration files, before launching the cluster.\n\n\nThe \nsnappy-ec2\n script is the entry point for these EC2 scripts and is derived from the \nspark-ec2\n script available in \nApache Spark 1.6\n.\n\n\nThe scripts are available on GitHub in the \nsnappy-cloud-tools repository\n and also as a \n.tar.gz\n file on \nthe release page\n file.\n\n\n\n\nNote\n\n\nThe EC2 scripts are provided on an experimental basis. Feel free to try it out and provide your feedback as via GitHub issues.\n\n\n\n\nThis section covers the following:\n\n   \nPrerequisites\n\n\n   \nDeploying SnappyData Cluster with EC2 Scripts\n\n\n   \nCluster Management\n\n\n   \nKnown Limitations\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources\n\n\n\n\n\n\nCreate an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster\n\nRefer to the Amazon Web Services EC2 documentation for more information on \ngenerating your own EC2 Key Pair\n.\n\n\n\n\n\n\nUsing the AWS Secret Access Key and the Access Key ID, set the two environment variables, \nAWS_SECRET_ACCESS_KEY\n and \nAWS_ACCESS_KEY_ID\n. You can find information about generating these keys in the AWS IAM console page.\n\nIf you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file.\n\n\n\n\n\n\nFor example:\n\n\nexport AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112\nexport AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10\n\n\n\n\n\n\nEnsure Python v 2.7 or later is installed on your local computer.\n\n\n\n\n\n\nDeploying SnappyData Cluster with EC2 Scripts\n\n\nIn the command prompt, go to the directory where the \nsnappydata-ec2-\nversion\n.tar.gz\n is extracted or to the\naws/ec2 directory where the \nSnappyData cloud tools repository\n is cloned locally.\n\n\nSyntax\n\n\n./snappy-ec2 -k \nyour-key-name\n -i \nyour-keyfile-path\n \naction\n \nyour-cluster-name\n [options]\n\n\n\n\nHere:\n\n\n\n\n\n\nyour-key-name\n refers to the EC2 key pair.\n\n\n\n\n\n\nyour-keyfile-path\n refers to the path to the key file.\n\n\n\n\n\n\naction\n refers to the action to be performed. Some of the available actions are \nlaunch\n, \ndestroy\n, \nstop\n, \nstart\n and \nreboot-cluster\n.\nUse \nlaunch\n action to create a new cluster while \nstop\n and \nstart\n actions work on existing clusters.\n\n\n\n\n\n\nBy default, the script starts one instance of a locator, lead, and server each.\nThe script identifies each cluster by its unique cluster name that you provide and internally ties the members\n(locators, leads, and stores/servers) of the cluster with EC2 security groups, whose names are derived from the cluster name.\n\n\nWhen running the script, you can also specify options to configure the cluster such as the number of stores in the\ncluster and the region where the EC2 instances should be launched.\n\n\nExample\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin --region=us-west-1 launch my-cluster\n\n\n\n\nThe above example launches a SnappyData cluster named \nmy-cluster\n with 2 stores or servers.\nThe locator is associated with security group named \nmy-cluster-locator\n and the servers are associated with \nmy-cluster-store\n security group.\n\n\nThe cluster is launched in the \nN. California (us-west-1)\n region on AWS and has an Apache Zeppelin server running on the instance where the lead is running.\n\n\nThe example assumes that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'.\n\n\n\n\nNote\n\n\nBy default, the cluster is launched in the \nN. Virginia (us-east-1)\n region on AWS. To launch the cluster in a specific region use option \n--region\n.\n\n\n\n\n\n\nCluster Management\n\n\nThis section covers the following:\n\n\n\n\nUsing custom build\n\n\nSpecifying Properties\n\n\nStopping the Cluster\n\n\nResuming the Cluster\n\n\nAdding Servers to the Cluster\n\n\nListing Members of the Cluster\n\n\nConnecting to the Cluster\n\n\nDestroying the Cluster\n\n\nStarting Cluster with Apache Zeppelin\n\n\nMore Options\n\n\n\n\n\n\nUsing custom build\n\n\nThis script by default uses the SnappyData OSS build available on the GitHub releases page to launch the cluster.\nTo select a version of the OSS build available on GitHub, use option \n--snappydata-version\n.\n\n\nYou can also provide your own SnappyData build to the script to launch the cluster, by using\noption \n--snappydata-tarball\n to the \nlaunch\n command.\nThe build can be present either on a local filesystem or as a resource on the web.\n\n\nFor example, to use \nSnappyData Enterprise\n build to launch the cluster, download the build tarball from\nwww.snappydata.io/download on your local machine and give its path as value to above option.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball=\n/home/ec2-user/snappydata/distributions/snappydata-1.0.2.1-bin.tar.gz\n \n\n\n\n\nAlternatively, you can also put your build file on a public web server and provide its URL to this option.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball=\nhttps://s3-us-east-2.amazonaws.com/mybucket/distributions/snappydata-1.0.2.1-bin.tar.gz\n \n\n\n\n\nThe build file should be in \n.tar.gz\n format.\n\n\n\n\nSpecifying Properties\n\n\nYou can specify the configuration for the cluster via command line options. Use \n--locator-conf\n to specify the\nconfiguration properties for all the locators in the cluster. Similarly, \n--server-conf\n and \n--lead-conf\n allow you\nto specify the configuration properties for servers and leads in the cluster, respectively.\n\n\nFollowing is a sample configuration for all the three processes in a SnappyData cluster:\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 launch my-cluster \\\n  --locator-conf=\n-peer-discovery-port=9999 -heap-size=1024m\n \\\n  --lead-conf=\n-spark.executor.cores=10 -heap-size=4096m -spark.ui.port=3333\n \\\n  --server-conf=\n-client-port=1530\n\n\n\n\n\nThe utility also reads \nsnappy-env.sh\n, if present in the directory where helper scripts are present.\n\n\n\n\nNote\n\n\n\n\n\n\nThe earlier method of specifying the configuration properties by placing the actual\n  configuration files in the directory, where helper scripts are available, is discontinued.\n\n\nEnsure that the configuration properties specified are correct. Otherwise, launching the\n  SnappyData cluster may fail but the EC2 instances would still be running.\n\n\n\n\n\n\nStopping the Cluster\n\n\nWhen you stop a cluster, it shuts down the EC2 instances and any data saved on the local instance stores is lost.\nHowever, the data saved on EBS volumes is retained, unless the spot-instances are used.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem stop cluster-name\n\n\n\n\n\n\nResuming the Cluster\n\n\nWhen you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem start cluster-name\n\n\n\n\n\n\nNote\n\n\nThe \nstart\n command, or \nlaunch\n command with \n--resume\n option, ignores the \n--locators\n, \n--leads\n, or \n--stores\n options and launches the SnappyData cluster on existing instances.\nHowever, if the configuration options are provided, they are read and processed, thus overriding their values that were provided when the cluster was launched or started previously.\n\n\n\n\n\n\nAdding Servers to the Cluster\n\n\nThis is not yet supported using the script. You must manually launch an instance with \n(cluster-name)-stores\n group and\nthen use \nlaunch\n command with the \n--resume\n option.\n\n\n\n\nListing Members of the Cluster\n\n\nTo get the first locator's hostname:\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem get-locator cluster-name\n\n\n\n\nUse the \nget-lead\n command to get the first lead's hostname.\n\n\n\n\nConnecting to the Cluster\n\n\nYou can connect to any instance of a cluster with SSH using the login command. It logs you into the first lead instance.\nYou can then use SSH to connect to any other member of the cluster without a password. \n\nThe SnappyData product directory is located at \n/opt/snappydata/\n on all the members.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem login cluster-name\n\n\n\n\n\n\nDestroying the Cluster\n\n\nDestroying a cluster permanently destroys all the data on the local instance stores and on the attached EBS volumes.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem destroy cluster-name\n\n\n\n\nThis also deletes the security groups created for this cluster.\n\n\n\n\nStarting Cluster with Apache Zeppelin\n\n\nOptionally, you can start an instance of Apache Zeppelin server with the cluster.\n\nApache Zeppelin\n provides a web-based interactive notebook that is pre-configured to\ncommunicate with the SnappyData cluster. The Zeppelin server is launched on the same EC2 instance where the lead node is running.\n\n\n./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --with-zeppelin launch cluster-name\n\n\n\n\n\n\nMore Options\n\n\nFor a complete list of options provided by the script, simply run \n./snappy-ec2\n. The options are also provided below\nfor quick reference.\n\n\nUsage: snappy-ec2 [options] \naction\n \ncluster_name\n\n\n\naction\n can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster\n\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  -s STORES, --stores=STORES\n                        Number of stores to launch (default: 1)\n  --locators=LOCATORS   Number of locator nodes to launch (default: 1)\n  --leads=LEADS         Number of lead nodes to launch (default: 1)\n  -w WAIT, --wait=WAIT  DEPRECATED (no longer necessary) - Seconds to wait for\n                        nodes to start\n  -k KEY_PAIR, --key-pair=KEY_PAIR\n                        Name of the key pair to use on instances\n  -i IDENTITY_FILE, --identity-file=IDENTITY_FILE\n                        SSH private key file to use for logging into instances\n  -p PROFILE, --profile=PROFILE\n                        If you have multiple profiles (AWS or boto config),\n                        you can configure additional, named profiles by using\n                        this option (default: none)\n  -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE\n                        Type of server and lead instance to launch (default:\n                        m4.large). WARNING: must be 64-bit; small instances\n                        won't work\n  --locator-instance-type=LOCATOR_INSTANCE_TYPE\n                        Locator instance type (default: t2.medium)\n  -r REGION, --region=REGION\n                        EC2 region used to launch instances in, or to find\n                        them in (default: us-east-1)\n  -z ZONE, --zone=ZONE  Availability zone to launch instances in, or 'all' to\n                        spread stores across multiple (an additional $0.01/Gb\n                        for bandwidthbetween zones applies) (default: a single\n                        zone chosen at random)\n  -a AMI, --ami=AMI     Amazon Machine Image ID to use\n  --snappydata-tarball=SNAPPYDATA_TARBALL\n                        HTTP URL or local file path of the SnappyData\n                        distribution tarball with which the cluster will be\n                        launched. (default: )\n  --locator-conf=LOCATOR_CONF\n                        Configuration properties for locators (default: )\n  --server-conf=SERVER_CONF\n                        Configuration properties for servers (default: )\n  --lead-conf=LEAD_CONF\n                        Configuration properties for leads (default: )\n  -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION\n                        Version of SnappyData to use: 'X.Y.Z' (default:\n                        LATEST)\n  --with-zeppelin       Launch Apache Zeppelin server with the cluster. It'll\n                        be launched on the same instance where lead node will\n                        be running.\n  --deploy-root-dir=DEPLOY_ROOT_DIR\n                        A directory to copy into / on the first locator. Must\n                        be absolute. Note that a trailing slash is handled as\n                        per rsync: If you omit it, the last directory of the\n                        --deploy-root-dir path will be created in / before\n                        copying its contents. If you append the trailing\n                        slash, the directory is not created and its contents\n                        are copied directly into /. (default: none).\n  -D [ADDRESS:]PORT     Use SSH dynamic port forwarding to create a SOCKS\n                        proxy at the given local address (for use with login)\n  --resume              Resume installation on a previously launched cluster\n                        (for debugging)\n  --root-ebs-vol-size=SIZE\n                        Size (in GB) of root EBS volume for servers and leads.\n                        SnappyData is installed on root volume.\n  --root-ebs-vol-size-locator=SIZE\n                        Size (in GB) of root EBS volume for locators.\n                        SnappyData is installed on root volume.\n  --ebs-vol-size=SIZE   Size (in GB) of each additional EBS volume to be\n                        attached.\n  --ebs-vol-type=EBS_VOL_TYPE\n                        EBS volume type (e.g. 'gp2', 'standard').\n  --ebs-vol-num=EBS_VOL_NUM\n                        Number of EBS volumes to attach to each node as\n                        /vol[x]. The volumes will be deleted when the\n                        instances terminate. Only possible on EBS-backed AMIs.\n                        EBS volumes are only attached if --ebs-vol-size \n 0.\n                        Only support up to 8 EBS volumes.\n  --placement-group=PLACEMENT_GROUP\n                        Which placement group to try and launch instances\n                        into. Assumes placement group is already created.\n  --spot-price=PRICE    If specified, launch stores as spot instances with the\n                        given maximum price (in dollars)\n  -u USER, --user=USER  The SSH user you want to connect as (default:\n                        ec2-user)\n  --delete-groups       When destroying a cluster, delete the security groups\n                        that were created\n  --use-existing-locator\n                        Launch fresh stores, but use an existing stopped\n                        locator if possible\n  --user-data=USER_DATA\n                        Path to a user-data file (most AMIs interpret this as\n                        an initialization script)\n  --authorized-address=AUTHORIZED_ADDRESS\n                        Address to authorize on created security groups\n                        (default: 0.0.0.0/0)\n  --additional-security-group=ADDITIONAL_SECURITY_GROUP\n                        Additional security group to place the machines in\n  --additional-tags=ADDITIONAL_TAGS\n                        Additional tags to set on the machines; tags are\n                        comma-separated, while name and value are colon\n                        separated; ex: \nTask:MySnappyProject,Env:production\n\n  --copy-aws-credentials\n                        Add AWS credentials to hadoop configuration to allow\n                        Snappy to access S3\n  --subnet-id=SUBNET_ID\n                        VPC subnet to launch instances in\n  --vpc-id=VPC_ID       VPC to launch instances in\n  --private-ips         Use private IPs for instances rather than public if\n                        VPC/subnet requires that.\n  --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR\n                        Whether instances should terminate when shut down or\n                        just stop\n  --instance-profile-name=INSTANCE_PROFILE_NAME\n                        IAM profile name to launch instances under\n\n\n\n\n\n\nKnown Limitations\n\n\n\n\n\n\nLaunching the cluster on custom AMI (specified via \n--ami\n option) does not work if the user 'ec2-user' does not have sudo permissions.\n\n\n\n\n\n\nSupport for option \n--user\n is incomplete.\n\n\n\n\n\n\n\n\nAWS Management Console\n\n\nYou can launch a SnappyData cluster on Amazon EC2 instance(s) using the AMI provided by SnappyData. For more information\non launching an EC2 instance, refer to the \nAWS documentation\n.\nThis section covers the following:\n\n\n\n\nPrerequisites\n\n\nLaunching the Instance\n\n\n\n\n\n\nPrerequisites\n\n\nEnsure that you have an existing AWS account with required permissions to launch the EC2 resources.\n\n\n\n\nDeploying SnappyData Cluster with AWS Management Console\n\n\nTo launch the instance and start the SnappyData cluster:\n\n\n\n\n\n\nOpen the \nAmazon EC2 console\n and sign in using your AWS login credentials.\n\n\n\n\n\n\nThe current region is displayed at the top of the screen. Select the region where you want to launch the instance.\n\n\n\n\n\n\nClick \nLaunch Instance\n from the Amazon EC2 console dashboard.\n\n\n\n\n\n\nOn the \nChoose an Amazon Machine Image (AMI)\n page, select \nCommunity AMIs\n from the left pane.\n\n\n\n\n\n\nEnter \nSnappyData\n in the search box, and press \nEnter\n on your keyboard.\n\n\n\n\n\n\nThe search result is displayed. From the search results, click \nSelect\n to choose the AMI with the latest release version.\n\n\n\n\n\n\nOn the \nChoose an Instance Type\n page, select the instance type as per the requirement of your use case and then click \nReview and Launch\n to launch the instance with default configurations. \n\n\n\n\nNote\n\n\n\n\n\n\nYou can also continue customizing your instance before you launch the instance. Refer to the AWS documentation for more information.\n\n\n\n\n\n\nWhen configuring the security groups, ensure that you open at least ports 22 (for SSH access to the EC2 instance) and 5050 (for access to Snappy UI).\n\n\n\n\n\n\n\n\n\n\n\n\nYou are directed to the last step \nReview Instance Launch\n. Check the details of your instance, and click \nLaunch\n.\n\n\n\n\n\n\nIn the \nSelect an existing key pair or create a new key pair\n dialog box, select a key pair.\n\n\n\n\n\n\nClick \nLaunch\n. The Launch Status page is displayed.\n\n\n\n\n\n\nClick \nView Instances\n. The dashboard which lists the instances is displayed.\n\n\n\n\n\n\nClick \nRefresh\n to view the updated list and the status of the instance creation.\n\n\n\n\n\n\nOnce the status of the instance changes to \nrunning\n, you have successfully created and launched the instance with the SnappyData AMI.\n\n\n\n\n\n\nUse SSH to connect to the instance using the \nUbuntu\n username. You require:\n\n\n\n\n\n\nThe private key file of the key pair with which the instance was launched, and\n\n\n\n\n\n\nDetails of the public hostname or IP address of the instance.\nRefer to the following documentation, for more information on \naccessing an EC2 instance\n.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThe public hostname/IP address information is available on the EC2 dashboard \n \nDescription\n tab. \n\n\n\n\n\n\nThe SnappyData product distribution is already downloaded and extracted in the \n/opt/snappydata\n directory and Java 8 is installed. \n\n\n\n\n\n\n\n\n\n\n\n\nGo to the \n/opt/snappydata\n directory. Run the following command to start a basic cluster with one data node, one lead, and one locator.\n\n\n./sbin/snappy-start-all.sh", 
            "title": "Amazon Web Services (AWS)"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#setting-up-cluster-on-amazon-web-services-aws", 
            "text": "Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that is important for SnappyData are Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3).\nYou can set up SnappyData cluster on Amazon Web Services using one of the following options:   SnappyData CloudBuilder  EC2 Scripts    AWS Management Console", 
            "title": "Setting up Cluster on Amazon Web Services (AWS)"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#snappydata-cloudbuilder", 
            "text": "SnappyData CloudBuilder  is a web based utility that allows you to quickly launch SnappyData cluster on AWS instances. It also launches Apache Zeppelin server that allows you to build notebooks that visually represent key elements of your business.  This method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started.   Prerequisites  Deploying SnappyData Cloud Cluster Using SnappyData CloudBuilder", 
            "title": "SnappyData CloudBuilder"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#prerequisites", 
            "text": "Ensure that you have an existing AWS account with required permissions to launch EC2 resources with AWS CloudFormation.    Sign into the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.    Create an EC2 Key Pair in the region where you want to launch the SnappyData cluster.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#deploying-snappydata-cluster-with-snappydata-cloudbuilder", 
            "text": "SnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData cluster.  It is recommended that you select an instance type with higher processing power and more memory for the leads and servers  of the cluster.", 
            "title": "Deploying SnappyData Cluster with SnappyData CloudBuilder"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#step-1-go-to-snappydata-cloudbuilder-page", 
            "text": "", 
            "title": "Step 1: Go to SnappyData CloudBuilder page"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#step-2-define-your-cluster", 
            "text": "", 
            "title": "Step 2: Define your Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#for-community-edition", 
            "text": "Pick your version  Select the  Community  option.  \n        Pick your Instance Select an instance based on the capacity that you require. \n        Enter your email : Provide your email address.  \n        Click  Generate .    The next page is displayed where you can  Select the Region and Launch your Cluster .", 
            "title": "For Community Edition"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#for-enterprise-edition", 
            "text": "Pick your version  Select the  Enterprise  option.     Make Locators   Leads Highly Available?    Select HA/Non-HA for the  Locators    Select HA/Non-HA for the  Leads         Pick total Memory   Disk  (GB):     Memory : Click and drag the bar to select the required memory.    Disk (3x Memory Recommended) : Click and drag the bar to select the required disk size. Currently, Amazon Elastic Block Storage (EBS) is provided.       Recommended Cluster : Select an instance based on the required capacity.    Add servers to support high availability? : Select this option to add servers to support high availability.    Do your workloads have high query volumes? : Select this option if your workloads have high query volumes.    Click the  Edit Nodes  option to modify the number of nodes.     Enter your email address and select the  Agree to terms of service  check-box.      Click  Generate . The next page is displayed where you can  Select the Region and Launch your Cluster .", 
            "title": "For Enterprise Edition"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#step-3-select-the-region-and-launch-your-cluster", 
            "text": "On this page, select the AWS region, and then click  Launch Cluster  to launch your single-node cluster.   Note    The region you select must match the EC2 Key Pair you created.    If you are not already logged into AWS, you are redirected to the AWS sign-in page.           On the  Select Template  page, the URL for the Amazon S3 template is pre-populated. Click  Next  to continue.        On the  Specify Details  page:   Stack name : You can change the stack name.    Note  The stack name must contain only letters, numbers, dashes, and should start with an alpha character.     KeyPairName : Enter a name of an existing EC2 KeyPair. This enables SSH access to the cluster. Refer to the Amazon documentation for more information on  generating your own EC2 Key Pair .    VPCID : From the drop-down list, select  default  Virtual Private Cloud (VPC) ID of your selected region. Your instances are launched within this VPC.  Click  Next  to continue.       Specify the tags (key-value pairs) for resources in your stack or leave the field empty and click  Next .    On the  Review  page, verify the details and click  Create  to create a stack.   Note  This operation may take a few minutes to complete.     The next page lists the existing stacks. Click  Refresh  to view the updated list and the status of the stack creation.\nWhen the cluster has started, the status of the stack changes to  CREATE_COMPLETE .       Click on the  Outputs  tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration.  \n       Note  If the status of the stack displays  ROLLBACK_IN_PROGRESS  or  DELETE_COMPLETE , the stack creation may have failed. Some common problems that might have caused the failure are:    Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.    Invalid Keypair : Verify that the EC2 key pair exists in the region you selected in the SnappyData CloudBuilder creation steps.    Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.*        Warning  To stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it.   For more information, refer to the  Apache Zeppelin  section or refer to the  Apache Zeppelin documentation .", 
            "title": "Step 3: Select the Region and Launch your Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#snappydata-ec2-scripts", 
            "text": "The SnappyData EC2 scripts enable you to quickly launch and manage SnappyData clusters on Amazon EC2 instances. They also allow you to provide custom configuration for the cluster via SnappyData configuration files, before launching the cluster.  The  snappy-ec2  script is the entry point for these EC2 scripts and is derived from the  spark-ec2  script available in  Apache Spark 1.6 .  The scripts are available on GitHub in the  snappy-cloud-tools repository  and also as a  .tar.gz  file on  the release page  file.   Note  The EC2 scripts are provided on an experimental basis. Feel free to try it out and provide your feedback as via GitHub issues.   This section covers the following:     Prerequisites      Deploying SnappyData Cluster with EC2 Scripts      Cluster Management      Known Limitations", 
            "title": "SnappyData EC2 Scripts"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#prerequisites_1", 
            "text": "Ensure that you have an existing AWS account with required permissions to launch EC2 resources    Create an EC2 Key Pair in the region where you want to launch the SnappyData Cloud cluster Refer to the Amazon Web Services EC2 documentation for more information on  generating your own EC2 Key Pair .    Using the AWS Secret Access Key and the Access Key ID, set the two environment variables,  AWS_SECRET_ACCESS_KEY  and  AWS_ACCESS_KEY_ID . You can find information about generating these keys in the AWS IAM console page. \nIf you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file.    For example:  export AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112\nexport AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10   Ensure Python v 2.7 or later is installed on your local computer.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#deploying-snappydata-cluster-with-ec2-scripts", 
            "text": "In the command prompt, go to the directory where the  snappydata-ec2- version .tar.gz  is extracted or to the\naws/ec2 directory where the  SnappyData cloud tools repository  is cloned locally.  Syntax  ./snappy-ec2 -k  your-key-name  -i  your-keyfile-path   action   your-cluster-name  [options]  Here:    your-key-name  refers to the EC2 key pair.    your-keyfile-path  refers to the path to the key file.    action  refers to the action to be performed. Some of the available actions are  launch ,  destroy ,  stop ,  start  and  reboot-cluster .\nUse  launch  action to create a new cluster while  stop  and  start  actions work on existing clusters.    By default, the script starts one instance of a locator, lead, and server each.\nThe script identifies each cluster by its unique cluster name that you provide and internally ties the members\n(locators, leads, and stores/servers) of the cluster with EC2 security groups, whose names are derived from the cluster name.  When running the script, you can also specify options to configure the cluster such as the number of stores in the\ncluster and the region where the EC2 instances should be launched.  Example  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 --with-zeppelin --region=us-west-1 launch my-cluster  The above example launches a SnappyData cluster named  my-cluster  with 2 stores or servers.\nThe locator is associated with security group named  my-cluster-locator  and the servers are associated with  my-cluster-store  security group.  The cluster is launched in the  N. California (us-west-1)  region on AWS and has an Apache Zeppelin server running on the instance where the lead is running.  The example assumes that you have the key file (my-ec2-key.pem) in your home directory for EC2 Key Pair named 'my-ec2-key'.   Note  By default, the cluster is launched in the  N. Virginia (us-east-1)  region on AWS. To launch the cluster in a specific region use option  --region .", 
            "title": "Deploying SnappyData Cluster with EC2 Scripts"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#cluster-management", 
            "text": "This section covers the following:   Using custom build  Specifying Properties  Stopping the Cluster  Resuming the Cluster  Adding Servers to the Cluster  Listing Members of the Cluster  Connecting to the Cluster  Destroying the Cluster  Starting Cluster with Apache Zeppelin  More Options", 
            "title": "Cluster Management"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#using-custom-build", 
            "text": "This script by default uses the SnappyData OSS build available on the GitHub releases page to launch the cluster.\nTo select a version of the OSS build available on GitHub, use option  --snappydata-version .  You can also provide your own SnappyData build to the script to launch the cluster, by using\noption  --snappydata-tarball  to the  launch  command.\nThe build can be present either on a local filesystem or as a resource on the web.  For example, to use  SnappyData Enterprise  build to launch the cluster, download the build tarball from\nwww.snappydata.io/download on your local machine and give its path as value to above option.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball= /home/ec2-user/snappydata/distributions/snappydata-1.0.2.1-bin.tar.gz    Alternatively, you can also put your build file on a public web server and provide its URL to this option.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem launch my-cluster --snappydata-tarball= https://s3-us-east-2.amazonaws.com/mybucket/distributions/snappydata-1.0.2.1-bin.tar.gz    The build file should be in  .tar.gz  format.", 
            "title": "Using custom build"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#specifying-properties", 
            "text": "You can specify the configuration for the cluster via command line options. Use  --locator-conf  to specify the\nconfiguration properties for all the locators in the cluster. Similarly,  --server-conf  and  --lead-conf  allow you\nto specify the configuration properties for servers and leads in the cluster, respectively.  Following is a sample configuration for all the three processes in a SnappyData cluster:  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --stores=2 launch my-cluster \\\n  --locator-conf= -peer-discovery-port=9999 -heap-size=1024m  \\\n  --lead-conf= -spark.executor.cores=10 -heap-size=4096m -spark.ui.port=3333  \\\n  --server-conf= -client-port=1530   The utility also reads  snappy-env.sh , if present in the directory where helper scripts are present.   Note    The earlier method of specifying the configuration properties by placing the actual\n  configuration files in the directory, where helper scripts are available, is discontinued.  Ensure that the configuration properties specified are correct. Otherwise, launching the\n  SnappyData cluster may fail but the EC2 instances would still be running.", 
            "title": "Specifying Properties"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#stopping-the-cluster", 
            "text": "When you stop a cluster, it shuts down the EC2 instances and any data saved on the local instance stores is lost.\nHowever, the data saved on EBS volumes is retained, unless the spot-instances are used.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem stop cluster-name", 
            "title": "Stopping the Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#resuming-the-cluster", 
            "text": "When you start a cluster, it uses the existing EC2 instances associated with the cluster name and launches SnappyData processes on them.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem start cluster-name   Note  The  start  command, or  launch  command with  --resume  option, ignores the  --locators ,  --leads , or  --stores  options and launches the SnappyData cluster on existing instances.\nHowever, if the configuration options are provided, they are read and processed, thus overriding their values that were provided when the cluster was launched or started previously.", 
            "title": "Resuming the Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#adding-servers-to-the-cluster", 
            "text": "This is not yet supported using the script. You must manually launch an instance with  (cluster-name)-stores  group and\nthen use  launch  command with the  --resume  option.", 
            "title": "Adding Servers to the Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#listing-members-of-the-cluster", 
            "text": "To get the first locator's hostname:  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem get-locator cluster-name  Use the  get-lead  command to get the first lead's hostname.", 
            "title": "Listing Members of the Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#connecting-to-the-cluster", 
            "text": "You can connect to any instance of a cluster with SSH using the login command. It logs you into the first lead instance.\nYou can then use SSH to connect to any other member of the cluster without a password.  \nThe SnappyData product directory is located at  /opt/snappydata/  on all the members.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem login cluster-name", 
            "title": "Connecting to the Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#destroying-the-cluster", 
            "text": "Destroying a cluster permanently destroys all the data on the local instance stores and on the attached EBS volumes.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem destroy cluster-name  This also deletes the security groups created for this cluster.", 
            "title": "Destroying the Cluster"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#starting-cluster-with-apache-zeppelin", 
            "text": "Optionally, you can start an instance of Apache Zeppelin server with the cluster. Apache Zeppelin  provides a web-based interactive notebook that is pre-configured to\ncommunicate with the SnappyData cluster. The Zeppelin server is launched on the same EC2 instance where the lead node is running.  ./snappy-ec2 -k my-ec2-key -i ~/my-ec2-key.pem --with-zeppelin launch cluster-name", 
            "title": "Starting Cluster with Apache Zeppelin"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#more-options", 
            "text": "For a complete list of options provided by the script, simply run  ./snappy-ec2 . The options are also provided below\nfor quick reference.  Usage: snappy-ec2 [options]  action   cluster_name  action  can be: launch, destroy, login, stop, start, get-locator, get-lead, reboot-cluster\n\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  -s STORES, --stores=STORES\n                        Number of stores to launch (default: 1)\n  --locators=LOCATORS   Number of locator nodes to launch (default: 1)\n  --leads=LEADS         Number of lead nodes to launch (default: 1)\n  -w WAIT, --wait=WAIT  DEPRECATED (no longer necessary) - Seconds to wait for\n                        nodes to start\n  -k KEY_PAIR, --key-pair=KEY_PAIR\n                        Name of the key pair to use on instances\n  -i IDENTITY_FILE, --identity-file=IDENTITY_FILE\n                        SSH private key file to use for logging into instances\n  -p PROFILE, --profile=PROFILE\n                        If you have multiple profiles (AWS or boto config),\n                        you can configure additional, named profiles by using\n                        this option (default: none)\n  -t INSTANCE_TYPE, --instance-type=INSTANCE_TYPE\n                        Type of server and lead instance to launch (default:\n                        m4.large). WARNING: must be 64-bit; small instances\n                        won't work\n  --locator-instance-type=LOCATOR_INSTANCE_TYPE\n                        Locator instance type (default: t2.medium)\n  -r REGION, --region=REGION\n                        EC2 region used to launch instances in, or to find\n                        them in (default: us-east-1)\n  -z ZONE, --zone=ZONE  Availability zone to launch instances in, or 'all' to\n                        spread stores across multiple (an additional $0.01/Gb\n                        for bandwidthbetween zones applies) (default: a single\n                        zone chosen at random)\n  -a AMI, --ami=AMI     Amazon Machine Image ID to use\n  --snappydata-tarball=SNAPPYDATA_TARBALL\n                        HTTP URL or local file path of the SnappyData\n                        distribution tarball with which the cluster will be\n                        launched. (default: )\n  --locator-conf=LOCATOR_CONF\n                        Configuration properties for locators (default: )\n  --server-conf=SERVER_CONF\n                        Configuration properties for servers (default: )\n  --lead-conf=LEAD_CONF\n                        Configuration properties for leads (default: )\n  -v SNAPPYDATA_VERSION, --snappydata-version=SNAPPYDATA_VERSION\n                        Version of SnappyData to use: 'X.Y.Z' (default:\n                        LATEST)\n  --with-zeppelin       Launch Apache Zeppelin server with the cluster. It'll\n                        be launched on the same instance where lead node will\n                        be running.\n  --deploy-root-dir=DEPLOY_ROOT_DIR\n                        A directory to copy into / on the first locator. Must\n                        be absolute. Note that a trailing slash is handled as\n                        per rsync: If you omit it, the last directory of the\n                        --deploy-root-dir path will be created in / before\n                        copying its contents. If you append the trailing\n                        slash, the directory is not created and its contents\n                        are copied directly into /. (default: none).\n  -D [ADDRESS:]PORT     Use SSH dynamic port forwarding to create a SOCKS\n                        proxy at the given local address (for use with login)\n  --resume              Resume installation on a previously launched cluster\n                        (for debugging)\n  --root-ebs-vol-size=SIZE\n                        Size (in GB) of root EBS volume for servers and leads.\n                        SnappyData is installed on root volume.\n  --root-ebs-vol-size-locator=SIZE\n                        Size (in GB) of root EBS volume for locators.\n                        SnappyData is installed on root volume.\n  --ebs-vol-size=SIZE   Size (in GB) of each additional EBS volume to be\n                        attached.\n  --ebs-vol-type=EBS_VOL_TYPE\n                        EBS volume type (e.g. 'gp2', 'standard').\n  --ebs-vol-num=EBS_VOL_NUM\n                        Number of EBS volumes to attach to each node as\n                        /vol[x]. The volumes will be deleted when the\n                        instances terminate. Only possible on EBS-backed AMIs.\n                        EBS volumes are only attached if --ebs-vol-size   0.\n                        Only support up to 8 EBS volumes.\n  --placement-group=PLACEMENT_GROUP\n                        Which placement group to try and launch instances\n                        into. Assumes placement group is already created.\n  --spot-price=PRICE    If specified, launch stores as spot instances with the\n                        given maximum price (in dollars)\n  -u USER, --user=USER  The SSH user you want to connect as (default:\n                        ec2-user)\n  --delete-groups       When destroying a cluster, delete the security groups\n                        that were created\n  --use-existing-locator\n                        Launch fresh stores, but use an existing stopped\n                        locator if possible\n  --user-data=USER_DATA\n                        Path to a user-data file (most AMIs interpret this as\n                        an initialization script)\n  --authorized-address=AUTHORIZED_ADDRESS\n                        Address to authorize on created security groups\n                        (default: 0.0.0.0/0)\n  --additional-security-group=ADDITIONAL_SECURITY_GROUP\n                        Additional security group to place the machines in\n  --additional-tags=ADDITIONAL_TAGS\n                        Additional tags to set on the machines; tags are\n                        comma-separated, while name and value are colon\n                        separated; ex:  Task:MySnappyProject,Env:production \n  --copy-aws-credentials\n                        Add AWS credentials to hadoop configuration to allow\n                        Snappy to access S3\n  --subnet-id=SUBNET_ID\n                        VPC subnet to launch instances in\n  --vpc-id=VPC_ID       VPC to launch instances in\n  --private-ips         Use private IPs for instances rather than public if\n                        VPC/subnet requires that.\n  --instance-initiated-shutdown-behavior=INSTANCE_INITIATED_SHUTDOWN_BEHAVIOR\n                        Whether instances should terminate when shut down or\n                        just stop\n  --instance-profile-name=INSTANCE_PROFILE_NAME\n                        IAM profile name to launch instances under", 
            "title": "More Options"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#known-limitations", 
            "text": "Launching the cluster on custom AMI (specified via  --ami  option) does not work if the user 'ec2-user' does not have sudo permissions.    Support for option  --user  is incomplete.", 
            "title": "Known Limitations"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#aws-management-console", 
            "text": "You can launch a SnappyData cluster on Amazon EC2 instance(s) using the AMI provided by SnappyData. For more information\non launching an EC2 instance, refer to the  AWS documentation .\nThis section covers the following:   Prerequisites  Launching the Instance", 
            "title": "AWS Management Console"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#prerequisites_2", 
            "text": "Ensure that you have an existing AWS account with required permissions to launch the EC2 resources.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/setting_up_cluster_on_amazon_web_services/#deploying-snappydata-cluster-with-aws-management-console", 
            "text": "To launch the instance and start the SnappyData cluster:    Open the  Amazon EC2 console  and sign in using your AWS login credentials.    The current region is displayed at the top of the screen. Select the region where you want to launch the instance.    Click  Launch Instance  from the Amazon EC2 console dashboard.    On the  Choose an Amazon Machine Image (AMI)  page, select  Community AMIs  from the left pane.    Enter  SnappyData  in the search box, and press  Enter  on your keyboard.    The search result is displayed. From the search results, click  Select  to choose the AMI with the latest release version.    On the  Choose an Instance Type  page, select the instance type as per the requirement of your use case and then click  Review and Launch  to launch the instance with default configurations.    Note    You can also continue customizing your instance before you launch the instance. Refer to the AWS documentation for more information.    When configuring the security groups, ensure that you open at least ports 22 (for SSH access to the EC2 instance) and 5050 (for access to Snappy UI).       You are directed to the last step  Review Instance Launch . Check the details of your instance, and click  Launch .    In the  Select an existing key pair or create a new key pair  dialog box, select a key pair.    Click  Launch . The Launch Status page is displayed.    Click  View Instances . The dashboard which lists the instances is displayed.    Click  Refresh  to view the updated list and the status of the instance creation.    Once the status of the instance changes to  running , you have successfully created and launched the instance with the SnappyData AMI.    Use SSH to connect to the instance using the  Ubuntu  username. You require:    The private key file of the key pair with which the instance was launched, and    Details of the public hostname or IP address of the instance.\nRefer to the following documentation, for more information on  accessing an EC2 instance .     Note    The public hostname/IP address information is available on the EC2 dashboard    Description  tab.     The SnappyData product distribution is already downloaded and extracted in the  /opt/snappydata  directory and Java 8 is installed.        Go to the  /opt/snappydata  directory. Run the following command to start a basic cluster with one data node, one lead, and one locator.  ./sbin/snappy-start-all.sh", 
            "title": "Deploying SnappyData Cluster with AWS Management Console"
        }, 
        {
            "location": "/kubernetes/", 
            "text": "Setting up Cluster on Kubernetes\n\n\nKubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes.\n\n\nThe following sections are included in this topic:\n\n\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nGetting Access to Kubernetes cluster\n\n\n\n\n\n\nDeploying SnappyData Chart on Kubernetes\n\n\n\n\n\n\nSetting up PKS Environment for Kubernetes\n\n\n\n\n\n\nInteracting with SnappyData Cluster on Kubernetes\n\n\n\n\n\n\nList of Configuration Parameters for SnappyData Chart\n\n\n\n\n\n\nKubernetes Obects Used in SnappyData Chart\n\n\n\n\n\n\nAccesing Logs and Configuring Log Level\n\n\n\n\n\n\n \n\n\nPrerequisites\n\n\nThe following prerequisites must be met to deploy SnappyData on Kubernetes:\n\n\n\n\n\n\nKubernetes cluster\n A running Kubernetes cluster of version 1.9 or higher. SnappyData has been tested on Google Container Engine(GKE) as well as on Pivotal Container Service (PKS). If Kubernetes cluster is not available, you can set it up as mentioned \nhere\n.\n\n\n\n\n\n\nHelm tool\n Helm tool must be deployed in the Kubernetes environment. Helm comprises of two parts, that is a client and a Tiller (Server portion of Helm) inside the kube-system namespace. Tiller runs inside the Kubernetes cluster and manages the deployment of charts or packages. You can follow the instructions \nhere\n to deploy Helm in your Kubernetes enviroment.\n\n\n\n\n\n\n \n\n\nGetting Access to Kubernetes Cluster\n\n\nIf you would like to deploy Kubernetes on-premises, you can use any of the following options:\n\n\nOption 1 - PKS\n\n\n\n\nPKS on vSphere: Follow these \ninstructions\n \n\n\nPKS on GCP: Follow these \ninstructions\n\n\nCreate a Kubernetes cluster using PKS CLI : After PKS is setup you will need to create a Kubernetes cluster as described \nhere\n\n\n\n\nOption 2 - Google Cloud Platform (GCP)\n\n\n\n\nLogin to your Google account and go to the \nCloud console\n to launch a GKE cluster.\n\n\n\n\nSteps to perform after Kubernetes cluster is available: \n\n\n\n\nIf using PKS, you must install the PKS command line tool. See instructions \nhere\n.\n\n\nInstall \nkubectl\n on your local development machine and configure access to the kubernetes/PKS cluster. See instructions for \nkubectl\n \nhere\n. \n\n\nIf you are using Google cloud, you will find instructions for setting up Google Cloud SDK ('gcloud') along with \nkubectl\n \nhere\n.\n\n\n\n\n \n\n\nDeploying SnappyData on Kubernetes\n\n\nSnappyData Helm \nchart is used to deploy SnappyData on Kubernetes.  It uses Kubernetes \nstatefulsets\n to launch the locator, lead, and server members. \n\n\nTo deploy SnappyData on Kubernetes:\n\n\n\n\n\n\nClone the \nspark-on-k8s\n repository and change to \ncharts\n directory.\n\n\ngit clone https://github.com/SnappyDataInc/spark-on-k8s\n\n\ncd spark-on-k8s/charts\n\n\n\n\n\n\nOptionally, you can edit the \nsnappydata \n values.yaml\n  file to change the default configurations in the SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer \nList of Configuration Parameters for SnappyData Chart\n\n\n\n\n\n\nInstall the \nsnappydata\n chart using the following command:\n\n\nhelm install --name snappydata --namespace snappy ./snappydata/\n\n\nThe above command installs the SnappyData chart in a namespace called \nsnappy\n and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console.\n\nBy default, \nSnappyData Helm \nchart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints.\n\n\n\n\n\n\nYou can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions \nhere\n. \n\n\nSnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted.\n\n\n \n\n\nInteracting with SnappyData Cluster on Kubernetes\n\n\nYou can interact with the SnappyData cluster on Kuberenetes in the same manner as you interact with a SnappyData cluster that runs locally or on-premise. All you require is the host IP address of the locator and the lead with their respective ports numbers.\n\n\nTo find the IP addresses and port numbers of the SnappyData processes, use command \nkubectl get svc --namespace=snappy\n. \nIn the \noutput\n, three services namely \nsnappydata-leader-public\n, \nsnappydata-locator-public\n and \n\nsnappydata-server-public\n  of type \nLoadBalancer\n are seen which expose the endpoints for locator, lead, and server respectively. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use.\n\n\nsnappydata-leader-public\n service exposes port \n5050\n for SnappyData Pulse and port \n8090\n to accept \nSnappyData jobs\n.\n\n\nsnappydata-locator-public\n service exposes port \n1527\n to accept \nJDBC/ODBC connections\n.\n\n\nYou can do the following on the SnappyData cluster that is deployed on Kubernetes:\n\n\n\n\n\n\nAccess SnappyData Pulse\n\n\n\n\n\n\nConnect SnappyData using JDBC Driver\n\n\n\n\n\n\nExecute Queries\n\n\n\n\n\n\nSubmit a SnappyData Job\n\n\n\n\n\n\nStop SnappyData Cluster on Kubernetes\n\n\n\n\n\n\n \n\n\nAccessing SnappyData Pulse\n\n\nThe dashboards on the SnappyData Pulse can be accessed using \nsnappydata-leader-public\n service. To view the dashboard, type the URL in the web browser in the format: \nexternalIp:5050\n.\nReplace \nexternalip\n with the external IP address of the \nsnappydata-leader-public \nservice.\n\n\nTo access SnappyData Pulse in Kubernetes:\n\n\n\n\n\n\nCheck the SnappyData services running in the Kubernetes cluster.\n\n\nkubectl get svc --namespace=snappy\n The output displays the external IP address of the \nsnappydata-leader-public\n service as shown in the following image: \n\n\n\n\n\n\nType \nexternalIp:5050\n in the browser. Here you must replace \nexternalip\n with the external IP address of the \nleader-public\n service.\n For example, 35.232.102.51:5050.\n\n\n\n\n\n\n \n\n\nConnecting SnappyData Using JDBC Driver\n\n\nFor Kubernetes deployments, JDBC clients can connect to SnappyData cluster using the JDBC URL that is derived from the \nsnappydata-locator-public\n service.\n\n\nTo connect to SnappyData using JDBC driver in Kubernetes:\n\n\n\n\n\n\nCheck the SnappyData services running in Kubernetes cluster.\n\n\nkubectl get svc --namespace=snappy\n\nThe output displays the external IP address  of the \nsnappydata-locator-public\n service and the port number for external connections as shown in the following image:\n\n\n\n\n\n\nUse the external IP address and port of the \nsnappydata-locator-public\n services to connect to SnappyData cluster using JDBC connections. For example, based on the above output, the JDBC URL to be used will be \njdbc:snappydata://104.198.47.162:1527/\n\n\n\n\n\n\nYou can refer to \nSnappyData documentation\n for an example of JDBC program and for instructions on how to obtain JDBC driver using Maven/SBT co-ordinates.\n\n\n \n\n\nExecuting Queries Using SnappyData Shell\n\n\nYou  can use SnappyData shell to connect to SnappyData and execute your queries. You can simply connect to one of the pods in the cluster and use the SnappyData Shell. Alternatively, you can download the SnappyData distribution from \nSnappyData github releases\n. SnappyData shell need not run within the Kubernetes cluster.\n\n\nTo execute queries in Kubernetes deployment:\n\n\n\n\n\n\nCheck the SnappyData services running in the Kubernetes cluster.\n\n\nkubectl get svc --namespace=snappy\n\nThe output displays the external IP address of the \nsnappydata-locator-public\n services  and the port number for external connections as shown in the following image:\n\n\n\n\n\n\nLaunch SnappyData shell and then create tables and execute queries. \nFollowing is an example of executing queries using SnappyData shell.\n\n\n\n\n\n\n# Connect to snappy-shell\n bin/snappy\n snappy\n connect client '104.198.47.162:1527';\n\n# Create tables and execute queries\n snappy\n create table t1(col1 int, col2 int) using column;\n snappy\n insert into t1 values(1, 1);\n 1 row inserted/updated/deleted \n\n\n\n\n \n\n\nSubmitting a SnappyData Job\n\n\nRefer to the \nHow Tos section\n in SnappyData documentation to understand how to submit SnappyData jobs.\nHowever, for submitting a SnappyData job in Kubernetes deployment, you need to use the \nsnappydata-leader-public\n service that exposes port \n8090\n to run the jobs.\n\n\nTo submit a SnappyData job in Kubernetes deployment:\n\n\n\n\n\n\nCheck the SnappyData services running in Kubernetes cluster.\n\n\nkubectl get svc --namespace=snappy\n\nThe output displays the external IP address of \nsnappydata-leader-public\n service which must be noted. \n\n\n\n\n\n\nChange to SnappyData product directory.\n\n\ncd $SNAPPY_HOME\n\n\n\n\n\n\nSubmit the job using the external IP of the \nsnappydata-leader-public\n service and the port number \n8090\n in the \n--lead\n option.\n Following is an example of submitting a SnappyData Job:\n\n\n\n\n\n\nbin/snappy-job.sh submit\n--app-name CreatePartitionedRowTable\n  --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable\n  --app-jar examples/jars/quickstart.jar\n  --lead 35.232.102.51:8090\n\n\n\n\nStopping the SnappyData Cluster on Kubernetes\n\n\nTo stop the SnappyData cluster on Kubernetes, you must delete the \nSnappyData Helm \nchart using the \nhelm delete\n command.\n\n\n$ helm delete --purge snappydata\n\n\n\n\nThe dynamically provisioned volumes and the data in it is retained, even if the chart deployment is deleted.\n\n\n\n\nNote\n\n\nIf the chart is deployed again with the same chart name and if the volume exists, then the existing volume is used instead of provisioning a new volume.\n\n\n\n\n \n\n\nList of Configuration Parameters for SnappyData Chart\n\n\nYou can modify the \nvalues.yaml\n  file to configure the SnappyData chart. The following table lists the configuration parameters available for this chart:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\nimage\n\n\nDocker repo from which the SnappyData Docker image is pulled.\n\n\nsnappydatainc/snappydata\n\n\n\n\n\n\nimageTag\n\n\nTag of the SnappyData Docker image that is pulled.\n\n\n\n\n\n\n\n\nimagePullPolicy\n\n\nPull policy for the image.\n\n\nIfNotPresent\n\n\n\n\n\n\nlocators.conf\n\n\nList of the configuration options that is passed to the locators.\n\n\n\n\n\n\n\n\nlocators.resources\n\n\nResource configuration for the locator Pods. User can configure CPU/memory requests and limit the usage.\n\n\nlocators.requests.memory\n is set to \n1024Mi\n.\n\n\n\n\n\n\nlocators.persistence.storageClass\n\n\nStorage class that is used while dynamically provisioning a volume.\n\n\nDefault value is not defined so \ndefault\n storage class for the cluster is chosen.\n\n\n\n\n\n\nlocators.persistence.accessMode\n\n\nAccess mode\n that is used for the dynamically provisioned volume.\n\n\nReadWriteOnce\n\n\n\n\n\n\nlocators.persistence.size\n\n\nSize of the dynamically provisioned volume.\n\n\n10Gi\n\n\n\n\n\n\nservers.replicaCount\n\n\nNumber of servers that are started in a SnappyData cluster.\n\n\n2\n\n\n\n\n\n\nservers.conf\n\n\nList of the configuration options that are passed to the servers.\n\n\n\n\n\n\n\n\nservers.resources\n\n\nResource configuration for the server Pods. You can configure CPU/memory requests and limit the usage.\n\n\nservers.requests.memory\n is set to \n4096Mi\n\n\n\n\n\n\nservers.persistence.storageClass\n\n\nStorage class that is used while dynamically provisioning a volume.\n\n\nDefault value is not defined so \ndefault\n storage class for the cluster will be chosen.\n\n\n\n\n\n\nservers.persistence.accessMode\n\n\nAccess mode\n for the dynamically provisioned volume.\n\n\nReadWriteOnce\n\n\n\n\n\n\nservers.persistence.size\n\n\nSize of the dynamically provisioned volume.\n\n\n10Gi\n\n\n\n\n\n\nleaders.conf\n\n\nList of configuration options that can be passed to the leaders.\n\n\n\n\n\n\n\n\nleaders.resources\n\n\nResource configuration for the server pods. You can configure CPU/memory requests and limits the usage.\n\n\nleaders.requests.memory\n is set to \n4096Mi\n\n\n\n\n\n\nleaders.persistence.storageClass\n\n\nStorage class that is used while dynamically provisioning a volume.\n\n\nDefault value is not defined so \ndefault\n storage class for the cluster will be chosen.\n\n\n\n\n\n\nleaders.persistence.accessMode\n\n\nAccess mode\n for the dynamically provisioned volume.\n\n\nReadWriteOnce\n\n\n\n\n\n\nleaders.persistence.size\n\n\nSize of the dynamically provisioned volume.\n\n\n10Gi\n\n\n\n\n\n\n\n\nThe following sample shows the configuration used to start four servers each with a heap size of 2048 MB:\n\n\nservers:\n  replicaCount: 4\n  ## config options for servers\n  conf: \n-heap-size=2048m\n\n\n\n\n\nYou can specify SnappyData \nconfiguration parameters\n in the \nservers.conf\n, \nlocators.conf\n, and \nleaders.conf\n attributes for servers, locators, and leaders respectively.\n\n\n \n\n\nKubernetes Obects Used in SnappyData Chart\n\n\nThis section provides details about the following Kubernetes objects that are used in SnappyData Chart:\n\n\n\n\n\n\nStatefulsets for Servers, Leaders, and Locators\n\n\n\n\n\n\nServices that Expose External Endpoints\n\n\n\n\n\n\nPersistent Volumes\n\n\n\n\n\n\n \n\n\nStatefulsets for Servers, Leaders, and Locators\n\n\nKubernetes statefulsets\n are used to manage stateful applications. Statefulsets provide many benefits such as stable and unique network identifiers, stable persistent storage, ordered deployment and scaling, graceful deletion, and rolling updates.\nSnappyData Helm chart deploys statefulsets for servers, leaders, and locators. By default the chart deploys two data servers, one locator, and one leader. Upon deletion of the Helm deployment, each pod gracefully terminates the SnappyData process that is running on it.\n\n\n \n\n\nServices that Expose External Endpoints\n\n\nSnappyData Helm chart creates services to allow you to make JDBC connections, execute Spark jobs, and access\nSnappyData Pulse etc.  Services of the type LoadBalancer have external IP address assigned and can be used to connect from outside of Kubernetes cluster.\nTo check the service created for SnappyData deployment, use command \nkubectl get svc --namespace=snappy\n. The following output is displayed:\n\n\n \n\n\nNAME                        TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                                        AGE\nsnappydata-leader           ClusterIP      None            \nnone\n           5050/TCP                                       5m\nsnappydata-leader-public    LoadBalancer   10.51.255.175   35.232.102.51    5050:31964/TCP,8090:32700/TCP,3768:32575/TCP   5m\nsnappydata-locator          ClusterIP      None            \nnone\n           10334/TCP,1527/TCP                             5m\nsnappydata-locator-public   LoadBalancer   10.51.241.224   104.198.47.162   1527:31957/TCP                                 5m\nsnappydata-server           ClusterIP      None            \nnone\n           1527/TCP                                       5m\nsnappydata-server-public    LoadBalancer   10.51.248.27    35.232.16.4      1527:31853/TCP                                 5m\n\n\n\n\n\nIn the above output, three services namely \nsnappydata-leader-public\n, \nsnappydata-locator-public\n and \n\nsnappydata-server-public\n  of type \nLoadBalancer\n are created. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use.\n\n\nsnappydata-leader-public\n service exposes port \n5050\n for SnappyData Pulse and port \n8090\n to accept SnappyData jobs.\n\nsnappydata-locator-public\n service exposes port \n1527\n to accept JDBC connections.\n\n\n \n\n\nPersistent Volumes\n\n\nA pod in a SnappyData deployment has a persistent volume mounted on it. This volume is dynamically provisioned and is used\nto store data directory for SnappyData. On each pod, the persistent volume is mounted on path \n/opt/snappydata/work\n. These volumes and the data in it is retained even if the chart deployment is deleted.\n\n\n \n\n\nAccessing Logs\n\n\nYou can access the logs when the \nSnappyData cluster is running\n as well as when the \nSnappyData cluster is not running\n. \n\n\n \n\n\nAccessing Logs When SnappyData Cluster is Running\n\n\nWhen a SnappyData cluster is running, you can open a session for a pod using \nkubectl\n command and then view the logs.\nThe following example shows how to access logs of \nsnappydata-server-0\n:\n\n\n# Connect to snappydata-server-0 pod and open a shell.\n$ kubectl exec -it snappydata-server-0 --namespace snappy -- /bin/bash\n\n# Switch to Snappydata work directory and view the logs.\n$ cd /opt/snappydata/work\n$ ls \n\n\n\n\n \n\n\nAccessing Logs When SnappyData Cluster is not Running\n\n\nWhen SnappyData cluster is not running, you can access the volumes used in SnappyData with a utility script \nsnappy-debug-pod.sh\n located in the \nutils\n directory of \nSpark on k8s\n repository.\nThis script launches a pod in the Kubernetes cluster with persistent volumes, specified via \n--pvc\n option, mounted on it and then returns a shell prompt. Volumes are mounted on the path starting with \n/data0 (volume1 on /data0 and so on)\n.\n\n\n\n\n\nIn the following example, the names of the persistent volume claims used by the cluster are retrieved and passed to the \nsnappy-debug-pod.sh\n script to be mounted on the pod.\n\n\n# Get the names of persistent volume claims used by SnappyData cluster installed in a namespace\n# called *snappy*. The PVCs used by SnappyData are prefixed with 'snappy-disk-claim-'.\n\n$ kubectl get  pvc --namespace snappy\nNAME                                     STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nsnappy-disk-claim-snappydata-leader-0    Bound     pvc-17cf9834-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\nsnappy-disk-claim-snappydata-locator-0   Bound     pvc-17d75411-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\nsnappy-disk-claim-snappydata-server-0    Bound     pvc-17de4f1a-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\nsnappy-disk-claim-snappydata-server-1    Bound     pvc-226d778d-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\n\n# To view logs for server-0 and server-1, use PVCs 'snappy-disk-claim-snappydata-server-0' and snappy-disk-claim-snappydata-server-1'\n\n$ ./utils/snappy-debug-pod.sh --pvc snappy-disk-claim-snappydata-server-0,snappy-disk-claim-snappydata-server-1 --namespace snappy\nVolume for snappy-disk-claim-snappydata-server-0 will be mounted on /data0\nVolume for snappy-disk-claim-snappydata-server-1 will be mounted on /data1\nLaunching the POD\nIf you don't see a command prompt, try pressing enter.\nbash-4.1# \n\n\n\n\nIn the above example, the second command opens a session with bash prompt for the pod on which the volumes corresponding to the mentioned PVCs are mounted on paths such as \n/data0\n, \n/data1\n and so on.\n\n\nYou can then examine the logs in these mounted paths. For example:\n\n\nbash-4.1# ls /data1\nlost+found  members.txt  snappydata-server-1\nbash-4.1# ls /data0\nlost+found  members.txt  snappydata-server-0\nbash-4.1# ls /data0/snappydata-server-0/\nbash-4.1# ls /data0/snappydata-server-0/*.*log\n\n\n\n\n \n\n\nConfiguring the Log Level\n\n\nYou can provide a \nlog4j.properties\n file while installing the SnappyData Helm chart. A template file \nlog4j.properties.template\n is provided in the \ncharts/snappydata/conf/\n directory. This template file can be renamed and used to configure log level as shown in the following example:\n\n\n$ cd charts/snappydata/conf/\n# copy the template file and edit it to configure log level\n$ cp log4j.properties.template log4j.properties\n\n\n\n\nWhen SnappyData chart is installed, the \nlog4.properties\n file  will be used to configure the log level.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/kubernetes/#setting-up-cluster-on-kubernetes", 
            "text": "Kubernetes is an open source project designed for container orchestration. SnappyData can be deployed on Kubernetes.  The following sections are included in this topic:    Prerequisites    Getting Access to Kubernetes cluster    Deploying SnappyData Chart on Kubernetes    Setting up PKS Environment for Kubernetes    Interacting with SnappyData Cluster on Kubernetes    List of Configuration Parameters for SnappyData Chart    Kubernetes Obects Used in SnappyData Chart    Accesing Logs and Configuring Log Level", 
            "title": "Setting up Cluster on Kubernetes"
        }, 
        {
            "location": "/kubernetes/#prerequisites", 
            "text": "The following prerequisites must be met to deploy SnappyData on Kubernetes:    Kubernetes cluster  A running Kubernetes cluster of version 1.9 or higher. SnappyData has been tested on Google Container Engine(GKE) as well as on Pivotal Container Service (PKS). If Kubernetes cluster is not available, you can set it up as mentioned  here .    Helm tool  Helm tool must be deployed in the Kubernetes environment. Helm comprises of two parts, that is a client and a Tiller (Server portion of Helm) inside the kube-system namespace. Tiller runs inside the Kubernetes cluster and manages the deployment of charts or packages. You can follow the instructions  here  to deploy Helm in your Kubernetes enviroment.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/kubernetes/#getting-access-to-kubernetes-cluster", 
            "text": "If you would like to deploy Kubernetes on-premises, you can use any of the following options:", 
            "title": "Getting Access to Kubernetes Cluster"
        }, 
        {
            "location": "/kubernetes/#option-1-pks", 
            "text": "PKS on vSphere: Follow these  instructions    PKS on GCP: Follow these  instructions  Create a Kubernetes cluster using PKS CLI : After PKS is setup you will need to create a Kubernetes cluster as described  here", 
            "title": "Option 1 - PKS"
        }, 
        {
            "location": "/kubernetes/#option-2-google-cloud-platform-gcp", 
            "text": "Login to your Google account and go to the  Cloud console  to launch a GKE cluster.   Steps to perform after Kubernetes cluster is available:    If using PKS, you must install the PKS command line tool. See instructions  here .  Install  kubectl  on your local development machine and configure access to the kubernetes/PKS cluster. See instructions for  kubectl   here .   If you are using Google cloud, you will find instructions for setting up Google Cloud SDK ('gcloud') along with  kubectl   here .", 
            "title": "Option 2 - Google Cloud Platform (GCP)"
        }, 
        {
            "location": "/kubernetes/#deploying-snappydata-on-kubernetes", 
            "text": "SnappyData Helm  chart is used to deploy SnappyData on Kubernetes.  It uses Kubernetes  statefulsets  to launch the locator, lead, and server members.   To deploy SnappyData on Kubernetes:    Clone the  spark-on-k8s  repository and change to  charts  directory.  git clone https://github.com/SnappyDataInc/spark-on-k8s  cd spark-on-k8s/charts    Optionally, you can edit the  snappydata   values.yaml   file to change the default configurations in the SnappyData chart. Configurations can be specified in the respective attributes for locators, leaders, and servers in this file. Refer  List of Configuration Parameters for SnappyData Chart    Install the  snappydata  chart using the following command:  helm install --name snappydata --namespace snappy ./snappydata/  The above command installs the SnappyData chart in a namespace called  snappy  and displays the Kubernetes objects (service, statefulsets etc.) created by the chart on the console. \nBy default,  SnappyData Helm  chart deploys a SnappyData cluster which consists of one locator, one lead, two servers and services to access SnappyData endpoints.    You can monitor the Kubernetes UI dashboard to check the status of the components as it takes few minutes for all the servers to be online. To access the Kubernetes UI refer to the instructions  here .   SnappyData chart dynamically provisions volumes for servers, locators, and leads. These volumes and the data in it are retained even after the chart deployment is deleted.", 
            "title": "Deploying SnappyData on Kubernetes"
        }, 
        {
            "location": "/kubernetes/#interacting-with-snappydata-cluster-on-kubernetes", 
            "text": "You can interact with the SnappyData cluster on Kuberenetes in the same manner as you interact with a SnappyData cluster that runs locally or on-premise. All you require is the host IP address of the locator and the lead with their respective ports numbers.  To find the IP addresses and port numbers of the SnappyData processes, use command  kubectl get svc --namespace=snappy . \nIn the  output , three services namely  snappydata-leader-public ,  snappydata-locator-public  and  snappydata-server-public   of type  LoadBalancer  are seen which expose the endpoints for locator, lead, and server respectively. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use.  snappydata-leader-public  service exposes port  5050  for SnappyData Pulse and port  8090  to accept  SnappyData jobs .  snappydata-locator-public  service exposes port  1527  to accept  JDBC/ODBC connections .  You can do the following on the SnappyData cluster that is deployed on Kubernetes:    Access SnappyData Pulse    Connect SnappyData using JDBC Driver    Execute Queries    Submit a SnappyData Job    Stop SnappyData Cluster on Kubernetes", 
            "title": "Interacting with SnappyData Cluster on Kubernetes"
        }, 
        {
            "location": "/kubernetes/#accessing-snappydata-pulse", 
            "text": "The dashboards on the SnappyData Pulse can be accessed using  snappydata-leader-public  service. To view the dashboard, type the URL in the web browser in the format:  externalIp:5050 .\nReplace  externalip  with the external IP address of the  snappydata-leader-public  service.  To access SnappyData Pulse in Kubernetes:    Check the SnappyData services running in the Kubernetes cluster.  kubectl get svc --namespace=snappy  The output displays the external IP address of the  snappydata-leader-public  service as shown in the following image:     Type  externalIp:5050  in the browser. Here you must replace  externalip  with the external IP address of the  leader-public  service.  For example, 35.232.102.51:5050.", 
            "title": "Accessing SnappyData Pulse"
        }, 
        {
            "location": "/kubernetes/#connecting-snappydata-using-jdbc-driver", 
            "text": "For Kubernetes deployments, JDBC clients can connect to SnappyData cluster using the JDBC URL that is derived from the  snappydata-locator-public  service.  To connect to SnappyData using JDBC driver in Kubernetes:    Check the SnappyData services running in Kubernetes cluster.  kubectl get svc --namespace=snappy \nThe output displays the external IP address  of the  snappydata-locator-public  service and the port number for external connections as shown in the following image:    Use the external IP address and port of the  snappydata-locator-public  services to connect to SnappyData cluster using JDBC connections. For example, based on the above output, the JDBC URL to be used will be  jdbc:snappydata://104.198.47.162:1527/    You can refer to  SnappyData documentation  for an example of JDBC program and for instructions on how to obtain JDBC driver using Maven/SBT co-ordinates.", 
            "title": "Connecting SnappyData Using JDBC Driver"
        }, 
        {
            "location": "/kubernetes/#executing-queries-using-snappydata-shell", 
            "text": "You  can use SnappyData shell to connect to SnappyData and execute your queries. You can simply connect to one of the pods in the cluster and use the SnappyData Shell. Alternatively, you can download the SnappyData distribution from  SnappyData github releases . SnappyData shell need not run within the Kubernetes cluster.  To execute queries in Kubernetes deployment:    Check the SnappyData services running in the Kubernetes cluster.  kubectl get svc --namespace=snappy \nThe output displays the external IP address of the  snappydata-locator-public  services  and the port number for external connections as shown in the following image:    Launch SnappyData shell and then create tables and execute queries.  Following is an example of executing queries using SnappyData shell.    # Connect to snappy-shell\n bin/snappy\n snappy  connect client '104.198.47.162:1527';\n\n# Create tables and execute queries\n snappy  create table t1(col1 int, col2 int) using column;\n snappy  insert into t1 values(1, 1);\n 1 row inserted/updated/deleted", 
            "title": "Executing Queries Using SnappyData Shell"
        }, 
        {
            "location": "/kubernetes/#submitting-a-snappydata-job", 
            "text": "Refer to the  How Tos section  in SnappyData documentation to understand how to submit SnappyData jobs.\nHowever, for submitting a SnappyData job in Kubernetes deployment, you need to use the  snappydata-leader-public  service that exposes port  8090  to run the jobs.  To submit a SnappyData job in Kubernetes deployment:    Check the SnappyData services running in Kubernetes cluster.  kubectl get svc --namespace=snappy \nThe output displays the external IP address of  snappydata-leader-public  service which must be noted.     Change to SnappyData product directory.  cd $SNAPPY_HOME    Submit the job using the external IP of the  snappydata-leader-public  service and the port number  8090  in the  --lead  option.  Following is an example of submitting a SnappyData Job:    bin/snappy-job.sh submit\n--app-name CreatePartitionedRowTable\n  --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable\n  --app-jar examples/jars/quickstart.jar\n  --lead 35.232.102.51:8090", 
            "title": "Submitting a SnappyData Job"
        }, 
        {
            "location": "/kubernetes/#stopping-the-snappydata-cluster-on-kubernetes", 
            "text": "To stop the SnappyData cluster on Kubernetes, you must delete the  SnappyData Helm  chart using the  helm delete  command.  $ helm delete --purge snappydata  The dynamically provisioned volumes and the data in it is retained, even if the chart deployment is deleted.   Note  If the chart is deployed again with the same chart name and if the volume exists, then the existing volume is used instead of provisioning a new volume.", 
            "title": "Stopping the SnappyData Cluster on Kubernetes"
        }, 
        {
            "location": "/kubernetes/#list-of-configuration-parameters-for-snappydata-chart", 
            "text": "You can modify the  values.yaml   file to configure the SnappyData chart. The following table lists the configuration parameters available for this chart:     Parameter  Description  Default      image  Docker repo from which the SnappyData Docker image is pulled.  snappydatainc/snappydata    imageTag  Tag of the SnappyData Docker image that is pulled.     imagePullPolicy  Pull policy for the image.  IfNotPresent    locators.conf  List of the configuration options that is passed to the locators.     locators.resources  Resource configuration for the locator Pods. User can configure CPU/memory requests and limit the usage.  locators.requests.memory  is set to  1024Mi .    locators.persistence.storageClass  Storage class that is used while dynamically provisioning a volume.  Default value is not defined so  default  storage class for the cluster is chosen.    locators.persistence.accessMode  Access mode  that is used for the dynamically provisioned volume.  ReadWriteOnce    locators.persistence.size  Size of the dynamically provisioned volume.  10Gi    servers.replicaCount  Number of servers that are started in a SnappyData cluster.  2    servers.conf  List of the configuration options that are passed to the servers.     servers.resources  Resource configuration for the server Pods. You can configure CPU/memory requests and limit the usage.  servers.requests.memory  is set to  4096Mi    servers.persistence.storageClass  Storage class that is used while dynamically provisioning a volume.  Default value is not defined so  default  storage class for the cluster will be chosen.    servers.persistence.accessMode  Access mode  for the dynamically provisioned volume.  ReadWriteOnce    servers.persistence.size  Size of the dynamically provisioned volume.  10Gi    leaders.conf  List of configuration options that can be passed to the leaders.     leaders.resources  Resource configuration for the server pods. You can configure CPU/memory requests and limits the usage.  leaders.requests.memory  is set to  4096Mi    leaders.persistence.storageClass  Storage class that is used while dynamically provisioning a volume.  Default value is not defined so  default  storage class for the cluster will be chosen.    leaders.persistence.accessMode  Access mode  for the dynamically provisioned volume.  ReadWriteOnce    leaders.persistence.size  Size of the dynamically provisioned volume.  10Gi     The following sample shows the configuration used to start four servers each with a heap size of 2048 MB:  servers:\n  replicaCount: 4\n  ## config options for servers\n  conf:  -heap-size=2048m   You can specify SnappyData  configuration parameters  in the  servers.conf ,  locators.conf , and  leaders.conf  attributes for servers, locators, and leaders respectively.", 
            "title": "List of Configuration Parameters for SnappyData Chart"
        }, 
        {
            "location": "/kubernetes/#kubernetes-obects-used-in-snappydata-chart", 
            "text": "This section provides details about the following Kubernetes objects that are used in SnappyData Chart:    Statefulsets for Servers, Leaders, and Locators    Services that Expose External Endpoints    Persistent Volumes", 
            "title": "Kubernetes Obects Used in SnappyData Chart"
        }, 
        {
            "location": "/kubernetes/#statefulsets-for-servers-leaders-and-locators", 
            "text": "Kubernetes statefulsets  are used to manage stateful applications. Statefulsets provide many benefits such as stable and unique network identifiers, stable persistent storage, ordered deployment and scaling, graceful deletion, and rolling updates.\nSnappyData Helm chart deploys statefulsets for servers, leaders, and locators. By default the chart deploys two data servers, one locator, and one leader. Upon deletion of the Helm deployment, each pod gracefully terminates the SnappyData process that is running on it.", 
            "title": "Statefulsets for Servers, Leaders, and Locators"
        }, 
        {
            "location": "/kubernetes/#services-that-expose-external-endpoints", 
            "text": "SnappyData Helm chart creates services to allow you to make JDBC connections, execute Spark jobs, and access\nSnappyData Pulse etc.  Services of the type LoadBalancer have external IP address assigned and can be used to connect from outside of Kubernetes cluster.\nTo check the service created for SnappyData deployment, use command  kubectl get svc --namespace=snappy . The following output is displayed:     NAME                        TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                                        AGE\nsnappydata-leader           ClusterIP      None             none            5050/TCP                                       5m\nsnappydata-leader-public    LoadBalancer   10.51.255.175   35.232.102.51    5050:31964/TCP,8090:32700/TCP,3768:32575/TCP   5m\nsnappydata-locator          ClusterIP      None             none            10334/TCP,1527/TCP                             5m\nsnappydata-locator-public   LoadBalancer   10.51.241.224   104.198.47.162   1527:31957/TCP                                 5m\nsnappydata-server           ClusterIP      None             none            1527/TCP                                       5m\nsnappydata-server-public    LoadBalancer   10.51.248.27    35.232.16.4      1527:31853/TCP                                 5m  In the above output, three services namely  snappydata-leader-public ,  snappydata-locator-public  and  snappydata-server-public   of type  LoadBalancer  are created. These services have external IP addresses assigned and therefore can be accessed from outside Kubernetes. The remaining services that do not have external IP addresses are those that are created for internal use.  snappydata-leader-public  service exposes port  5050  for SnappyData Pulse and port  8090  to accept SnappyData jobs. snappydata-locator-public  service exposes port  1527  to accept JDBC connections.", 
            "title": "Services that Expose External Endpoints"
        }, 
        {
            "location": "/kubernetes/#persistent-volumes", 
            "text": "A pod in a SnappyData deployment has a persistent volume mounted on it. This volume is dynamically provisioned and is used\nto store data directory for SnappyData. On each pod, the persistent volume is mounted on path  /opt/snappydata/work . These volumes and the data in it is retained even if the chart deployment is deleted.", 
            "title": "Persistent Volumes"
        }, 
        {
            "location": "/kubernetes/#accessing-logs", 
            "text": "You can access the logs when the  SnappyData cluster is running  as well as when the  SnappyData cluster is not running .", 
            "title": "Accessing Logs"
        }, 
        {
            "location": "/kubernetes/#accessing-logs-when-snappydata-cluster-is-running", 
            "text": "When a SnappyData cluster is running, you can open a session for a pod using  kubectl  command and then view the logs.\nThe following example shows how to access logs of  snappydata-server-0 :  # Connect to snappydata-server-0 pod and open a shell.\n$ kubectl exec -it snappydata-server-0 --namespace snappy -- /bin/bash\n\n# Switch to Snappydata work directory and view the logs.\n$ cd /opt/snappydata/work\n$ ls", 
            "title": "Accessing Logs When SnappyData Cluster is Running"
        }, 
        {
            "location": "/kubernetes/#accessing-logs-when-snappydata-cluster-is-not-running", 
            "text": "When SnappyData cluster is not running, you can access the volumes used in SnappyData with a utility script  snappy-debug-pod.sh  located in the  utils  directory of  Spark on k8s  repository.\nThis script launches a pod in the Kubernetes cluster with persistent volumes, specified via  --pvc  option, mounted on it and then returns a shell prompt. Volumes are mounted on the path starting with  /data0 (volume1 on /data0 and so on) .   In the following example, the names of the persistent volume claims used by the cluster are retrieved and passed to the  snappy-debug-pod.sh  script to be mounted on the pod.  # Get the names of persistent volume claims used by SnappyData cluster installed in a namespace\n# called *snappy*. The PVCs used by SnappyData are prefixed with 'snappy-disk-claim-'.\n\n$ kubectl get  pvc --namespace snappy\nNAME                                     STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nsnappy-disk-claim-snappydata-leader-0    Bound     pvc-17cf9834-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\nsnappy-disk-claim-snappydata-locator-0   Bound     pvc-17d75411-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\nsnappy-disk-claim-snappydata-server-0    Bound     pvc-17de4f1a-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\nsnappy-disk-claim-snappydata-server-1    Bound     pvc-226d778d-68c3-11e8-ab38-42010a8001a3   10Gi       RWO            standard       50d\n\n# To view logs for server-0 and server-1, use PVCs 'snappy-disk-claim-snappydata-server-0' and snappy-disk-claim-snappydata-server-1'\n\n$ ./utils/snappy-debug-pod.sh --pvc snappy-disk-claim-snappydata-server-0,snappy-disk-claim-snappydata-server-1 --namespace snappy\nVolume for snappy-disk-claim-snappydata-server-0 will be mounted on /data0\nVolume for snappy-disk-claim-snappydata-server-1 will be mounted on /data1\nLaunching the POD\nIf you don't see a command prompt, try pressing enter.\nbash-4.1#   In the above example, the second command opens a session with bash prompt for the pod on which the volumes corresponding to the mentioned PVCs are mounted on paths such as  /data0 ,  /data1  and so on.  You can then examine the logs in these mounted paths. For example:  bash-4.1# ls /data1\nlost+found  members.txt  snappydata-server-1\nbash-4.1# ls /data0\nlost+found  members.txt  snappydata-server-0\nbash-4.1# ls /data0/snappydata-server-0/\nbash-4.1# ls /data0/snappydata-server-0/*.*log", 
            "title": "Accessing Logs When SnappyData Cluster is not Running"
        }, 
        {
            "location": "/kubernetes/#configuring-the-log-level", 
            "text": "You can provide a  log4j.properties  file while installing the SnappyData Helm chart. A template file  log4j.properties.template  is provided in the  charts/snappydata/conf/  directory. This template file can be renamed and used to configure log level as shown in the following example:  $ cd charts/snappydata/conf/\n# copy the template file and edit it to configure log level\n$ cp log4j.properties.template log4j.properties  When SnappyData chart is installed, the  log4.properties  file  will be used to configure the log level.", 
            "title": "Configuring the Log Level"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/", 
            "text": "Getting Started with Docker Image\n\n\nSnappyData comes with a pre-configured Docker image. It has binaries for SnappyData, which enables you to try the quick start program and more, with SnappyData.\n\n\nThis section assumes you have already installed and configured Docker properly. Refer to \nDocker documentation\n for more details.\n\n\nVerify that Docker is Installed\n\n\nIn the command prompt run the command:\n\n\n$ docker run hello-world\n\n\n\n\n\n\n\nNote\n\n\nEnsure that the Docker containers have access to at least 4GB of RAM on your machine.\n\n\n\n\nGet the Docker Image\n\n\nIn the command prompt, type the following command to get the Docker image. This starts the container and takes you to the Spark shell.\n\n\n$  docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell\n\n\n\n\nThe latest image files start downloading to your local machine. Depending on your network connection, it may take some time. \n\n\nOnce you have launched the Spark shell, in the \n$ scala\n prompt, follow the steps explained \nhere\n.\n\n\nFor more details about SnappyData Docker image see \nSnappy Cloud Tools\n.", 
            "title": "Docker"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/#getting-started-with-docker-image", 
            "text": "SnappyData comes with a pre-configured Docker image. It has binaries for SnappyData, which enables you to try the quick start program and more, with SnappyData.  This section assumes you have already installed and configured Docker properly. Refer to  Docker documentation  for more details.", 
            "title": "Getting Started with Docker Image"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/#verify-that-docker-is-installed", 
            "text": "In the command prompt run the command:  $ docker run hello-world   Note  Ensure that the Docker containers have access to at least 4GB of RAM on your machine.", 
            "title": "Verify that Docker is Installed"
        }, 
        {
            "location": "/quickstart/getting_started_with_docker_image/#get-the-docker-image", 
            "text": "In the command prompt, type the following command to get the Docker image. This starts the container and takes you to the Spark shell.  $  docker run -it -p 5050:5050 snappydatainc/snappydata bin/spark-shell  The latest image files start downloading to your local machine. Depending on your network connection, it may take some time.   Once you have launched the Spark shell, in the  $ scala  prompt, follow the steps explained  here .  For more details about SnappyData Docker image see  Snappy Cloud Tools .", 
            "title": "Get the Docker Image"
        }, 
        {
            "location": "/install/building_from_source/", 
            "text": "Building from Source\n\n\n\n\nNote\n\n\nBuilding SnappyData requires JDK 8 installation (\nOracle Java SE\n).\n\n\n\n\nBuild all Components of SnappyData\n\n\nLatest release branch\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git -b branch-\nrelease-version\n --recursive\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nMaster\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git --recursive\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nThe product is in \nbuild-artifacts/scala-2.11/snappy\n\n\nBuild only the Top-level Components\n\n\nUse this option if you want to build only the top-level SnappyData project and pull in jars for other projects (spark, store, spark-jobserver):\n\n\nLatest release branch\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git -b branch-\nrelease-version\n\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nMaster\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nRepository Layout\n\n\n\n\n\n\ncore\n - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between \nspark\n and \nstore\n (GemFireXD). For example, SnappyContext, row and column store, streaming additions etc.\n\n\n\n\n\n\ncluster\n - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc.\n\n\n\n\n\n\nThis component depends on \ncore\n and \nstore\n. The code in the \ncluster\n depends on the \ncore\n but not the other way round.\n\n\n\n\n\n\nspark\n - \nApache Spark\n code with SnappyData enhancements.\n\n\n\n\n\n\nstore\n - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch.\n\n\n\n\n\n\nspark-jobserver\n - Fork of \nspark-jobserver\n project with some additions to integrate with SnappyData.\n\n\n\n\n\n\nThe \nspark\n, \nstore\n, and \nspark-jobserver\n directories are required to be clones of the respective SnappyData repositories and are integrated into the top-level SnappyData project as git submodules. When working with submodules, updating the repositories follows the normal \ngit submodules\n. One can add some aliases in gitconfig to aid pull/push as follows:\n\n\n[alias]\n  spull = !git pull \n git submodule sync --recursive \n git submodule update --init --recursive\n  spush = push --recurse-submodules=on-demand\n\n\n\n\nThe above aliases can serve as useful shortcuts to pull and push all projects from top-level \nsnappydata\n repository.\n\n\nBuilding\n\n\nGradle is the build tool used for all the SnappyData projects. Changes to \nApache Spark\n and \nspark-jobserver\n forks include the addition of Gradle build scripts to allow building them independently as well as a sub-project of SnappyData. The only requirement for the build is a JDK 8 installation. The Gradle wrapper script downloads all the other build dependencies as required.\n\n\nIf you do not want to deal with sub-modules and only work on a SnappyData project, you can clone only the SnappyData repository (without the \n--recursive\n option) and the build pulls those SnappyData project jar dependencies from Maven central.\n\n\nIf working on all the separate projects integrated inside the top-level SnappyData clone, the Gradle build recognizes the same and build those projects too and includes the same in the top-level product distribution jar. The \nspark\n and \nstore\n submodules can also be built and published independently.\n\n\nUseful build and test targets:\n\n\n./gradlew assemble      -  build all the sources\n./gradlew testClasses   -  build all the tests\n./gradlew product       -  build and place the product distribution\n                           (in build-artifacts/scala_2.11/snappy)\n./gradlew distTar       -  create a tar.gz archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew distZip       -  create a zip archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew buildAll      -  build all sources, tests, product, packages (all targets above)\n./gradlew checkAll      -  run testsuites of snappydata components\n./gradlew cleanAll      -  clean all build and test output\n./gradlew runQuickstart -  run the quickstart suite (the \nGetting Started\n section of docs)\n./gradlew precheckin    -  cleanAll, buildAll, scalaStyle, build docs,\n                           and run full snappydata testsuite including quickstart\n./gradlew precheckin -Pstore  -  cleanAll, buildAll, scalaStyle, build docs,\n                           run full snappydata testsuite including quickstart\n                           and also full SnappyData store testsuite\n./gradlew buildDtests   -  To build the Distributed tests\n\n\n\n\nThe default build directory is \nbuild-artifacts/scala-2.11\n for projects. An exception is \nstore\n project, where the default build directory is \nbuild-artifacts/\n; where; \nos\n is \nlinux\n on Linux systems, \nosx\n on Mac, \nwindows\n on Windows.\n\n\nThe usual Gradle test run targets (\ntest\n, \ncheck\n) work as expected for JUnit tests. Separate targets have been provided for running Scala tests (\nscalaTest\n) while the \ncheck\n target runs both the JUnit and ScalaTests. One can run a single Scala test suite class with \nsingleSuite\n option while running a single test within some suite works with the \n--tests\n option:\n\n\n ./gradlew snappy-core:scalaTest -PsingleSuite=**.ColumnTableTest  # run all tests in the class\n\n ./gradlew snappy-core:scalaTest \\\n\n    --tests \nTest the creation/dropping of table using SQL\n  # run a single test (use full name)\n\n\n\n\nRunning individual tests within some suite works using the \n--tests\n argument.\n\n\nAll ScalaTest build targets can be found by running the following command (case sensitive):\n\n./gradlew tasks --all | grep scalaTest\n\n\nSetting up IntelliJ IDEA with Gradle\n\n\nIntelliJ IDEA is the IDE commonly used by developers at SnappyData. Users who prefer to use Eclipse can try the Scala-IDE and Gradle support, however, it is recommended to use IntelliJ IDEA. \n\nSteps required for setting up SnappyData with all its components in IDEA are listed below.\n\n\nTo import into IntelliJ IDEA:\n\n\n\n\n\n\nUpgrade IntelliJ IDEA to at least version 2016.x, preferably 2018.x or more, including the latest Scala plug-in. Older versions have trouble dealing with scala code particularly some of the code in Spark. Newer versions have trouble running tests with gradle import, since they do not honor the build output directory as set in gradle. Ensure JDK 8 is installed and IDEA can find it (either in PATH or via JAVA_HOME).\n\n\n\n\n\n\nIncrease the \nXmx\n to 2g or more (4g, if possible) in the \nIDEA global vmoptions\n (in product bin directory, files named \nidea64.vmoptions\n for 64-bit and \nidea.vmoptions\n for 32-bit).\n\n\n\n\n\n\nIf using Java 8 release 144 or later, also add \n-Djdk.util.zip.ensureTrailingSlash=false\n to the \nglobal vmoptions\n file to fix an \nIDEA issue\n.\n\n\n\n\n\n\nIncrease the available JVM heap size for IDEA. Open \nbin/idea64.vmoptions\n (assuming 64-bit JVM) and increase \n-Xmx\n option to be something like \n-Xmx2g\n for comfortable use.\n\n\n\n\n\n\nSelect \nImport Project\n, and then select the SnappyData directory. Use external Gradle import. Click \nNext\n in the following screen. Clear the \nCreate separate module per source set\n option, while other options can continue with the default. Click \nNext\n in the following screens.\n\n\n\n\nNote\n\n\n\n\n\n\nIgnore the \n\"Gradle location is unknown warning\"\n.\n\n\n\n\n\n\nEnsure that the JDK 8 installation has been selected.\n\n\n\n\n\n\nIgnore and dismiss the \n\"Unindexed remote Maven repositories found\"\n warning message if seen.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen import is completed, \n\n\n\n\n\n\nGo to \nFile\n Settings\n Editor\n Code Style\n Scala\n. Set the scheme as \nProject\n. \n\n\n\n\n\n\nIn the same window, select \nJava\n code style and set the scheme as \nProject\n. \n\n\n\n\n\n\nClick \nOK\n to apply and close the window. \n\n\n\n\n\n\nCopy \ncodeStyleSettings.xml\n located in the SnappyData top-level directory, to the \n.idea\n directory created by IDEA. \n\n\n\n\n\n\nVerify that the settings are now applied in \nFile\n Settings\n Editor\n Code Style\n Java\n which should display indent as 2 and continuation indent as 4 (same as Scala).\n\n\n\n\n\n\n\n\n\n\nIf the Gradle tab is not visible immediately, then select it from option available at the bottom-left of IDE. Click on that window list icon for the tabs to be displayed permanently.\n\n\n\n\n\n\nGenerate Apache Avro and SnappyData required sources by expanding: \nsnappydata_2.11\n Tasks\n other\n. Right-click on \ngenerateSources\n and run it. The \nRun\n option may not be available if indexing is still in progress, wait for indexing to complete, and then try again. \n The first run may take some time to complete, as it downloads the jar files and other required files. This step has to be done the first time, or if \n./gradlew clean\n has been run, or if you have made changes to \njavacc/avro/messages.xml\n source files.\n\n\n\n\n\n\nIf you get unexpected \nDatabase not found\n or \nNullPointerException\n errors in SnappyData-store/GemFireXD layer, run the \ngenerateSources\n target (Gradle tab) again.\n\n\n\n\n\n\nIf you get \nNullPointerException\n error when reading the \nspark-version-info.properties\n file, right-click and run the \ncopyResourcesAll\n target from \nsnappydata_2.11\n Tasks\n other\n (Gradle tab) to copy the required resources.\n\n\n\n\n\n\nIncrease the compiler heap sizes or else the build can take a long time to complete, especially with integrated \nspark\n and \nstore\n. In \nFile\n Settings\n Build, Execution, Deployment\n Compiler\n option increase the \nBuild process heap size\n to 1536 or 2048. Similarly, in \nLanguages \n Frameworks\n Scala Compiler Server\n option, increase the JVM maximum heap size to 1536 or 2048.\n\n\n\n\n\n\nTest the full build.\n\n\n\n\n\n\nFor JUnit tests configuration also append \n/build-artifacts\n to the working directory. That is, open \nRun\n Edit Configurations\n, expand \nDefaults\n and select \nJUnit\n, the working directory should be \n\\$MODULE_DIR\\$/build-artifacts\n. Likewise, append \nbuild-artifacts\n to the working directory for ScalaTest. Without this, all intermediate log and other files pollute the source tree and will have to be cleaned manually.\n\n\n\n\n\n\nIf you see the following error while building the project, open module settings, select the module \nsnappy-cluster_2.11\n, go to its \nDependencies\n tab and ensure that \nsnappy-spark-unsafe_2.11\n comes before \nspark-unsafe\n or just find \nsnappy-spark-unsafe_2.11\n and move it to the top.\n\n\n\n\n\n\nError:(236, 18) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String\n    if (source.getByte(i) == first \n matchAt(source, target, i)) return true\n    Error:(233, 24) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String\n    val first = target.getByte(0)\n\n\n\n\n\nEven with the above, running unit tests in IDEA may result in more runtime errors due to unexpected \nslf4j\n versions. A more comprehensive way to correct, both the compilation and unit test problems in IDEA, is to update the snappy-cluster or for whichever module unit tests are to be run and have the \nTEST\n imports at the end. \n\n\nThe easiest way to do that is to close IDEA, open the module IML file (\n.idea/modules/cluster/snappy-cluster_2.11.iml\n in this case) in an editor. Search for \nscope=\"TEST\"\n and move all those lines to the bottom just before \n/component\n close tag.\n\n\nRunning a ScalaTest/JUnit\n\n\nRunning Scala/JUnit tests from IntelliJ IDEA is straightforward.\n\n\n\n\n\n\nWhen selecting a run configuration for JUnit/ScalaTest, avoid selecting the Gradle one (green round icon) otherwise, an external Gradle process is launched that can start building the project again is not cleanly integrated with IDEA. Use the normal JUnit (red+green arrows icon) or ScalaTest (JUnit like with red overlay).\n\n\n\n\n\n\nFor JUnit tests, ensure that the working directory is the top-level \n\\$MODULE_DIR\\$/build-artifacts\n as mentioned earlier. Otherwise, many SnappyData-store tests fail to find the resource files required in tests. They also pollute the files, so when launched, this allows those to go into \nbuild-artifacts\n that are easier to clean. For that reason, it is preferable to do the same for ScalaTests.\n\n\n\n\n\n\nSome of the tests use data files from the \ntests-common\n directory. For such tests, run the Gradle task \nsnappydata_2.11\n Tasks\n other\n copyResourcesAll\n to copy the resources in build area where IDEA runs can find it.", 
            "title": "Building from Source"
        }, 
        {
            "location": "/install/building_from_source/#building-from-source", 
            "text": "Note  Building SnappyData requires JDK 8 installation ( Oracle Java SE ).", 
            "title": "Building from Source"
        }, 
        {
            "location": "/install/building_from_source/#build-all-components-of-snappydata", 
            "text": "Latest release branch   git clone https://github.com/SnappyDataInc/snappydata.git -b branch- release-version  --recursive  cd snappydata  ./gradlew product  Master   git clone https://github.com/SnappyDataInc/snappydata.git --recursive  cd snappydata  ./gradlew product  The product is in  build-artifacts/scala-2.11/snappy", 
            "title": "Build all Components of SnappyData"
        }, 
        {
            "location": "/install/building_from_source/#build-only-the-top-level-components", 
            "text": "Use this option if you want to build only the top-level SnappyData project and pull in jars for other projects (spark, store, spark-jobserver):  Latest release branch   git clone https://github.com/SnappyDataInc/snappydata.git -b branch- release-version   cd snappydata  ./gradlew product  Master   git clone https://github.com/SnappyDataInc/snappydata.git  cd snappydata  ./gradlew product", 
            "title": "Build only the Top-level Components"
        }, 
        {
            "location": "/install/building_from_source/#repository-layout", 
            "text": "core  - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between  spark  and  store  (GemFireXD). For example, SnappyContext, row and column store, streaming additions etc.    cluster  - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc.    This component depends on  core  and  store . The code in the  cluster  depends on the  core  but not the other way round.    spark  -  Apache Spark  code with SnappyData enhancements.    store  - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch.    spark-jobserver  - Fork of  spark-jobserver  project with some additions to integrate with SnappyData.    The  spark ,  store , and  spark-jobserver  directories are required to be clones of the respective SnappyData repositories and are integrated into the top-level SnappyData project as git submodules. When working with submodules, updating the repositories follows the normal  git submodules . One can add some aliases in gitconfig to aid pull/push as follows:  [alias]\n  spull = !git pull   git submodule sync --recursive   git submodule update --init --recursive\n  spush = push --recurse-submodules=on-demand  The above aliases can serve as useful shortcuts to pull and push all projects from top-level  snappydata  repository.", 
            "title": "Repository Layout"
        }, 
        {
            "location": "/install/building_from_source/#building", 
            "text": "Gradle is the build tool used for all the SnappyData projects. Changes to  Apache Spark  and  spark-jobserver  forks include the addition of Gradle build scripts to allow building them independently as well as a sub-project of SnappyData. The only requirement for the build is a JDK 8 installation. The Gradle wrapper script downloads all the other build dependencies as required.  If you do not want to deal with sub-modules and only work on a SnappyData project, you can clone only the SnappyData repository (without the  --recursive  option) and the build pulls those SnappyData project jar dependencies from Maven central.  If working on all the separate projects integrated inside the top-level SnappyData clone, the Gradle build recognizes the same and build those projects too and includes the same in the top-level product distribution jar. The  spark  and  store  submodules can also be built and published independently.  Useful build and test targets:  ./gradlew assemble      -  build all the sources\n./gradlew testClasses   -  build all the tests\n./gradlew product       -  build and place the product distribution\n                           (in build-artifacts/scala_2.11/snappy)\n./gradlew distTar       -  create a tar.gz archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew distZip       -  create a zip archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew buildAll      -  build all sources, tests, product, packages (all targets above)\n./gradlew checkAll      -  run testsuites of snappydata components\n./gradlew cleanAll      -  clean all build and test output\n./gradlew runQuickstart -  run the quickstart suite (the  Getting Started  section of docs)\n./gradlew precheckin    -  cleanAll, buildAll, scalaStyle, build docs,\n                           and run full snappydata testsuite including quickstart\n./gradlew precheckin -Pstore  -  cleanAll, buildAll, scalaStyle, build docs,\n                           run full snappydata testsuite including quickstart\n                           and also full SnappyData store testsuite\n./gradlew buildDtests   -  To build the Distributed tests  The default build directory is  build-artifacts/scala-2.11  for projects. An exception is  store  project, where the default build directory is  build-artifacts/ ; where;  os  is  linux  on Linux systems,  osx  on Mac,  windows  on Windows.  The usual Gradle test run targets ( test ,  check ) work as expected for JUnit tests. Separate targets have been provided for running Scala tests ( scalaTest ) while the  check  target runs both the JUnit and ScalaTests. One can run a single Scala test suite class with  singleSuite  option while running a single test within some suite works with the  --tests  option:   ./gradlew snappy-core:scalaTest -PsingleSuite=**.ColumnTableTest  # run all tests in the class  ./gradlew snappy-core:scalaTest \\     --tests  Test the creation/dropping of table using SQL   # run a single test (use full name)  Running individual tests within some suite works using the  --tests  argument.  All ScalaTest build targets can be found by running the following command (case sensitive): ./gradlew tasks --all | grep scalaTest", 
            "title": "Building"
        }, 
        {
            "location": "/install/building_from_source/#setting-up-intellij-idea-with-gradle", 
            "text": "IntelliJ IDEA is the IDE commonly used by developers at SnappyData. Users who prefer to use Eclipse can try the Scala-IDE and Gradle support, however, it is recommended to use IntelliJ IDEA.  \nSteps required for setting up SnappyData with all its components in IDEA are listed below.  To import into IntelliJ IDEA:    Upgrade IntelliJ IDEA to at least version 2016.x, preferably 2018.x or more, including the latest Scala plug-in. Older versions have trouble dealing with scala code particularly some of the code in Spark. Newer versions have trouble running tests with gradle import, since they do not honor the build output directory as set in gradle. Ensure JDK 8 is installed and IDEA can find it (either in PATH or via JAVA_HOME).    Increase the  Xmx  to 2g or more (4g, if possible) in the  IDEA global vmoptions  (in product bin directory, files named  idea64.vmoptions  for 64-bit and  idea.vmoptions  for 32-bit).    If using Java 8 release 144 or later, also add  -Djdk.util.zip.ensureTrailingSlash=false  to the  global vmoptions  file to fix an  IDEA issue .    Increase the available JVM heap size for IDEA. Open  bin/idea64.vmoptions  (assuming 64-bit JVM) and increase  -Xmx  option to be something like  -Xmx2g  for comfortable use.    Select  Import Project , and then select the SnappyData directory. Use external Gradle import. Click  Next  in the following screen. Clear the  Create separate module per source set  option, while other options can continue with the default. Click  Next  in the following screens.   Note    Ignore the  \"Gradle location is unknown warning\" .    Ensure that the JDK 8 installation has been selected.    Ignore and dismiss the  \"Unindexed remote Maven repositories found\"  warning message if seen.       When import is completed,     Go to  File  Settings  Editor  Code Style  Scala . Set the scheme as  Project .     In the same window, select  Java  code style and set the scheme as  Project .     Click  OK  to apply and close the window.     Copy  codeStyleSettings.xml  located in the SnappyData top-level directory, to the  .idea  directory created by IDEA.     Verify that the settings are now applied in  File  Settings  Editor  Code Style  Java  which should display indent as 2 and continuation indent as 4 (same as Scala).      If the Gradle tab is not visible immediately, then select it from option available at the bottom-left of IDE. Click on that window list icon for the tabs to be displayed permanently.    Generate Apache Avro and SnappyData required sources by expanding:  snappydata_2.11  Tasks  other . Right-click on  generateSources  and run it. The  Run  option may not be available if indexing is still in progress, wait for indexing to complete, and then try again.   The first run may take some time to complete, as it downloads the jar files and other required files. This step has to be done the first time, or if  ./gradlew clean  has been run, or if you have made changes to  javacc/avro/messages.xml  source files.    If you get unexpected  Database not found  or  NullPointerException  errors in SnappyData-store/GemFireXD layer, run the  generateSources  target (Gradle tab) again.    If you get  NullPointerException  error when reading the  spark-version-info.properties  file, right-click and run the  copyResourcesAll  target from  snappydata_2.11  Tasks  other  (Gradle tab) to copy the required resources.    Increase the compiler heap sizes or else the build can take a long time to complete, especially with integrated  spark  and  store . In  File  Settings  Build, Execution, Deployment  Compiler  option increase the  Build process heap size  to 1536 or 2048. Similarly, in  Languages   Frameworks  Scala Compiler Server  option, increase the JVM maximum heap size to 1536 or 2048.    Test the full build.    For JUnit tests configuration also append  /build-artifacts  to the working directory. That is, open  Run  Edit Configurations , expand  Defaults  and select  JUnit , the working directory should be  \\$MODULE_DIR\\$/build-artifacts . Likewise, append  build-artifacts  to the working directory for ScalaTest. Without this, all intermediate log and other files pollute the source tree and will have to be cleaned manually.    If you see the following error while building the project, open module settings, select the module  snappy-cluster_2.11 , go to its  Dependencies  tab and ensure that  snappy-spark-unsafe_2.11  comes before  spark-unsafe  or just find  snappy-spark-unsafe_2.11  and move it to the top.    Error:(236, 18) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String\n    if (source.getByte(i) == first   matchAt(source, target, i)) return true\n    Error:(233, 24) value getByte is not a member of org.apache.spark.unsafe.types.UTF8String\n    val first = target.getByte(0)  Even with the above, running unit tests in IDEA may result in more runtime errors due to unexpected  slf4j  versions. A more comprehensive way to correct, both the compilation and unit test problems in IDEA, is to update the snappy-cluster or for whichever module unit tests are to be run and have the  TEST  imports at the end.   The easiest way to do that is to close IDEA, open the module IML file ( .idea/modules/cluster/snappy-cluster_2.11.iml  in this case) in an editor. Search for  scope=\"TEST\"  and move all those lines to the bottom just before  /component  close tag.", 
            "title": "Setting up IntelliJ IDEA with Gradle"
        }, 
        {
            "location": "/install/building_from_source/#running-a-scalatestjunit", 
            "text": "Running Scala/JUnit tests from IntelliJ IDEA is straightforward.    When selecting a run configuration for JUnit/ScalaTest, avoid selecting the Gradle one (green round icon) otherwise, an external Gradle process is launched that can start building the project again is not cleanly integrated with IDEA. Use the normal JUnit (red+green arrows icon) or ScalaTest (JUnit like with red overlay).    For JUnit tests, ensure that the working directory is the top-level  \\$MODULE_DIR\\$/build-artifacts  as mentioned earlier. Otherwise, many SnappyData-store tests fail to find the resource files required in tests. They also pollute the files, so when launched, this allows those to go into  build-artifacts  that are easier to clean. For that reason, it is preferable to do the same for ScalaTests.    Some of the tests use data files from the  tests-common  directory. For such tests, run the Gradle task  snappydata_2.11  Tasks  other  copyResourcesAll  to copy the resources in build area where IDEA runs can find it.", 
            "title": "Running a ScalaTest/JUnit"
        }, 
        {
            "location": "/install/upgrade/", 
            "text": "Upgrade Instructions\n\n\nThis guide provides information for upgrading systems running an earlier version of SnappyData. We assume that you have SnappyData already installed, and you are upgrading to the latest version of SnappyData.\n\n\nBefore you begin the upgrade, ensure that you understand the new features and any specific requirements for that release.\n\n\nBefore You Upgrade\n\n\n\n\n\n\nConfirm that your system meets the hardware and software requirements described in \nSystem Requirements\n section.\n\n\n\n\n\n\nBackup the existing environment: \nCreate a backup of the locator, lead, and server configuration files that exist in the \nconf\n folder located in the SnappyData home directory.\n\n\n\n\n\n\nStop the cluster and verify that all members are stopped: You can shut down the cluster using the \nsbin/snappy-stop-all.sh\n command. \nTo ensure that all the members have been shut down correctly, use the \nsbin/snappy-status-all.sh\n command.\n\n\n\n\n\n\nCreate a \nbackup of the operational disk store files\n for all members in the distributed system.\n\n\n\n\n\n\nReinstall SnappyData: After you have stopped the cluster, \ninstall the latest version of SnappyData\n.\n\n\n\n\n\n\nReconfigure your cluster using the locator, lead, and server configuration files you backed up in step 1.\n\n\n\n\n\n\nTo ensure that the restore script (restore.sh) copies files back to their original locations, make sure that the disk files are available at the original location before restarting the cluster with the latest version of SnappyData.\n\n\n\n\n\n\nUpgrading SnappyData Version from 1.0.1 to 1.0.2\n\n\nThe following steps must be specifically followed to upgrade to SnappyData version 1.0.2: \n\n\n\n\n\n\nIn addition to any existing configuration options, add \n-J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\n property to the \nconf/locators\n files for each of the locator as shown:\n\n\nconf/locators\nlocator1 -dir=\npath1\n -peer-discovery-port=\nport1\n -locators=\nlocator2:port2\n -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\nlocator2 -dir=\npath2\n -peer-discovery-port=\nport2\n -locators= \nlocator1:port1\n -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\n\n\n\n\n\n\n\nStart the cluster\n and verify that cluster is up and running. Also verify that all the tables are present along with their data. \n\n\n\n\n\n\nIn the existing running cluster, start one more locator without the given system property and ensure to provide \n-locators=oldlocator:port\n property in the conf files for the locator as shown in the following example. In this example configuration below locator3 is added apart from locator1 and locator2. Also locator3 does not specify the property '-J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true`. \n\n\nlocator1 -dir=\npath1\n -peer-discovery-port=\nport1\n -locators=\nlocator2:port2\n -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\nlocator2 -dir=\npath2\n -peer-discovery-port=\nport2\n -locators= \nlocator1:port1\n -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\nlocator3 -dir=\npath3\n -peer-discovery-port=\nport3\n -locators= \nlocator1:port1\n ,\nlocator2:port2\n\n\nNow start the cluster:\n./sbin/snappy-start-all.sh\n\n\n\n\n\n\n\nShutdown the old locators that were started with the system property, using the commands as shown:\n\n\n./bin/snappy locator stop -dir=-dir=\npath1\n\n./bin/snappy locator stop -dir=-dir=\npath2\n\n\n\n\n\n\n\n\nRemove the directory contents from the stopped locators and start those locators without this property:\n \n-J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\n as shown:\n\n\nRemove the old locator directory contents as follows:\nrm -rf \npath1\n/*\nrm -rf \npath2\n/*\n\nEdit the conf/locators file as follows:\nlocator1 -dir=\npath1\n -peer-discovery-port=\nport1\n -locators=\nlocator2:port2\n,\nlocator3:port3\n\nlocator2 -dir=\npath2\n -peer-discovery-port=\nport2\n -locators=\nlocator1:port1\n,\nlocator3:port3\n\nlocator3 -dir=\npath3\n -peer-discovery-port=\nport3\n -locators=\nlocator1:port1\n,\nlocator2:port2\n\n\nNow start the cluster:\n./sbin/snappy-start-all.sh\n\n\n\n\n\n\n\nVerify the tables and data.\n\n\n\n\n\n\nUpgrading from SnappyData Version 1.0.0\n\n\nFor best performance, it is recommended that you recreate any large column tables after you upgrade from 1.0.0 to 1.0.1/1.0.2. The following two improvements provided in 1.0.1 take effect:\n\n\n\n\n\n\nCompression for on-disk and over the network data.\n\n\n\n\n\n\nSeparate disk-store for column delta store. This improves the compactor performance significantly for cases of frequent JDBC/ODBC inserts or small inserts where the delta store gets used frequently.\n\n\n\n\n\n\n\n\nNote\n\n\nEnsure that no operations are currently running on the system.\n\n\n\n\nThe following example demonstrates how you can re-create your column tables using a Parquet-based external table:\n\n\nsnappy\n create external table table1Parquet using parquet options (path '...') as select * from table1;\n\n\n\n\nsnappy\n drop table table1;\n\n\n\n\nsnappy\n create table table1 ...;\n\n\n\n\nsnappy\n insert into table1 as select * from table1Parquet;\n\n\n\n\nsnappy\n drop table table1Parquet;\n\n\n\n\n\n\nNote\n\n\n\n\n\u200b    Use a path for the Parquet file that has enough space to hold the table data. Once the re-import has completed successfully, make sure that the Parquet files are deleted explicitly.", 
            "title": "Upgrade Instructions"
        }, 
        {
            "location": "/install/upgrade/#upgrade-instructions", 
            "text": "This guide provides information for upgrading systems running an earlier version of SnappyData. We assume that you have SnappyData already installed, and you are upgrading to the latest version of SnappyData.  Before you begin the upgrade, ensure that you understand the new features and any specific requirements for that release.", 
            "title": "Upgrade Instructions"
        }, 
        {
            "location": "/install/upgrade/#before-you-upgrade", 
            "text": "Confirm that your system meets the hardware and software requirements described in  System Requirements  section.    Backup the existing environment:  Create a backup of the locator, lead, and server configuration files that exist in the  conf  folder located in the SnappyData home directory.    Stop the cluster and verify that all members are stopped: You can shut down the cluster using the  sbin/snappy-stop-all.sh  command.  To ensure that all the members have been shut down correctly, use the  sbin/snappy-status-all.sh  command.    Create a  backup of the operational disk store files  for all members in the distributed system.    Reinstall SnappyData: After you have stopped the cluster,  install the latest version of SnappyData .    Reconfigure your cluster using the locator, lead, and server configuration files you backed up in step 1.    To ensure that the restore script (restore.sh) copies files back to their original locations, make sure that the disk files are available at the original location before restarting the cluster with the latest version of SnappyData.", 
            "title": "Before You Upgrade"
        }, 
        {
            "location": "/install/upgrade/#upgrading-snappydata-version-from-101-to-102", 
            "text": "The following steps must be specifically followed to upgrade to SnappyData version 1.0.2:     In addition to any existing configuration options, add  -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true  property to the  conf/locators  files for each of the locator as shown:  conf/locators\nlocator1 -dir= path1  -peer-discovery-port= port1  -locators= locator2:port2  -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\nlocator2 -dir= path2  -peer-discovery-port= port2  -locators=  locator1:port1  -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true    Start the cluster  and verify that cluster is up and running. Also verify that all the tables are present along with their data.     In the existing running cluster, start one more locator without the given system property and ensure to provide  -locators=oldlocator:port  property in the conf files for the locator as shown in the following example. In this example configuration below locator3 is added apart from locator1 and locator2. Also locator3 does not specify the property '-J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true`.   locator1 -dir= path1  -peer-discovery-port= port1  -locators= locator2:port2  -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\nlocator2 -dir= path2  -peer-discovery-port= port2  -locators=  locator1:port1  -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true\nlocator3 -dir= path3  -peer-discovery-port= port3  -locators=  locator1:port1  , locator2:port2 \n\nNow start the cluster:\n./sbin/snappy-start-all.sh    Shutdown the old locators that were started with the system property, using the commands as shown:  ./bin/snappy locator stop -dir=-dir= path1 \n./bin/snappy locator stop -dir=-dir= path2     Remove the directory contents from the stopped locators and start those locators without this property:   -J-Dsnappydata.DISALLOW_METASTORE_ON_LOCATOR=true  as shown:  Remove the old locator directory contents as follows:\nrm -rf  path1 /*\nrm -rf  path2 /*\n\nEdit the conf/locators file as follows:\nlocator1 -dir= path1  -peer-discovery-port= port1  -locators= locator2:port2 , locator3:port3 \nlocator2 -dir= path2  -peer-discovery-port= port2  -locators= locator1:port1 , locator3:port3 \nlocator3 -dir= path3  -peer-discovery-port= port3  -locators= locator1:port1 , locator2:port2 \n\nNow start the cluster:\n./sbin/snappy-start-all.sh    Verify the tables and data.", 
            "title": "Upgrading SnappyData Version from 1.0.1 to 1.0.2"
        }, 
        {
            "location": "/install/upgrade/#upgrading-from-snappydata-version-100", 
            "text": "For best performance, it is recommended that you recreate any large column tables after you upgrade from 1.0.0 to 1.0.1/1.0.2. The following two improvements provided in 1.0.1 take effect:    Compression for on-disk and over the network data.    Separate disk-store for column delta store. This improves the compactor performance significantly for cases of frequent JDBC/ODBC inserts or small inserts where the delta store gets used frequently.     Note  Ensure that no operations are currently running on the system.   The following example demonstrates how you can re-create your column tables using a Parquet-based external table:  snappy  create external table table1Parquet using parquet options (path '...') as select * from table1;  snappy  drop table table1;  snappy  create table table1 ...;  snappy  insert into table1 as select * from table1Parquet;  snappy  drop table table1Parquet;   Note   \u200b    Use a path for the Parquet file that has enough space to hold the table data. Once the re-import has completed successfully, make sure that the Parquet files are deleted explicitly.", 
            "title": "Upgrading from SnappyData Version 1.0.0"
        }, 
        {
            "location": "/howto/", 
            "text": "Overview\n\n\nThis section introduces you to several common operations such as starting a cluster, working with tables (load, query, update), working with streams and running approximate queries.\n\n\nRunning the Examples:\n\nTopics in this section refer to source code examples that are shipped with the product. Instructions to run these examples can be found in the source code.\n\n\nSource code for these examples is located in the \nquickstart/src/main/scala/org/apache/spark/examples/snappydata\n and in \nquickstart/python\n directories of the SnappyData product distribution. \n\n\nYou can run the examples in any of the following ways:\n\n\n\n\n\n\nIn the Local Mode\n: By using the \nbin/run-example\n script (to run Scala examples) or by using the \nbin/spark-submit\n script (to run Python examples). These examples run colocated with Spark + SnappyData Store in the same JVM. \n\n\n\n\n\n\nAs a Job\n: Many of the Scala examples are also implemented as a SnappyData job. In this case, examples can be submitted as a job to a running SnappyData cluster. Refer to the \njobs\n section for details on how to run a job.\n\n\n\n\n\n\n\n\nNote\n\n\nSnappyData also supports Java API. Refer to the \ndocumentation\n for more details on Java API.\n\n\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nHow to Start a SnappyData Cluster\n\n\n\n\n\n\nHow to Check the Status of a SnappyData Cluster\n\n\n\n\n\n\nHow to Stop a SnappyData Cluster\n\n\n\n\n\n\nHow to Run a Spark Job inside the Cluster\n\n\n\n\n\n\nHow to Access SnappyData Store from an existing Spark Installation using Smart Connector\n\n\n\n\n\n\nHow to Use Snappy SQL shell (snappy-sql)\n\n\n\n\n\n\nHow to Create Row Tables and Run Queries\n\n\n\n\n\n\nHow to Create Column Tables and Run Queries\n\n\n\n\n\n\nHow to Load Data into SnappyData Tables\n\n\n\n\n\n\nHow to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc)\n\n\n\n\n\n\nHow to Perform a Colocated Join\n\n\n\n\n\n\nHow to Connect using JDBC Driver\n\n\n\n\n\n\nHow to Store and Query JSON Objects\n\n\n\n\n\n\nHow to Store and Query Objects\n\n\n\n\n\n\nHow to use Stream Processing with SnappyData\n\n\n\n\n\n\nHow to use Transactions Isolation Levels\n\n\n\n\n\n\nHow to use Synopsis Data Engine to Run Approximate Queries\n\n\n\n\n\n\nHow to use Python to Create Tables and Run Queries\n\n\n\n\n\n\nHow to connect using ODBC Driver\n\n\n\n\n\n\nHow to connect to the Cluster from External Clients\n\n\n\n\n\n\nHow to import data from a Hive Table into a SnappyData Table\n\n\n\n\n\n\nHow to Export and Restore table data to HDFS\n\n\n\n\n\n\nHow to use Apache Zeppelin with SnappyData\n\n\n\n\n\n\nHow to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster", 
            "title": "How Tos"
        }, 
        {
            "location": "/howto/#overview", 
            "text": "This section introduces you to several common operations such as starting a cluster, working with tables (load, query, update), working with streams and running approximate queries.  Running the Examples: \nTopics in this section refer to source code examples that are shipped with the product. Instructions to run these examples can be found in the source code.  Source code for these examples is located in the  quickstart/src/main/scala/org/apache/spark/examples/snappydata  and in  quickstart/python  directories of the SnappyData product distribution.   You can run the examples in any of the following ways:    In the Local Mode : By using the  bin/run-example  script (to run Scala examples) or by using the  bin/spark-submit  script (to run Python examples). These examples run colocated with Spark + SnappyData Store in the same JVM.     As a Job : Many of the Scala examples are also implemented as a SnappyData job. In this case, examples can be submitted as a job to a running SnappyData cluster. Refer to the  jobs  section for details on how to run a job.     Note  SnappyData also supports Java API. Refer to the  documentation  for more details on Java API.   The following topics are covered in this section:    How to Start a SnappyData Cluster    How to Check the Status of a SnappyData Cluster    How to Stop a SnappyData Cluster    How to Run a Spark Job inside the Cluster    How to Access SnappyData Store from an existing Spark Installation using Smart Connector    How to Use Snappy SQL shell (snappy-sql)    How to Create Row Tables and Run Queries    How to Create Column Tables and Run Queries    How to Load Data into SnappyData Tables    How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc)    How to Perform a Colocated Join    How to Connect using JDBC Driver    How to Store and Query JSON Objects    How to Store and Query Objects    How to use Stream Processing with SnappyData    How to use Transactions Isolation Levels    How to use Synopsis Data Engine to Run Approximate Queries    How to use Python to Create Tables and Run Queries    How to connect using ODBC Driver    How to connect to the Cluster from External Clients    How to import data from a Hive Table into a SnappyData Table    How to Export and Restore table data to HDFS    How to use Apache Zeppelin with SnappyData    How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster", 
            "title": "Overview"
        }, 
        {
            "location": "/howto/start_snappy_cluster/", 
            "text": "How to Start a SnappyData Cluster\n\n\nStarting SnappyData Cluster on a Single Machine\n\n\nIf you have \ndownloaded and extracted\n the SnappyData product distribution, navigate to the SnappyData product root directory.\n\n\nStart the Cluster\n: Run the \n./sbin/snappy-start-all.sh\n script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server.\n\n\n$ ./sbin/snappy-start-all.sh\n\n\n\n\nIt may take 30 seconds or more to bootstrap the entire cluster on your local machine.\n\n\nSample Output\n: The sample output for \nsnappy-start-all.sh\n is displayed as:\n\n\nStarting SnappyData Locator using peer discovery on: localhost[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user/snappyData/work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 9368 status: running\nStarting SnappyData Server using locators for peer discovery: user1-laptop[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user1/snappyData/work/localhost-server-1/snappyserver.log\nSnappyData Server pid: 9519 status: running\n  Distributed system now has 2 members.\n  Other members: localhost(9368:locator)\nv0\n:16944\nStarting SnappyData Leader using locators for peer discovery: user1-laptop[10334]\nLogs generated in /home/user1/snappyData/work/localhost-lead-1/snappyleader.log\nSnappyData Leader pid: 9699 status: running\n  Distributed system now has 3 members.\n  Other members: localhost(9368:locator)\nv0\n:16944, 192.168.63.1(9519:datastore)\nv1\n:46966\n\n\n\n\nStarting the SnappyData Cluster on Multiple Hosts\n\n\nTo start the cluster on multiple hosts:\n\n\n\n\n\n\nThe easiest way to run SnappyData on multiple nodes is to use a shared file system such as NFS on all the nodes.\n You can also extract the product distribution on each node of the cluster. If all nodes have NFS access, install SnappyData on any one of the nodes.\n\n\n\n\n\n\nCreate the configuration files using the templates provided in the \nconf\n folder. Copy the existing template files (\nservers.template\n, \nlocators.template\n and \nleads.template\n) and rename them to \nservers\n, \nlocators\n, \nleads\n.\n\n Edit the files to include the hostnames on which to start the server, locator, and lead. Refer to the \nconfiguration\n section for more information on properties.\n\n\n\n\n\n\nStart the cluster using \n./sbin/snappy-start-all.sh\n. SnappyData starts the cluster using SSH.\n\n\n\n\n\n\n\n\nNote\n\n\nIt is recommended that you set up passwordless SSH on all hosts in the cluster. Refer to the documentation for more details on \ninstallation\n and \ncluster configuration\n.\n\n\n\n\nStarting Individual Components\n\n\nInstead of starting SnappyData cluster using the \nsnappy-start-all.sh\n script, individual components can be started on a system locally using the following commands:\n\n\n\n\nTip\n\n\nAll \nconfiguration parameters\n are provided as command line arguments rather than reading from a configuration file.\n\n\n\n\nFor example, you can run any of the following commands depending on the individual component that you want to start.\n\n\n$ ./bin/snappy locator start  -dir=/node-a/locator1\n$ ./bin/snappy server start  -dir=/node-b/server1  -locators=localhost[10334] -heap-size=16g\n$ ./bin/snappy leader start  -dir=/node-c/lead1  -locators=localhost[10334] -spark.executor.cores=32\n\n\n\n\n\n\nNote\n\n\nThe path mentioned for \n-dir\n should exist. Otherwise, the command will fail with \nFileNotFoundException\n.", 
            "title": "How to Start a SnappyData Cluster"
        }, 
        {
            "location": "/howto/start_snappy_cluster/#how-to-start-a-snappydata-cluster", 
            "text": "", 
            "title": "How to Start a SnappyData Cluster"
        }, 
        {
            "location": "/howto/start_snappy_cluster/#starting-snappydata-cluster-on-a-single-machine", 
            "text": "If you have  downloaded and extracted  the SnappyData product distribution, navigate to the SnappyData product root directory.  Start the Cluster : Run the  ./sbin/snappy-start-all.sh  script to start the SnappyData cluster on your single machine using default settings. This starts a lead node, a locator, and a data server.  $ ./sbin/snappy-start-all.sh  It may take 30 seconds or more to bootstrap the entire cluster on your local machine.  Sample Output : The sample output for  snappy-start-all.sh  is displayed as:  Starting SnappyData Locator using peer discovery on: localhost[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user/snappyData/work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 9368 status: running\nStarting SnappyData Server using locators for peer discovery: user1-laptop[10334]\nStarting DRDA server for SnappyData at address localhost/127.0.0.1[1527]\nLogs generated in /home/user1/snappyData/work/localhost-server-1/snappyserver.log\nSnappyData Server pid: 9519 status: running\n  Distributed system now has 2 members.\n  Other members: localhost(9368:locator) v0 :16944\nStarting SnappyData Leader using locators for peer discovery: user1-laptop[10334]\nLogs generated in /home/user1/snappyData/work/localhost-lead-1/snappyleader.log\nSnappyData Leader pid: 9699 status: running\n  Distributed system now has 3 members.\n  Other members: localhost(9368:locator) v0 :16944, 192.168.63.1(9519:datastore) v1 :46966", 
            "title": "Starting SnappyData Cluster on a Single Machine"
        }, 
        {
            "location": "/howto/start_snappy_cluster/#starting-the-snappydata-cluster-on-multiple-hosts", 
            "text": "To start the cluster on multiple hosts:    The easiest way to run SnappyData on multiple nodes is to use a shared file system such as NFS on all the nodes.  You can also extract the product distribution on each node of the cluster. If all nodes have NFS access, install SnappyData on any one of the nodes.    Create the configuration files using the templates provided in the  conf  folder. Copy the existing template files ( servers.template ,  locators.template  and  leads.template ) and rename them to  servers ,  locators ,  leads .  Edit the files to include the hostnames on which to start the server, locator, and lead. Refer to the  configuration  section for more information on properties.    Start the cluster using  ./sbin/snappy-start-all.sh . SnappyData starts the cluster using SSH.     Note  It is recommended that you set up passwordless SSH on all hosts in the cluster. Refer to the documentation for more details on  installation  and  cluster configuration .", 
            "title": "Starting the SnappyData Cluster on Multiple Hosts"
        }, 
        {
            "location": "/howto/start_snappy_cluster/#starting-individual-components", 
            "text": "Instead of starting SnappyData cluster using the  snappy-start-all.sh  script, individual components can be started on a system locally using the following commands:   Tip  All  configuration parameters  are provided as command line arguments rather than reading from a configuration file.   For example, you can run any of the following commands depending on the individual component that you want to start.  $ ./bin/snappy locator start  -dir=/node-a/locator1\n$ ./bin/snappy server start  -dir=/node-b/server1  -locators=localhost[10334] -heap-size=16g\n$ ./bin/snappy leader start  -dir=/node-c/lead1  -locators=localhost[10334] -spark.executor.cores=32   Note  The path mentioned for  -dir  should exist. Otherwise, the command will fail with  FileNotFoundException .", 
            "title": "Starting Individual Components"
        }, 
        {
            "location": "/howto/check_status_cluster/", 
            "text": "How to Check the Status of the SnappyData Cluster\n\n\nYou can check the status of a running cluster using the following command:\n\n\n$ ./sbin/snappy-status-all.sh\nSnappyData Locator pid: 9748 status: running\nSnappyData Server pid: 9887 status: running\nSnappyData Leader pid: 10468 status: running\n\n\n\n\nYou can check the SnappyData UI by opening \nhttp://\nleadHostname\n:5050\n in your browser, where \nleadHostname\n is the host name of your lead node. Use \nSnappy SQL shell\n to connect to the cluster and perform various SQL operations.\n\n\nRelated Topics\n\n\n\n\nSnappyData Pulse", 
            "title": "How to Check the Status of a SnappyData Cluster"
        }, 
        {
            "location": "/howto/check_status_cluster/#how-to-check-the-status-of-the-snappydata-cluster", 
            "text": "You can check the status of a running cluster using the following command:  $ ./sbin/snappy-status-all.sh\nSnappyData Locator pid: 9748 status: running\nSnappyData Server pid: 9887 status: running\nSnappyData Leader pid: 10468 status: running  You can check the SnappyData UI by opening  http:// leadHostname :5050  in your browser, where  leadHostname  is the host name of your lead node. Use  Snappy SQL shell  to connect to the cluster and perform various SQL operations.  Related Topics   SnappyData Pulse", 
            "title": "How to Check the Status of the SnappyData Cluster"
        }, 
        {
            "location": "/howto/stop_snappy_cluster/", 
            "text": "How to Stop a SnappyData Cluster\n\n\nStopping the Cluster\n\n\nYou can stop the cluster using the \n./sbin/snappy-stop-all.sh\n command:\n\n\n$ ./sbin/snappy-stop-all.sh\nThe SnappyData Leader has stopped.\nThe SnappyData Server has stopped.\nThe SnappyData Locator has stopped.\n\n\n\n\n\n\nNote\n\n\nEnsure that all write operations on column table have finished execution when you stop a cluster, else it can lead to a partial write.\n\n\n\n\nStopping Individual Components\n\n\nInstead of stopping the SnappyData cluster using the \nsnappy-stop-all.sh\n script, individual components can be stopped on a system locally using the following commands:\n\n\n\n\nTip\n\n\nAll \nconfiguration parameters\n are provided as command line arguments rather than reading from a configuration file.\n\n\n\n\n$ ./bin/snappy locator stop -dir=/node-a/locator1\n$ ./bin/snappy server stop -dir=/node-b/server1\n$ ./bin/snappy leader stop -dir=/node-c/lead1", 
            "title": "How to Stop a SnappyData Cluster"
        }, 
        {
            "location": "/howto/stop_snappy_cluster/#how-to-stop-a-snappydata-cluster", 
            "text": "", 
            "title": "How to Stop a SnappyData Cluster"
        }, 
        {
            "location": "/howto/stop_snappy_cluster/#stopping-the-cluster", 
            "text": "You can stop the cluster using the  ./sbin/snappy-stop-all.sh  command:  $ ./sbin/snappy-stop-all.sh\nThe SnappyData Leader has stopped.\nThe SnappyData Server has stopped.\nThe SnappyData Locator has stopped.   Note  Ensure that all write operations on column table have finished execution when you stop a cluster, else it can lead to a partial write.", 
            "title": "Stopping the Cluster"
        }, 
        {
            "location": "/howto/stop_snappy_cluster/#stopping-individual-components", 
            "text": "Instead of stopping the SnappyData cluster using the  snappy-stop-all.sh  script, individual components can be stopped on a system locally using the following commands:   Tip  All  configuration parameters  are provided as command line arguments rather than reading from a configuration file.   $ ./bin/snappy locator stop -dir=/node-a/locator1\n$ ./bin/snappy server stop -dir=/node-b/server1\n$ ./bin/snappy leader stop -dir=/node-c/lead1", 
            "title": "Stopping Individual Components"
        }, 
        {
            "location": "/howto/run_spark_job_inside_cluster/", 
            "text": "How to Run Spark Job inside the Cluster\n\n\nSpark program that runs inside a SnappyData cluster is implemented as a SnappyData job.\n\n\nImplementing a Job\n: \nA SnappyData job is a class or object that implements SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. In the \nrunSnappyJob\n method of the job, you implement the logic for your Spark program using SnappySession object instance passed to it. You can perform all operations such as create a table, load data, execute queries using the SnappySession. \n\nAny of the Spark APIs can be invoked by a SnappyJob.\n\n\nclass CreatePartitionedRowTable extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappySession, jobConfig: Config): Any\n\n  /**\n  SnappyData calls this function to validate the job input and reject invalid job requests.\n  You can implement custom validations here, for example, validating the configuration parameters\n  **/\n  def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation\n}\n\n\n\n\nDependencies\n:\nTo compile your job, use the Maven/SBT dependencies for the latest released version of SnappyData.\n\n\nExample: Maven dependency\n:\n\n\n// https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\n\ndependency\n\n    \ngroupId\nio.snappydata\n/groupId\n\n    \nartifactId\nsnappydata-cluster_2.11\n/artifactId\n\n    \nversion\n1.0.2.1\n/version\n\n\n/dependency\n\n\n\n\n\nExample: SBT dependency\n:\n\n\n// https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies += \nio.snappydata\n % \nsnappydata-cluster_2.11\n % \n1.0.2.1\n\n\n\n\n\n\n\nNote\n\n\nIf your project fails while resolving the above dependency (ie. it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file. \n As a workaround, add the below code to the \nbuild.sbt\n:\n\n\n\n\nval workaround = {\n  sys.props += \npackaging.type\n -\n \njar\n\n  ()\n}\n\n\n\n\nFor more details, refer \nhttps://github.com/sbt/sbt/issues/3618\n.\n\n\nRunning the Job\n: \nOnce you create a jar file for SnappyData job, use the \n./bin/snappy-job.sh\n to submit the job in the SnappyData cluster, and then run the job. This is similar to \nspark-submit\n for any Spark application. \n\n\nFor example, to run the job implemented in \nCreatePartitionedRowTable.scala\n you can use the following command. The command submits the job and runs it as:\n\n\n # first change the directory to the SnappyData product directory\n $ cd $SNAPPY_HOME\n $ ./bin/snappy-job.sh submit\n    --app-name CreatePartitionedRowTable\n    --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable\n    --app-jar examples/jars/quickstart.jar\n    --lead localhost:8090\n\n\n\n\nIn the above command, \nquickstart.jar\n contains the program and is bundled in the product distribution and the \n--lead\n option specifies the host name of the lead node along with the port on which it accepts jobs (default 8090).\n\n\nOutput\n: It returns output similar to:\n\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  }\n}\n\n\n\n\nCheck Status\n: You can check the status of the job using the Job ID listed above:\n\n\n./bin/snappy-job.sh status --lead localhost:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n\n\n\nRefer to the \nBuilding SnappyData applications using Spark API\n section of the documentation for more details.", 
            "title": "How to Run Spark Job inside the Cluster"
        }, 
        {
            "location": "/howto/run_spark_job_inside_cluster/#how-to-run-spark-job-inside-the-cluster", 
            "text": "Spark program that runs inside a SnappyData cluster is implemented as a SnappyData job.  Implementing a Job : \nA SnappyData job is a class or object that implements SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. In the  runSnappyJob  method of the job, you implement the logic for your Spark program using SnappySession object instance passed to it. You can perform all operations such as create a table, load data, execute queries using the SnappySession.  \nAny of the Spark APIs can be invoked by a SnappyJob.  class CreatePartitionedRowTable extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappySession, jobConfig: Config): Any\n\n  /**\n  SnappyData calls this function to validate the job input and reject invalid job requests.\n  You can implement custom validations here, for example, validating the configuration parameters\n  **/\n  def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation\n}  Dependencies :\nTo compile your job, use the Maven/SBT dependencies for the latest released version of SnappyData.  Example: Maven dependency :  // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 dependency \n     groupId io.snappydata /groupId \n     artifactId snappydata-cluster_2.11 /artifactId \n     version 1.0.2.1 /version  /dependency   Example: SBT dependency :  // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies +=  io.snappydata  %  snappydata-cluster_2.11  %  1.0.2.1    Note  If your project fails while resolving the above dependency (ie. it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file.   As a workaround, add the below code to the  build.sbt :   val workaround = {\n  sys.props +=  packaging.type  -   jar \n  ()\n}  For more details, refer  https://github.com/sbt/sbt/issues/3618 .  Running the Job : \nOnce you create a jar file for SnappyData job, use the  ./bin/snappy-job.sh  to submit the job in the SnappyData cluster, and then run the job. This is similar to  spark-submit  for any Spark application.   For example, to run the job implemented in  CreatePartitionedRowTable.scala  you can use the following command. The command submits the job and runs it as:   # first change the directory to the SnappyData product directory\n $ cd $SNAPPY_HOME\n $ ./bin/snappy-job.sh submit\n    --app-name CreatePartitionedRowTable\n    --class org.apache.spark.examples.snappydata.CreatePartitionedRowTable\n    --app-jar examples/jars/quickstart.jar\n    --lead localhost:8090  In the above command,  quickstart.jar  contains the program and is bundled in the product distribution and the  --lead  option specifies the host name of the lead node along with the port on which it accepts jobs (default 8090).  Output : It returns output similar to:  {\n   status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  }\n}  Check Status : You can check the status of the job using the Job ID listed above:  ./bin/snappy-job.sh status --lead localhost:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48  Refer to the  Building SnappyData applications using Spark API  section of the documentation for more details.", 
            "title": "How to Run Spark Job inside the Cluster"
        }, 
        {
            "location": "/howto/spark_installation_using_smart_connector/", 
            "text": "How to Access SnappyData Store from an existing Spark Installation using Smart Connector\n\n\nSnappyData comes with a Smart Connector that enables Spark applications to work with the SnappyData cluster, from any compatible Spark cluster (you can use any distribution that is compatible with Apache Spark 2.1.1). The Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. This is similar to how Spark applications today work with stores like Cassandra, Redis, etc.\n\n\nFor more information on the various modes, refer to the \nSnappyData Smart Connector\n section of the documentation.\n\n\nCode Example\n\n\nThe code example for this mode is in \nSmartConnectorExample.scala\n\n\nConfigure a SnappySession\n:\n\nThe code below shows how to initialize a SparkSession. Here the property \nsnappydata.connection\n instructs the connector to acquire cluster connectivity, catalog metadata and register it locally in the Spark cluster. The values consists of \nlocator host\n and \nJDBC client port\n on which the locator listens for connections (default 1527).\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nSmartConnectorExample\n)\n    // It can be any master URL\n    .master(\nlocal[4]\n)\n    // snappydata.connection property enables the application to interact with SnappyData store\n    .config(\nsnappydata.connection\n, \nlocalhost:1527\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate Table and Run Queries\n: \nYou can now create tables and run queries in SnappyData store using your Apache Spark program.\n\n\n// reading an already created SnappyStore table SNAPPY_COL_TABLE\nval colTable = snSession.table(\nSNAPPY_COL_TABLE\n)\ncolTable.show(10)\n\nsnSession.dropTable(\nTestColumnTable\n, ifExists = true)\n\n// Creating a table from a DataFrame\nval dataFrame = snSession.range(1000).selectExpr(\nid\n, \nfloor(rand() * 10000) as k\n)\n\nsnSession.sql(\ncreate table TestColumnTable (id bigint not null, k bigint not null) using column\n)\n\n// insert data in TestColumnTable\ndataFrame.write.insertInto(\nTestColumnTable\n)\n\n\n\n\nRunning a Smart Connector Application\n\n\nStart a SnappyData cluster and create a table.\n\n\n$ ./sbin/snappy-start-all.sh\n\n$ ./bin/snappy\nSnappyData version 1.0.2.1\nsnappy\n  connect client 'localhost:1527';\nUsing CONNECTION0\nsnappy\n CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN;\nsnappy\n insert into SNAPPY_COL_TABLE VALUES(1,1);\n1 row inserted/updated/deleted\nsnappy\n insert into SNAPPY_COL_TABLE VALUES(2,2);\n1 row inserted/updated/deleted\nexit;\n\n\n\n\nThe Smart Connector Application can now connect to this SnappyData cluster. \n\n\nThe following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside the SnappyData cluster. \nSnappyData package has to be specified along with the application jar to run the Smart Connector application.\n\n\n$ ./bin/spark-submit --master local[*] --conf snappydata.connection=localhost:1527  --class org.apache.spark.examples.snappydata.SmartConnectorExample --packages SnappyDataInc:snappydata:1.0.2.1-s_2.11       $SNAPPY_HOME/examples/jars/quickstart.jar\n\n\n\n\nExecute a Smart Connector Application\n\n\nStart a SnappyData cluster and create a table inside it.\n\n\n$ ./sbin/snappy-start-all.sh\n\n$ ./bin/snappy\nSnappyData version 1.0.2.1\nsnappy\n  connect client 'localhost:1527';\nUsing CONNECTION0\nsnappy\n CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN;\nsnappy\n insert into SNAPPY_COL_TABLE VALUES(1,1);\n1 row inserted/updated/deleted\nsnappy\n insert into SNAPPY_COL_TABLE VALUES(2,2);\n1 row inserted/updated/deleted\nexit;\n\n\n\n\nA Smart Connector Application can now connect to this SnappyData cluster. The following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside SnappyData cluster. SnappyData package has to be specified along with the application jar to run the Smart Connector application. \n\n\n$ ./bin/spark-submit --master local[*] --conf spark.snappydata.connection=localhost:1527  --class org.apache.spark.examples.snappydata.SmartConnectorExample   --packages SnappyDataInc:snappydata:1.0.2.1-s_2.11 $SNAPPY_HOME/examples/jars/quickstart.jar", 
            "title": "How to Access SnappyData Store from existing Spark Installation using Smart Connector"
        }, 
        {
            "location": "/howto/spark_installation_using_smart_connector/#how-to-access-snappydata-store-from-an-existing-spark-installation-using-smart-connector", 
            "text": "SnappyData comes with a Smart Connector that enables Spark applications to work with the SnappyData cluster, from any compatible Spark cluster (you can use any distribution that is compatible with Apache Spark 2.1.1). The Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. This is similar to how Spark applications today work with stores like Cassandra, Redis, etc.  For more information on the various modes, refer to the  SnappyData Smart Connector  section of the documentation.", 
            "title": "How to Access SnappyData Store from an existing Spark Installation using Smart Connector"
        }, 
        {
            "location": "/howto/spark_installation_using_smart_connector/#code-example", 
            "text": "The code example for this mode is in  SmartConnectorExample.scala  Configure a SnappySession : \nThe code below shows how to initialize a SparkSession. Here the property  snappydata.connection  instructs the connector to acquire cluster connectivity, catalog metadata and register it locally in the Spark cluster. The values consists of  locator host  and  JDBC client port  on which the locator listens for connections (default 1527).  val spark: SparkSession = SparkSession\n    .builder\n    .appName( SmartConnectorExample )\n    // It can be any master URL\n    .master( local[4] )\n    // snappydata.connection property enables the application to interact with SnappyData store\n    .config( snappydata.connection ,  localhost:1527 )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)  Create Table and Run Queries : \nYou can now create tables and run queries in SnappyData store using your Apache Spark program.  // reading an already created SnappyStore table SNAPPY_COL_TABLE\nval colTable = snSession.table( SNAPPY_COL_TABLE )\ncolTable.show(10)\n\nsnSession.dropTable( TestColumnTable , ifExists = true)\n\n// Creating a table from a DataFrame\nval dataFrame = snSession.range(1000).selectExpr( id ,  floor(rand() * 10000) as k )\n\nsnSession.sql( create table TestColumnTable (id bigint not null, k bigint not null) using column )\n\n// insert data in TestColumnTable\ndataFrame.write.insertInto( TestColumnTable )", 
            "title": "Code Example"
        }, 
        {
            "location": "/howto/spark_installation_using_smart_connector/#running-a-smart-connector-application", 
            "text": "Start a SnappyData cluster and create a table.  $ ./sbin/snappy-start-all.sh\n\n$ ./bin/snappy\nSnappyData version 1.0.2.1\nsnappy   connect client 'localhost:1527';\nUsing CONNECTION0\nsnappy  CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN;\nsnappy  insert into SNAPPY_COL_TABLE VALUES(1,1);\n1 row inserted/updated/deleted\nsnappy  insert into SNAPPY_COL_TABLE VALUES(2,2);\n1 row inserted/updated/deleted\nexit;  The Smart Connector Application can now connect to this SnappyData cluster.   The following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside the SnappyData cluster.  SnappyData package has to be specified along with the application jar to run the Smart Connector application.  $ ./bin/spark-submit --master local[*] --conf snappydata.connection=localhost:1527  --class org.apache.spark.examples.snappydata.SmartConnectorExample --packages SnappyDataInc:snappydata:1.0.2.1-s_2.11       $SNAPPY_HOME/examples/jars/quickstart.jar", 
            "title": "Running a Smart Connector Application"
        }, 
        {
            "location": "/howto/spark_installation_using_smart_connector/#execute-a-smart-connector-application", 
            "text": "Start a SnappyData cluster and create a table inside it.  $ ./sbin/snappy-start-all.sh\n\n$ ./bin/snappy\nSnappyData version 1.0.2.1\nsnappy   connect client 'localhost:1527';\nUsing CONNECTION0\nsnappy  CREATE TABLE SNAPPY_COL_TABLE(r1 Integer, r2 Integer) USING COLUMN;\nsnappy  insert into SNAPPY_COL_TABLE VALUES(1,1);\n1 row inserted/updated/deleted\nsnappy  insert into SNAPPY_COL_TABLE VALUES(2,2);\n1 row inserted/updated/deleted\nexit;  A Smart Connector Application can now connect to this SnappyData cluster. The following command executes an example that queries SNAPPY_COL_TABLE and creates a new table inside SnappyData cluster. SnappyData package has to be specified along with the application jar to run the Smart Connector application.   $ ./bin/spark-submit --master local[*] --conf spark.snappydata.connection=localhost:1527  --class org.apache.spark.examples.snappydata.SmartConnectorExample   --packages SnappyDataInc:snappydata:1.0.2.1-s_2.11 $SNAPPY_HOME/examples/jars/quickstart.jar", 
            "title": "Execute a Smart Connector Application"
        }, 
        {
            "location": "/howto/use_snappy_shell/", 
            "text": "How to Use Snappy SQL shell (snappy-sql)\n\n\nThe Snappy SQL shell can be used to execute SQL on SnappyData cluster. In the background, \nsnappy-sql\n uses JDBC connections to execute SQL.\n\n\nConnect to a SnappyData Cluster\n: \n\nUse the \nsnappy-sql\n and \nconnect client\n commands on the Snappy SQL shell as follows:\n\n\n$ ./bin/snappy-sql\nsnappy\n connect client '\nlocatorHostName\n:1527';\n\n\n\n\nHere, the \nlocatorHostName\n is the host name of the node on which the locator is started and \n1527\n is the default port on which the locator listens for connections.\n\n\nExecute SQL queries\n:\n Once connected, you can execute SQL queries using \nsnappy-sql\n\n\nsnappy\n CREATE TABLE APP.PARTSUPP (PS_PARTKEY INTEGER NOT NULL PRIMARY KEY, PS_SUPPKEY INTEGER NOT NULL, PS_AVAILQTY INTEGER NOT NULL, PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL) USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') ;\n\nsnappy\n INSERT INTO APP.PARTSUPP VALUES(100, 1, 5000, 100);\n\nsnappy\n INSERT INTO APP.PARTSUPP VALUES(200, 2, 50, 10);\n\nsnappy\n SELECT * FROM APP.PARTSUPP;\nPS_PARTKEY |PS_SUPPKEY |PS_AVAILQTY|PS_SUPPLYCOST    \n-----------------------------------------------------\n100        |1          |5000       |100.00           \n200        |2          |50         |10.00            \n\n2 rows selected\n\n\n\n\nView the members of cluster\n: \n\nUse the \nshow members\n command.\n\n\nsnappy\n show members;\nID                            |HOST                          |KIND                          |STATUS              |NETSERVERS                    |SERVERGROUPS                  \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n192.168.63.1(21412)\nv1\n:61964 |192.168.63.1                  |datastore(normal)             |RUNNING             |localhost/127.0.0.1[1528]     |                              \n192.168.63.1(21594)\nv2\n:29474 |192.168.63.1                  |accessor(normal)              |RUNNING             |                              |IMPLICIT_LEADER_SERVERGROUP   \nlocalhost(21262)\nv0\n:22770    |localhost                     |locator(normal)               |RUNNING             |localhost/127.0.0.1[1527]     |                              \n\n3 rows selected\n\n\n\n\nView the list tables in a schema\n: \n\nUse \nshow tables in \nschema\n command.\n\n\nsnappy\n show tables in app;\nTABLE_SCHEM         |TABLE_NAME                    |TABLE_TYPE|REMARKS             \n-----------------------------------------------------------------------------------\nAPP                 |PARTSUPP                      |TABLE     |                    \n\n1 row selected", 
            "title": "How to Use Snappy SQL shell (snappy-sql)"
        }, 
        {
            "location": "/howto/use_snappy_shell/#how-to-use-snappy-sql-shell-snappy-sql", 
            "text": "The Snappy SQL shell can be used to execute SQL on SnappyData cluster. In the background,  snappy-sql  uses JDBC connections to execute SQL.  Connect to a SnappyData Cluster :  \nUse the  snappy-sql  and  connect client  commands on the Snappy SQL shell as follows:  $ ./bin/snappy-sql\nsnappy  connect client ' locatorHostName :1527';  Here, the  locatorHostName  is the host name of the node on which the locator is started and  1527  is the default port on which the locator listens for connections.  Execute SQL queries :  Once connected, you can execute SQL queries using  snappy-sql  snappy  CREATE TABLE APP.PARTSUPP (PS_PARTKEY INTEGER NOT NULL PRIMARY KEY, PS_SUPPKEY INTEGER NOT NULL, PS_AVAILQTY INTEGER NOT NULL, PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL) USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') ;\n\nsnappy  INSERT INTO APP.PARTSUPP VALUES(100, 1, 5000, 100);\n\nsnappy  INSERT INTO APP.PARTSUPP VALUES(200, 2, 50, 10);\n\nsnappy  SELECT * FROM APP.PARTSUPP;\nPS_PARTKEY |PS_SUPPKEY |PS_AVAILQTY|PS_SUPPLYCOST    \n-----------------------------------------------------\n100        |1          |5000       |100.00           \n200        |2          |50         |10.00            \n\n2 rows selected  View the members of cluster :  \nUse the  show members  command.  snappy  show members;\nID                            |HOST                          |KIND                          |STATUS              |NETSERVERS                    |SERVERGROUPS                  \n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n192.168.63.1(21412) v1 :61964 |192.168.63.1                  |datastore(normal)             |RUNNING             |localhost/127.0.0.1[1528]     |                              \n192.168.63.1(21594) v2 :29474 |192.168.63.1                  |accessor(normal)              |RUNNING             |                              |IMPLICIT_LEADER_SERVERGROUP   \nlocalhost(21262) v0 :22770    |localhost                     |locator(normal)               |RUNNING             |localhost/127.0.0.1[1527]     |                              \n\n3 rows selected  View the list tables in a schema :  \nUse  show tables in  schema  command.  snappy  show tables in app;\nTABLE_SCHEM         |TABLE_NAME                    |TABLE_TYPE|REMARKS             \n-----------------------------------------------------------------------------------\nAPP                 |PARTSUPP                      |TABLE     |                    \n\n1 row selected", 
            "title": "How to Use Snappy SQL shell (snappy-sql)"
        }, 
        {
            "location": "/howto/create_row_tables_and_run_queries/", 
            "text": "How to Create Row Tables and Run Queries\n\n\nEach record in a Row table is managed in contiguous memory, and therefore, optimized for selective queries (For example. key based point lookup ) or updates. \nA row table can either be replicated to all nodes or partitioned across nodes. It can be created by using DataFrame API or using SQL.\n\n\nRefer to the \nRow and column tables\n documentation for complete list of attributes for row tables.\n\n\nFull source code, for example, to create and perform operations on replicated and partitioned row table can be found in \nCreateReplicatedRowTable.scala\n and \nCreatePartitionedRowTable.scala\n\n\nCreate a Row Table using DataFrame API:\n\n\nThe code snippet below shows how to create a replicated row table using API.\n\n\nGet a SnappySession\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nCreateReplicatedRowTable\n)\n    .master(\nlocal[*]\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\nimport org.apache.spark.sql.types._\n\n\n\n\nCreate the Table using API\n:\nFirst, define the table schema and then create the table using createTable API\n\n\nval schema = StructType(Array(StructField(\nS_SUPPKEY\n, IntegerType, false),\n  StructField(\nS_NAME\n, StringType, false),\n  StructField(\nS_ADDRESS\n, StringType, false),\n  StructField(\nS_NATIONKEY\n, IntegerType, false),\n  StructField(\nS_PHONE\n, StringType, false),\n  StructField(\nS_ACCTBAL\n, DecimalType(15, 2), false),\n  StructField(\nS_COMMENT\n, StringType, false)\n))\n\n// props1 map specifies the properties for the table to be created\n// \nPERSISTENCE\n flag indicates that the table data should be persisted to\n// disk asynchronously\nval props1 = Map(\nPERSISTENCE\n -\n \nasynchronous\n)\n// create a row table using createTable API\nsnSession.createTable(\nSUPPLIER\n, \nrow\n, schema, props1)\n\n\n\n\nCreating a Row table using SQL\n:\nThe same table can be created using SQL as shown below:\n\n\n// First drop the table if it exists\nsnSession.sql(\nDROP TABLE IF EXISTS SUPPLIER\n)\n// Create a row table using SQL\n// \nPERSISTENCE\n that the table data should be persisted to disk asynchronously\n// For complete list of attributes refer the documentation\nsnSession.sql(\n  \nCREATE TABLE SUPPLIER ( \n +\n      \nS_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n +\n      \nS_NAME STRING NOT NULL, \n +\n      \nS_ADDRESS STRING NOT NULL, \n +\n      \nS_NATIONKEY INTEGER NOT NULL, \n +\n      \nS_PHONE STRING NOT NULL, \n +\n      \nS_ACCTBAL DECIMAL(15, 2) NOT NULL, \n +\n      \nS_COMMENT STRING NOT NULL \n +\n      \n) USING ROW OPTIONS (PERSISTENCE 'asynchronous')\n)\n\n\n\n\nYou can perform various operations such as inset data, mutate it (update/delete), select data from the table. All these operations can be done either through APIs or by using SQL queries.\nFor example:\n\n\nTo insert data in the SUPPLIER table:\n \n\n\nsnSession.sql(\nINSERT INTO SUPPLIER VALUES(1, 'SUPPLIER1', 'CHICAGO, IL', 0, '555-543-789', 10000, ' ')\n)\nsnSession.sql(\nINSERT INTO SUPPLIER VALUES(2, 'SUPPLIER2', 'BOSTON, MA', 0, '555-234-489', 20000, ' ')\n)\nsnSession.sql(\nINSERT INTO SUPPLIER VALUES(3, 'SUPPLIER3', 'NEWYORK, NY', 0, '555-743-785', 34000, ' ')\n)\nsnSession.sql(\nINSERT INTO SUPPLIER VALUES(4, 'SUPPLIER4', 'SANHOSE, CA', 0, '555-321-098', 1000, ' ')\n)\n\n\n\n\nTo print the contents of the SUPPLIER table:\n \n\n\nvar tableData = snSession.sql(\nSELECT * FROM SUPPLIER\n).collect()\ntableData.foreach(println)\n\n\n\n\nTo update the table account balance for SUPPLIER4:\n \n\n\nsnSession.sql(\nUPDATE SUPPLIER SET S_ACCTBAL = 50000 WHERE S_NAME = 'SUPPLIER4'\n)\n\n\n\n\nTo print contents of the SUPPLIER table after update\n \n\n\ntableData = snSession.sql(\nSELECT * FROM SUPPLIER\n).collect()\ntableData.foreach(println)\n\n\n\n\nTo delete the records for SUPPLIER2 and SUPPLIER3\n \n\n\nsnSession.sql(\nDELETE FROM SUPPLIER WHERE S_NAME = 'SUPPLIER2' OR S_NAME = 'SUPPLIER3'\n)\n\n\n\n\n\nTo print the contents of the SUPPLIER table after delete\n\n\ntableData = snSession.sql(\nSELECT * FROM SUPPLIER\n).collect()\ntableData.foreach(println)", 
            "title": "How to Create Row Tables and Run Queries"
        }, 
        {
            "location": "/howto/create_row_tables_and_run_queries/#how-to-create-row-tables-and-run-queries", 
            "text": "Each record in a Row table is managed in contiguous memory, and therefore, optimized for selective queries (For example. key based point lookup ) or updates. \nA row table can either be replicated to all nodes or partitioned across nodes. It can be created by using DataFrame API or using SQL.  Refer to the  Row and column tables  documentation for complete list of attributes for row tables.  Full source code, for example, to create and perform operations on replicated and partitioned row table can be found in  CreateReplicatedRowTable.scala  and  CreatePartitionedRowTable.scala", 
            "title": "How to Create Row Tables and Run Queries"
        }, 
        {
            "location": "/howto/create_row_tables_and_run_queries/#create-a-row-table-using-dataframe-api", 
            "text": "The code snippet below shows how to create a replicated row table using API.  Get a SnappySession  val spark: SparkSession = SparkSession\n    .builder\n    .appName( CreateReplicatedRowTable )\n    .master( local[*] )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\nimport org.apache.spark.sql.types._  Create the Table using API :\nFirst, define the table schema and then create the table using createTable API  val schema = StructType(Array(StructField( S_SUPPKEY , IntegerType, false),\n  StructField( S_NAME , StringType, false),\n  StructField( S_ADDRESS , StringType, false),\n  StructField( S_NATIONKEY , IntegerType, false),\n  StructField( S_PHONE , StringType, false),\n  StructField( S_ACCTBAL , DecimalType(15, 2), false),\n  StructField( S_COMMENT , StringType, false)\n))\n\n// props1 map specifies the properties for the table to be created\n//  PERSISTENCE  flag indicates that the table data should be persisted to\n// disk asynchronously\nval props1 = Map( PERSISTENCE  -   asynchronous )\n// create a row table using createTable API\nsnSession.createTable( SUPPLIER ,  row , schema, props1)  Creating a Row table using SQL :\nThe same table can be created using SQL as shown below:  // First drop the table if it exists\nsnSession.sql( DROP TABLE IF EXISTS SUPPLIER )\n// Create a row table using SQL\n//  PERSISTENCE  that the table data should be persisted to disk asynchronously\n// For complete list of attributes refer the documentation\nsnSession.sql(\n   CREATE TABLE SUPPLIER (   +\n       S_SUPPKEY INTEGER NOT NULL PRIMARY KEY,   +\n       S_NAME STRING NOT NULL,   +\n       S_ADDRESS STRING NOT NULL,   +\n       S_NATIONKEY INTEGER NOT NULL,   +\n       S_PHONE STRING NOT NULL,   +\n       S_ACCTBAL DECIMAL(15, 2) NOT NULL,   +\n       S_COMMENT STRING NOT NULL   +\n       ) USING ROW OPTIONS (PERSISTENCE 'asynchronous') )  You can perform various operations such as inset data, mutate it (update/delete), select data from the table. All these operations can be done either through APIs or by using SQL queries.\nFor example:  To insert data in the SUPPLIER table:    snSession.sql( INSERT INTO SUPPLIER VALUES(1, 'SUPPLIER1', 'CHICAGO, IL', 0, '555-543-789', 10000, ' ') )\nsnSession.sql( INSERT INTO SUPPLIER VALUES(2, 'SUPPLIER2', 'BOSTON, MA', 0, '555-234-489', 20000, ' ') )\nsnSession.sql( INSERT INTO SUPPLIER VALUES(3, 'SUPPLIER3', 'NEWYORK, NY', 0, '555-743-785', 34000, ' ') )\nsnSession.sql( INSERT INTO SUPPLIER VALUES(4, 'SUPPLIER4', 'SANHOSE, CA', 0, '555-321-098', 1000, ' ') )  To print the contents of the SUPPLIER table:    var tableData = snSession.sql( SELECT * FROM SUPPLIER ).collect()\ntableData.foreach(println)  To update the table account balance for SUPPLIER4:    snSession.sql( UPDATE SUPPLIER SET S_ACCTBAL = 50000 WHERE S_NAME = 'SUPPLIER4' )  To print contents of the SUPPLIER table after update    tableData = snSession.sql( SELECT * FROM SUPPLIER ).collect()\ntableData.foreach(println)  To delete the records for SUPPLIER2 and SUPPLIER3    snSession.sql( DELETE FROM SUPPLIER WHERE S_NAME = 'SUPPLIER2' OR S_NAME = 'SUPPLIER3' )  To print the contents of the SUPPLIER table after delete  tableData = snSession.sql( SELECT * FROM SUPPLIER ).collect()\ntableData.foreach(println)", 
            "title": "Create a Row Table using DataFrame API:"
        }, 
        {
            "location": "/howto/create_column_tables_and_run_queries/", 
            "text": "How to Create Column Tables and Run Queries\n\n\nColumn tables organize and manage data in a columnar form such that modern day CPUs can traverse and run computations like a sum or an average fast (as the values are available in contiguous memory).\n\n\nRefer to the \nRow and column tables\n documentation for the complete list of attributes for column tables.\n\n\nFull source code, for example, to create and perform operations on column table can be found in \nCreateColumnTable.scala\n\n\nCreate a Column Table using DataFrame API\n\n\nThe code snippet below shows how to create a column table using DataFrame API.\n\n\nGet a SnappySession\n:\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nCreateColumnTable\n)\n    .master(\nlocal[*]\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nDefine the table schema\n\n\nval tableSchema = StructType(Array(StructField(\nC_CUSTKEY\n, IntegerType, false),\n    StructField(\nC_NAME\n, StringType, false),\n    StructField(\nC_ADDRESS\n, StringType, false),\n    StructField(\nC_NATIONKEY\n, IntegerType, false),\n    StructField(\nC_PHONE\n, StringType, false),\n    StructField(\nC_ACCTBAL\n, DecimalType(15, 2), false),\n    StructField(\nC_MKTSEGMENT\n, StringType, false),\n    StructField(\nC_COMMENT\n, StringType, false)\n    ))\n\n\n\n\nCreate the table and load data from CSV\n\n\n// props1 map specifies the properties for the table to be created\n// \nPARTITION_BY\n attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\nval props1 = Map(\nPARTITION_BY\n -\n \nC_CUSTKEY\n)\nsnSession.createTable(\nCUSTOMER\n, \ncolumn\n, tableSchema, props1)\n\nval tableSchema = snSession.table(\nCUSTOMER\n).schema\n// insert some data in it\n// loading data in CUSTOMER table from a text file with delimited columns\nval customerDF = snSession.read.schema(schema = tableSchema).csv(\nquickstart/src/main/resources/customer.csv\n)\ncustomerDF.write.insertInto(\nCUSTOMER\n)\n\n\n\n\nCreate a Column Table using SQL\n\n\nThe same table can be created using SQL as shown below:\n\n\nsnSession.sql(\nCREATE TABLE CUSTOMER ( \n +\n    \nC_CUSTKEY     INTEGER NOT NULL,\n +\n    \nC_NAME        VARCHAR(25) NOT NULL,\n +\n    \nC_ADDRESS     VARCHAR(40) NOT NULL,\n +\n    \nC_NATIONKEY   INTEGER NOT NULL,\n +\n    \nC_PHONE       VARCHAR(15) NOT NULL,\n +\n    \nC_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n +\n    \nC_MKTSEGMENT  VARCHAR(10) NOT NULL,\n +\n    \nC_COMMENT     VARCHAR(117) NOT NULL)\n +\n    \nUSING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\n)\n\n\n\n\nYou can execute selected queries on a column table, join the column table with other tables, and append data to it.", 
            "title": "How to Create Column Tables and Run Queries"
        }, 
        {
            "location": "/howto/create_column_tables_and_run_queries/#how-to-create-column-tables-and-run-queries", 
            "text": "Column tables organize and manage data in a columnar form such that modern day CPUs can traverse and run computations like a sum or an average fast (as the values are available in contiguous memory).  Refer to the  Row and column tables  documentation for the complete list of attributes for column tables.  Full source code, for example, to create and perform operations on column table can be found in  CreateColumnTable.scala", 
            "title": "How to Create Column Tables and Run Queries"
        }, 
        {
            "location": "/howto/create_column_tables_and_run_queries/#create-a-column-table-using-dataframe-api", 
            "text": "The code snippet below shows how to create a column table using DataFrame API.  Get a SnappySession :  val spark: SparkSession = SparkSession\n    .builder\n    .appName( CreateColumnTable )\n    .master( local[*] )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)  Define the table schema  val tableSchema = StructType(Array(StructField( C_CUSTKEY , IntegerType, false),\n    StructField( C_NAME , StringType, false),\n    StructField( C_ADDRESS , StringType, false),\n    StructField( C_NATIONKEY , IntegerType, false),\n    StructField( C_PHONE , StringType, false),\n    StructField( C_ACCTBAL , DecimalType(15, 2), false),\n    StructField( C_MKTSEGMENT , StringType, false),\n    StructField( C_COMMENT , StringType, false)\n    ))  Create the table and load data from CSV  // props1 map specifies the properties for the table to be created\n//  PARTITION_BY  attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\nval props1 = Map( PARTITION_BY  -   C_CUSTKEY )\nsnSession.createTable( CUSTOMER ,  column , tableSchema, props1)\n\nval tableSchema = snSession.table( CUSTOMER ).schema\n// insert some data in it\n// loading data in CUSTOMER table from a text file with delimited columns\nval customerDF = snSession.read.schema(schema = tableSchema).csv( quickstart/src/main/resources/customer.csv )\ncustomerDF.write.insertInto( CUSTOMER )", 
            "title": "Create a Column Table using DataFrame API"
        }, 
        {
            "location": "/howto/create_column_tables_and_run_queries/#create-a-column-table-using-sql", 
            "text": "The same table can be created using SQL as shown below:  snSession.sql( CREATE TABLE CUSTOMER (   +\n     C_CUSTKEY     INTEGER NOT NULL,  +\n     C_NAME        VARCHAR(25) NOT NULL,  +\n     C_ADDRESS     VARCHAR(40) NOT NULL,  +\n     C_NATIONKEY   INTEGER NOT NULL,  +\n     C_PHONE       VARCHAR(15) NOT NULL,  +\n     C_ACCTBAL     DECIMAL(15,2)   NOT NULL,  +\n     C_MKTSEGMENT  VARCHAR(10) NOT NULL,  +\n     C_COMMENT     VARCHAR(117) NOT NULL)  +\n     USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') )  You can execute selected queries on a column table, join the column table with other tables, and append data to it.", 
            "title": "Create a Column Table using SQL"
        }, 
        {
            "location": "/howto/load_data_into_snappydata_tables/", 
            "text": "How to Load Data into SnappyData Tables\n\n\nSnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. By integrating the loading mechanism with the Query engine (Catalyst optimizer) it is often possible to push down filters and projections all the way to the data source minimizing data transfer. Here is the list of important features:\n\n\nSupport for many Sources\n \nThere is built-in support for many data sources as well as data formats. Data can be accessed from S3, file system, HDFS, Hive, RDB, etc. And the loaders have built-in support to handle CSV, Parquet, ORC, Avro, JSON, Java/Scala Objects, etc as the data formats. \n\n\nAccess virtually any modern data store\n Virtually all major data providers have a native Spark connector that complies with the Data Sources API. For e.g. you can load data from any RDB like Amazon Redshift, Cassandra, Redis, Elastic Search, Neo4J, etc. While these connectors are not built-in, you can easily deploy these connectors as dependencies into a SnappyData cluster. All the connectors are typically registered in spark-packages.org\n\n\nAvoid Schema wrangling\n \nSpark supports schema inference. Which means, all you need to do is point to the external source in your 'create table' DDL (or Spark SQL API) and schema definition is learned by reading in the data. There is no need to explicitly define each column and type. This is extremely useful when dealing with disparate, complex and wide data sets. \n\n\nRead nested, sparse data sets\n When data is accessed from a source, the schema inference occurs by not just reading a header but often by reading the entire data set. For instance, when reading JSON files the structure could change from document to document. The inference engine builds up the schema as it reads each record and keeps unioning them to create a unified schema. This approach allows developers to become very productive with disparate data sets.\n\n\nLoad using Spark API or SQL\n \n You can use SQL to point to any data source or use the native Spark Scala/Java API to load. \nFor instance, you can first \ncreate an external table\n. \n\n\nCREATE EXTERNAL TABLE \ntablename\n USING \nany-data-source-supported\n OPTIONS \noptions\n\n\n\n\n\nNext, use it in any SQL query or DDL. For example,\n\n\nCREATE EXTERNAL TABLE STAGING_CUSTOMER USING parquet OPTIONS(path 'quickstart/src/main/resources/customerparquet')\n\nCREATE TABLE CUSTOMER USING column OPTIONS(buckets '8') AS ( SELECT * FROM STAGING_CUSTOMER)\n\n\n\n\nExample - Load from CSV\n\n\nYou can either explicitly define the schema or infer the schema and the column data types. To infer the column names, we need the CSV header to specify the names. In this example we don't have the names, so we explicitly define the schema. \n\n\n// Get a SnappySession in a local cluster\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nCreateColumnTable\n)\n    .master(\nlocal[*]\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nWe explicitly define the table definition first ....\n\n\nsnSession.sql(\nCREATE TABLE CUSTOMER ( \n +\n    \nC_CUSTKEY     INTEGER NOT NULL,\n +\n    \nC_NAME        VARCHAR(25) NOT NULL,\n +\n    \nC_ADDRESS     VARCHAR(40) NOT NULL,\n +\n    \nC_NATIONKEY   INTEGER NOT NULL,\n +\n    \nC_PHONE       VARCHAR(15) NOT NULL,\n +\n    \nC_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n +\n    \nC_MKTSEGMENT  VARCHAR(10) NOT NULL,\n +\n    \nC_COMMENT     VARCHAR(117) NOT NULL)\n +\n    \nUSING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\n)\n\n\n\n\nLoad data in the CUSTOMER table from a CSV file by using Data Sources API\n\n\nval tableSchema = snSession.table(\nCUSTOMER\n).schema\nval customerDF = snSession.read.schema(schema = tableSchema).csv(s\n$dataFolder/customer.csv\n)\ncustomerDF.write.insertInto(\nCUSTOMER\n)\n\n\n\n\nThe \nSpark SQL programming guide\n provides a full description of the Data Sources API \n\n\nExample - Load from Parquet files\n\n\nval customerDF = snSession.read.parquet(s\n$dataDir/customer_parquet\n)\ncustomerDF.write.insertInto(\nCUSTOMER\n)\n\n\n\n\nInferring schema from data file\n\n\nA schema for the table can be inferred from the data file. Data is first introspected to learn the schema (column names and types) without requring this input from the user. The example below illustrates reading a parquet data source and creates a new columnar table in SnappyData. The schema is automatically defined when the Parquet data files are read. \n\n\nval customerDF = snSession.read.parquet(s\nquickstart/src/main/resources/customerparquet\n)\n// props1 map specifies the properties for the table to be created\n// \nPARTITION_BY\n attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\nval props1 = Map(\nPARTITION_BY\n -\n \nC_CUSTKEY\n)\ncustomerDF.write.format(\ncolumn\n).mode(\nappend\n).options(props1).saveAsTable(\nCUSTOMER\n)\n\n\n\n\nIn the code snippet below a schema is inferred from a CSV file. Column names are derived from the header in the file.\n\n\nval customer_csv_DF = snSession.read.option(\nheader\n, \ntrue\n)\n    .option(\ninferSchema\n, \ntrue\n).csv(\nquickstart/src/main/resources/customer_with_headers.csv\n)\n\n// props1 map specifies the properties for the table to be created\n// \nPARTITION_BY\n attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY),\n// For complete list of attributes refer the documentation\nval props1 = Map(\nPARTITION_BY\n -\n \nC_CUSTKEY\n)\ncustomer_csv_DF.write.format(\ncolumn\n).mode(\nappend\n).options(props1).saveAsTable(\nCUSTOMER\n)\n\n\n\n\nThe source code to load the data from a CSV/Parquet files is in \nCreateColumnTable.scala\n. \n\n\nExample - reading JSON documents\n\nAs mentioned before when dealing with JSON you have two challenges - (1) the data can be highly nested (2) the structure of the documents can keep changing. \n\n\nHere is a simple example that loads multiple JSON records that show dealing with schema changes across documents -   \nWorkingWithJson.scala\n\n\n\n\nNote\n\n\nWhen loading data from sources like CSV or Parquet the files would need to be accessible from all the cluster members in SnappyData. Make sure it is NFS mounted or made accessible through the Cloud solution (shared storage like S3).", 
            "title": "How to Load Data into SnappyData Tables"
        }, 
        {
            "location": "/howto/load_data_into_snappydata_tables/#how-to-load-data-into-snappydata-tables", 
            "text": "SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. By integrating the loading mechanism with the Query engine (Catalyst optimizer) it is often possible to push down filters and projections all the way to the data source minimizing data transfer. Here is the list of important features:  Support for many Sources   There is built-in support for many data sources as well as data formats. Data can be accessed from S3, file system, HDFS, Hive, RDB, etc. And the loaders have built-in support to handle CSV, Parquet, ORC, Avro, JSON, Java/Scala Objects, etc as the data formats.   Access virtually any modern data store  Virtually all major data providers have a native Spark connector that complies with the Data Sources API. For e.g. you can load data from any RDB like Amazon Redshift, Cassandra, Redis, Elastic Search, Neo4J, etc. While these connectors are not built-in, you can easily deploy these connectors as dependencies into a SnappyData cluster. All the connectors are typically registered in spark-packages.org  Avoid Schema wrangling   Spark supports schema inference. Which means, all you need to do is point to the external source in your 'create table' DDL (or Spark SQL API) and schema definition is learned by reading in the data. There is no need to explicitly define each column and type. This is extremely useful when dealing with disparate, complex and wide data sets.   Read nested, sparse data sets  When data is accessed from a source, the schema inference occurs by not just reading a header but often by reading the entire data set. For instance, when reading JSON files the structure could change from document to document. The inference engine builds up the schema as it reads each record and keeps unioning them to create a unified schema. This approach allows developers to become very productive with disparate data sets.  Load using Spark API or SQL    You can use SQL to point to any data source or use the native Spark Scala/Java API to load. \nFor instance, you can first  create an external table .   CREATE EXTERNAL TABLE  tablename  USING  any-data-source-supported  OPTIONS  options   Next, use it in any SQL query or DDL. For example,  CREATE EXTERNAL TABLE STAGING_CUSTOMER USING parquet OPTIONS(path 'quickstart/src/main/resources/customerparquet')\n\nCREATE TABLE CUSTOMER USING column OPTIONS(buckets '8') AS ( SELECT * FROM STAGING_CUSTOMER)  Example - Load from CSV  You can either explicitly define the schema or infer the schema and the column data types. To infer the column names, we need the CSV header to specify the names. In this example we don't have the names, so we explicitly define the schema.   // Get a SnappySession in a local cluster\nval spark: SparkSession = SparkSession\n    .builder\n    .appName( CreateColumnTable )\n    .master( local[*] )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)  We explicitly define the table definition first ....  snSession.sql( CREATE TABLE CUSTOMER (   +\n     C_CUSTKEY     INTEGER NOT NULL,  +\n     C_NAME        VARCHAR(25) NOT NULL,  +\n     C_ADDRESS     VARCHAR(40) NOT NULL,  +\n     C_NATIONKEY   INTEGER NOT NULL,  +\n     C_PHONE       VARCHAR(15) NOT NULL,  +\n     C_ACCTBAL     DECIMAL(15,2)   NOT NULL,  +\n     C_MKTSEGMENT  VARCHAR(10) NOT NULL,  +\n     C_COMMENT     VARCHAR(117) NOT NULL)  +\n     USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') )  Load data in the CUSTOMER table from a CSV file by using Data Sources API  val tableSchema = snSession.table( CUSTOMER ).schema\nval customerDF = snSession.read.schema(schema = tableSchema).csv(s $dataFolder/customer.csv )\ncustomerDF.write.insertInto( CUSTOMER )  The  Spark SQL programming guide  provides a full description of the Data Sources API   Example - Load from Parquet files  val customerDF = snSession.read.parquet(s $dataDir/customer_parquet )\ncustomerDF.write.insertInto( CUSTOMER )  Inferring schema from data file  A schema for the table can be inferred from the data file. Data is first introspected to learn the schema (column names and types) without requring this input from the user. The example below illustrates reading a parquet data source and creates a new columnar table in SnappyData. The schema is automatically defined when the Parquet data files are read.   val customerDF = snSession.read.parquet(s quickstart/src/main/resources/customerparquet )\n// props1 map specifies the properties for the table to be created\n//  PARTITION_BY  attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY)\nval props1 = Map( PARTITION_BY  -   C_CUSTKEY )\ncustomerDF.write.format( column ).mode( append ).options(props1).saveAsTable( CUSTOMER )  In the code snippet below a schema is inferred from a CSV file. Column names are derived from the header in the file.  val customer_csv_DF = snSession.read.option( header ,  true )\n    .option( inferSchema ,  true ).csv( quickstart/src/main/resources/customer_with_headers.csv )\n\n// props1 map specifies the properties for the table to be created\n//  PARTITION_BY  attribute specifies partitioning key for CUSTOMER table(C_CUSTKEY),\n// For complete list of attributes refer the documentation\nval props1 = Map( PARTITION_BY  -   C_CUSTKEY )\ncustomer_csv_DF.write.format( column ).mode( append ).options(props1).saveAsTable( CUSTOMER )  The source code to load the data from a CSV/Parquet files is in  CreateColumnTable.scala .   Example - reading JSON documents \nAs mentioned before when dealing with JSON you have two challenges - (1) the data can be highly nested (2) the structure of the documents can keep changing.   Here is a simple example that loads multiple JSON records that show dealing with schema changes across documents -    WorkingWithJson.scala   Note  When loading data from sources like CSV or Parquet the files would need to be accessible from all the cluster members in SnappyData. Make sure it is NFS mounted or made accessible through the Cloud solution (shared storage like S3).", 
            "title": "How to Load Data into SnappyData Tables"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/", 
            "text": "How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc)\n\n\nSnappyData comes bundled with the libraries to access HDFS (Apache compatible). You can load your data using SQL or DataFrame API.\n\n\nExample - Loading data from CSV file using SQL\n\n\n// Create an external table based on CSV file\nCREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING csv OPTIONS (path '../../quickstart/src/main/resources/customer_with_headers.csv', header 'true', inferSchema 'true');\n\n// Create a SnappyData table and load data into CUSTOMER table\nCREATE TABLE CUSTOMER using column options() as (select * from CUSTOMER_STAGING_1);\n\n\n\n\n\n\nTip\n\n\nSimilarly, you can create an external table for all data sources and use SQL \"insert into\" query to load data. For more information on creating external tables refer to, \nCREATE EXTERNAL TABLE\n\n\n\n\nExample - Loading CSV Files from HDFS using API\n\n\nThe example below demonstrates how you can read CSV files from HDFS using an API:\n\n\nval dataDF=snc.read.option(\nheader\n,\ntrue\n).csv (\nhdfs://namenode-uri:port/path/to/customer_with_headers.csv\n)\n\n// Drop table if it exists\nsnc.sql(\ndrop table if exists CUSTOMER\n)\n\n// Load data into table\ndataDF.write.format(\ncolumn\n).saveAsTable(\nCUSTOMER\n)\n\n\n\n\nExample - Loading and Enriching CSV Data from HDFS\n\n\nThe example below demonstrates how you can load and enrich CSV Data from HDFS:\n\n\nval dataDF = snappy.read.option(\nheader\n, \ntrue\n)\n    .csv(\nhdfs://namenode-uri:port/path/to/customers.csv\n)\n\n// Drop table if it exists and create it with only required fields\nsnappy.sql(\ndrop table if exists CUSTOMER\n)\nsnappy.sql(\ncreate table CUSTOMER(C_CUSTKEY INTEGER NOT NULL\n +\n    \n, C_NAME VARCHAR(25) NOT NULL,\n +\n    \n C_ADDRESS VARCHAR(40) NOT NULL,\n +\n    \n C_NATIONKEY INTEGER NOT NULL,\n +\n    \n C_PHONE VARCHAR(15) NOT NULL,\n +\n    \n C_ACCTBAL DECIMAL(15,2) NOT NULL,\n +\n    \n C_MKTSEGMENT VARCHAR(10) NOT NULL,\n +\n    \n C_COMMENT VARCHAR(117) NOT NULL) using column options()\n)\n\n// Project and transform data from df and load it in table.\nimport snappy.implicits._\ndataDF.select($\nC_CUSTKEY\n,\n  $\nC_NAME\n,\n  $\nC_ADDRESS\n,\n  $\nC_NATIONKEY\n,\n  $\nC_PHONE\n,\n  $\nC_ACCTBAL\n + 100,\n  $\nC_MKTSEGMENT\n,\n  $\nC_COMMENT\n.substr(1, 5).alias(\nSHORT_COMMENT\n)).write.insertInto(\nCUSTOMER\n)\n\n\n\n\nExample - Loading from Hive\n\n\nAs SnappyData manages the catalog at all times and it is not possible to configure an external Hive catalog service like in Spark when using a SnappySession. But, it is still possible to access Hive using the native SparkSession (with \nenableHiveSupport\n set to \ntrue\n). \nHere is an example using the SparkSession(spark object below) to access a Hive table as a DataFrame, then converted to an RDD so it can be passed to a SnappySession to store it in a SnappyData Table. \n\n\nval ds = spark.table(\nhiveTable\n)\nval rdd = ds.rdd\nval session = new SnappySession(sparkContext)\nval df = session.createDataFrame(rdd, ds.schema)\ndf.write.format(\ncolumn\n).saveAsTable(\ncolumnTable\n)\n\n\n\n\nImporting Data using JDBC from a relational DB\n\n\n\n\nNote\n\n\nBefore you begin, you must install the corresponding JDBC driver. To do so, copy the JDBC driver jar file in \n/jars\n directory located in the home directory and then restart the cluster.\n\n\n\n\n\n\n\nThe example below demonstrates how to connect to any SQL database using JDBC:\n\n\n\n\n\n\nVerify and load the SQL Driver:\n\n\nClass.forName(\"com.mysql.jdbc.Driver\")\n\n\n\n\n\n\n\nSpecify all the properties to access the database\n\n\nimport java.util.Properties\nval jdbcUsername = \"USER_NAME\"\nval jdbcPassword = \"PASSWORD\"\nval jdbcHostname = \"HOSTNAME\"\nval jdbcPort = 3306\nval jdbcDatabase =\"DATABASE\"\nval jdbcUrl = s\"jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}?user=${jdbcUsername}\npassword=${jdbcPassword}\nrelaxAutoCommit=true\"\n\nval connectionProperties = new Properties()\nconnectionProperties.put(\"user\", \"USERNAME\")\nconnectionProperties.put(\"password\", \"PASSWORD\")\n\n\n\n\n\n\n\nFetch the table meta data from the RDB and creates equivalent column tables \n\n\nval connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword)\nconnection.isClosed()\nval md:DatabaseMetaData = connection.getMetaData();\nval rs:ResultSet = md.getTables(null, null, \"%\", null);\nwhile (rs.next()) {\n\nval tableName=rs.getString(3)\nval df=snc.read.jdbc(jdbcUrl, tableName, connectionProperties)\ndf.printSchema\ndf.show()\n// Create and load a column table with same schema as that of source table \n   df.write.format(\"column\").mode(SaveMode.Append).saveAsTable(tableName)\n}\n\n\n\n\n\n\n\n Using SQL to access external RDB tables \n\nYou can also use plain SQL to access any external RDB using external tables. Create external table on RDBMS table and query it directly from SnappyData as described below:\n\n\nsnc.sql(\ndrop table if exists external_table\n)\nsnc.sql(s\nCREATE  external TABLE external_table USING jdbc OPTIONS (dbtable 'tweet', driver 'com.mysql.jdbc.Driver',  user 'root',  password 'root',  url '$jdbcUrl')\n)\nsnc.sql(\nselect * from external_table\n).show\n\n\n\n\nRefer to the \nSpark SQL JDBC source access for how to parallelize access when dealing with large data sets\n.\n\n\nLoading Data from NoSQL store (Cassandra)\n\n\nThe example below demonstrates how you can load data from a NoSQL store:\n\n\n\n\nNote\n\n\nBefore you begin, you must install the corresponding Spark-Cassandra connector jar. To do so, copy the Spark-Cassandra connector jar file to the \n/jars\n directory located in the home directory and then restart the cluster.\n\n\n\n\n\n\n\nval df = snc.read.format(\norg.apache.spark.sql.cassandra\n).options(Map( \ntable\n -\n \nCUSTOMER\n, \nkeyspace\n -\n \ntest\n)) .load\ndf.write.format(\ncolumn\n).mode(SaveMode.Append).saveAsTable(\nCUSTOMER\n)\nsnc.sql(\nselect * from CUSTOMER\n).show", 
            "title": "How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc)"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/#how-to-load-data-from-external-data-stores-eg-hdfs-cassandra-hive-etc", 
            "text": "SnappyData comes bundled with the libraries to access HDFS (Apache compatible). You can load your data using SQL or DataFrame API.", 
            "title": "How to Load Data from External Data Stores (e.g. HDFS, Cassandra, Hive, etc)"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/#example-loading-data-from-csv-file-using-sql", 
            "text": "// Create an external table based on CSV file\nCREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING csv OPTIONS (path '../../quickstart/src/main/resources/customer_with_headers.csv', header 'true', inferSchema 'true');\n\n// Create a SnappyData table and load data into CUSTOMER table\nCREATE TABLE CUSTOMER using column options() as (select * from CUSTOMER_STAGING_1);   Tip  Similarly, you can create an external table for all data sources and use SQL \"insert into\" query to load data. For more information on creating external tables refer to,  CREATE EXTERNAL TABLE", 
            "title": "Example - Loading data from CSV file using SQL"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/#example-loading-csv-files-from-hdfs-using-api", 
            "text": "The example below demonstrates how you can read CSV files from HDFS using an API:  val dataDF=snc.read.option( header , true ).csv ( hdfs://namenode-uri:port/path/to/customer_with_headers.csv )\n\n// Drop table if it exists\nsnc.sql( drop table if exists CUSTOMER )\n\n// Load data into table\ndataDF.write.format( column ).saveAsTable( CUSTOMER )", 
            "title": "Example - Loading CSV Files from HDFS using API"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/#example-loading-and-enriching-csv-data-from-hdfs", 
            "text": "The example below demonstrates how you can load and enrich CSV Data from HDFS:  val dataDF = snappy.read.option( header ,  true )\n    .csv( hdfs://namenode-uri:port/path/to/customers.csv )\n\n// Drop table if it exists and create it with only required fields\nsnappy.sql( drop table if exists CUSTOMER )\nsnappy.sql( create table CUSTOMER(C_CUSTKEY INTEGER NOT NULL  +\n     , C_NAME VARCHAR(25) NOT NULL,  +\n      C_ADDRESS VARCHAR(40) NOT NULL,  +\n      C_NATIONKEY INTEGER NOT NULL,  +\n      C_PHONE VARCHAR(15) NOT NULL,  +\n      C_ACCTBAL DECIMAL(15,2) NOT NULL,  +\n      C_MKTSEGMENT VARCHAR(10) NOT NULL,  +\n      C_COMMENT VARCHAR(117) NOT NULL) using column options() )\n\n// Project and transform data from df and load it in table.\nimport snappy.implicits._\ndataDF.select($ C_CUSTKEY ,\n  $ C_NAME ,\n  $ C_ADDRESS ,\n  $ C_NATIONKEY ,\n  $ C_PHONE ,\n  $ C_ACCTBAL  + 100,\n  $ C_MKTSEGMENT ,\n  $ C_COMMENT .substr(1, 5).alias( SHORT_COMMENT )).write.insertInto( CUSTOMER )", 
            "title": "Example - Loading and Enriching CSV Data from HDFS"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/#example-loading-from-hive", 
            "text": "As SnappyData manages the catalog at all times and it is not possible to configure an external Hive catalog service like in Spark when using a SnappySession. But, it is still possible to access Hive using the native SparkSession (with  enableHiveSupport  set to  true ). \nHere is an example using the SparkSession(spark object below) to access a Hive table as a DataFrame, then converted to an RDD so it can be passed to a SnappySession to store it in a SnappyData Table.   val ds = spark.table( hiveTable )\nval rdd = ds.rdd\nval session = new SnappySession(sparkContext)\nval df = session.createDataFrame(rdd, ds.schema)\ndf.write.format( column ).saveAsTable( columnTable )", 
            "title": "Example - Loading from Hive"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/#importing-data-using-jdbc-from-a-relational-db", 
            "text": "Note  Before you begin, you must install the corresponding JDBC driver. To do so, copy the JDBC driver jar file in  /jars  directory located in the home directory and then restart the cluster.    The example below demonstrates how to connect to any SQL database using JDBC:    Verify and load the SQL Driver:  Class.forName(\"com.mysql.jdbc.Driver\")    Specify all the properties to access the database  import java.util.Properties\nval jdbcUsername = \"USER_NAME\"\nval jdbcPassword = \"PASSWORD\"\nval jdbcHostname = \"HOSTNAME\"\nval jdbcPort = 3306\nval jdbcDatabase =\"DATABASE\"\nval jdbcUrl = s\"jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}?user=${jdbcUsername} password=${jdbcPassword} relaxAutoCommit=true\"\n\nval connectionProperties = new Properties()\nconnectionProperties.put(\"user\", \"USERNAME\")\nconnectionProperties.put(\"password\", \"PASSWORD\")    Fetch the table meta data from the RDB and creates equivalent column tables   val connection = DriverManager.getConnection(jdbcUrl, jdbcUsername, jdbcPassword)\nconnection.isClosed()\nval md:DatabaseMetaData = connection.getMetaData();\nval rs:ResultSet = md.getTables(null, null, \"%\", null);\nwhile (rs.next()) {\n\nval tableName=rs.getString(3)\nval df=snc.read.jdbc(jdbcUrl, tableName, connectionProperties)\ndf.printSchema\ndf.show()\n// Create and load a column table with same schema as that of source table \n   df.write.format(\"column\").mode(SaveMode.Append).saveAsTable(tableName)\n}     Using SQL to access external RDB tables  \nYou can also use plain SQL to access any external RDB using external tables. Create external table on RDBMS table and query it directly from SnappyData as described below:  snc.sql( drop table if exists external_table )\nsnc.sql(s CREATE  external TABLE external_table USING jdbc OPTIONS (dbtable 'tweet', driver 'com.mysql.jdbc.Driver',  user 'root',  password 'root',  url '$jdbcUrl') )\nsnc.sql( select * from external_table ).show  Refer to the  Spark SQL JDBC source access for how to parallelize access when dealing with large data sets .", 
            "title": "Importing Data using JDBC from a relational DB"
        }, 
        {
            "location": "/howto/load_data_from_external_data_stores/#loading-data-from-nosql-store-cassandra", 
            "text": "The example below demonstrates how you can load data from a NoSQL store:   Note  Before you begin, you must install the corresponding Spark-Cassandra connector jar. To do so, copy the Spark-Cassandra connector jar file to the  /jars  directory located in the home directory and then restart the cluster.    val df = snc.read.format( org.apache.spark.sql.cassandra ).options(Map(  table  -   CUSTOMER ,  keyspace  -   test )) .load\ndf.write.format( column ).mode(SaveMode.Append).saveAsTable( CUSTOMER )\nsnc.sql( select * from CUSTOMER ).show", 
            "title": "Loading Data from NoSQL store (Cassandra)"
        }, 
        {
            "location": "/howto/perform_a_colocated_join/", 
            "text": "How to Perform a Colocated Join\n\n\nWhen two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData server. Colocating the data of two tables based on a partitioning column's value is a best practice if you frequently perform queries on those tables that join on that column.\nWhen colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data.\n\n\nCode Example: ORDERS table is colocated with CUSTOMER table\n\n\nA partitioned table can be colocated with another partitioned table by using the \"COLOCATE_WITH\" attribute in the table options. \n\nFor example, in the code snippet below, the ORDERS table is colocated with the CUSTOMER table. The complete source for this example can be found in the file \nColocatedJoinExample.scala\n\n\nGet a SnappySession\n:\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nColocatedJoinExample\n)\n    .master(\nlocal[*]\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate Table Customer:\n\n\nsnSession.sql(\nCREATE TABLE CUSTOMER ( \n +\n    \nC_CUSTKEY     INTEGER NOT NULL,\n +\n    \nC_NAME        VARCHAR(25) NOT NULL,\n +\n    \nC_ADDRESS     VARCHAR(40) NOT NULL,\n +\n    \nC_NATIONKEY   INTEGER NOT NULL,\n +\n    \nC_PHONE       VARCHAR(15) NOT NULL,\n +\n    \nC_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n +\n    \nC_MKTSEGMENT  VARCHAR(10) NOT NULL,\n +\n    \nC_COMMENT     VARCHAR(117) NOT NULL)\n +\n    \nUSING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\n)\n\n\n\n\nCreate Table Orders:\n\n\nsnSession.sql(\nCREATE TABLE ORDERS  ( \n +\n    \nO_ORDERKEY       INTEGER NOT NULL,\n +\n    \nO_CUSTKEY        INTEGER NOT NULL,\n +\n    \nO_ORDERSTATUS    CHAR(1) NOT NULL,\n +\n    \nO_TOTALPRICE     DECIMAL(15,2) NOT NULL,\n +\n    \nO_ORDERDATE      DATE NOT NULL,\n +\n    \nO_ORDERPRIORITY  CHAR(15) NOT NULL,\n +\n    \nO_CLERK          CHAR(15) NOT NULL,\n +\n    \nO_SHIPPRIORITY   INTEGER NOT NULL,\n +\n    \nO_COMMENT        VARCHAR(79) NOT NULL) \n +\n    \nUSING COLUMN OPTIONS (PARTITION_BY 'O_CUSTKEY', \n +\n    \nCOLOCATE_WITH 'CUSTOMER' )\n)\n\n\n\n\nPerform a Colocate join:\n \n\n\n// Selecting orders for all customers\nval result = snSession.sql(\nSELECT C_CUSTKEY, C_NAME, O_ORDERKEY, O_ORDERSTATUS, O_ORDERDATE, \n +\n    \nO_TOTALPRICE FROM CUSTOMER, ORDERS WHERE C_CUSTKEY = O_CUSTKEY\n).collect()", 
            "title": "How to Perform a Colocated Join"
        }, 
        {
            "location": "/howto/perform_a_colocated_join/#how-to-perform-a-colocated-join", 
            "text": "When two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData server. Colocating the data of two tables based on a partitioning column's value is a best practice if you frequently perform queries on those tables that join on that column.\nWhen colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data.  Code Example: ORDERS table is colocated with CUSTOMER table  A partitioned table can be colocated with another partitioned table by using the \"COLOCATE_WITH\" attribute in the table options.  \nFor example, in the code snippet below, the ORDERS table is colocated with the CUSTOMER table. The complete source for this example can be found in the file  ColocatedJoinExample.scala  Get a SnappySession :  val spark: SparkSession = SparkSession\n    .builder\n    .appName( ColocatedJoinExample )\n    .master( local[*] )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)  Create Table Customer:  snSession.sql( CREATE TABLE CUSTOMER (   +\n     C_CUSTKEY     INTEGER NOT NULL,  +\n     C_NAME        VARCHAR(25) NOT NULL,  +\n     C_ADDRESS     VARCHAR(40) NOT NULL,  +\n     C_NATIONKEY   INTEGER NOT NULL,  +\n     C_PHONE       VARCHAR(15) NOT NULL,  +\n     C_ACCTBAL     DECIMAL(15,2)   NOT NULL,  +\n     C_MKTSEGMENT  VARCHAR(10) NOT NULL,  +\n     C_COMMENT     VARCHAR(117) NOT NULL)  +\n     USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') )  Create Table Orders:  snSession.sql( CREATE TABLE ORDERS  (   +\n     O_ORDERKEY       INTEGER NOT NULL,  +\n     O_CUSTKEY        INTEGER NOT NULL,  +\n     O_ORDERSTATUS    CHAR(1) NOT NULL,  +\n     O_TOTALPRICE     DECIMAL(15,2) NOT NULL,  +\n     O_ORDERDATE      DATE NOT NULL,  +\n     O_ORDERPRIORITY  CHAR(15) NOT NULL,  +\n     O_CLERK          CHAR(15) NOT NULL,  +\n     O_SHIPPRIORITY   INTEGER NOT NULL,  +\n     O_COMMENT        VARCHAR(79) NOT NULL)   +\n     USING COLUMN OPTIONS (PARTITION_BY 'O_CUSTKEY',   +\n     COLOCATE_WITH 'CUSTOMER' ) )  Perform a Colocate join:    // Selecting orders for all customers\nval result = snSession.sql( SELECT C_CUSTKEY, C_NAME, O_ORDERKEY, O_ORDERSTATUS, O_ORDERDATE,   +\n     O_TOTALPRICE FROM CUSTOMER, ORDERS WHERE C_CUSTKEY = O_CUSTKEY ).collect()", 
            "title": "How to Perform a Colocated Join"
        }, 
        {
            "location": "/howto/connect_using_jdbc_driver/", 
            "text": "How to Connect using JDBC Driver\n\n\nYou can connect to and execute queries against SnappyData cluster using JDBC driver. \nThe connection URL typically points to one of the locators. The locator passes the information of all available servers, based on which the driver automatically connects to one of the servers.\n\n\nTo connect to the SnappyData cluster using JDBC, use URL of the form \njdbc:snappydata://\nlocatorHostName\n:\nlocatorClientPort\n/\n\n\nWhere the \nlocatorHostName\n is the hostname of the node on which the locator is started and \nlocatorClientPort\n is the port on which the locator accepts client connections (default 1527).\n\n\nYou can use Maven or SBT dependencies to get the latest SnappyData JBDC driver which is used for establishing the JDBC connection with SnappyData. Other than this you can also directly download the JDBC driver from the SnappyData release page. \n\n\nUsing Maven/SBT Dependencies\n\n\nYou can use the Maven or the SBT dependencies to get the latest released version of SnappyData JDBC driver.\n\n\nExample: Maven dependency\n\n\n!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client --\n\n\ndependency\n\n    \ngroupId\nio.snappydata\n/groupId\n\n    \nartifactId\nsnappydata-jdbc_2.11\n/artifactId\n\n    \nversion\n1.0.2.2\n/version\n\n\n/dependency\n\n\n\n\n\nExample: SBT dependency\n\n\n// https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client\nlibraryDependencies += \nio.snappydata\n % \nsnappydata-jdbc_2.11\n % \n1.0.2.2\n\n\n\n\n\n\n\nNote\n\n\nIf your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due to an issue with its pom file. \nAs a workaround, add the below code to the \nbuild.sbt\n:\n\n\n\n\nval workaround = {\n  sys.props += \npackaging.type\n -\n \njar\n\n  ()\n}\n\n\n\n\nFor more details, refer \nhttps://github.com/sbt/sbt/issues/3618\n.\n\n\nDowloading SnappyData JDBC Driver Jar\n\n\nYou can directly \ndownload the SnappyData JDBC driver\n from the latest SnappyData release page. Scroll down to download the SnappyData JDBC driver jar which is listed in the \nDescription of download Artifacts\n \n \nAssets\n section.\n\n\nCode Example\n\n\nConnect to a SnappyData cluster using JDBC on default client port\n\n\nThe code snippet shows how to connect to a SnappyData cluster using JDBC on default client port 1527. The complete source code of the example is located at \nJDBCExample.scala\n\n\nval url: String = s\njdbc:snappydata://localhost:1527/\n\nval conn1 = DriverManager.getConnection(url)\n\nval stmt1 = conn1.createStatement()\n// Creating a table (PARTSUPP) using JDBC connection\nstmt1.execute(\nDROP TABLE IF EXISTS APP.PARTSUPP\n)\nstmt1.execute(\nCREATE TABLE APP.PARTSUPP ( \n +\n     \nPS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,\n +\n     \nPS_SUPPKEY     INTEGER NOT NULL,\n +\n     \nPS_AVAILQTY    INTEGER NOT NULL,\n +\n     \nPS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)\n +\n    \nUSING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY')\n)\n\n// Inserting records in PARTSUPP table via batch inserts\nval preparedStmt1 = conn1.prepareStatement(\nINSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?)\n)\n\nvar x = 0\nfor (x \n- 1 to 10) {\n  preparedStmt1.setInt(1, x*100)\n  preparedStmt1.setInt(2, x)\n  preparedStmt1.setInt(3, x*1000)\n  preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2))\n  preparedStmt1.addBatch()\n}\npreparedStmt1.executeBatch()\npreparedStmt1.close()\n\n\n\n\n\n\nNote\n\n\nIf the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the \nio.snappydata.jdbc.ClientDriver\n class.\n\n\n\n\n\n\nConnecting with JDBC Client Pool Driver\n\n\nJDBC client pool driver provides built-in connection pooling and relies on the non-pooled \nJDBC driver\n. The driver initializes the pool when the first connection is created using this driver. Thereafter, for every request, the connection is returned from the pool instead of establishing a new connection with the server. \nWe recommend using the pooled driver for low latency operations such as point lookups and when using the Spark JDBC data source API (see example below). When you access SnappyData from Java frameworks such as Spring, we recommend using pooling provided in the framework and switch to using the non-pooled driver. \n\n\n\n\nImportant\n\n\nThe underlying pool is uniquely associated with the set of properties that are passed while creating the connection. If any of the properties change, a new pool is created.\n\n\n\n\nTo connect to SnappyData Cluster using JDBC client pool driver\n, use the url of the form: \n \njdbc:snappydata:pool://\nhost\n:\nport\n\nWhere \nhost\n is the hostname of the node on which the locator is started and \nport\n is the port on which the locator accepts client connections (default 1527).\n\n\nThe client pool driver class name is \nio.snappydata.jdbc.ClientPoolDriver\n.\n\n\nThe following pool related properties can be used to tune the JDBC client pool driver:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npool.user\n\n\nThe username to be passed to the JDBC client pool driver to establish a connection.\n\n\n\n\n\n\npool.password\n\n\nThe password to be passed to the JDBC  client pool driver to establish a connection.\n\n\n\n\n\n\npool.initialSize\n\n\nThe initial number of connections that are created when the pool is started. Default value is \nmax(256, availableProcessors * 8)\n.\n\n\n\n\n\n\npool.maxActive\n\n\nThe maximum number of active connections that can be allocated from this pool at a time. The default value is \nmax(256, availableProcessors * 8)\n.\n\n\n\n\n\n\npool.minIdle\n\n\nThe minimum number of established connections that should be maintained in the client pool. Default value is \n1\n.\n\n\n\n\n\n\npool.maxIdle\n\n\nThe maximum number of connections that should be maintained in the client pool. Default value is \nmaxActive:\nmax(256, availableProcessors * 8)\n. Idle connections are checked periodically, if enabled, and the connections that are idle for more than the time set in \nminEvictableIdleTimeMillis\n are released.\n\n\n\n\n\n\npool.maxWait\n\n\n(int) The maximum waiting period, in milliseconds, for establishing a connection after which an exception is thrown. Default value is 30000 (30 seconds).\n\n\n\n\n\n\npool.removeAbandoned\n\n\nFlag to remove the abandoned connections, in case they exceed the settings for \nremoveAbandonedTimeout\n. If set to true a connection is considered abandoned and eligible for removal, if its no longer in use than the settings for \nremoveAbandonedTimeout\n. Setting this to \ntrue\n can recover db connections from applications that fail to close a connection. The default value is \nfalse\n.\n\n\n\n\n\n\npool.removeAbandonedTimeout\n\n\nTimeout in seconds before an abandoned connection, that was in use, can be removed. The default value is 60 seconds. The value should be set to the time required for the longest running query in your applications.\n\n\n\n\n\n\npool.timeBetweenEvictionRunsMillis\n\n\nTime period required to sleep between runs of the idle connection validation/cleaner thread. You should always set this value above one second. This time period determines how often we check for idle and abandoned connections and how often to validate the idle connections. The default value is 5000 (5 seconds).\n\n\n\n\n\n\npool.minEvictableIdleTimeMillis\n\n\nThe minimum time period, in milliseconds, for which an object can be idle in the pool before it qualifies for eviction. The default value is 60000 (60 seconds).\n\n\n\n\n\n\ndriver\n\n\nio.snappydata.jdbc.ClientPoolDriver\nThis should be passed through Spark JDBC API for loading and using the driver.\n\n\n\n\n\n\npool.testOnBorrow\n\n\nIndicates if the objects are validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and will attempt to borrow another. In order to have a more efficient validation, see \npool.validationInterval\n. Default value is \ntrue\n.\n\n\n\n\n\n\npool.validationInterval\n\n\nAvoid excess validation, only run validation at most at this frequency - time in milliseconds. If a connection is due for validation, but has been validated previously within this interval, it will not be validated again. The default value is 10000 (10 seconds).\n\n\n\n\n\n\n\n\nExample Code Snippet:\n\n\nval properties = new Properties()\nproperties.setProperty(\npool.user\n, \nuser\n)\nproperties.setProperty(\npool.password\n, \npass\n)\nproperties.setProperty(\ndriver\n, \u201c\u201cio.snappydata.jdbc.ClientPoolDriver\u201d\u201d)\n\nval builder = SparkSession\n.builder.\nappName(\napp\n)\n.master(\nlocal[*]\n)\n\nval spark: SparkSession = builder.getOrCreate\n\nval df = spark.read.jdbc(\u201cjdbc:snappydata:pool://localhost:1527\u201d, \nTable_X\n, properties)\n\n\n\n\n\nLimitations\n\n\nIf you set any of the following properties for a pooled connection, it gets auto-reset to the default values whenever you obtain a new pooled connection.\n\n\n\n\nsetAutoCommit\n\n\nsetTransactionIsolation\n\n\nsetReadOnly\n\n\n\n\nHowever, if you have set any of the other properties (e.g. spark or snappy AQP related properties), it does not get auto-reset when you obtain a new pooled connection.", 
            "title": "How to Connect using JDBC Driver"
        }, 
        {
            "location": "/howto/connect_using_jdbc_driver/#how-to-connect-using-jdbc-driver", 
            "text": "You can connect to and execute queries against SnappyData cluster using JDBC driver. \nThe connection URL typically points to one of the locators. The locator passes the information of all available servers, based on which the driver automatically connects to one of the servers.  To connect to the SnappyData cluster using JDBC, use URL of the form  jdbc:snappydata:// locatorHostName : locatorClientPort /  Where the  locatorHostName  is the hostname of the node on which the locator is started and  locatorClientPort  is the port on which the locator accepts client connections (default 1527).  You can use Maven or SBT dependencies to get the latest SnappyData JBDC driver which is used for establishing the JDBC connection with SnappyData. Other than this you can also directly download the JDBC driver from the SnappyData release page.", 
            "title": "How to Connect using JDBC Driver"
        }, 
        {
            "location": "/howto/connect_using_jdbc_driver/#using-mavensbt-dependencies", 
            "text": "You can use the Maven or the SBT dependencies to get the latest released version of SnappyData JDBC driver.  Example: Maven dependency  !-- https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client --  dependency \n     groupId io.snappydata /groupId \n     artifactId snappydata-jdbc_2.11 /artifactId \n     version 1.0.2.2 /version  /dependency   Example: SBT dependency  // https://mvnrepository.com/artifact/io.snappydata/snappydata-store-client\nlibraryDependencies +=  io.snappydata  %  snappydata-jdbc_2.11  %  1.0.2.2    Note  If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due to an issue with its pom file.  As a workaround, add the below code to the  build.sbt :   val workaround = {\n  sys.props +=  packaging.type  -   jar \n  ()\n}  For more details, refer  https://github.com/sbt/sbt/issues/3618 .", 
            "title": "Using Maven/SBT Dependencies"
        }, 
        {
            "location": "/howto/connect_using_jdbc_driver/#dowloading-snappydata-jdbc-driver-jar", 
            "text": "You can directly  download the SnappyData JDBC driver  from the latest SnappyData release page. Scroll down to download the SnappyData JDBC driver jar which is listed in the  Description of download Artifacts     Assets  section.", 
            "title": "Dowloading SnappyData JDBC Driver Jar"
        }, 
        {
            "location": "/howto/connect_using_jdbc_driver/#code-example", 
            "text": "Connect to a SnappyData cluster using JDBC on default client port  The code snippet shows how to connect to a SnappyData cluster using JDBC on default client port 1527. The complete source code of the example is located at  JDBCExample.scala  val url: String = s jdbc:snappydata://localhost:1527/ \nval conn1 = DriverManager.getConnection(url)\n\nval stmt1 = conn1.createStatement()\n// Creating a table (PARTSUPP) using JDBC connection\nstmt1.execute( DROP TABLE IF EXISTS APP.PARTSUPP )\nstmt1.execute( CREATE TABLE APP.PARTSUPP (   +\n      PS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,  +\n      PS_SUPPKEY     INTEGER NOT NULL,  +\n      PS_AVAILQTY    INTEGER NOT NULL,  +\n      PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)  +\n     USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') )\n\n// Inserting records in PARTSUPP table via batch inserts\nval preparedStmt1 = conn1.prepareStatement( INSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?) )\n\nvar x = 0\nfor (x  - 1 to 10) {\n  preparedStmt1.setInt(1, x*100)\n  preparedStmt1.setInt(2, x)\n  preparedStmt1.setInt(3, x*1000)\n  preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2))\n  preparedStmt1.addBatch()\n}\npreparedStmt1.executeBatch()\npreparedStmt1.close()   Note  If the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the  io.snappydata.jdbc.ClientDriver  class.", 
            "title": "Code Example"
        }, 
        {
            "location": "/howto/connect_using_jdbc_driver/#connecting-with-jdbc-client-pool-driver", 
            "text": "JDBC client pool driver provides built-in connection pooling and relies on the non-pooled  JDBC driver . The driver initializes the pool when the first connection is created using this driver. Thereafter, for every request, the connection is returned from the pool instead of establishing a new connection with the server. \nWe recommend using the pooled driver for low latency operations such as point lookups and when using the Spark JDBC data source API (see example below). When you access SnappyData from Java frameworks such as Spring, we recommend using pooling provided in the framework and switch to using the non-pooled driver.    Important  The underlying pool is uniquely associated with the set of properties that are passed while creating the connection. If any of the properties change, a new pool is created.   To connect to SnappyData Cluster using JDBC client pool driver , use the url of the form:    jdbc:snappydata:pool:// host : port \nWhere  host  is the hostname of the node on which the locator is started and  port  is the port on which the locator accepts client connections (default 1527).  The client pool driver class name is  io.snappydata.jdbc.ClientPoolDriver .  The following pool related properties can be used to tune the JDBC client pool driver:     Property  Description      pool.user  The username to be passed to the JDBC client pool driver to establish a connection.    pool.password  The password to be passed to the JDBC  client pool driver to establish a connection.    pool.initialSize  The initial number of connections that are created when the pool is started. Default value is  max(256, availableProcessors * 8) .    pool.maxActive  The maximum number of active connections that can be allocated from this pool at a time. The default value is  max(256, availableProcessors * 8) .    pool.minIdle  The minimum number of established connections that should be maintained in the client pool. Default value is  1 .    pool.maxIdle  The maximum number of connections that should be maintained in the client pool. Default value is  maxActive: max(256, availableProcessors * 8) . Idle connections are checked periodically, if enabled, and the connections that are idle for more than the time set in  minEvictableIdleTimeMillis  are released.    pool.maxWait  (int) The maximum waiting period, in milliseconds, for establishing a connection after which an exception is thrown. Default value is 30000 (30 seconds).    pool.removeAbandoned  Flag to remove the abandoned connections, in case they exceed the settings for  removeAbandonedTimeout . If set to true a connection is considered abandoned and eligible for removal, if its no longer in use than the settings for  removeAbandonedTimeout . Setting this to  true  can recover db connections from applications that fail to close a connection. The default value is  false .    pool.removeAbandonedTimeout  Timeout in seconds before an abandoned connection, that was in use, can be removed. The default value is 60 seconds. The value should be set to the time required for the longest running query in your applications.    pool.timeBetweenEvictionRunsMillis  Time period required to sleep between runs of the idle connection validation/cleaner thread. You should always set this value above one second. This time period determines how often we check for idle and abandoned connections and how often to validate the idle connections. The default value is 5000 (5 seconds).    pool.minEvictableIdleTimeMillis  The minimum time period, in milliseconds, for which an object can be idle in the pool before it qualifies for eviction. The default value is 60000 (60 seconds).    driver  io.snappydata.jdbc.ClientPoolDriver This should be passed through Spark JDBC API for loading and using the driver.    pool.testOnBorrow  Indicates if the objects are validated before being borrowed from the pool. If the object fails to validate, it will be dropped from the pool, and will attempt to borrow another. In order to have a more efficient validation, see  pool.validationInterval . Default value is  true .    pool.validationInterval  Avoid excess validation, only run validation at most at this frequency - time in milliseconds. If a connection is due for validation, but has been validated previously within this interval, it will not be validated again. The default value is 10000 (10 seconds).     Example Code Snippet:  val properties = new Properties()\nproperties.setProperty( pool.user ,  user )\nproperties.setProperty( pool.password ,  pass )\nproperties.setProperty( driver , \u201c\u201cio.snappydata.jdbc.ClientPoolDriver\u201d\u201d)\n\nval builder = SparkSession\n.builder.\nappName( app )\n.master( local[*] )\n\nval spark: SparkSession = builder.getOrCreate\n\nval df = spark.read.jdbc(\u201cjdbc:snappydata:pool://localhost:1527\u201d,  Table_X , properties)", 
            "title": "Connecting with JDBC Client Pool Driver"
        }, 
        {
            "location": "/howto/connect_using_jdbc_driver/#limitations", 
            "text": "If you set any of the following properties for a pooled connection, it gets auto-reset to the default values whenever you obtain a new pooled connection.   setAutoCommit  setTransactionIsolation  setReadOnly   However, if you have set any of the other properties (e.g. spark or snappy AQP related properties), it does not get auto-reset when you obtain a new pooled connection.", 
            "title": "Limitations"
        }, 
        {
            "location": "/howto/using_snappydata_for_any_spark_dist/", 
            "text": "How to use SnappyData for any Spark Distribution\n\n\nThe \nsnappydata-jdbc Spark \npackage adds extensions to Spark\u2019s inbuilt JDBC data source provider to work better with SnappyData. This allows SnappyData to be treated as a regular JDBC data source with all versions of Spark which are greater or equal to 2.1, while also providing speed to direct SnappyData embedded cluster for many types of queries.\n\n\nFollowing is a sample of Spark JDBC extension setup and usage: \n\n\n\n\n\n\nInclude the \nsnappydata-jdbc\n package in the Spark job with spark-submit or spark-shell:\n\n\n$SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2\n\n\n\n\n\n\n\nSet the session properties.\nThe SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in \nspark-defaults.conf \n along with all the other Spark properties.\n Following is a sample code of configuring the properties in \nSparkConf\n:\n\n\n$SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2 --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.user=\nuser\n --conf spark.snappydata.password=\npassword\n\n\n\n\nOverloads of the above methods accepting \nuser+password\n and \nhost+port\n is also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the \nDataFrameReader.jdbc\n method.\n\n\n\n\n\n\nImport the required implicits in the job/shell code as follows:\n\n\nimport io.snappydata.sql.implicits._\n\n\n\n\n\n\n\nAfter the required session properties are set (connection and user/password etc.), you can run the queries/DMLs without any other configuration as shown here:\n\n\n// execute DDL\nspark.snappyExecute(\"create table testTable1 (id long, data string) using column\")\n// DML\nspark.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\")\n// bulk insert from external table in embedded mode\nspark.snappyExecute(\"insert into testTable1 select * from externalTable1\")\n// query\nval dataSet = spark.snappyQuery(\u201cselect count(*) from testTable1\u201d)\n\n\n\n\n\n\n\nFor Java\n\n    When using Java, the wrapper must be created explicitly as shown in the following sample:\n\n\n    import org.apache.spark.sql.*;\n\n    JdbcExecute exec = new JdbcExecute(spark);\n    exec.snappyExecute(\u201ccreate table testTable1 (id long, data string) using column\u201d);\n    exec.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\");\n    DataFrame df = exec.snappyQuery(...);\n    ...", 
            "title": "How to use SnappyData for any Spark Distribution"
        }, 
        {
            "location": "/howto/using_snappydata_for_any_spark_dist/#how-to-use-snappydata-for-any-spark-distribution", 
            "text": "The  snappydata-jdbc Spark  package adds extensions to Spark\u2019s inbuilt JDBC data source provider to work better with SnappyData. This allows SnappyData to be treated as a regular JDBC data source with all versions of Spark which are greater or equal to 2.1, while also providing speed to direct SnappyData embedded cluster for many types of queries.  Following is a sample of Spark JDBC extension setup and usage:     Include the  snappydata-jdbc  package in the Spark job with spark-submit or spark-shell:  $SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2    Set the session properties. The SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in  spark-defaults.conf   along with all the other Spark properties.  Following is a sample code of configuring the properties in  SparkConf :  $SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2 --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.user= user  --conf spark.snappydata.password= password   Overloads of the above methods accepting  user+password  and  host+port  is also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the  DataFrameReader.jdbc  method.    Import the required implicits in the job/shell code as follows:  import io.snappydata.sql.implicits._    After the required session properties are set (connection and user/password etc.), you can run the queries/DMLs without any other configuration as shown here:  // execute DDL\nspark.snappyExecute(\"create table testTable1 (id long, data string) using column\")\n// DML\nspark.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\")\n// bulk insert from external table in embedded mode\nspark.snappyExecute(\"insert into testTable1 select * from externalTable1\")\n// query\nval dataSet = spark.snappyQuery(\u201cselect count(*) from testTable1\u201d)    For Java \n    When using Java, the wrapper must be created explicitly as shown in the following sample:      import org.apache.spark.sql.*;\n\n    JdbcExecute exec = new JdbcExecute(spark);\n    exec.snappyExecute(\u201ccreate table testTable1 (id long, data string) using column\u201d);\n    exec.snappyExecute(\"insert into testTable1 values (1, \u2018data1\u2019)\");\n    DataFrame df = exec.snappyQuery(...);\n    ...", 
            "title": "How to use SnappyData for any Spark Distribution"
        }, 
        {
            "location": "/howto/store_retrieve_complex_datatypes_JDBC/", 
            "text": "How to Store and Retrieve Complex Data Types in JDBC Programs\n\n\nIf you want to store/retrieve objects for complex data types (Array, Map and Struct) using JDBC programs, SnappyData provides \ncom.pivotal.gemfirexd.snappy.ComplexTypeSerializer\n utility class to serialize/deserialize those objects:\n\n\n\n\nA column of type \nARRAY\n can store array of Java objects (Object[]), typed arrays, java.util.Collection, and scala.collection.Seq.\n\n\nA column of type \nMAP\n can store java.util.Map or scala.collection.Map.\n\n\nA column of type \nSTRUCT\n can store array of Java objects (Object[]), typed arrays, java.util.Collection, scala.collection.Seq, or scala.Product\n\n\n\n\n\n\nNote\n\n\nComplex data types are supported only for column tables.\n\n\n\n\nCode Example: Storing and Retrieving Array Data in a JDBC program\n\n\nThe following scala code snippets show how to perform insert and select operations on columns of complex data types. Complete source code for example is available at \nJDBCWithComplexTypes.scala\n \n\n\n// create a JDBC connection\nval url: String = s\njdbc:snappydata://localhost:1527/\n\nval conn = DriverManager.getConnection(url)\n\nval stmt = conn.createStatement()\n// create a table with a column of type array\nstmt.execute(\nCREATE TABLE TABLE_WITH_COMPLEX_TYPES (col1 Int, col2 Array\nDecimal\n) USING column options()\n)\n\n\n\n\nInserting Data\n\n\n\n\n\n\nInsert a single row having a complex type (array)\n\n\nval arrDecimal = Array(Decimal(\"4.92\"), Decimal(\"51.98\"))\nval pstmt = conn.prepareStatement(\"insert into TABLE_WITH_COMPLEX_TYPE values (?, ?)\")\n\n\n\n\n\n\n\nCreate a serializer that can be used to serialize array data and insert into the table.\n\n\nval serializer1 = ComplexTypeSerializer.create(tableName, \"col2\", conn)\npstmt.setInt(1, 1)\npstmt.setBytes(2, serializer1.serialize(arrDecimal))\npstmt.execute\npstmt.close()\n\n\n\n\n\n\n\nSelecting Data\n\n\n// Select array data as a JSON string\n\nvar rs = stmt.executeQuery(s\nSELECT * FROM $tableName\n)\nwhile (rs.next()) {\n // read the column as a String\n val res1 = rs.getString(\ncol2\n)\n println(s\nres1 = $res1\n)\n // Alternate way, read the same column as a Clob\n val res2 = rs.getClob(\ncol2\n)\n println(s\nres2 = \n + res2.getSubString(1, res2.length.asInstanceOf[Int]))\n}\n\n// Reading array data as  BLOB and Bytes and then forming a Scala array\nval serializer = ComplexTypeSerializer.create(tableName, \ncol2\n, conn)\nrs = stmt.executeQuery(s\nSELECT * FROM $tableName --+ complexTypeAsJson(0)\n)\nwhile (rs.next()) {\n  // Read the same column as a byte[] and then deserialize it into an Array\n  val res1 = serializer.deserialize(rs.getBytes(\ncol2\n))\n  println(s\nres1 = $res1\n)\n  // alternate way, read the same column as a Blob an then deserialize it into an Array\n  val res2 = serializer.deserialize(rs.getBlob(\ncol2\n))\n  println(s\nres2 = $res2\n)\n}\n\n\nvar rs = stmt.executeQuery(s\nSELECT * FROM $tableName\n)\nwhile (rs.next()) {\n // read the column as a String\n val res1 = rs.getString(\ncol2\n)\n println(s\nres1 = $res1\n)\n // alternate way, read the same column as a Clob\n val res2 = rs.getClob(\ncol2\n)\n println(s\nres2 = \n + res2.getSubString(1, res2.length.asInstanceOf[Int]))\n}\n\n// reading array data as  BLOB and Bytes and then forming a Scala array\nval serializer = ComplexTypeSerializer.create(tableName, \ncol2\n, conn)\nrs = stmt.executeQuery(s\nSELECT * FROM $tableName --+ complexTypeAsJson(0)\n)\nwhile (rs.next()) {\n  // read the same column as a byte[] and then deserialize it into an Array\n  val res1 = serializer.deserialize(rs.getBytes(\ncol2\n))\n  println(s\nres1 = $res1\n)\n  // alternate way, read the same column as a Blob an then deserialize it into an Array\n  val res2 = serializer.deserialize(rs.getBlob(\ncol2\n))\n  println(s\nres2 = $res2\n)\n}\n\n\n\n\n\n\nSee also:\nHow to connect using JDBC driver", 
            "title": "How to Store and Retrieve Complex Data Types in JDBC Programs"
        }, 
        {
            "location": "/howto/store_retrieve_complex_datatypes_JDBC/#how-to-store-and-retrieve-complex-data-types-in-jdbc-programs", 
            "text": "If you want to store/retrieve objects for complex data types (Array, Map and Struct) using JDBC programs, SnappyData provides  com.pivotal.gemfirexd.snappy.ComplexTypeSerializer  utility class to serialize/deserialize those objects:   A column of type  ARRAY  can store array of Java objects (Object[]), typed arrays, java.util.Collection, and scala.collection.Seq.  A column of type  MAP  can store java.util.Map or scala.collection.Map.  A column of type  STRUCT  can store array of Java objects (Object[]), typed arrays, java.util.Collection, scala.collection.Seq, or scala.Product    Note  Complex data types are supported only for column tables.", 
            "title": "How to Store and Retrieve Complex Data Types in JDBC Programs"
        }, 
        {
            "location": "/howto/store_retrieve_complex_datatypes_JDBC/#code-example-storing-and-retrieving-array-data-in-a-jdbc-program", 
            "text": "The following scala code snippets show how to perform insert and select operations on columns of complex data types. Complete source code for example is available at  JDBCWithComplexTypes.scala    // create a JDBC connection\nval url: String = s jdbc:snappydata://localhost:1527/ \nval conn = DriverManager.getConnection(url)\n\nval stmt = conn.createStatement()\n// create a table with a column of type array\nstmt.execute( CREATE TABLE TABLE_WITH_COMPLEX_TYPES (col1 Int, col2 Array Decimal ) USING column options() )", 
            "title": "Code Example: Storing and Retrieving Array Data in a JDBC program"
        }, 
        {
            "location": "/howto/store_retrieve_complex_datatypes_JDBC/#inserting-data", 
            "text": "Insert a single row having a complex type (array)  val arrDecimal = Array(Decimal(\"4.92\"), Decimal(\"51.98\"))\nval pstmt = conn.prepareStatement(\"insert into TABLE_WITH_COMPLEX_TYPE values (?, ?)\")    Create a serializer that can be used to serialize array data and insert into the table.  val serializer1 = ComplexTypeSerializer.create(tableName, \"col2\", conn)\npstmt.setInt(1, 1)\npstmt.setBytes(2, serializer1.serialize(arrDecimal))\npstmt.execute\npstmt.close()", 
            "title": "Inserting Data"
        }, 
        {
            "location": "/howto/store_retrieve_complex_datatypes_JDBC/#selecting-data", 
            "text": "// Select array data as a JSON string\n\nvar rs = stmt.executeQuery(s SELECT * FROM $tableName )\nwhile (rs.next()) {\n // read the column as a String\n val res1 = rs.getString( col2 )\n println(s res1 = $res1 )\n // Alternate way, read the same column as a Clob\n val res2 = rs.getClob( col2 )\n println(s res2 =   + res2.getSubString(1, res2.length.asInstanceOf[Int]))\n}\n\n// Reading array data as  BLOB and Bytes and then forming a Scala array\nval serializer = ComplexTypeSerializer.create(tableName,  col2 , conn)\nrs = stmt.executeQuery(s SELECT * FROM $tableName --+ complexTypeAsJson(0) )\nwhile (rs.next()) {\n  // Read the same column as a byte[] and then deserialize it into an Array\n  val res1 = serializer.deserialize(rs.getBytes( col2 ))\n  println(s res1 = $res1 )\n  // alternate way, read the same column as a Blob an then deserialize it into an Array\n  val res2 = serializer.deserialize(rs.getBlob( col2 ))\n  println(s res2 = $res2 )\n}\n\n\nvar rs = stmt.executeQuery(s SELECT * FROM $tableName )\nwhile (rs.next()) {\n // read the column as a String\n val res1 = rs.getString( col2 )\n println(s res1 = $res1 )\n // alternate way, read the same column as a Clob\n val res2 = rs.getClob( col2 )\n println(s res2 =   + res2.getSubString(1, res2.length.asInstanceOf[Int]))\n}\n\n// reading array data as  BLOB and Bytes and then forming a Scala array\nval serializer = ComplexTypeSerializer.create(tableName,  col2 , conn)\nrs = stmt.executeQuery(s SELECT * FROM $tableName --+ complexTypeAsJson(0) )\nwhile (rs.next()) {\n  // read the same column as a byte[] and then deserialize it into an Array\n  val res1 = serializer.deserialize(rs.getBytes( col2 ))\n  println(s res1 = $res1 )\n  // alternate way, read the same column as a Blob an then deserialize it into an Array\n  val res2 = serializer.deserialize(rs.getBlob( col2 ))\n  println(s res2 = $res2 )\n}  See also: How to connect using JDBC driver", 
            "title": "Selecting Data"
        }, 
        {
            "location": "/howto/store_and_query_json_objects/", 
            "text": "How to Store and Query JSON Objects\n\n\nYou can insert JSON data in SnappyData tables and execute queries on the tables.\n\n\nCode Example: Loads JSON data from a JSON file into a column table and executes query\n\n\nThe code snippet loads JSON data from a JSON file into a column table and executes the query against it.\nThe source code for JSON example is located at \nWorkingWithJson.scala\n. After creating SnappySession, the JSON file is read using Spark API and loaded into a SnappyData table.\n\n\nGet a SnappySession\n:\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nWorkingWithJson\n)\n    .master(\nlocal[*]\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate a DataFrame from the JSON file\n:\n\n\nval some_people_path = s\nquickstart/src/main/resources/some_people.json\n\n// Read a JSON file using Spark API\nval people = snSession.read.json(some_people_path)\npeople.printSchema()\n\n\n\n\nCreate a SnappyData table and insert the JSON data in it using the DataFrame\n:\n\n\n//Drop the table if it exists\nsnSession.dropTable(\npeople\n, ifExists = true)\n\n//Create a columnar table with the Json DataFrame schema\nsnSession.createTable(tableName = \npeople\n,\n  provider = \ncolumn\n,\n  schema = people.schema,\n  options = Map.empty[String,String],\n  allowExisting = false)\n\n// Write the created DataFrame to the columnar table\npeople.write.insertInto(\npeople\n)\n\n\n\n\nAppend more data from a second JSON file\n:\n\n\n// Append more people to the column table\nval more_people_path = s\nquickstart/src/main/resources/more_people.json\n\n\n//Explicitly passing schema to handle record level field mismatch\n// e.g. some records have \ndistrict\n field while some do not.\nval morePeople = snSession.read.schema(people.schema).json(more_people_path)\nmorePeople.write.insertInto(\npeople\n)\n\n//print schema of the table\nprintln(\nPrint Schema of the table\\n################\n)\nprintln(snSession.table(\npeople\n).schema)\n\n\n\n\nExecute queries and return the results\n\n\n// Query it like any other table\nval nameAndAddress = snSession.sql(\nSELECT \n +\n    \nname, \n +\n    \naddress.city, \n +\n    \naddress.state, \n +\n    \naddress.district, \n +\n    \naddress.lane \n +\n    \nFROM people\n)\n\nnameAndAddress.toJSON.show()", 
            "title": "How to Store and Query JSON Objects"
        }, 
        {
            "location": "/howto/store_and_query_json_objects/#how-to-store-and-query-json-objects", 
            "text": "You can insert JSON data in SnappyData tables and execute queries on the tables.  Code Example: Loads JSON data from a JSON file into a column table and executes query  The code snippet loads JSON data from a JSON file into a column table and executes the query against it.\nThe source code for JSON example is located at  WorkingWithJson.scala . After creating SnappySession, the JSON file is read using Spark API and loaded into a SnappyData table.  Get a SnappySession :  val spark: SparkSession = SparkSession\n    .builder\n    .appName( WorkingWithJson )\n    .master( local[*] )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)  Create a DataFrame from the JSON file :  val some_people_path = s quickstart/src/main/resources/some_people.json \n// Read a JSON file using Spark API\nval people = snSession.read.json(some_people_path)\npeople.printSchema()  Create a SnappyData table and insert the JSON data in it using the DataFrame :  //Drop the table if it exists\nsnSession.dropTable( people , ifExists = true)\n\n//Create a columnar table with the Json DataFrame schema\nsnSession.createTable(tableName =  people ,\n  provider =  column ,\n  schema = people.schema,\n  options = Map.empty[String,String],\n  allowExisting = false)\n\n// Write the created DataFrame to the columnar table\npeople.write.insertInto( people )  Append more data from a second JSON file :  // Append more people to the column table\nval more_people_path = s quickstart/src/main/resources/more_people.json \n\n//Explicitly passing schema to handle record level field mismatch\n// e.g. some records have  district  field while some do not.\nval morePeople = snSession.read.schema(people.schema).json(more_people_path)\nmorePeople.write.insertInto( people )\n\n//print schema of the table\nprintln( Print Schema of the table\\n################ )\nprintln(snSession.table( people ).schema)  Execute queries and return the results  // Query it like any other table\nval nameAndAddress = snSession.sql( SELECT   +\n     name,   +\n     address.city,   +\n     address.state,   +\n     address.district,   +\n     address.lane   +\n     FROM people )\n\nnameAndAddress.toJSON.show()", 
            "title": "How to Store and Query JSON Objects"
        }, 
        {
            "location": "/howto/store_and_query_objects/", 
            "text": "How to Store and Query Objects\n\n\nYou can use domain object to load data into SnappyData tables and select the data by executing queries against the table.\n\n\nCode Example: Insert Person objects into the column table\n\n\nThe code snippet below inserts Person objects into a column table. The source code for this example is located at \nWorkingWithObjects.scala\n. After creating SnappySession, the Person objects are inserted using Spark API and loads into a SnappyData table.\n\n\nGet a SnappySession\n:\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nCreateReplicatedRowTable\n)\n    .master(\nlocal[4]\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nCreate DataFrame objects\n:\n\n\n//Import the implicits for automatic conversion between Objects to DataSets.\nimport snSession.implicits._\n\n// Create a Dataset using Spark APIs\nval people = Seq(Person(\nTom\n, Address(\nColumbus\n, \nOhio\n), Map(\nfrnd1\n-\n \n8998797979\n, \nfrnd2\n -\n \n09878786886\n))\n  , Person(\nNed\n, Address(\nSan Diego\n, \nCalifornia\n), Map.empty[String,String])).toDS()\n\n\n\n\nCreate a SnappyData table and insert data into it\n:\n\n\n//Drop the table if it exists.\nsnSession.dropTable(\nPersons\n, ifExists = true)\n\n//Create a columnar table with a Struct to store Address\nsnSession.sql(\nCREATE table Persons(name String, address Struct\ncity: String, state:String\n, \n +\n     \nemergencyContacts Map\nString,String\n) using column options()\n)\n\n// Write the created DataFrame to the columnar table.\npeople.write.insertInto(\nPersons\n)\n\n//print schema of the table\nprintln(\nPrint Schema of the table\\n################\n)\nprintln(snSession.table(\nPersons\n).schema)\n\n// Append more people to the column table\nval morePeople = Seq(Person(\nJon Snow\n, Address(\nColumbus\n, \nOhio\n), Map.empty[String,String]),\n  Person(\nRob Stark\n, Address(\nSan Diego\n, \nCalifornia\n), Map.empty[String,String]),\n  Person(\nMichael\n, Address(\nNull\n, \nCalifornia\n), Map.empty[String,String])).toDS()\n\nmorePeople.write.insertInto(\nPersons\n)\n\n\n\n\nExecute query on the table and return results\n:\n\n\n// Query it like any other table\nval nameAndAddress = snSession.sql(\nSELECT name, address, emergencyContacts FROM Persons\n)\n\n//Reconstruct the objects from obtained Row\nval allPersons = nameAndAddress.as[Person]\n//allPersons is a Spark Dataset of Person objects. \n// Use of the Dataset APIs to transform, query this data set.", 
            "title": "How to Store and Query Objects"
        }, 
        {
            "location": "/howto/store_and_query_objects/#how-to-store-and-query-objects", 
            "text": "You can use domain object to load data into SnappyData tables and select the data by executing queries against the table.  Code Example: Insert Person objects into the column table  The code snippet below inserts Person objects into a column table. The source code for this example is located at  WorkingWithObjects.scala . After creating SnappySession, the Person objects are inserted using Spark API and loads into a SnappyData table.  Get a SnappySession :  val spark: SparkSession = SparkSession\n    .builder\n    .appName( CreateReplicatedRowTable )\n    .master( local[4] )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)  Create DataFrame objects :  //Import the implicits for automatic conversion between Objects to DataSets.\nimport snSession.implicits._\n\n// Create a Dataset using Spark APIs\nval people = Seq(Person( Tom , Address( Columbus ,  Ohio ), Map( frnd1 -   8998797979 ,  frnd2  -   09878786886 ))\n  , Person( Ned , Address( San Diego ,  California ), Map.empty[String,String])).toDS()  Create a SnappyData table and insert data into it :  //Drop the table if it exists.\nsnSession.dropTable( Persons , ifExists = true)\n\n//Create a columnar table with a Struct to store Address\nsnSession.sql( CREATE table Persons(name String, address Struct city: String, state:String ,   +\n      emergencyContacts Map String,String ) using column options() )\n\n// Write the created DataFrame to the columnar table.\npeople.write.insertInto( Persons )\n\n//print schema of the table\nprintln( Print Schema of the table\\n################ )\nprintln(snSession.table( Persons ).schema)\n\n// Append more people to the column table\nval morePeople = Seq(Person( Jon Snow , Address( Columbus ,  Ohio ), Map.empty[String,String]),\n  Person( Rob Stark , Address( San Diego ,  California ), Map.empty[String,String]),\n  Person( Michael , Address( Null ,  California ), Map.empty[String,String])).toDS()\n\nmorePeople.write.insertInto( Persons )  Execute query on the table and return results :  // Query it like any other table\nval nameAndAddress = snSession.sql( SELECT name, address, emergencyContacts FROM Persons )\n\n//Reconstruct the objects from obtained Row\nval allPersons = nameAndAddress.as[Person]\n//allPersons is a Spark Dataset of Person objects. \n// Use of the Dataset APIs to transform, query this data set.", 
            "title": "How to Store and Query Objects"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/", 
            "text": "How to use Stream Processing with SnappyData\n\n\nSnappyData supports both the older \nSpark Streaming model (based on DStreams)\n as well as the newer \nStructured Streaming model\n. Unlike the Spark streaming DStreams model, that is based on RDDs, SnappyData supports Spark SQL in both models.\n\n\n \n\n\nSpark Streaming DStreams Model\n\n\nSnappyData\u2019s streaming functionality builds on top of Spark Streaming and is primarily aimed at making it simpler to build streaming applications and to integrate with the built-in store. In SnappyData, you can define streams declaratively from any SQL client, register continuous queries on streams, mutate SnappyData tables based on the streaming data. For more information on streaming, refer to this \nsection\n.\n\n\nCode Sample\n\n\nCode example for streaming is in \nStreamingExample.scala\n. The code snippets in the following sections show how to declare a stream table, register continuous queries(CQ), and update SnappyData table using the stream data.\n\n\nUsing Stream Processing with SnappyData\n\n\nFirst get a SnappySession and a SnappyStreamingContext\n: \n\nHere SnappyStreamingContext is initialized in a batch duration of one second.\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(getClass.getSimpleName)\n    .master(\nlocal[*]\n)\n    .getOrCreate\n\nval snsc = new SnappyStreamingContext(spark.sparkContext, Seconds(1))\n\n\n\n\nThe example starts an embedded Kafka instance on which a few messages are published. SnappyData processes these message and updates a table based on the stream data.\n\n\nThe SQL below shows how to declare a stream table using SQL. The rowConverter attribute specifies a class used to return Row objects from the received stream messages.\n\n\n snsc.sql(\n      \ncreate stream table adImpressionStream (\n +\n        \n time_stamp timestamp,\n +\n        \n publisher string,\n +\n        \n advertiser string,\n +\n        \n website string,\n +\n        \n geo string,\n +\n        \n bid double,\n +\n        \n cookie string) \n + \n using kafka_stream options(\n +\n        \n rowConverter 'org.apache.spark.examples.snappydata.RowsConverter',\n +\n        s\n kafkaParams 'bootstrap.servers-\n$add;\n +\n        \nkey.deserializer-\norg.apache.kafka.common.serialization.StringDeserializer;\n +\n        \nvalue.deserializer-\norg.apache.kafka.common.serialization.StringDeserializer;\n +\n        s\ngroup.id-\n$groupId;auto.offset.reset-\nearliest',\n +\n        s\n startingOffsets '$startingOffsets', \n +\n        s\n subscribe '$topic')\n\n    )\n\n\n\n\nRowsConverter decodes a stream message consisting of comma-separated fields and forms a Row object from it.\n\n\nclass RowsConverter extends StreamToRowsConverter with Serializable {\n\n  override def toRows(message: Any): Seq[Row] = {\n    val log = message.asInstanceOf[String]\n    val fields = log.split(\n,\n)\n    val rows = Seq(Row.fromSeq(Seq(new java.sql.Timestamp(fields(0).toLong),\n      fields(1),\n      fields(2),\n      fields(3),\n      fields(4),\n      fields(5).toDouble,\n      fields(6)\n    )))\n\n    rows\n  }\n}\n\n\n\n\n\nTo create a row table that is updated based on the streaming data\n:\n\n\nsnsc.sql(\ncreate table publisher_bid_counts(publisher string, bidCount int) using row\n)\n\n\n\n\nTo declare a continuous query that is executed on the streaming data\n: This query returns a number of bids per publisher in one batch.\n\n\nval resultStream: SchemaDStream = snsc.registerCQ(\nselect publisher, count(bid) as bidCount from \n +\n    \nadImpressionStream window (duration 1 seconds, slide 1 seconds) group by publisher\n)\n\n\n\n\nTo process that the result of above continuous query to update the row table publisher_bid_counts\n:\n\n\n// this conf is used to get a JDBC connection\nval conf = new ConnectionConfBuilder(snsc.snappySession).build()\n\nresultStream.foreachDataFrame(df =\n {\n    println(\nData received in streaming window\n)\n    df.show()\n\n    println(\nUpdating table publisher_bid_counts\n)\n    val conn = ConnectionUtil.getConnection(conf)\n    val result = df.collect()\n    val stmt = conn.prepareStatement(\nupdate publisher_bid_counts set \n +\n        s\nbidCount = bidCount + ? where publisher = ?\n)\n\n    result.foreach(row =\n {\n      val publisher = row.getString(0)\n      val bidCount = row.getLong(1)\n      stmt.setLong(1, bidCount)\n      stmt.setString(2, publisher)\n      stmt.addBatch()\n        }\n        )\n    stmt.executeBatch()\n    conn.close()\n  }\n})\n\n\n\n\nTo display the total bids by each publisher by querying publisher_bid_counts table\n:\n\n\nsnsc.snappySession.sql(\nselect publisher, bidCount from publisher_bid_counts\n).show()\n\n\n\n\n \n\n\nStructured Streaming\n\n\nThe SnappyData structured streaming programming model is the same as \nSpark structured streaming\n. \n\n\nThe only difference is support for ingesting streaming dataframes into SnappyData tables through a built-in \nSink\n. The \nSink\n supports idempotent writes, ensuring consistency of data when failures occur, as well as support for all mutation operations such as inserts, appends, updates, puts, and deletes. \n\n\nThe output data source name for SnappyData is \nsnappysink\n.\n\n\nCode Sample\n\n\nA minimal code example for structured streaming with snappysink is available \nhere\n.\n\n\nUsing SnappyData Structured Streaming API\n\n\nThe following code snippet, from the example, explains the usage of SnappyData's \n Structured Streaming API\n:\n\n\n    val streamingQuery = structDF\n        .filter(_.signal \n 10)\n        .writeStream\n        .format(\nsnappysink\n)           // Required to ingest into SnappyData tables\n        .queryName(\nDevices\n)\n        .trigger(ProcessingTime(\n1 seconds\n))\n        .option(\nstreamQueryId\n, \nDevices\n)     // Required: must be unique across a snappydata cluster\n        .option(\ntableName\n, \ndevices\n)     // Required: where should the data be saved ? \n        .option(\ncheckpointLocation\n, checkpointDirectory)\n        .start()\n\n\n\n\nSnappyData Specific options\n\n\nThe following are SnappyData specific options which can be configured for Structured Streaming:\n\n\n\n\n\n\n\n\nOptions\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstreamQueryId\n\n\nThis is internally used by SnappyData to track the progress of a stream query. The value of this property must be kept unique for each stream query across the SnappyData cluster.  The property is case-insensitive and is mandatory.\n\n\n\n\n\n\ntableName\n\n\nName of the SnappyData table where the streaming data is ingested. The property is case-insensitive and is mandatory.\n\n\n\n\n\n\nconflation\n\n\nThis is an optional boolean property with the default value set to \nfalse\n. Conflation is enabled only when you set this property to \ntrue\n. \nIf this property is set to \ntrue\n and if the incoming streaming batch contains multiple events on the same key, \nSnappyData Sink\n automatically reduces this to a single operation. This is typically the last operation on any given key for the batch that is being processed. This property is only applicable when the \n_eventType\n column is available (see \nbelow\n) and the target table has Keys defined. For more information, see \nhere\n.\n\n\n\n\n\n\n \nsinkCallback\n\n\nThis is an optional property which is used to override default \nSnappyData Sink\n behavior. To override the default behavior, client codes should implement \nSnappySinkCallback\n trait and pass the fully qualified name of the implementing class against this property value.\n\n\n\n\n\n\n\n\nHandling Inserts, Updates and Deletes\n\n\nA common use case for streaming is capturing writes into another store (Operational database such as RDB or NoSQL DB) and streaming the events through Kafka, applying Spark transformations, and ingesting into an analytics datastore such as SnappyData. This pattern is commonly referred to as \nChange-Data-Capture (CDC)\n.\n\n\n \n\nTo support this use case, \nSnappyData Sink\n supports events to signal if these are Inserts, Updates, or Deletes. The application is required to inject a column called \n_eventType\n as described below. \n\n\nTo support \nCDC\n, the source DataFrame must have the following:\n\n\n\n\n\n\nAn \nIntegerType\n column named \n_eventType\n.  The value in the \n_eventType\n column can be any of the following:\n\n\n\n\n0\n for insert events\n\n\n1\n for update events\n\n\n2\n for delete events\n\n\n\n\nIn case the input data is following a different convention for event types, then it must be transformed to match the above-mentioned format.\n\n\n\n\nNote\n\n\nRecords which have \n_eventType\n value other than the above-mentioned ones are skipped.\n\n\n\n\n\n\n\n\nThe target SnappyData table must have key columns defined for a column table or primary key defined for a row table.\n\n\n\n\n\n\nAn example explaining the \nCDC\n use case is available \nhere\n.\n\n\nIf the \n_eventType\n column is not provided as part of source dataframe, then the following is observed:\n \n\n\n\n\nIn a target table with key columns/primary key defined, the \nput into\n operation is applied to all events.\n\n\nIn a target table without key columns/primary key defined, the \ninsert\n operation is applied to all the events.\n\n\n\n\n \n\n\nEvent Processing Order\n\n\nCurrently, the ordering of events across partitions is NOT supported. Event processing occurs independently in each partition. Hence, your application must ensure that all the events, that are associated with a key, are always delivered on the same partition (shard on the key).\nIf your incoming stream is not partitioned on the key column(s), the application should first repartition the dataframe on the key column(s). You can ignore this requirement, if your incoming streams are continuously appending (For example, time series) or when replacing data where ordering is irrelevant.\n\n\n \n\nIf \nconflation\n property is set to \ntrue\n, \nSnappyData Sink\n will first conflate the incoming batch independently by each partition. This results in a batch, where there is at most a single entry per key. \nThen, the writes occur by grouping the events in the following manner (for performance optimization of the columnar store) and is only applicable when your DataFrame has an \n_eventType\n column:\n\n\n\n\nProcesses all delete events\n (deletes relevant records from target table).\n\n\nProcesses all insert events\n (inserts relevant records into the target table).\n\n\nProcesses all update events \n(applies \nPutInto\n operation).\n\n\n\n\nThis above grouping semantics is followed even when the \nconflation\n property is set to \nfalse\n.  When \n_eventType\n column is not available, all records are merged into the target table using \nPutInto\n semantics.\n\n\nBy default the \nconflation\n property is set to \nfalse\n. Therefore, the current ordering semantics only ensures consistency when incoming events in a batch are for unique key column(s).\n\n\nFor example:\nIf an incoming batch contains an \nInsert(key1)\n event followed by a\n Delete(key1)\n event, the record for \nkey1\n is shown in the target table after the batch is processed. This is because all the Delete events are processed before Insert events as per the event processing order explained \nabove\n.\nIn such cases, you should enable the Conflation by setting the \nconflation\n property to \ntrue\n. Now, if a batch contains \nInsert(key1)\n event followed by a \nDelete(key1)\n event, then \n SnappyData Sink\n conflates these two events into a single event by selecting the last event which is \nDelete(key1)\n and only that event is processed for \nkey1\n.\nProcessing \nDelete(key1)\n event without processing \nInsert(key1)\n event do not result in a failure, as Delete events are ignored, if corresponding records do not exist in the target table.\n\n\n\n\nNote\n\n\nApplications can override the default \nSnappyData Sink\n semantics by explicitly implementing the \nsinkCallback\n.\n\n\n\n\nLimitations\n\n\nLimitations of \nSnappyData Sink\n are as follows:\n\n\n\n\n\n\nWhen the data coming from the source is not partitioned by key columns, then using \nSnappyData Sink\n may result in inconsistent data. This is because each partition independently processes the data using the \nabove-mentioned logic\n.\n\n\n\n\n\n\nWhen key columns are not defined on the target table and the input dataframe does not contain \n_eventType\n column, then \nSnappyData Sink\n cannot guarantee idempotent behavior. This is because inserts cannot be converted into \nput into\n, as there are no key columns on the table. In such a scenario, \nSnappyData Sink\n may insert duplicate records after an abrupt failure of the streaming job.\n\n\n\n\n\n\nThe default \nSnappyData Sink\n implementation does not support partial records for updates. Which means that there is no support to merge updates on a few columns into the store. For all update events, the incoming records must provide values into all the columns of the target table.", 
            "title": "How to use Stream Processing with SnappyData"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#how-to-use-stream-processing-with-snappydata", 
            "text": "SnappyData supports both the older  Spark Streaming model (based on DStreams)  as well as the newer  Structured Streaming model . Unlike the Spark streaming DStreams model, that is based on RDDs, SnappyData supports Spark SQL in both models.", 
            "title": "How to use Stream Processing with SnappyData"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#spark-streaming-dstreams-model", 
            "text": "SnappyData\u2019s streaming functionality builds on top of Spark Streaming and is primarily aimed at making it simpler to build streaming applications and to integrate with the built-in store. In SnappyData, you can define streams declaratively from any SQL client, register continuous queries on streams, mutate SnappyData tables based on the streaming data. For more information on streaming, refer to this  section .", 
            "title": "Spark Streaming DStreams Model"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#code-sample", 
            "text": "Code example for streaming is in  StreamingExample.scala . The code snippets in the following sections show how to declare a stream table, register continuous queries(CQ), and update SnappyData table using the stream data.", 
            "title": "Code Sample"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#using-stream-processing-with-snappydata", 
            "text": "First get a SnappySession and a SnappyStreamingContext :  \nHere SnappyStreamingContext is initialized in a batch duration of one second.  val spark: SparkSession = SparkSession\n    .builder\n    .appName(getClass.getSimpleName)\n    .master( local[*] )\n    .getOrCreate\n\nval snsc = new SnappyStreamingContext(spark.sparkContext, Seconds(1))  The example starts an embedded Kafka instance on which a few messages are published. SnappyData processes these message and updates a table based on the stream data.  The SQL below shows how to declare a stream table using SQL. The rowConverter attribute specifies a class used to return Row objects from the received stream messages.   snsc.sql(\n       create stream table adImpressionStream (  +\n          time_stamp timestamp,  +\n          publisher string,  +\n          advertiser string,  +\n          website string,  +\n          geo string,  +\n          bid double,  +\n          cookie string)   +   using kafka_stream options(  +\n          rowConverter 'org.apache.spark.examples.snappydata.RowsConverter',  +\n        s  kafkaParams 'bootstrap.servers- $add;  +\n         key.deserializer- org.apache.kafka.common.serialization.StringDeserializer;  +\n         value.deserializer- org.apache.kafka.common.serialization.StringDeserializer;  +\n        s group.id- $groupId;auto.offset.reset- earliest',  +\n        s  startingOffsets '$startingOffsets',   +\n        s  subscribe '$topic') \n    )  RowsConverter decodes a stream message consisting of comma-separated fields and forms a Row object from it.  class RowsConverter extends StreamToRowsConverter with Serializable {\n\n  override def toRows(message: Any): Seq[Row] = {\n    val log = message.asInstanceOf[String]\n    val fields = log.split( , )\n    val rows = Seq(Row.fromSeq(Seq(new java.sql.Timestamp(fields(0).toLong),\n      fields(1),\n      fields(2),\n      fields(3),\n      fields(4),\n      fields(5).toDouble,\n      fields(6)\n    )))\n\n    rows\n  }\n}  To create a row table that is updated based on the streaming data :  snsc.sql( create table publisher_bid_counts(publisher string, bidCount int) using row )  To declare a continuous query that is executed on the streaming data : This query returns a number of bids per publisher in one batch.  val resultStream: SchemaDStream = snsc.registerCQ( select publisher, count(bid) as bidCount from   +\n     adImpressionStream window (duration 1 seconds, slide 1 seconds) group by publisher )  To process that the result of above continuous query to update the row table publisher_bid_counts :  // this conf is used to get a JDBC connection\nval conf = new ConnectionConfBuilder(snsc.snappySession).build()\n\nresultStream.foreachDataFrame(df =  {\n    println( Data received in streaming window )\n    df.show()\n\n    println( Updating table publisher_bid_counts )\n    val conn = ConnectionUtil.getConnection(conf)\n    val result = df.collect()\n    val stmt = conn.prepareStatement( update publisher_bid_counts set   +\n        s bidCount = bidCount + ? where publisher = ? )\n\n    result.foreach(row =  {\n      val publisher = row.getString(0)\n      val bidCount = row.getLong(1)\n      stmt.setLong(1, bidCount)\n      stmt.setString(2, publisher)\n      stmt.addBatch()\n        }\n        )\n    stmt.executeBatch()\n    conn.close()\n  }\n})  To display the total bids by each publisher by querying publisher_bid_counts table :  snsc.snappySession.sql( select publisher, bidCount from publisher_bid_counts ).show()", 
            "title": "Using Stream Processing with SnappyData"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#structured-streaming", 
            "text": "The SnappyData structured streaming programming model is the same as  Spark structured streaming .   The only difference is support for ingesting streaming dataframes into SnappyData tables through a built-in  Sink . The  Sink  supports idempotent writes, ensuring consistency of data when failures occur, as well as support for all mutation operations such as inserts, appends, updates, puts, and deletes.   The output data source name for SnappyData is  snappysink .", 
            "title": "Structured Streaming"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#code-sample_1", 
            "text": "A minimal code example for structured streaming with snappysink is available  here .", 
            "title": "Code Sample"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#using-snappydata-structured-streaming-api", 
            "text": "The following code snippet, from the example, explains the usage of SnappyData's   Structured Streaming API :      val streamingQuery = structDF\n        .filter(_.signal   10)\n        .writeStream\n        .format( snappysink )           // Required to ingest into SnappyData tables\n        .queryName( Devices )\n        .trigger(ProcessingTime( 1 seconds ))\n        .option( streamQueryId ,  Devices )     // Required: must be unique across a snappydata cluster\n        .option( tableName ,  devices )     // Required: where should the data be saved ? \n        .option( checkpointLocation , checkpointDirectory)\n        .start()", 
            "title": "Using SnappyData Structured Streaming API"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#snappydata-specific-options", 
            "text": "The following are SnappyData specific options which can be configured for Structured Streaming:     Options  Description      streamQueryId  This is internally used by SnappyData to track the progress of a stream query. The value of this property must be kept unique for each stream query across the SnappyData cluster.  The property is case-insensitive and is mandatory.    tableName  Name of the SnappyData table where the streaming data is ingested. The property is case-insensitive and is mandatory.    conflation  This is an optional boolean property with the default value set to  false . Conflation is enabled only when you set this property to  true .  If this property is set to  true  and if the incoming streaming batch contains multiple events on the same key,  SnappyData Sink  automatically reduces this to a single operation. This is typically the last operation on any given key for the batch that is being processed. This property is only applicable when the  _eventType  column is available (see  below ) and the target table has Keys defined. For more information, see  here .      sinkCallback  This is an optional property which is used to override default  SnappyData Sink  behavior. To override the default behavior, client codes should implement  SnappySinkCallback  trait and pass the fully qualified name of the implementing class against this property value.", 
            "title": "SnappyData Specific options"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#handling-inserts-updates-and-deletes", 
            "text": "A common use case for streaming is capturing writes into another store (Operational database such as RDB or NoSQL DB) and streaming the events through Kafka, applying Spark transformations, and ingesting into an analytics datastore such as SnappyData. This pattern is commonly referred to as  Change-Data-Capture (CDC) .    \nTo support this use case,  SnappyData Sink  supports events to signal if these are Inserts, Updates, or Deletes. The application is required to inject a column called  _eventType  as described below.   To support  CDC , the source DataFrame must have the following:    An  IntegerType  column named  _eventType .  The value in the  _eventType  column can be any of the following:   0  for insert events  1  for update events  2  for delete events   In case the input data is following a different convention for event types, then it must be transformed to match the above-mentioned format.   Note  Records which have  _eventType  value other than the above-mentioned ones are skipped.     The target SnappyData table must have key columns defined for a column table or primary key defined for a row table.    An example explaining the  CDC  use case is available  here .  If the  _eventType  column is not provided as part of source dataframe, then the following is observed:     In a target table with key columns/primary key defined, the  put into  operation is applied to all events.  In a target table without key columns/primary key defined, the  insert  operation is applied to all the events.", 
            "title": "Handling Inserts, Updates and Deletes"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#event-processing-order", 
            "text": "Currently, the ordering of events across partitions is NOT supported. Event processing occurs independently in each partition. Hence, your application must ensure that all the events, that are associated with a key, are always delivered on the same partition (shard on the key). If your incoming stream is not partitioned on the key column(s), the application should first repartition the dataframe on the key column(s). You can ignore this requirement, if your incoming streams are continuously appending (For example, time series) or when replacing data where ordering is irrelevant.    \nIf  conflation  property is set to  true ,  SnappyData Sink  will first conflate the incoming batch independently by each partition. This results in a batch, where there is at most a single entry per key.  Then, the writes occur by grouping the events in the following manner (for performance optimization of the columnar store) and is only applicable when your DataFrame has an  _eventType  column:   Processes all delete events  (deletes relevant records from target table).  Processes all insert events  (inserts relevant records into the target table).  Processes all update events  (applies  PutInto  operation).   This above grouping semantics is followed even when the  conflation  property is set to  false .  When  _eventType  column is not available, all records are merged into the target table using  PutInto  semantics.  By default the  conflation  property is set to  false . Therefore, the current ordering semantics only ensures consistency when incoming events in a batch are for unique key column(s).  For example: If an incoming batch contains an  Insert(key1)  event followed by a  Delete(key1)  event, the record for  key1  is shown in the target table after the batch is processed. This is because all the Delete events are processed before Insert events as per the event processing order explained  above . In such cases, you should enable the Conflation by setting the  conflation  property to  true . Now, if a batch contains  Insert(key1)  event followed by a  Delete(key1)  event, then   SnappyData Sink  conflates these two events into a single event by selecting the last event which is  Delete(key1)  and only that event is processed for  key1 . Processing  Delete(key1)  event without processing  Insert(key1)  event do not result in a failure, as Delete events are ignored, if corresponding records do not exist in the target table.   Note  Applications can override the default  SnappyData Sink  semantics by explicitly implementing the  sinkCallback .", 
            "title": "Event Processing Order"
        }, 
        {
            "location": "/howto/use_stream_processing_with_snappydata/#limitations", 
            "text": "Limitations of  SnappyData Sink  are as follows:    When the data coming from the source is not partitioned by key columns, then using  SnappyData Sink  may result in inconsistent data. This is because each partition independently processes the data using the  above-mentioned logic .    When key columns are not defined on the target table and the input dataframe does not contain  _eventType  column, then  SnappyData Sink  cannot guarantee idempotent behavior. This is because inserts cannot be converted into  put into , as there are no key columns on the table. In such a scenario,  SnappyData Sink  may insert duplicate records after an abrupt failure of the streaming job.    The default  SnappyData Sink  implementation does not support partial records for updates. Which means that there is no support to merge updates on a few columns into the store. For all update events, the incoming records must provide values into all the columns of the target table.", 
            "title": "Limitations"
        }, 
        {
            "location": "/howto/use_transactions_isolation_levels/", 
            "text": "How to use Transactions Isolation Levels\n\n\nSnappyData supports transaction isolation levels when using JDBC or ODBC connections. The default transaction level in SnappyData is set to NONE. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads.\n\n\nSnappyData also supports \nREAD_COMMITTED\n and \nREPEATABLE_READ\n transaction isolation levels. A detailed description of the transaction's semantics in SnappyData can be found in the \nOverview of SnappyData Distributed Transactions\n section.\n\n\n\n\nNote\n\n\nIf you set the isolation level to \nREAD_COMMITTED\n or \nREPEATABLE_READ\n, queries on column table report an error if \nautocommit\n is set to \noff\n (\nfalse\n). \n Queries on column tables are supported when isolation level is set to \nNONE\n. SnappyData internally sets autocommit to \ntrue\n in this case.\n\n\nQueries on row tables are supported when \nautocommit\n is set to \nfalse\n and isolation level is set to other \nREAD_COMMITTED\n or \nREPEATABLE_READ\n.\n\n\n\n\nExamples\n\n\n\n\nNote\n\n\nBefore you try these examples, ensure that you have \nstarted the SnappyData cluster\n.\n\n\n\n\nThe following examples provide JDBC example code snippets that explain how to use transactions isolation levels.\n\n\nExample 1\n\n\nFor row tables, \nautocommit\n can be set to \nfalse\n or \ntrue\n\n\nimport java.sql.{Connection, Statement}\n\n...\n...\n\nval url: String = \njdbc:snappydata://1527/\n\nval conn1 = DriverManager.getConnection(url)\nval stmt1 = conn1.createStatement()\n\n// create a row table\nstmt1.execute(\nCREATE TABLE APP.PARTSUPP ( \n +\n        \nPS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,\n +\n        \nPS_SUPPKEY     INTEGER NOT NULL,\n +\n        \nPS_AVAILQTY    INTEGER NOT NULL,\n +\n        \nPS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)\n +\n        \nUSING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY')\n)\n\n// set the tx isolation level\nconn1.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED)\n// set autocommit to false\nconn1.setAutoCommit(false)\n\nval preparedStmt1 = conn1.prepareStatement(\nINSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?)\n)\nfor (x \n- 1 to 10) {\n  preparedStmt1.setInt(1, x*100)\n  preparedStmt1.setInt(2, x)\n  preparedStmt1.setInt(3, x*1000)\n  preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2))\n  preparedStmt1.executeUpdate()\n}\n\n// commit the transaction\nconn1.commit()\n\nval rs1 = stmt1.executeQuery(\nSELECT * FROM APP.PARTSUPP\n)\nwhile (rs1.next()) {\n  println(rs1.getInt(1) + \n,\n + rs1.getInt(2) + \n,\n + rs1.getInt(3))\n}\nrs1.close()\nstmt1.close()\n\nconn1.close()\n\n\n\n\nExample 2\n\n\nFor column tables, \nautocommit\n must be set to \ntrue\n, otherwise, an error is reported when the query is executed.\n\n\nval conn2 = DriverManager.getConnection(url)\nval stmt2 = conn2.createStatement()\n\n// create a column table\nstmt2.execute(\nCREATE TABLE CUSTOMER ( \n +\n    \nC_CUSTKEY     INTEGER,\n +\n    \nC_NAME        VARCHAR(25),\n +\n    \nC_ADDRESS     VARCHAR(40),\n +\n    \nC_NATIONKEY   INTEGER,\n +\n    \nC_PHONE       VARCHAR(15),\n +\n    \nC_ACCTBAL     DECIMAL(15,2),\n +\n    \nC_MKTSEGMENT  VARCHAR(10),\n +\n    \nC_COMMENT     VARCHAR(117))\n +\n    \nUSING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY')\n)\n\n// set the tx isolation level\nconn2.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED)\n// set autocommit to true otherwise opeartions on column table will error out\nconn2.setAutoCommit(true)\nstmt2.execute(\nINSERT INTO CUSTOMER VALUES(20000, 'Customer20000', \n +\n        \n'Chicago, IL', 1, '555-101-782', 3500, 'MKTSEGMENT', '')\n)\nstmt2.execute(\nINSERT INTO CUSTOMER VALUES(30000, 'Customer30000', \n +\n        \n'San Hose, CA', 1, '555-201-562', 4500, 'MKTSEGMENT', '')\n)\nval rs2 = stmt2.executeQuery(\nSELECT * FROM APP.CUSTOMER\n)\nwhile (rs2.next()) {\n  println(rs2.getInt(1) + \n,\n + rs2.getString(2))\n}\nrs2.close()\n\n\n\n\nUnsupported operations when \nautocommit\n is set to false for column tables\n\n\n// if autocommit is set to false, queries throw an error if column tables are involved\nconn2.setAutoCommit(false)\n// invalid query\nstmt2.execute(\nSELECT * FROM APP.CUSTOMER\n)\n// the above statement throws an error as given below\nEXCEPTION: java.sql.SQLException: (SQLState=XJ218 Severity=20000) (Server=localhost/127.0.0.1[25299] Thread=pool-14-thread-3) Operations on column tables are not supported when query routing is disabled or autocommit is false\n\n\n\n\nMore information\n\n\n\n\n\n\nOverview of SnappyData Distributed Transactions\n\n\n\n\n\n\nBest Practices for SnappyData Distributed Transactions", 
            "title": "How to use Transactions Isolation Levels"
        }, 
        {
            "location": "/howto/use_transactions_isolation_levels/#how-to-use-transactions-isolation-levels", 
            "text": "SnappyData supports transaction isolation levels when using JDBC or ODBC connections. The default transaction level in SnappyData is set to NONE. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads.  SnappyData also supports  READ_COMMITTED  and  REPEATABLE_READ  transaction isolation levels. A detailed description of the transaction's semantics in SnappyData can be found in the  Overview of SnappyData Distributed Transactions  section.   Note  If you set the isolation level to  READ_COMMITTED  or  REPEATABLE_READ , queries on column table report an error if  autocommit  is set to  off  ( false ).   Queries on column tables are supported when isolation level is set to  NONE . SnappyData internally sets autocommit to  true  in this case.  Queries on row tables are supported when  autocommit  is set to  false  and isolation level is set to other  READ_COMMITTED  or  REPEATABLE_READ .", 
            "title": "How to use Transactions Isolation Levels"
        }, 
        {
            "location": "/howto/use_transactions_isolation_levels/#examples", 
            "text": "Note  Before you try these examples, ensure that you have  started the SnappyData cluster .   The following examples provide JDBC example code snippets that explain how to use transactions isolation levels.", 
            "title": "Examples"
        }, 
        {
            "location": "/howto/use_transactions_isolation_levels/#example-1", 
            "text": "For row tables,  autocommit  can be set to  false  or  true  import java.sql.{Connection, Statement}\n\n...\n...\n\nval url: String =  jdbc:snappydata://1527/ \nval conn1 = DriverManager.getConnection(url)\nval stmt1 = conn1.createStatement()\n\n// create a row table\nstmt1.execute( CREATE TABLE APP.PARTSUPP (   +\n         PS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,  +\n         PS_SUPPKEY     INTEGER NOT NULL,  +\n         PS_AVAILQTY    INTEGER NOT NULL,  +\n         PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)  +\n         USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY') )\n\n// set the tx isolation level\nconn1.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED)\n// set autocommit to false\nconn1.setAutoCommit(false)\n\nval preparedStmt1 = conn1.prepareStatement( INSERT INTO APP.PARTSUPP VALUES(?, ?, ?, ?) )\nfor (x  - 1 to 10) {\n  preparedStmt1.setInt(1, x*100)\n  preparedStmt1.setInt(2, x)\n  preparedStmt1.setInt(3, x*1000)\n  preparedStmt1.setBigDecimal(4, java.math.BigDecimal.valueOf(100.2))\n  preparedStmt1.executeUpdate()\n}\n\n// commit the transaction\nconn1.commit()\n\nval rs1 = stmt1.executeQuery( SELECT * FROM APP.PARTSUPP )\nwhile (rs1.next()) {\n  println(rs1.getInt(1) +  ,  + rs1.getInt(2) +  ,  + rs1.getInt(3))\n}\nrs1.close()\nstmt1.close()\n\nconn1.close()", 
            "title": "Example 1"
        }, 
        {
            "location": "/howto/use_transactions_isolation_levels/#example-2", 
            "text": "For column tables,  autocommit  must be set to  true , otherwise, an error is reported when the query is executed.  val conn2 = DriverManager.getConnection(url)\nval stmt2 = conn2.createStatement()\n\n// create a column table\nstmt2.execute( CREATE TABLE CUSTOMER (   +\n     C_CUSTKEY     INTEGER,  +\n     C_NAME        VARCHAR(25),  +\n     C_ADDRESS     VARCHAR(40),  +\n     C_NATIONKEY   INTEGER,  +\n     C_PHONE       VARCHAR(15),  +\n     C_ACCTBAL     DECIMAL(15,2),  +\n     C_MKTSEGMENT  VARCHAR(10),  +\n     C_COMMENT     VARCHAR(117))  +\n     USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') )\n\n// set the tx isolation level\nconn2.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED)\n// set autocommit to true otherwise opeartions on column table will error out\nconn2.setAutoCommit(true)\nstmt2.execute( INSERT INTO CUSTOMER VALUES(20000, 'Customer20000',   +\n         'Chicago, IL', 1, '555-101-782', 3500, 'MKTSEGMENT', '') )\nstmt2.execute( INSERT INTO CUSTOMER VALUES(30000, 'Customer30000',   +\n         'San Hose, CA', 1, '555-201-562', 4500, 'MKTSEGMENT', '') )\nval rs2 = stmt2.executeQuery( SELECT * FROM APP.CUSTOMER )\nwhile (rs2.next()) {\n  println(rs2.getInt(1) +  ,  + rs2.getString(2))\n}\nrs2.close()", 
            "title": "Example 2"
        }, 
        {
            "location": "/howto/use_transactions_isolation_levels/#unsupported-operations-when-autocommit-is-set-to-false-for-column-tables", 
            "text": "// if autocommit is set to false, queries throw an error if column tables are involved\nconn2.setAutoCommit(false)\n// invalid query\nstmt2.execute( SELECT * FROM APP.CUSTOMER )\n// the above statement throws an error as given below\nEXCEPTION: java.sql.SQLException: (SQLState=XJ218 Severity=20000) (Server=localhost/127.0.0.1[25299] Thread=pool-14-thread-3) Operations on column tables are not supported when query routing is disabled or autocommit is false  More information    Overview of SnappyData Distributed Transactions    Best Practices for SnappyData Distributed Transactions", 
            "title": "Unsupported operations when autocommit is set to false for column tables"
        }, 
        {
            "location": "/howto/use_synopsis_data_engine_to_run_approximate_queries/", 
            "text": "How to use Synopsis Data Engine to Run Approximate Queries\n\n\nSynopsis Data Engine (SDE) uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time.\nFor more information on  SDE, refer to \nSDE documentation\n.\n\n\nCode Example\n:\nThe complete code example for SDE is in \nSynopsisDataExample.scala\n. The code below creates a sample table and executes queries that run on the sample table.\n\n\nGet a SnappySession\n:\n\n\nval spark: SparkSession = SparkSession\n    .builder\n    .appName(\nSynopsisDataExample\n)\n    .master(\nlocal[*]\n)\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nThe base column table(AIRLINE) is created from temporary parquet table as follows\n:\n\n\n// Create temporary staging table to load parquet data\nsnSession.sql(\nCREATE EXTERNAL TABLE STAGING_AIRLINE \n +\n    \nUSING parquet OPTIONS(path \n + s\n'${dataFolder}/airlineParquetData')\n)\n\n// Create a column table AIRLINE\nsnSession.sql(\nCREATE TABLE AIRLINE USING column AS (SELECT Year AS Year_, \n +\n    \nMonth AS Month_ , DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, \n +\n    \nCRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, \n +\n    \nCRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance, \n +\n    \nTaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay, \n +\n    \nWeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, \n +\n    \nArrDelaySlot FROM STAGING_AIRLINE)\n)\n\n\n\n\nCreate a sample table for the above base table\n:\nAttribute 'qcs' in the statement below specifies the columns used for stratification and attribute 'fraction' specifies how big the sample needs to be (3% of the base table AIRLINE in this case). For more information on Synopsis Data Engine, refer to the \nSDE documentation\n.\n\n\nsnSession.sql(\nCREATE SAMPLE TABLE AIRLINE_SAMPLE ON AIRLINE OPTIONS\n +\n    \n(qcs 'UniqueCarrier, Year_, Month_', fraction '0.03')  \n +\n    \nAS (SELECT Year_, Month_ , DayOfMonth, \n +\n    \nDayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier, \n +\n    \nFlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime, \n +\n    \nArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, \n +\n    \nCancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay, \n +\n    \nNASDelay, SecurityDelay, LateAircraftDelay, ArrDelaySlot FROM AIRLINE)\n)\n\n\n\n\nExecute queries that return approximate results using sample tables\n:\nThe query below returns airlines by number of flights in descending order. The 'with error 0.20' clause in the query below signals query engine to execute the query on the sample table instead of the base table and maximum 20% error is allowed.\n\n\nvar result = snSession.sql(\nselect  count(*) flightRecCount, description AirlineName, \n +\n    \nUniqueCarrier carrierCode ,Year_ from airline , airlineref where \n +\n    \nairline.UniqueCarrier = airlineref.code group by \n +\n    \nUniqueCarrier,description, Year_ order by flightRecCount desc limit \n +\n    \n10 with error 0.20\n).collect()\nresult.foreach(r =\n println(r(0) + \n, \n + r(1) + \n, \n + r(2) + \n, \n + r(3)))\n\n\n\n\nJoin the sample table with a reference table\n:\nYou can join the sample table with a reference table to execute queries. The example below illustrates how a reference table (AIRLINEREF) is created as from a parquet data file.\n\n\n// create temporary staging table to load parquet data\nsnSession.sql(\nCREATE EXTERNAL TABLE STAGING_AIRLINEREF USING \n +\n    \nparquet OPTIONS(path \n + s\n'${dataFolder}/airportcodeParquetData')\n)\nsnSession.sql(\nCREATE TABLE AIRLINEREF USING row AS (SELECT CODE, \n +\n    \nDESCRIPTION FROM STAGING_AIRLINEREF)\n)\n\n\n\n\nJoin the sample table and reference table to find out which airlines arrive on schedule\n:\n\n\nresult = snSession.sql(\nselect AVG(ArrDelay) arrivalDelay, \n +\n    \nrelative_error(arrivalDelay) rel_err, description AirlineName, \n +\n    \nUniqueCarrier carrier from airline, airlineref \n +\n    \nwhere airline.UniqueCarrier = airlineref.Code \n +\n    \ngroup by UniqueCarrier, description order by arrivalDelay \n +\n    \nwith error\n).collect()\n   result.foreach(r =\n println(r(0) + \n, \n + r(1) + \n, \n + r(2) + \n, \n + r(3)))", 
            "title": "How to use Synopsis Data Engine to Run Approximate Queries"
        }, 
        {
            "location": "/howto/use_synopsis_data_engine_to_run_approximate_queries/#how-to-use-synopsis-data-engine-to-run-approximate-queries", 
            "text": "Synopsis Data Engine (SDE) uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time.\nFor more information on  SDE, refer to  SDE documentation .  Code Example :\nThe complete code example for SDE is in  SynopsisDataExample.scala . The code below creates a sample table and executes queries that run on the sample table.  Get a SnappySession :  val spark: SparkSession = SparkSession\n    .builder\n    .appName( SynopsisDataExample )\n    .master( local[*] )\n    .getOrCreate\n\nval snSession = new SnappySession(spark.sparkContext)  The base column table(AIRLINE) is created from temporary parquet table as follows :  // Create temporary staging table to load parquet data\nsnSession.sql( CREATE EXTERNAL TABLE STAGING_AIRLINE   +\n     USING parquet OPTIONS(path   + s '${dataFolder}/airlineParquetData') )\n\n// Create a column table AIRLINE\nsnSession.sql( CREATE TABLE AIRLINE USING column AS (SELECT Year AS Year_,   +\n     Month AS Month_ , DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime,   +\n     CRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime,   +\n     CRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance,   +\n     TaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay,   +\n     WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay,   +\n     ArrDelaySlot FROM STAGING_AIRLINE) )  Create a sample table for the above base table :\nAttribute 'qcs' in the statement below specifies the columns used for stratification and attribute 'fraction' specifies how big the sample needs to be (3% of the base table AIRLINE in this case). For more information on Synopsis Data Engine, refer to the  SDE documentation .  snSession.sql( CREATE SAMPLE TABLE AIRLINE_SAMPLE ON AIRLINE OPTIONS  +\n     (qcs 'UniqueCarrier, Year_, Month_', fraction '0.03')    +\n     AS (SELECT Year_, Month_ , DayOfMonth,   +\n     DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier,   +\n     FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime,   +\n     ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut,   +\n     Cancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay,   +\n     NASDelay, SecurityDelay, LateAircraftDelay, ArrDelaySlot FROM AIRLINE) )  Execute queries that return approximate results using sample tables :\nThe query below returns airlines by number of flights in descending order. The 'with error 0.20' clause in the query below signals query engine to execute the query on the sample table instead of the base table and maximum 20% error is allowed.  var result = snSession.sql( select  count(*) flightRecCount, description AirlineName,   +\n     UniqueCarrier carrierCode ,Year_ from airline , airlineref where   +\n     airline.UniqueCarrier = airlineref.code group by   +\n     UniqueCarrier,description, Year_ order by flightRecCount desc limit   +\n     10 with error 0.20 ).collect()\nresult.foreach(r =  println(r(0) +  ,   + r(1) +  ,   + r(2) +  ,   + r(3)))  Join the sample table with a reference table :\nYou can join the sample table with a reference table to execute queries. The example below illustrates how a reference table (AIRLINEREF) is created as from a parquet data file.  // create temporary staging table to load parquet data\nsnSession.sql( CREATE EXTERNAL TABLE STAGING_AIRLINEREF USING   +\n     parquet OPTIONS(path   + s '${dataFolder}/airportcodeParquetData') )\nsnSession.sql( CREATE TABLE AIRLINEREF USING row AS (SELECT CODE,   +\n     DESCRIPTION FROM STAGING_AIRLINEREF) )  Join the sample table and reference table to find out which airlines arrive on schedule :  result = snSession.sql( select AVG(ArrDelay) arrivalDelay,   +\n     relative_error(arrivalDelay) rel_err, description AirlineName,   +\n     UniqueCarrier carrier from airline, airlineref   +\n     where airline.UniqueCarrier = airlineref.Code   +\n     group by UniqueCarrier, description order by arrivalDelay   +\n     with error ).collect()\n   result.foreach(r =  println(r(0) +  ,   + r(1) +  ,   + r(2) +  ,   + r(3)))", 
            "title": "How to use Synopsis Data Engine to Run Approximate Queries"
        }, 
        {
            "location": "/howto/use_python_to_create_tables_and_run_queries/", 
            "text": "How to use Python to Create Tables and Run Queries\n\n\nDevelopers can write programs in Python to use SnappyData features. \n\n\nFirst create a SnappySession\n:\n\n\n from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)\n\n\n\n\nCreate table using SnappySession\n:\n\n\n# Creating partitioned table PARTSUPP using SQL\nsnappy.sql(\nDROP TABLE IF EXISTS PARTSUPP\n)\n# \nPARTITION_BY\n attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY),\n# For complete list of table attributes refer the documentation\n# http://snappydatainc.github.io/snappydata/programming_guide\nsnappy.sql(\nCREATE TABLE PARTSUPP ( \n +\n      \nPS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,\n +\n      \nPS_SUPPKEY     INTEGER NOT NULL,\n +\n      \nPS_AVAILQTY    INTEGER NOT NULL,\n +\n      \nPS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)\n +\n      \nUSING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY' )\n)\n\n\n\n\nInserting data in table using INSERT query\n:\n\n\nsnappy.sql(\nINSERT INTO PARTSUPP VALUES(100, 1, 5000, 100)\n)\nsnappy.sql(\nINSERT INTO PARTSUPP VALUES(200, 2, 50, 10)\n)\nsnappy.sql(\nINSERT INTO PARTSUPP VALUES(300, 3, 1000, 20)\n)\nsnappy.sql(\nINSERT INTO PARTSUPP VALUES(400, 4, 200, 30)\n)\n# Printing the contents of the PARTSUPP table\nsnappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nUpdate the data using SQL\n:\n\n\n# Update the available quantity for PARTKEY 100\nsnappy.sql(\nUPDATE PARTSUPP SET PS_AVAILQTY = 50000 WHERE PS_PARTKEY = 100\n)\n# Printing the contents of the PARTSUPP table after update\nsnappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nDelete records from the table\n:\n\n\n# Delete the records for PARTKEY 400\nsnappy.sql(\nDELETE FROM PARTSUPP WHERE PS_PARTKEY = 400\n)\n# Printing the contents of the PARTSUPP table after delete\nsnappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nCreate table using API\n:\nThis same table can be created by using createTable API. First create a schema and then create the table, and then mutate the table data using API:\n\n\n# drop the table if it exists\nsnappy.dropTable('PARTSUPP', True)\n\nschema = StructType([StructField('PS_PARTKEY', IntegerType(), False),\n      StructField('PS_SUPPKEY', IntegerType(), False),\n      StructField('PS_AVAILQTY', IntegerType(),False),\n      StructField('PS_SUPPLYCOST', DecimalType(15, 2), False)\n      ])\n\n # \nPARTITION_BY\n attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY)\n # For complete list of table attributes refer the documentation at\n # http://snappydatainc.github.io/snappydata/programming_guide\n snappy.createTable('PARTSUPP', 'row', schema, False, PARTITION_BY = 'PS_PARTKEY')\n\n # Inserting data in PARTSUPP table using DataFrame\ntuples = [(100, 1, 5000, Decimal(100)), (200, 2, 50, Decimal(10)),\n         (300, 3, 1000, Decimal(20)), (400, 4, 200, Decimal(30))]\nrdd = sc.parallelize(tuples)\ntuplesDF = snappy.createDataFrame(rdd, schema)\ntuplesDF.write.insertInto(\nPARTSUPP\n)\n#Printing the contents of the PARTSUPP table\nsnappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n# Update the available quantity for PARTKEY 100\nsnappy.update(\nPARTSUPP\n, \nPS_PARTKEY =100\n, [50000], [\nPS_AVAILQTY\n])\n# Printing the contents of the PARTSUPP table after update\nsnappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n# Delete the records for PARTKEY 400\nsnappy.delete(\nPARTSUPP\n, \nPS_PARTKEY =400\n)\n# Printing the contents of the PARTSUPP table after delete\nsnappy.sql(\nSELECT * FROM PARTSUPP\n).show()\n\n\n\n\nThe complete source code for the above example is in \nCreateTable.py\n\n\nRelated Topics:\n\n\n\n\nRunning Python Applications", 
            "title": "How to use Python to Create Tables and Run Queries"
        }, 
        {
            "location": "/howto/use_python_to_create_tables_and_run_queries/#how-to-use-python-to-create-tables-and-run-queries", 
            "text": "Developers can write programs in Python to use SnappyData features.   First create a SnappySession :   from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)  Create table using SnappySession :  # Creating partitioned table PARTSUPP using SQL\nsnappy.sql( DROP TABLE IF EXISTS PARTSUPP )\n#  PARTITION_BY  attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY),\n# For complete list of table attributes refer the documentation\n# http://snappydatainc.github.io/snappydata/programming_guide\nsnappy.sql( CREATE TABLE PARTSUPP (   +\n       PS_PARTKEY     INTEGER NOT NULL PRIMARY KEY,  +\n       PS_SUPPKEY     INTEGER NOT NULL,  +\n       PS_AVAILQTY    INTEGER NOT NULL,  +\n       PS_SUPPLYCOST  DECIMAL(15,2)  NOT NULL)  +\n       USING ROW OPTIONS (PARTITION_BY 'PS_PARTKEY' ) )  Inserting data in table using INSERT query :  snappy.sql( INSERT INTO PARTSUPP VALUES(100, 1, 5000, 100) )\nsnappy.sql( INSERT INTO PARTSUPP VALUES(200, 2, 50, 10) )\nsnappy.sql( INSERT INTO PARTSUPP VALUES(300, 3, 1000, 20) )\nsnappy.sql( INSERT INTO PARTSUPP VALUES(400, 4, 200, 30) )\n# Printing the contents of the PARTSUPP table\nsnappy.sql( SELECT * FROM PARTSUPP ).show()  Update the data using SQL :  # Update the available quantity for PARTKEY 100\nsnappy.sql( UPDATE PARTSUPP SET PS_AVAILQTY = 50000 WHERE PS_PARTKEY = 100 )\n# Printing the contents of the PARTSUPP table after update\nsnappy.sql( SELECT * FROM PARTSUPP ).show()  Delete records from the table :  # Delete the records for PARTKEY 400\nsnappy.sql( DELETE FROM PARTSUPP WHERE PS_PARTKEY = 400 )\n# Printing the contents of the PARTSUPP table after delete\nsnappy.sql( SELECT * FROM PARTSUPP ).show()  Create table using API :\nThis same table can be created by using createTable API. First create a schema and then create the table, and then mutate the table data using API:  # drop the table if it exists\nsnappy.dropTable('PARTSUPP', True)\n\nschema = StructType([StructField('PS_PARTKEY', IntegerType(), False),\n      StructField('PS_SUPPKEY', IntegerType(), False),\n      StructField('PS_AVAILQTY', IntegerType(),False),\n      StructField('PS_SUPPLYCOST', DecimalType(15, 2), False)\n      ])\n\n #  PARTITION_BY  attribute specifies partitioning key for PARTSUPP table(PS_PARTKEY)\n # For complete list of table attributes refer the documentation at\n # http://snappydatainc.github.io/snappydata/programming_guide\n snappy.createTable('PARTSUPP', 'row', schema, False, PARTITION_BY = 'PS_PARTKEY')\n\n # Inserting data in PARTSUPP table using DataFrame\ntuples = [(100, 1, 5000, Decimal(100)), (200, 2, 50, Decimal(10)),\n         (300, 3, 1000, Decimal(20)), (400, 4, 200, Decimal(30))]\nrdd = sc.parallelize(tuples)\ntuplesDF = snappy.createDataFrame(rdd, schema)\ntuplesDF.write.insertInto( PARTSUPP )\n#Printing the contents of the PARTSUPP table\nsnappy.sql( SELECT * FROM PARTSUPP ).show()\n\n# Update the available quantity for PARTKEY 100\nsnappy.update( PARTSUPP ,  PS_PARTKEY =100 , [50000], [ PS_AVAILQTY ])\n# Printing the contents of the PARTSUPP table after update\nsnappy.sql( SELECT * FROM PARTSUPP ).show()\n\n# Delete the records for PARTKEY 400\nsnappy.delete( PARTSUPP ,  PS_PARTKEY =400 )\n# Printing the contents of the PARTSUPP table after delete\nsnappy.sql( SELECT * FROM PARTSUPP ).show()  The complete source code for the above example is in  CreateTable.py  Related Topics:   Running Python Applications", 
            "title": "How to use Python to Create Tables and Run Queries"
        }, 
        {
            "location": "/howto/connect_using_odbc_driver/", 
            "text": "How to Connect using ODBC Driver\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nYou can connect to SnappyData Cluster using SnappyData ODBC Driver and can execute SQL queries by connecting to any of the servers in the cluster.\n\n\n\n\nStep 1: Install Visual C++ Redistributable for Visual Studio 2013\n\n\nTo download and install the Visual C++ Redistributable for Visual Studio 2013:\n\n\n\n\n\n\nDownload Visual C++ Redistributable for Visual Studio 2013\n\n\n\n\n\n\nSelect \nRun\n to start the installation and follow the steps to complete the installation.\n\n\n\n\n\n\n\n\nStep 2: Install SnappyData ODBC Driver\n\n\nTo download and install the ODBC driver:\n\n\n\n\n\n\nDownload the SnappyData 1.0.2.1 Enterprise Version\n by registering on the SnappyData website. The downloaded file contains the SnappyData ODBC driver installers.\n\n\n\n\n\n\nDepending on your Windows installation, extract the contents of the 32-bit or 64-bit version of the SnappyData ODBC Driver.\n\n\n\n\n\n\n\n\nVersion\n\n\nODBC Driver\n\n\n\n\n\n\n\n\n\n\n32-bit for 32-bit platform\n\n\nsnappydata-1.0.2-odbc32.zip\n\n\n\n\n\n\n32-bit for 64-bit platform\n\n\nsnappydata-1.0.2-odbc32_64.zip\n\n\n\n\n\n\n64-bit for 64-bit platform\n\n\nsnappydata-1.0.2-odbc64.zip\n\n\n\n\n\n\n\n\n\n\n\n\nDouble-click on the \nSnappyDataODBCDriverInstaller.msi\n file, and follow the steps to complete the installation.\n\n\n\n\nNote\n\n\nEnsure that \nSnappyData is installed\n and the \nSnappyData cluster is running\n.\n\n\n\n\n\n\n\n\nConnect to the SnappyData Cluster\n\n\nOnce you have installed the SnappyData ODBC Driver, you can connect to SnappyData cluster in any of the following ways:\n\n\n\n\n\n\nUse the SnappyData Driver Connection URL:\n\n\nDriver=SnappyData ODBC Driver;server=\nServerHost\n;port=\nServerPort\n;user=\nuserName\n;password=\npassword\n\n\n\n\n\n\n\n\nCreate a SnappyData DSN (Data Source Name) using the installed SnappyData ODBC Driver. Refer to the Windows documentation relevant to your operating system for more information on creating a DSN. \n\nWhen prompted, select the SnappyData ODBC Driver from the list of drivers and enter a Data Source name, SnappyData Server Host, Port, User Name and Password.\nRefer to the documentation for detailed information on \nSetting Up SnappyData ODBC Driver\n.", 
            "title": "How to Connect using ODBC Driver"
        }, 
        {
            "location": "/howto/connect_using_odbc_driver/#how-to-connect-using-odbc-driver", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   You can connect to SnappyData Cluster using SnappyData ODBC Driver and can execute SQL queries by connecting to any of the servers in the cluster.", 
            "title": "How to Connect using ODBC Driver"
        }, 
        {
            "location": "/howto/connect_using_odbc_driver/#step-1-install-visual-c-redistributable-for-visual-studio-2013", 
            "text": "To download and install the Visual C++ Redistributable for Visual Studio 2013:    Download Visual C++ Redistributable for Visual Studio 2013    Select  Run  to start the installation and follow the steps to complete the installation.", 
            "title": "Step 1: Install Visual C++ Redistributable for Visual Studio 2013"
        }, 
        {
            "location": "/howto/connect_using_odbc_driver/#step-2-install-snappydata-odbc-driver", 
            "text": "To download and install the ODBC driver:    Download the SnappyData 1.0.2.1 Enterprise Version  by registering on the SnappyData website. The downloaded file contains the SnappyData ODBC driver installers.    Depending on your Windows installation, extract the contents of the 32-bit or 64-bit version of the SnappyData ODBC Driver.     Version  ODBC Driver      32-bit for 32-bit platform  snappydata-1.0.2-odbc32.zip    32-bit for 64-bit platform  snappydata-1.0.2-odbc32_64.zip    64-bit for 64-bit platform  snappydata-1.0.2-odbc64.zip       Double-click on the  SnappyDataODBCDriverInstaller.msi  file, and follow the steps to complete the installation.   Note  Ensure that  SnappyData is installed  and the  SnappyData cluster is running .", 
            "title": "Step 2: Install SnappyData ODBC Driver"
        }, 
        {
            "location": "/howto/connect_using_odbc_driver/#connect-to-the-snappydata-cluster", 
            "text": "Once you have installed the SnappyData ODBC Driver, you can connect to SnappyData cluster in any of the following ways:    Use the SnappyData Driver Connection URL:  Driver=SnappyData ODBC Driver;server= ServerHost ;port= ServerPort ;user= userName ;password= password     Create a SnappyData DSN (Data Source Name) using the installed SnappyData ODBC Driver. Refer to the Windows documentation relevant to your operating system for more information on creating a DSN.  \nWhen prompted, select the SnappyData ODBC Driver from the list of drivers and enter a Data Source name, SnappyData Server Host, Port, User Name and Password.\nRefer to the documentation for detailed information on  Setting Up SnappyData ODBC Driver .", 
            "title": "Connect to the SnappyData Cluster"
        }, 
        {
            "location": "/howto/connect_to_the_cluster_from_external_clients/", 
            "text": "How to Connect to the Cluster from External Network\n\n\nYou can also connect to the SnappyData cluster from a different network as a client (DbVisualizer, SQuirreL SQL etc.). \nFor example, to connect to a cluster on AWS from your local machine set the following properties in the \nconf/locators\n and \nconf/servers\n files:\n\n\n\n\n\n\nclient-bind-address\n: Set the hostname or IP address to which the locator or server listens on, for JDBC/ODBC/thrift client connections.\n\n\n\n\n\n\nhostname-for-clients\n: Set the IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the \nclient-bind-address\n to be given to clients. \n This value can be different from \nclient-bind-address\n for cases where the servers/locators are behind a NAT firewall (AWS for example) where \nclient-bind-address\n needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in \nclient-bind-address\n resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required.\n\n\n\n\nNote\n\n\nBy default, the locator or server binds to localhost. You may need to set either or both these properties to enable connection from external clients. If not set, external client connections may fail.\n\n\n\n\n\n\n\n\nPort Settings: Locator or server listens on the default port 1527 for client connections. Ensure that this port is open in your firewall settings. \n You can also change the default port by setting the \nclient-port\n property in the \nconf/locators\n and \nconf/servers\n.\n\n\n\n\n\n\n\n\nNote\n\n\nFor ODBC clients, you must use the host and port details of the server and not the locator.", 
            "title": "How to Connect to the Cluster from External Network"
        }, 
        {
            "location": "/howto/connect_to_the_cluster_from_external_clients/#how-to-connect-to-the-cluster-from-external-network", 
            "text": "You can also connect to the SnappyData cluster from a different network as a client (DbVisualizer, SQuirreL SQL etc.).  For example, to connect to a cluster on AWS from your local machine set the following properties in the  conf/locators  and  conf/servers  files:    client-bind-address : Set the hostname or IP address to which the locator or server listens on, for JDBC/ODBC/thrift client connections.    hostname-for-clients : Set the IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the  client-bind-address  to be given to clients.   This value can be different from  client-bind-address  for cases where the servers/locators are behind a NAT firewall (AWS for example) where  client-bind-address  needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in  client-bind-address  resolves to internal IP address from inside and to the public IP address from outside, but for other cases, this property is required.   Note  By default, the locator or server binds to localhost. You may need to set either or both these properties to enable connection from external clients. If not set, external client connections may fail.     Port Settings: Locator or server listens on the default port 1527 for client connections. Ensure that this port is open in your firewall settings.   You can also change the default port by setting the  client-port  property in the  conf/locators  and  conf/servers .     Note  For ODBC clients, you must use the host and port details of the server and not the locator.", 
            "title": "How to Connect to the Cluster from External Network"
        }, 
        {
            "location": "/howto/import_from_hive_table/", 
            "text": "How to Import Data from Hive Table into SnappyData Table\n\n\nOption 1\n \n\n\nIf Hive tables have data stored in Apache Parquet format or Optimized Row Columnar (ORC) format the data can be copied directly into SnappyData tables.\n\n\nFor example,\n\n\nCREATE EXTERNAL TABLE \nhive_external_table_name\n USING parquet OPTIONS(path path-to-parquet-or-orc)\n\nCREATE TABLE \ntable_name\n USING COLUMN AS (select * from hive_external_table_name)\n\n\n\n\nFor more information on creating an external table, refer to \nCREATE EXTERNAL TABLE\n.\n\n\nOption 2\n\n\nTake the RDD[Row] from Dataset of Hive Table and insert it into column table.\n\n\nFor example,\n\n\nval ds = spark.table(\nHive_Table_Name\n)\nval df = snappy.createDataFrame(ds.rdd, ds.schema)\ndf.write.format(\ncolumn\n).saveAsTable(\nSnappy_Table_Name\n)\n\n\n\n\nIn above example, 'spark' is of type SparkSession and 'snappy' is of type SnappySession.", 
            "title": "How to Import Data from Hive Table into SnappyData Table"
        }, 
        {
            "location": "/howto/import_from_hive_table/#how-to-import-data-from-hive-table-into-snappydata-table", 
            "text": "Option 1    If Hive tables have data stored in Apache Parquet format or Optimized Row Columnar (ORC) format the data can be copied directly into SnappyData tables.  For example,  CREATE EXTERNAL TABLE  hive_external_table_name  USING parquet OPTIONS(path path-to-parquet-or-orc)\n\nCREATE TABLE  table_name  USING COLUMN AS (select * from hive_external_table_name)  For more information on creating an external table, refer to  CREATE EXTERNAL TABLE .  Option 2  Take the RDD[Row] from Dataset of Hive Table and insert it into column table.  For example,  val ds = spark.table( Hive_Table_Name )\nval df = snappy.createDataFrame(ds.rdd, ds.schema)\ndf.write.format( column ).saveAsTable( Snappy_Table_Name )  In above example, 'spark' is of type SparkSession and 'snappy' is of type SnappySession.", 
            "title": "How to Import Data from Hive Table into SnappyData Table"
        }, 
        {
            "location": "/howto/export_hdfs/", 
            "text": "How to Export and Restore Table Data using HDFS\n\n\nIn SnappyData, table data is stored in memory and on disk (depending on the configuration). As SnappyData supports Spark APIs, table data can be exported to HDFS using Spark APIs. This can be used to backup your tables to HDFS. \n\n\n\n\nTip\n\n\nWhen performing a backup of your tables to HDFS, it is a good practice to export data during a period of low activity in your system. The export does not block any activities in the distributed system, but it does use file system resources on all hosts in your distributed system and can affect performance.\n\n\n\n\nFor example, as shown below you can create a DataFrame for a table and save it as parquet file.\n\n\n// created a DataFrame for table \nAPP.CUSTOMER\n\nval df = snappySession.table(\nAPP.CUSTOMER\n)\n// save it as parquet file on HDFS\ndf.write.parquet(\nhdfs://127.0.0.1:9000/customer\n)\n\n\n\n\nRefer to \nHow to Run Spark Code inside the Cluster\n to understand how to write a SnappyData job. The above can be added to the \nrunSnappyJob()\n function of the SnappyData job.\n\n\nYou can also import this data back into SnappyData tables.\n\n\nFor example using SQL, create an external table and import the data:\n\n\nsnappy\n CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING parquet OPTIONS (path 'hdfs://127.0.0.1:9000/customer', header 'true', inferSchema 'true');\nsnappy\n insert into customer select * from CUSTOMER_STAGING_1;\n\n\n\n\nOr by using APIs (as a part of SnappyData job). Refer to \nHow to Run Spark Code inside the Cluster\n for more information. \n\n\n// create a DataFrame using parquet \nval df2 = snappySession.read.parquet(\nhdfs://127.0.0.1:9000/customer\n)\n// insetert the data into table\ndf2.write.mode(SaveMode.Append).saveAsTable(\nAPP.CUSTOMER\n)", 
            "title": "How to Export and Restore Table Data using HDFS"
        }, 
        {
            "location": "/howto/export_hdfs/#how-to-export-and-restore-table-data-using-hdfs", 
            "text": "In SnappyData, table data is stored in memory and on disk (depending on the configuration). As SnappyData supports Spark APIs, table data can be exported to HDFS using Spark APIs. This can be used to backup your tables to HDFS.    Tip  When performing a backup of your tables to HDFS, it is a good practice to export data during a period of low activity in your system. The export does not block any activities in the distributed system, but it does use file system resources on all hosts in your distributed system and can affect performance.   For example, as shown below you can create a DataFrame for a table and save it as parquet file.  // created a DataFrame for table  APP.CUSTOMER \nval df = snappySession.table( APP.CUSTOMER )\n// save it as parquet file on HDFS\ndf.write.parquet( hdfs://127.0.0.1:9000/customer )  Refer to  How to Run Spark Code inside the Cluster  to understand how to write a SnappyData job. The above can be added to the  runSnappyJob()  function of the SnappyData job.  You can also import this data back into SnappyData tables.  For example using SQL, create an external table and import the data:  snappy  CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 USING parquet OPTIONS (path 'hdfs://127.0.0.1:9000/customer', header 'true', inferSchema 'true');\nsnappy  insert into customer select * from CUSTOMER_STAGING_1;  Or by using APIs (as a part of SnappyData job). Refer to  How to Run Spark Code inside the Cluster  for more information.   // create a DataFrame using parquet \nval df2 = snappySession.read.parquet( hdfs://127.0.0.1:9000/customer )\n// insetert the data into table\ndf2.write.mode(SaveMode.Append).saveAsTable( APP.CUSTOMER )", 
            "title": "How to Export and Restore Table Data using HDFS"
        }, 
        {
            "location": "/howto/tableauconnect/", 
            "text": "How to Connect Tableau to SnappyData\n\n\nDownload and install SnappyData Enterprise edition to connect Tableau to SnappyData. You can connect Tableau using one of the following options:\n\n\n\n\nThrift Server\n compatible with Apache HiveServer2 (HS2)\n\n\nSnappyData ODBC driver\n\n\n\n\n\n\nConnect Tableau using Thrift Server\n\n\nUse the following steps to connect Tableau to SnappyData using Thrift Server that is compatible with Apache HiveServer2. This is also fully compatible with Spark's Thrift.\n\n\nStep 1: Enable Thrift Server in SnappyData Cluster\n\n\n\n\nDownload and Install the SnappyData Enterprise version 1.0.2.1 from the \nSnappyData Release page\n. \n\n\nConfigure the SnappyData Cluster\n.\n\n\nIn the \nLead node configuration\n, set the following property:\nsnappydata.hiveServer.enabled=true\n\n\nLaunch the SnappyData cluster. \n./sbin/snappy-start-all.sh\n\n\n\n\nStep 2: Connect Tableau Desktop to SnappyData\n\n\n\n\nDownload and install \nTableau Desktop v2018.3.x\n from the \nTableau Download page\n. You may also need to register your product.\n\n\nOpen the Tableau Desktop application, on the left panel, from the \nTo A Server \nsection, select \nSpark SQL connector\n option. \n    \n\n\n\n\nIn the \nSpark SQL\n configuration dialog box, enter the following details:\n\n\n\n\n\n\nEnter the host/IP of the Lead node in SnappyData cluster. The default port used by the Hive thrift server is 10000.\n\n\nSelect \nSparkThriftServer\n option from \nType\n dropdown.\n\n\nSelect \nusername and password\n option from the \nAuthentication\n dropdown.\n\n\nSet \nTransport\n field to \nSASL\n.\n\n\nProvide a username/password. You could choose to use \nAPP/APP\n for username/password if authentication was not configured in the cluster.\n\n\n\n\n\n\nNote\n\n\nFor more information about Spark SQL configurations, click \nhere\n.\n\n\n\n\n\n\n\n\nClick the \nSign In \nbutton to connect to SnappyData. Tableau displays the page where you can browse and select Schema and Tables as per your requirements to create data visualizations.\n\n\n\n\nNote\n\n\nIf you have not installed the Simba Spark ODBC Driver on your system already, the \nSign In\n button is disabled. To enable it, click the \nDownload and Install the drivers \nlink and install the Simba Spark ODBC Driver. After this, the \nSign in\n button is enabled.\n\n\n\n\n\n\n\n\nHandling Large Size Tableau Extracts in SnappyData\n\n\nWhen you are using the \nTableau extract\n feature and if your extracted data set will be large, you may need to do the following:\n\n\n\n\nSet the max result size allowed by SnappyData\n By default, SnappyData will terminate a query whose result exceeds 1GB. You can set the \nspark.driver.MaxResultSize\n property on the Lead node and bounce your cluster.\n\n\nConfigure streaming of the result set to Tableau from SnappyData Hive server\nTableau permits \nInitial SQL\n to be sent to the server when creating a data source connection as described \nhere\n.\n \nIn the \nInitial SQL\n dialog box, type the following: \nset spark.sql.thriftServer.incrementalCollect=true\n\n\n\n\n\n\nConnect Tableau using SnappyData ODBC Driver\n\n\nGet the latest version of SnappyData and SnappyData ODBC driver from \nSnappyData Release page\n. \n\n\nUse the following instructions to connect Tableau using SnappyData ODBC driver:\n\n\nStep 1: Setup SnappyData ODBC Driver\n\n\nFollow the instructions provided \nhere\n to setup SnappyData ODBC Driver.\n\n\nStep 2: Install Tableau Desktop (10.1 or Higher)\n\n\nTo install Tableau desktop:\n\n\n\n\n\n\nDownload Tableau Desktop\n.\n\n\n\n\n\n\nDepending on your Windows installation, download the 32-bit or 64-bit version of the installer.\n\n\n\n\n\n\nFollow the steps to complete the installation and ensure that you register and activate your product.\n\n\n\n\n\n\nStep 3: Connect Tableau Desktop to SnappyData Server\n\n\nWhen using Tableau with the SnappyData ODBC Driver for the first time, you must add the \nodbc-snappydata.tdc\n file that is available in the downloaded \nsnappydata-odbc-1.0.2.zip\n.\n\n\nTo connect the Tableau Desktop to the SnappyData Server:\n\n\n\n\n\n\nCopy the \nodbc-snappydata.tdc\n file to the \nUser_Home_Path\n/Documents/My Tableau Repository/Datasources directory.\n\n\n\n\n\n\nOpen the Tableau Desktop application.\n\n\n\n\n\n\nOn the Start Page,\n\n\na. Under \nConnect\n \n \nTo a Server\n, click \nOther Databases (ODBC)\n. The Other Databases (ODBC) window is displayed.\n\n\nb. In the DSN drop-down list, select the name that you provided for your SnappyData ODBC connection (for example \nsnappydsn\n), and then click \nConnect\n.\n\n\n\n\n\n\nWhen the connection to the SnappyData server is established, the \nSign In\n option is enabled. Click \nSign In\n to log into Tableau.\n\n\n\n\n\n\nFrom the \nSchema\n drop-down list, select a schema. For example, \napp\n. \nAll tables from the selected schema are listed.\n\n\n\n\n\n\nSelect the required table(s) and drag it to the canvas. A view generated using the selected tables is displayed. \nIf you make changes to the table, click \nUpdate Now\n to refresh and view your changes.\n\n\n\n\n\n\nIn the \nWorksheets\n tab, click \nsheet\n to start the analysis.\n \n\n\n\n\n\n\nOn this screen, you can click and drag a field from the \nDimensions\n area to \nRows\n or \nColumns\n.\n Refer to the Tableau documentation for more information on data visualization.", 
            "title": "How to Connect Tableau to SnappyData"
        }, 
        {
            "location": "/howto/tableauconnect/#how-to-connect-tableau-to-snappydata", 
            "text": "Download and install SnappyData Enterprise edition to connect Tableau to SnappyData. You can connect Tableau using one of the following options:   Thrift Server  compatible with Apache HiveServer2 (HS2)  SnappyData ODBC driver", 
            "title": "How to Connect Tableau to SnappyData"
        }, 
        {
            "location": "/howto/tableauconnect/#connect-tableau-using-thrift-server", 
            "text": "Use the following steps to connect Tableau to SnappyData using Thrift Server that is compatible with Apache HiveServer2. This is also fully compatible with Spark's Thrift.", 
            "title": "Connect Tableau using Thrift Server"
        }, 
        {
            "location": "/howto/tableauconnect/#step-1-enable-thrift-server-in-snappydata-cluster", 
            "text": "Download and Install the SnappyData Enterprise version 1.0.2.1 from the  SnappyData Release page .   Configure the SnappyData Cluster .  In the  Lead node configuration , set the following property: snappydata.hiveServer.enabled=true  Launch the SnappyData cluster.  ./sbin/snappy-start-all.sh", 
            "title": "Step 1: Enable Thrift Server in SnappyData Cluster"
        }, 
        {
            "location": "/howto/tableauconnect/#step-2-connect-tableau-desktop-to-snappydata", 
            "text": "Download and install  Tableau Desktop v2018.3.x  from the  Tableau Download page . You may also need to register your product.  Open the Tableau Desktop application, on the left panel, from the  To A Server  section, select  Spark SQL connector  option. \n       In the  Spark SQL  configuration dialog box, enter the following details:    Enter the host/IP of the Lead node in SnappyData cluster. The default port used by the Hive thrift server is 10000.  Select  SparkThriftServer  option from  Type  dropdown.  Select  username and password  option from the  Authentication  dropdown.  Set  Transport  field to  SASL .  Provide a username/password. You could choose to use  APP/APP  for username/password if authentication was not configured in the cluster.    Note  For more information about Spark SQL configurations, click  here .     Click the  Sign In  button to connect to SnappyData. Tableau displays the page where you can browse and select Schema and Tables as per your requirements to create data visualizations.   Note  If you have not installed the Simba Spark ODBC Driver on your system already, the  Sign In  button is disabled. To enable it, click the  Download and Install the drivers  link and install the Simba Spark ODBC Driver. After this, the  Sign in  button is enabled.", 
            "title": "Step 2: Connect Tableau Desktop to SnappyData"
        }, 
        {
            "location": "/howto/tableauconnect/#handling-large-size-tableau-extracts-in-snappydata", 
            "text": "When you are using the  Tableau extract  feature and if your extracted data set will be large, you may need to do the following:   Set the max result size allowed by SnappyData  By default, SnappyData will terminate a query whose result exceeds 1GB. You can set the  spark.driver.MaxResultSize  property on the Lead node and bounce your cluster.  Configure streaming of the result set to Tableau from SnappyData Hive server Tableau permits  Initial SQL  to be sent to the server when creating a data source connection as described  here .  \nIn the  Initial SQL  dialog box, type the following:  set spark.sql.thriftServer.incrementalCollect=true", 
            "title": "Handling Large Size Tableau Extracts in SnappyData"
        }, 
        {
            "location": "/howto/tableauconnect/#connect-tableau-using-snappydata-odbc-driver", 
            "text": "Get the latest version of SnappyData and SnappyData ODBC driver from  SnappyData Release page .   Use the following instructions to connect Tableau using SnappyData ODBC driver:", 
            "title": "Connect Tableau using SnappyData ODBC Driver"
        }, 
        {
            "location": "/howto/tableauconnect/#step-1-setup-snappydata-odbc-driver", 
            "text": "Follow the instructions provided  here  to setup SnappyData ODBC Driver.", 
            "title": "Step 1: Setup SnappyData ODBC Driver"
        }, 
        {
            "location": "/howto/tableauconnect/#step-2-install-tableau-desktop-101-or-higher", 
            "text": "To install Tableau desktop:    Download Tableau Desktop .    Depending on your Windows installation, download the 32-bit or 64-bit version of the installer.    Follow the steps to complete the installation and ensure that you register and activate your product.", 
            "title": "Step 2: Install Tableau Desktop (10.1 or Higher)"
        }, 
        {
            "location": "/howto/tableauconnect/#step-3-connect-tableau-desktop-to-snappydata-server", 
            "text": "When using Tableau with the SnappyData ODBC Driver for the first time, you must add the  odbc-snappydata.tdc  file that is available in the downloaded  snappydata-odbc-1.0.2.zip .  To connect the Tableau Desktop to the SnappyData Server:    Copy the  odbc-snappydata.tdc  file to the  User_Home_Path /Documents/My Tableau Repository/Datasources directory.    Open the Tableau Desktop application.    On the Start Page,  a. Under  Connect     To a Server , click  Other Databases (ODBC) . The Other Databases (ODBC) window is displayed.  b. In the DSN drop-down list, select the name that you provided for your SnappyData ODBC connection (for example  snappydsn ), and then click  Connect .    When the connection to the SnappyData server is established, the  Sign In  option is enabled. Click  Sign In  to log into Tableau.    From the  Schema  drop-down list, select a schema. For example,  app .  All tables from the selected schema are listed.    Select the required table(s) and drag it to the canvas. A view generated using the selected tables is displayed.  If you make changes to the table, click  Update Now  to refresh and view your changes.    In the  Worksheets  tab, click  sheet  to start the analysis.      On this screen, you can click and drag a field from the  Dimensions  area to  Rows  or  Columns .  Refer to the Tableau documentation for more information on data visualization.", 
            "title": "Step 3: Connect Tableau Desktop to SnappyData Server"
        }, 
        {
            "location": "/howto/use_apache_zeppelin_with_snappydata/", 
            "text": "How to Use Apache Zeppelin with SnappyData\n\n\nStep 1: Download, Install and Configure SnappyData\n\n\n\n\n\n\nDownload and Install SnappyData\n \n\n The table below lists the version of the SnappyData Zeppelin Interpreter and Apache Zeppelin Installer for the supported SnappyData Releases.\n\n\n\n\n\n\n\n\nSnappyData Zeppelin Interpreter\n\n\nApache Zeppelin Binary Package\n\n\nSnappyData Release\n\n\n\n\n\n\n\n\n\n\nVersion 0.7.3.4\n\n\nVersion 0.7.3\n\n\nRelease 1.0.2.1\n\n\n\n\n\n\nVersion 0.7.3.2\n\n\nVersion 0.7.3\n\n\nRelease 1.0.2\n\n\n\n\n\n\nVersion 0.7.3\n\n\nVersion 0.7.3\n\n\nRelease 1.0.1\n\n\n\n\n\n\nVersion 0.7.2\n\n\nVersion 0.7.2\n\n\nRelease 1.0.0\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure the SnappyData Cluster\n.\n\n\n\n\n\n\nIn \nlead node configuration\n set the following properties:\n\n\n\n\n\n\nEnable the SnappyData Zeppelin interpreter by adding \n-zeppelin.interpreter.enable=true\n \n\n\n\n\n\n\nIn the classpath option, define the location where the SnappyData Interpreter is downloaded by adding\n\n\n-classpath=/\ndownload_location\n/snappydata-zeppelin-\nversion_number\n.jar\n\n\n\n\n\n\nIn the \nconf/spark-env.sh\n file, set the \nSPARK_PUBLIC_DNS\n property to the public DNS name of the lead node. This enables the Member Logs to be displayed correctly to users accessing the \nSnappyData Pulse UI\n from outside the network.\n\n\n\n\n\n\n\n\n\n\nStart the SnappyData cluster\n.\n\n\n\n\n\n\nExtract the contents of the Zeppelin binary package. \n \n\n\n\n\n\n\nInstall the SnappyData Zeppelin interpreter in Apache Zeppelin by executing the following command from Zeppelin's bin directory: \n\n\n./install-interpreter.sh --name snappydata --artifact io.snappydata:snappydata-zeppelin:\nsnappydata_interpreter_version_number\n\n\n\n\nZeppelin interpreter allows the SnappyData interpreter to be plugged into Zeppelin using which, you can run queries.\n\n\n\n\n\n\nRename the \nzeppelin-site.xml.template\n file (located in zeppelin-\nversion_number\n-bin-all/conf directory) to \nzeppelin-site.xml\n.\n\n\n\n\n\n\nEdit the \nzeppelin-site.xml\n file: \n\n\nIn the \nzeppelin.interpreters\n property, add the following interpreter class names:             \norg.apache.zeppelin.interpreter.SnappyDataZeppelinInterpreter,org.apache.zeppelin.interpreter.SnappyDataSqlZeppelinInterpreter\n\n\n\n\n\n\nDownload the predefined SnappyData notebooks \nnotebook.tar.gz\n. \n Extract and copy the contents of the notebook.tar.gz  compressed file to the \nnotebook\n folder in the Zeppelin installation on your local machine.\n\n\n\n\n\n\nStart the Zeppelin daemon using the command: \n \nbin/zeppelin-daemon.sh start\n\n\n\n\n\n\nTo ensure that the installation is successful, log into the Zeppelin UI (\nhttp://localhost:8080\n) from your web browser.\n\n\n\n\n\n\nStep 2: Configure Interpreter Settings\n\n\n\n\n\n\nLog on to Zeppelin from your web browser and select \nInterpreter\n from the \nSettings\n option.\n\n\n\n\n\n\nClick \nCreate\n to add an interpreter.\n \n  \n\n\n\n\n\n\nFrom the \nInterpreter group\n drop-down select \nsnappydata\n.\n     \n\n\n\n\nNote\n\n\nIf \nsnappydata\n is not displayed in the \nInterpreter group\n drop-down list, try the following options, and then restart Zeppelin daemon: \n\n\n\n\n\n\nDelete the \ninterpreter.json\n file located in the \nconf\n directory (in the Zeppelin home directory).\n\n\n\n\n\n\nDelete the \nzeppelin-spark_\nversion_number\n.jar\n file located in the \ninterpreter/snappydata\n directory (in the Zeppelin home directory).\n\n\n\n\n\n\n\n\n\n\n\n\nClick the \nConnect to existing process\n option. The fields \nHost\n and \nPort\n are displayed.\n\n\n\n\n\n\nSpecify the host on which the SnappyData lead node is executing, and the SnappyData Zeppelin Port (Default is 3768).\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault Values\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHost\n\n\nlocalhost\n\n\nSpecify host on which the SnappyData lead node is executing\n\n\n\n\n\n\nPort\n\n\n3768\n\n\nSpecify the Zeppelin server port\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure the interpreter properties. \nThe table lists the properties required for SnappyData.\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndefault.url\n\n\njdbc:snappydata://localhost:1527/\n\n\nSpecify the JDBC URL for SnappyData cluster in the format \njdbc:snappydata://\nlocator_hostname\n:1527\n\n\n\n\n\n\ndefault.driver\n\n\nio.snappydata.jdbc.ClientDriver\n\n\nSpecify the JDBC driver for SnappyData\n\n\n\n\n\n\nsnappydata.connection\n\n\nlocalhost:1527\n\n\nSpecify the \nhost:clientPort\n combination of the locator for the JDBC connection\n\n\n\n\n\n\nmaster\n\n\nlocal[*]\n\n\nSpecify the URI of the spark master (only local/split mode)\n\n\n\n\n\n\nzeppelin.jdbc.concurrent.use\n\n\ntrue\n\n\nSpecify the Zeppelin scheduler to be used. \nSelect \nTrue\n for Fair and \nFalse\n for FIFO\n\n\n\n\n\n\n\n\n\n\n\n\nIf required, edit other properties, and then click \nSave\n to apply your changes.\n\n\n\n\n\n\n\n\nNote\n\n\nYou can modify the default port number of the Zeppelin interpreter by setting the property:\n\n\n-zeppelin.interpreter.port=\nport_number\n in \nlead node configuration\n. \n\n\n\n\nAdditional Settings\n\n\n\n\n\n\nCreate a note and bind the interpreter by setting SnappyData as the default interpreter.\n SnappyData Zeppelin Interpreter group consist of two interpreters. Click and drag \nInterpreter_Name\n to the top of the list to set it as the default interpreter.\n\n\n\n\n\n\n\n\nInterpreter Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%snappydata.snappydata or \n %snappydata.spark\n\n\nThis interpreter is used to write Scala code in the paragraph. SnappyContext is injected in this interpreter and can be accessed using variable \nsnc\n\n\n\n\n\n\n%snappydata.sql\n\n\nThis interpreter is used to execute SQL queries on the SnappyData cluster. It also has features of executing approximate queries on the SnappyData cluster.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n to apply your changes.\n\n\n\n\n\n\n Known Issue\n\n\nIf you are using SnappyData Zeppelin Interpreter 0.7.1 and Zeppelin Installer 0.7 with SnappyData 0.8 or future releases, the approximate result does not work on the sample table, when you execute a paragraph with the \n%sql show-instant-results-first\n directive.", 
            "title": "How to Use Apache Zeppelin with SnappyData"
        }, 
        {
            "location": "/howto/use_apache_zeppelin_with_snappydata/#how-to-use-apache-zeppelin-with-snappydata", 
            "text": "", 
            "title": "How to Use Apache Zeppelin with SnappyData"
        }, 
        {
            "location": "/howto/use_apache_zeppelin_with_snappydata/#step-1-download-install-and-configure-snappydata", 
            "text": "Download and Install SnappyData   \n The table below lists the version of the SnappyData Zeppelin Interpreter and Apache Zeppelin Installer for the supported SnappyData Releases.     SnappyData Zeppelin Interpreter  Apache Zeppelin Binary Package  SnappyData Release      Version 0.7.3.4  Version 0.7.3  Release 1.0.2.1    Version 0.7.3.2  Version 0.7.3  Release 1.0.2    Version 0.7.3  Version 0.7.3  Release 1.0.1    Version 0.7.2  Version 0.7.2  Release 1.0.0       Configure the SnappyData Cluster .    In  lead node configuration  set the following properties:    Enable the SnappyData Zeppelin interpreter by adding  -zeppelin.interpreter.enable=true      In the classpath option, define the location where the SnappyData Interpreter is downloaded by adding  -classpath=/ download_location /snappydata-zeppelin- version_number .jar    In the  conf/spark-env.sh  file, set the  SPARK_PUBLIC_DNS  property to the public DNS name of the lead node. This enables the Member Logs to be displayed correctly to users accessing the  SnappyData Pulse UI  from outside the network.      Start the SnappyData cluster .    Extract the contents of the Zeppelin binary package.       Install the SnappyData Zeppelin interpreter in Apache Zeppelin by executing the following command from Zeppelin's bin directory:   ./install-interpreter.sh --name snappydata --artifact io.snappydata:snappydata-zeppelin: snappydata_interpreter_version_number   Zeppelin interpreter allows the SnappyData interpreter to be plugged into Zeppelin using which, you can run queries.    Rename the  zeppelin-site.xml.template  file (located in zeppelin- version_number -bin-all/conf directory) to  zeppelin-site.xml .    Edit the  zeppelin-site.xml  file:   In the  zeppelin.interpreters  property, add the following interpreter class names:              org.apache.zeppelin.interpreter.SnappyDataZeppelinInterpreter,org.apache.zeppelin.interpreter.SnappyDataSqlZeppelinInterpreter    Download the predefined SnappyData notebooks  notebook.tar.gz .   Extract and copy the contents of the notebook.tar.gz  compressed file to the  notebook  folder in the Zeppelin installation on your local machine.    Start the Zeppelin daemon using the command:    bin/zeppelin-daemon.sh start    To ensure that the installation is successful, log into the Zeppelin UI ( http://localhost:8080 ) from your web browser.", 
            "title": "Step 1: Download, Install and Configure SnappyData"
        }, 
        {
            "location": "/howto/use_apache_zeppelin_with_snappydata/#step-2-configure-interpreter-settings", 
            "text": "Log on to Zeppelin from your web browser and select  Interpreter  from the  Settings  option.    Click  Create  to add an interpreter.         From the  Interpreter group  drop-down select  snappydata .\n        Note  If  snappydata  is not displayed in the  Interpreter group  drop-down list, try the following options, and then restart Zeppelin daemon:     Delete the  interpreter.json  file located in the  conf  directory (in the Zeppelin home directory).    Delete the  zeppelin-spark_ version_number .jar  file located in the  interpreter/snappydata  directory (in the Zeppelin home directory).       Click the  Connect to existing process  option. The fields  Host  and  Port  are displayed.    Specify the host on which the SnappyData lead node is executing, and the SnappyData Zeppelin Port (Default is 3768).     Property  Default Values  Description      Host  localhost  Specify host on which the SnappyData lead node is executing    Port  3768  Specify the Zeppelin server port       Configure the interpreter properties.  The table lists the properties required for SnappyData.     Property  Value  Description      default.url  jdbc:snappydata://localhost:1527/  Specify the JDBC URL for SnappyData cluster in the format  jdbc:snappydata:// locator_hostname :1527    default.driver  io.snappydata.jdbc.ClientDriver  Specify the JDBC driver for SnappyData    snappydata.connection  localhost:1527  Specify the  host:clientPort  combination of the locator for the JDBC connection    master  local[*]  Specify the URI of the spark master (only local/split mode)    zeppelin.jdbc.concurrent.use  true  Specify the Zeppelin scheduler to be used.  Select  True  for Fair and  False  for FIFO       If required, edit other properties, and then click  Save  to apply your changes.     Note  You can modify the default port number of the Zeppelin interpreter by setting the property:  -zeppelin.interpreter.port= port_number  in  lead node configuration .", 
            "title": "Step 2: Configure Interpreter Settings"
        }, 
        {
            "location": "/howto/use_apache_zeppelin_with_snappydata/#additional-settings", 
            "text": "Create a note and bind the interpreter by setting SnappyData as the default interpreter.  SnappyData Zeppelin Interpreter group consist of two interpreters. Click and drag  Interpreter_Name  to the top of the list to set it as the default interpreter.     Interpreter Name  Description      %snappydata.snappydata or   %snappydata.spark  This interpreter is used to write Scala code in the paragraph. SnappyContext is injected in this interpreter and can be accessed using variable  snc    %snappydata.sql  This interpreter is used to execute SQL queries on the SnappyData cluster. It also has features of executing approximate queries on the SnappyData cluster.       Click  Save  to apply your changes.     Known Issue  If you are using SnappyData Zeppelin Interpreter 0.7.1 and Zeppelin Installer 0.7 with SnappyData 0.8 or future releases, the approximate result does not work on the sample table, when you execute a paragraph with the  %sql show-instant-results-first  directive.", 
            "title": "Additional Settings"
        }, 
        {
            "location": "/howto/concurrent_apache_zeppelin_access_to_secure_snappydata/", 
            "text": "How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster\n\n\nMultiple users can concurrently access a secure SnappyData cluster by configuring the JDBC interpreter setting in Apache Zeppelin. The JDBC interpreter allows you to create a JDBC connection to a SnappyData cluster.\n\n\n\n\nNote\n\n\n\n\n\n\nCurrently, only the \n%jdbc\n interpreter is supported with a secure SnappyData cluster.\n\n\n\n\n\n\nEach user accessing the secure SnappyData cluster should configure the \n%jdbc\n interpreter in Apache Zeppelin as described in this section.\n\n\n\n\n\n\n\n\nStep 1: Download, Install and Configure SnappyData\n\n\n\n\n\n\nDownload and install SnappyData Enterprise Edition\n \n\n\n\n\n\n\nConfigure the SnappyData cluster with security enabled\n.\n\n\n\n\n\n\nStart the SnappyData cluster\n.\n\n\n\n\n\n\nCreate a table and load data.\n\n\n\n\n\n\nGrant the required permissions for the users accessing the table.\n\n\nFor example:\n\n\nsnappy\n GRANT SELECT ON Table airline TO user2;\nsnappy\n GRANT INSERT ON Table airline TO user3;\nsnappy\n GRANT UPDATE ON Table airline TO user4;\n\n\n\n\n\n\n\n\n\nNote\n\n\nUser requiring INSERT, UPDATE or DELETE permissions also require explicit SELECT permission on a table.\n\n\n\n\n\n\n\n\nExtract the contents of the Zeppelin binary package. \n \n\n\n\n\n\n\nStart the Zeppelin daemon using the command: \n \n./bin/zeppelin-daemon.sh start\n\n\n\n\n\n\nConfigure the JDBC Interpreter\n\n\nLog on to Zeppelin from your web browser and configure the \nJDBC Interpreter\n.\n\n\n    Zeppelin web server is started on port 8080\n    http://\nIP address\n:8080/#/\n\n\n\nConfigure the Interpreter\n\n\n\n\n\n\nLog on to Zeppelin from your web browser and select \nInterpreter\n from the \nSettings\n option.\n\n\n\n\n\n\nEdit the existing \n%jdbc\n interpreter and configure the interpreter properties.\n    The table lists the properties required for SnappyData:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndefault.url\n\n\njdbc:snappydata://localhost:1527/\n\n\nSpecify the JDBC URL for SnappyData cluster in the format \njdbc:snappydata://\nlocator_hostname\n:1527\n\n\n\n\n\n\ndefault.driver\n\n\nio.snappydata.jdbc.ClientDriver\n\n\nSpecify the JDBC driver for SnappyData\n\n\n\n\n\n\ndefault.password\n\n\nuser123\n\n\nThe JDBC user password\n\n\n\n\n\n\ndefault.user\n\n\nuser1\n\n\nThe JDBC username\n\n\n\n\n\n\n\n\n\n\n\n\nDependency settings\n Since Zeppelin includes only PostgreSQL driver jar by default, you need to add the Client (JDBC) JAR file path for   SnappyData. The SnappyData Client (JDBC) JAR file (snappydata-jdbc_2.11-1.0.2.2.jar) is available on \nthe release page\n. \n\n    The SnappyData Client (JDBC) JAR file can also be placed under \n/interpreter/jdbc\n before starting Zeppelin instead of providing it in the dependency setting.\n\n\n\n\n\n\nIf required, edit other properties, and then click \nSave\n to apply your changes. \n\n\n\n\n\n\nSee also\n\n\n\n\nHow to Use Apache Zeppelin with SnappyData\n\n\nHow to connect using JDBC driver", 
            "title": "How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster"
        }, 
        {
            "location": "/howto/concurrent_apache_zeppelin_access_to_secure_snappydata/#how-to-configure-apache-zeppelin-to-securely-and-concurrently-access-the-snappydata-cluster", 
            "text": "Multiple users can concurrently access a secure SnappyData cluster by configuring the JDBC interpreter setting in Apache Zeppelin. The JDBC interpreter allows you to create a JDBC connection to a SnappyData cluster.   Note    Currently, only the  %jdbc  interpreter is supported with a secure SnappyData cluster.    Each user accessing the secure SnappyData cluster should configure the  %jdbc  interpreter in Apache Zeppelin as described in this section.", 
            "title": "How to Configure Apache Zeppelin to Securely and Concurrently access the SnappyData Cluster"
        }, 
        {
            "location": "/howto/concurrent_apache_zeppelin_access_to_secure_snappydata/#step-1-download-install-and-configure-snappydata", 
            "text": "Download and install SnappyData Enterprise Edition      Configure the SnappyData cluster with security enabled .    Start the SnappyData cluster .    Create a table and load data.    Grant the required permissions for the users accessing the table.  For example:  snappy  GRANT SELECT ON Table airline TO user2;\nsnappy  GRANT INSERT ON Table airline TO user3;\nsnappy  GRANT UPDATE ON Table airline TO user4;     Note  User requiring INSERT, UPDATE or DELETE permissions also require explicit SELECT permission on a table.     Extract the contents of the Zeppelin binary package.       Start the Zeppelin daemon using the command:    ./bin/zeppelin-daemon.sh start", 
            "title": "Step 1: Download, Install and Configure SnappyData"
        }, 
        {
            "location": "/howto/concurrent_apache_zeppelin_access_to_secure_snappydata/#configure-the-jdbc-interpreter", 
            "text": "Log on to Zeppelin from your web browser and configure the  JDBC Interpreter .      Zeppelin web server is started on port 8080\n    http:// IP address :8080/#/", 
            "title": "Configure the JDBC Interpreter"
        }, 
        {
            "location": "/howto/concurrent_apache_zeppelin_access_to_secure_snappydata/#configure-the-interpreter", 
            "text": "Log on to Zeppelin from your web browser and select  Interpreter  from the  Settings  option.    Edit the existing  %jdbc  interpreter and configure the interpreter properties.\n    The table lists the properties required for SnappyData:     Property  Value  Description      default.url  jdbc:snappydata://localhost:1527/  Specify the JDBC URL for SnappyData cluster in the format  jdbc:snappydata:// locator_hostname :1527    default.driver  io.snappydata.jdbc.ClientDriver  Specify the JDBC driver for SnappyData    default.password  user123  The JDBC user password    default.user  user1  The JDBC username       Dependency settings  Since Zeppelin includes only PostgreSQL driver jar by default, you need to add the Client (JDBC) JAR file path for   SnappyData. The SnappyData Client (JDBC) JAR file (snappydata-jdbc_2.11-1.0.2.2.jar) is available on  the release page .  \n    The SnappyData Client (JDBC) JAR file can also be placed under  /interpreter/jdbc  before starting Zeppelin instead of providing it in the dependency setting.    If required, edit other properties, and then click  Save  to apply your changes.     See also   How to Use Apache Zeppelin with SnappyData  How to connect using JDBC driver", 
            "title": "Configure the Interpreter"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture Overview\n\n\nThis section presents a high-level overview of SnappyData\u2019s core components. It also explains how our data pipeline (as streams) are ingested into our in-memory store and subsequently interacted with and analyzed.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nCore Components\n\n\n\n\n\n\nData Ingestion Pipeline\n\n\n\n\n\n\nHybrid Cluster Manager\n\n\n\n\n\n\nSnappyData Cluster Architecture", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#architecture-overview", 
            "text": "This section presents a high-level overview of SnappyData\u2019s core components. It also explains how our data pipeline (as streams) are ingested into our in-memory store and subsequently interacted with and analyzed.  The following topics are covered in this section:    Core Components    Data Ingestion Pipeline    Hybrid Cluster Manager    SnappyData Cluster Architecture", 
            "title": "Architecture Overview"
        }, 
        {
            "location": "/architecture/core_components/", 
            "text": "Core Components\n\n\nThe following depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, standard components, such as security and monitoring have been omitted.\n\n\n\n\nThe storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row-oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. Refer to the \nRow/Column table\n section for details on the syntax and available features. \n\n\nTwo primary programming models are supported by SnappyData \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions, to make the language compatible with the SQL standard. One could perceive SnappyData as an SQL database that uses Spark API as its language for stored procedures. Our \nstream processing\n is primarily through Spark Streaming, but it is integrated and runs within our store.\n\n\nThe OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). All OLTP operations are routed immediately to appropriate data partitions without incurring any scheduling overhead.\n\n\nTo support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, SnappyData uses a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.\n\n\nIn addition to the \u201cexact\u201d Dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Synopsis Data Engine (SDE) and exploits appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.\n\n\nTo understand the data flow architecture, you are first walked through a real-time use case that involves stream processing, ingesting into an in-memory store and interactive analytics.", 
            "title": "Core Components"
        }, 
        {
            "location": "/architecture/core_components/#core-components", 
            "text": "The following depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, standard components, such as security and monitoring have been omitted.   The storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row-oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. Refer to the  Row/Column table  section for details on the syntax and available features.   Two primary programming models are supported by SnappyData \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions, to make the language compatible with the SQL standard. One could perceive SnappyData as an SQL database that uses Spark API as its language for stored procedures. Our  stream processing  is primarily through Spark Streaming, but it is integrated and runs within our store.  The OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). All OLTP operations are routed immediately to appropriate data partitions without incurring any scheduling overhead.  To support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, SnappyData uses a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.  In addition to the \u201cexact\u201d Dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Synopsis Data Engine (SDE) and exploits appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.  To understand the data flow architecture, you are first walked through a real-time use case that involves stream processing, ingesting into an in-memory store and interactive analytics.", 
            "title": "Core Components"
        }, 
        {
            "location": "/architecture/data_ingestion_pipeline/", 
            "text": "Data Ingestion Pipeline\n\n\nThe data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive architecture for real-time applications. The steps to support these tasks are depicted in the following figure and explained below.\n\n\n\n\n\n\n\n\nOnce the SnappyData cluster is started and before any live streams can be processed, ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (for example, HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables.\n\n\n\n\n\n\nSpark Streaming\u2019s parallel receivers are relied on to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.\n\n\n\n\n\n\nNext, SQL is used to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), these data structures are seamlessly combined in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high-velocity streams, SnappyData can still provide answers in real time by resorting to approximation.\n\n\n\n\n\n\nThe stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (for example, maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.\n\n\n\n\n\n\nTo prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.\n\n\n\n\n\n\nOnce ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance with users\u2019 requested accuracy.", 
            "title": "Data Ingestion Pipeline"
        }, 
        {
            "location": "/architecture/data_ingestion_pipeline/#data-ingestion-pipeline", 
            "text": "The data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive architecture for real-time applications. The steps to support these tasks are depicted in the following figure and explained below.     Once the SnappyData cluster is started and before any live streams can be processed, ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (for example, HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables.    Spark Streaming\u2019s parallel receivers are relied on to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.    Next, SQL is used to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), these data structures are seamlessly combined in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high-velocity streams, SnappyData can still provide answers in real time by resorting to approximation.    The stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (for example, maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.    To prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.    Once ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance with users\u2019 requested accuracy.", 
            "title": "Data Ingestion Pipeline"
        }, 
        {
            "location": "/architecture/hybrid_cluster_manager/", 
            "text": "Hybrid Cluster Manager\n\n\nSpark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (for example, YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors. This is represented in the following figure.\n\n\n\n\nWhile Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet the following additional requirements as an operational database.\n\n\n\n\n\n\nHigh Concurrency\n: SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.\n\n\n\n\n\n\nState Sharing\n: Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real-time data sharing.\n\n\n\n\n\n\nHigh Availability (HA)\n: As a highly concurrent distributed system that offers low latency access to data, applications must be protected from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations, therefore, become an important requirement for SnappyData.\n\n\n\n\n\n\nConsistency\n: As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture, how SnappyData meets each of these requirements is explained in the subsequent sections.", 
            "title": "Hybrid Cluster Manager"
        }, 
        {
            "location": "/architecture/hybrid_cluster_manager/#hybrid-cluster-manager", 
            "text": "Spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (for example, YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors. This is represented in the following figure.   While Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet the following additional requirements as an operational database.    High Concurrency : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.    State Sharing : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real-time data sharing.    High Availability (HA) : As a highly concurrent distributed system that offers low latency access to data, applications must be protected from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations, therefore, become an important requirement for SnappyData.    Consistency : As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture, how SnappyData meets each of these requirements is explained in the subsequent sections.", 
            "title": "Hybrid Cluster Manager"
        }, 
        {
            "location": "/architecture/cluster_architecture/", 
            "text": "SnappyData Cluster Architecture\n\n\nA SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members as represented in the below figure.\n\n\n\n\n\n\nLocator: Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n\n\n\n\n\n\nLead Node: The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n\n\n\n\n\n\nData Servers: A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or pass it to the lead node for execution by Spark SQL.\n\n\n\n\n\n\n\n\nSnappyData also has multiple deployment options. For more information refer to, \nDeployment Options\n.\n\n\nInteracting with SnappyData\n\n\n\n\nNote\n\n\nFor the section on the Spark API, it is assumed that users have some familiarity with \ncore Spark, Spark SQL, and Spark Streaming concepts\n.\nAnd, you can try out the Spark \nQuick Start\n. All the commands and programs listed in the Spark guides work in SnappyData as well.\nFor the section on SQL, no Spark knowledge is necessary.\n\n\n\n\nTo interact with SnappyData, interfaces are provided for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self-contained Spark application or can share the state with other jobs using the SnappyData store.\n\n\nUnlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.\n\n\n\n\n\n\nLong running executors\n: Executors are running within the SnappyData store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors.\n\n\n\n\n\n\nDriver runs in HA configuration\n: Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shut down, taking down all cached state with it. Instead, SnappyData leverages the \nSpark JobServer\n to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA).\n\n\n\n\n\n\nIn this document, mostly the same set of features via the Spark API or using SQL is showcased. If you are familiar with Scala and understand Spark concepts you can choose to skip the SQL part go directly to the \nSpark API section\n.\n\n\nHigh Concurrency in SnappyData\n\n\nThousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into low latency requests and high latency ones.\n\n\nFor low latency operations, Spark\u2019s scheduling mechanism is completely bypassed and directly operate on the data. High latency operations (for example, compute intensive queries) are routed through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.\n\n\nState Sharing in SnappyData\n\n\nA SnappyData cluster is designed to be a long-running clustered database. The state is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "SnappyData Cluster Architecture"
        }, 
        {
            "location": "/architecture/cluster_architecture/#snappydata-cluster-architecture", 
            "text": "A SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members as represented in the below figure.    Locator: Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.    Lead Node: The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.    Data Servers: A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node or pass it to the lead node for execution by Spark SQL.     SnappyData also has multiple deployment options. For more information refer to,  Deployment Options .", 
            "title": "SnappyData Cluster Architecture"
        }, 
        {
            "location": "/architecture/cluster_architecture/#interacting-with-snappydata", 
            "text": "Note  For the section on the Spark API, it is assumed that users have some familiarity with  core Spark, Spark SQL, and Spark Streaming concepts .\nAnd, you can try out the Spark  Quick Start . All the commands and programs listed in the Spark guides work in SnappyData as well.\nFor the section on SQL, no Spark knowledge is necessary.   To interact with SnappyData, interfaces are provided for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self-contained Spark application or can share the state with other jobs using the SnappyData store.  Unlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.    Long running executors : Executors are running within the SnappyData store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors.    Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shut down, taking down all cached state with it. Instead, SnappyData leverages the  Spark JobServer  to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA).    In this document, mostly the same set of features via the Spark API or using SQL is showcased. If you are familiar with Scala and understand Spark concepts you can choose to skip the SQL part go directly to the  Spark API section .", 
            "title": "Interacting with SnappyData"
        }, 
        {
            "location": "/architecture/cluster_architecture/#high-concurrency-in-snappydata", 
            "text": "Thousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into low latency requests and high latency ones.  For low latency operations, Spark\u2019s scheduling mechanism is completely bypassed and directly operate on the data. High latency operations (for example, compute intensive queries) are routed through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.", 
            "title": "High Concurrency in SnappyData"
        }, 
        {
            "location": "/architecture/cluster_architecture/#state-sharing-in-snappydata", 
            "text": "A SnappyData cluster is designed to be a long-running clustered database. The state is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "State Sharing in SnappyData"
        }, 
        {
            "location": "/configuration/", 
            "text": "Configuring the Cluster\n\n\nSnappyData has three main components - Locator, Server, and Lead.\n\n\nThe Lead node embeds a Spark driver and the Server node embeds a Spark Executor. The server node also embeds a SnappyData store.\n\n\nSnappyData cluster can be started with the default configurations using script \nsbin/snappy-start-all.sh\n. This script starts up a locator, one data server, and one lead node. However, SnappyData can be configured to start multiple components on different nodes. \n\nAlso, each component can be configured individually using configuration files. In this section, you can learn how the components can be individually configured and also learn about various other configurations of SnappyData.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nConfiguration Files\n\n\n\n\n\n\nConfiguring Locators\n\n\n\n\n\n\nConfiguring Leads\n\n\n\n\n\n\nConfiguring Data Servers\n\n\n\n\n\n\nConfiguring SnappyData Smart Connector\n\n\n\n\n\n\nEnvironment Settings\n\n\n\n\n\n\nHadoop Provided Settings\n\n\n\n\n\n\nSnappyData Command Line Utility\n\n\n\n\n\n\n\n\n\n\nLogging\n\n\n\n\n\n\n\n\n\n\nList of Properties\n\n\n\n\n\n\nFirewalls and Connections", 
            "title": "Configuring the Cluster"
        }, 
        {
            "location": "/configuration/#configuring-the-cluster", 
            "text": "SnappyData has three main components - Locator, Server, and Lead.  The Lead node embeds a Spark driver and the Server node embeds a Spark Executor. The server node also embeds a SnappyData store.  SnappyData cluster can be started with the default configurations using script  sbin/snappy-start-all.sh . This script starts up a locator, one data server, and one lead node. However, SnappyData can be configured to start multiple components on different nodes.  \nAlso, each component can be configured individually using configuration files. In this section, you can learn how the components can be individually configured and also learn about various other configurations of SnappyData.  The following topics are covered in this section:    Configuration Files    Configuring Locators    Configuring Leads    Configuring Data Servers    Configuring SnappyData Smart Connector    Environment Settings    Hadoop Provided Settings    SnappyData Command Line Utility      Logging      List of Properties    Firewalls and Connections", 
            "title": "Configuring the Cluster"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/", 
            "text": "Configuration\n\n\nConfiguration files for locator, lead, and server should be created in the \nconf\n folder located in the SnappyData home directory with names \nlocators\n, \nleads\n, and \nservers\n.\n\n\nTo do so, you can copy the existing template files \nservers.template\n, \nlocators.template\n, \nleads.template\n, and rename them to \nservers\n, \nlocators\n, \nleads\n.\nThese files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members.\n\n\n\n\nTip\n\n\n\n\n\n\nFor system properties (set in the conf/lead, conf/servers and conf/locators file), -D and -XX: can be used. All other JVM properties need the \n-J\n prefix.\n\n\n\n\n\n\nInstead of starting the SnappyData cluster, you can \nstart\n and \nstop\n individual components on a system locally.\n\n\n\n\n\n\n\n\n\n\nConfiguring Locators\n\n\nLocators provide discovery service for the cluster. Clients (for example, JDBC) connect to the locator and discover the lead and data servers in the cluster. The clients automatically connect to the data servers upon discovery (upon initial connection). Cluster members (Data servers, Lead nodes) also discover each other using the locator. Refer to the \nArchitecture\n section for more information on the core components.\n\n\nIt is recommended to configure two locators (for HA) in production using the \nconf/locators\n file located in the \nSnappyData_home\n/conf\n directory. \n\n\nIn this file, you can specify:\n\n\n\n\n\n\nThe hostname on which a SnappyData locator is started.\n\n\n\n\n\n\nThe startup directory where the logs and configuration files for that locator instance are located.\n\n\n\n\n\n\nSnappyData specific properties that can be passed.\n\n\n\n\n\n\nYou can refer to the \nconf/locators.template\n file for some examples.\n\n\nList of Locator Properties\n\n\nRefer to the \nSnappyData properties\n for the complete list of SnappyData properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-bind-address\n\n\nIP address on which the locator is bound. The default behavior is to bind to all local addresses.\n\n\n\n\n\n\n-classpath\n\n\nLocation of user classes required by the SnappyData Server.\nThis path is appended to the current classpath.\n\n\n\n\n\n\n-client-port\n\n\nThe port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527.\n\n\n\n\n\n\n-dir\n\n\nThe working directory of the server that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory).\n\n\n\n\n\n\n-heap-size\n\n\n Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. \nFor example, -heap-size=1024m. \nIf you use the \n-heap-size\n option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the \neviction-heap-percentage\n to 85.5% of the \ncritical-heap-percentage\n. \nSnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them.\n\n\n\n\n\n\n-J\n\n\nJVM option passed to the spawned SnappyData server JVM. \nFor example, use -J-Xmx1024m to set the JVM heap to 1GB.\n\n\n\n\n\n\n-J-Dsnappydata.enable-rls\n\n\nEnables the system for row level security when set to true.  By default, this is off. If this property is set to true, then the Smart Connector access to SnappyData fails.\n\n\n\n\n\n\n-locators\n\n\nList of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. \nThe list must include all locators in use and must be configured consistently for every member of the distributed system.\n\n\n\n\n\n\n-log-file\n\n\nPath of the file to which this member writes log messages. The default is \nsnappylocator.log\n in the working directory. In case logging is set via log4j, the default log file is \nsnappydata.log\n.\n\n\n\n\n\n\n-member-timeout\n\n\nUses the \nmember-timeout\n server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:\n 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.\n 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. \nValid values are in the range 1000..600000.\n\n\n\n\n\n\n-peer-discovery-address\n\n\nUse this as value for the port in the \"host:port\" value of \"-locators\" property\n\n\n\n\n\n\n-peer-discovery-port\n\n\nThe port on which the locator listens for peer discovery (includes servers as well as other locators).  \nValid values are in the range 1-65535, with a default of 10334.\n\n\n\n\n\n\nProperties for SSL Encryption\n\n\nssl-enabled\n, \nssl-ciphers\n, \nssl-protocols\n, \nssl-require-authentication\n.\n\n\n\n\n\n\n\n\n\n\nExample\n: To start two locators on node-a:9999 and node-b:8888, update the configuration file as follows:\n\n\n$ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999\n\n\n\n\n\n\nConfiguring Leads\n\n\nLead Nodes primarily runs the SnappyData managed Spark driver. There is one primary lead node at any given instance, but there can be multiple secondary lead node instances on standby for fault tolerance. Applications can run Jobs using the REST service provided by the Lead node. Most of the SQL queries are automatically routed to the Lead to be planned and executed through a scheduler. You can refer to the \nconf/leads.template\n file for some examples. \n\n\nCreate the configuration file (\nleads\n) for leads in the \nSnappyData_home\n/conf\n directory.\n\n\n\n\nNote\n\n\nIn the \nconf/spark-env.sh\n file set the \nSPARK_PUBLIC_DNS\n property to the public DNS name of the lead node. This enables the Member Logs to be displayed correctly to users accessing SnappyData Pulse from outside the network.\n\n\n\n\nList of Lead Properties\n\n\nRefer to the \nSnappyData properties\n for the complete list of SnappyData properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-bind-address\n\n\nIP address on which the lead is bound. The default behavior is to bind to all local addresses.\n\n\n\n\n\n\n-classpath\n\n\nLocation of user classes required by the SnappyData Server.\nThis path is appended to the current classpath.\n\n\n\n\n\n\n-critical-heap-percentage\n\n\nSets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. \nIf you set \n-heap-size\n, the default value for \ncritical-heap-percentage\n is set to 95% of the heap size. \nUse this switch to override the default.\nWhen this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.\n\n\n\n\n\n\n-dir\n\n\nThe working directory of the lead that contains the SnappyData Lead status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory).\n\n\n\n\n\n\n-eviction-heap-percentage\n\n\nSets the memory usage percentage threshold (0-100) that the Resource Manager uses to evict data from the heap. By default, the eviction threshold is 85.5% of whatever is set for \n-critical-heap-percentage\n.\nUse this switch to override the default.\n\n\n\n\n\n\n-heap-size\n\n\n Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. \nFor example, \n-heap-size=8g\n \n It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the \n-heap-size\n option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the \neviction-heap-percentage\n to 85.5% of the \ncritical-heap-percentage\n. \nSnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them.\n\n\n\n\n\n\n-J\n\n\nJVM option passed to the spawned SnappyData Lead JVM. \nFor example, use -J-Xmx1024m to set the JVM heap to 1GB.\n\n\n\n\n\n\n-J-Dsnappydata.enable-rls\n\n\nEnables the system for row level security when set to true.  By default, this is off. If this property is set to true,  then the Smart Connector access to SnappyData fails.\n\n\n\n\n\n\n-J-Dsnappydata.RESTRICT_TABLE_CREATION\n\n\nApplicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false.\n\n\n\n\n\n\njobserver.waitForInitialization\n\n\nWhen this property is set to true, the cluster startup waits for the Spark jobserver to be fully initialized before marking the lead node as \nRUNNING\n. The default is false.\n\n\n\n\n\n\n-locators\n\n\nList of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. \nThe list must include all locators in use and must be configured consistently for every member of the distributed system.\n\n\n\n\n\n\n-log-file\n\n\nPath of the file to which this member writes log messages. The default \nsnappyleader.log\n in the working directory. In case logging is set via log4j, the default log file is \nsnappydata.log\n.\n\n\n\n\n\n\n-member-timeout\n\n\nUses the \nmember-timeout\n configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:\n 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.\n 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. \nValid values are in the range 1000..600000.\n\n\n\n\n\n\n-memory-size\n\n\nSpecifies the total memory that can be used by the node for column storage and execution in off-heap. However, lead member do not need off-heap memory.  You can configure the off-heap memory for leads only when you are planning to increase the broadcast limit to a large value. This is generally not recommended and you must preferably limit the broadcast to a smaller value. The default off-heap size for leads is 0.\n\n\n\n\n\n\n-snappydata.column.batchSize\n\n\nThe default size of blocks to use for storage in the SnappyData column store. The default value is 24M.\n\n\n\n\n\n\nspark.context-settings.num-cpu-cores\n\n\nThe number of cores that can be allocated. The default is 4.\n\n\n\n\n\n\nspark.context-settings.memory-per-node\n\n\nThe executor memory per node (-Xmx style. For example: 512m, 1G). The default is 512m.\n\n\n\n\n\n\nspark.context-settings.streaming.batch_interval\n\n\nThe batch interval for Spark Streaming contexts in milliseconds. The default is 1000.\n\n\n\n\n\n\nspark.context-settings.streaming.stopGracefully\n\n\nIf set to true, the streaming stops gracefully by waiting for the completion of processing of all the received data. The default is true.\n\n\n\n\n\n\nspark.context-settings.streaming.stopSparkContext\n\n\nif set to true, the SparkContext is stopped along with the StreamingContext. The default is true.\n\n\n\n\n\n\n-spark.driver.maxResultSize\n\n\nLimit of the total size of serialized results of all partitions for each action (for example, collect). The value should be at least 1MB or 0 for unlimited. Jobs are aborted if the total size of the results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB\n\n\n\n\n\n\n-spark.executor.cores\n\n\nThe number of cores to use on each server.\n\n\n\n\n\n\n-spark.jobserver.port\n\n\nThe port on which to run the jobserver. Default port is 8090.\n\n\n\n\n\n\n-spark.jobserver.bind-address\n\n\nThe address on which the jobserver listens. Default address is 0.0.0.\n\n\n\n\n\n\n-spark.jobserver.job-result-cache-size\n\n\nThe number of job results to keep per JobResultActor/context. The default is 5000.\n\n\n\n\n\n\n-spark.jobserver.max-jobs-per-context\n\n\nThe number of jobs that can be run simultaneously in the context. The default is 8.\n\n\n\n\n\n\n-spark.local.dir\n\n\nDirectory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks.\n\n\n\n\n\n\n-spark.network.timeout\n\n\nThe default timeout for all network interactions while running queries.\n\n\n\n\n\n\n-spark.sql.codegen.cacheSize\n\n\nSize of the generated code cache that is used by Spark, in the  SnappyData Spark distribution, and by SnappyData. The default is 2000.\n\n\n\n\n\n\n-spark.ssl.enabled\n\n\nEnables or disables Spark layer encryption. The default is false.\n\n\n\n\n\n\n-spark.ssl.keyPassword\n\n\nThe password to the private key in the key store.\n\n\n\n\n\n\n-spark.ssl.keyStore\n\n\nPath to the key store file. The path can be absolute or relative to the directory in which the process is started.\n\n\n\n\n\n\n-spark.ssl.keyStorePassword\n\n\nThe password used to access the keystore.\n\n\n\n\n\n\n-spark.ssl.trustStore\n\n\nPath to the trust store file. The path can be absolute or relative to the directory in which the process is started.\n\n\n\n\n\n\n-spark.ssl.trustStorePassword\n\n\nThe password used to access the truststore.\n\n\n\n\n\n\n-spark.ssl.protocol\n\n\nThe protocol that must be supported by JVM. For example, TLS.\n\n\n\n\n\n\n-spark.ui.port\n\n\nPort for your SnappyData Pulse, which shows tables, memory and workload data. The default is 5050.\n\n\n\n\n\n\nProperties for SSL Encryption\n\n\nssl-enabled\n, \nssl-ciphers\n, \nssl-protocols\n, \nssl-require-authentication\n. \n These properties need not be added to  the Lead members in case of a client-server connection.\n\n\n\n\n\n\n\n\nExample\n: To start a lead (node-l), set \nspark.executor.cores\n as 10 on all servers, and change the Spark UI port from 5050 to 9090, update the configuration file as follows:\n\n\n$ cat conf/leads\nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10\n\n\n\n\n\n\nConfiguring Secondary Lead\n\n\nTo configure secondary leads, you must add the required number of entries in the \nconf/leads\n file. \n\n\nFor example:\n\n\n$ cat conf/leads\nnode-l1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-l2 -heap-size=4096m -locators=node-b:8888,node-a:9999\n\n\n\n\n\nIn this example, two leads (one on node-l1 and another on node-l2) are configured. Using \nsbin/snappy-start-all.sh\n, when you launch the cluster, one of them becomes the primary lead and the other becomes the secondary lead.\n\n\n\n\nConfiguring Data Servers\n\n\nData Servers hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than the Spark engine. Data servers use intelligent query routing to either execute the query directly on the node or to pass it to the lead node for execution by Spark SQL. You can refer to the \nconf/servers.template\n file for some examples. \n\n\nCreate the configuration file (\nservers\n) for data servers in the \nSnappyData_home\n/conf\n directory. \n\n\nList of Server Properties\n\n\nRefer to the \nSnappyData properties\n for the complete list of SnappyData properties.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-bind-address\n\n\nIP address on which the server is bound. The default behavior is to bind to all local addresses.\n\n\n\n\n\n\n-classpath\n\n\nLocation of user classes required by the SnappyData Server.\nThis path is appended to the current classpath.\n\n\n\n\n\n\n-client-port\n\n\nThe port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527.\n\n\n\n\n\n\n-critical-heap-percentage\n\n\nSets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. \nIf you set \n-heap-size\n, the default value for \ncritical-heap-percentage\n is set to 95% of the heap size. \nUse this switch to override the default.\nWhen this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.\n\n\n\n\n\n\n-critical-off-heap-percentage\n\n\nSets the critical threshold for off-heap memory usage in percentage, 0-100. \nWhen this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory.\n\n\n\n\n\n\n-dir\n\n\nThe working directory of the server that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory). \nwork\n is the default current working directory.\n\n\n\n\n\n\n-eviction-heap-percentage\n\n\nSets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 85.5% of whatever is set for \n-critical-heap-percentage\n.\nUse this switch to override the default.\n\n\n\n\n\n\n-eviction-off-heap-percentage\n\n\nSets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory. \nBy default, the eviction threshold is 85.5% of the value that is set for \n-critical-off-heap-percentage\n. \nUse this switch to override the default.\n\n\n\n\n\n\n-heap-size\n\n\n Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. \nFor example, -heap-size=1024m. \nIf you use the \n-heap-size\n option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the \neviction-heap-percentage\n to 85.5% of the \ncritical-heap-percentage\n. \nSnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them.\n\n\n\n\n\n\n-memory-size\n\n\nSpecifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in \nspecific scenarios\n.\n\n\n\n\n\n\n-J\n\n\nJVM option passed to the spawned SnappyData server JVM. \nFor example, use \n-J-XX:+PrintGCDetails\n to print the GC details in JVM logs.\n\n\n\n\n\n\n-J-Dgemfirexd.hostname-for-clients\n\n\nThe IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the \nclient-bind-address\n to be given to clients. \n This value can be different from \nclient-bind-address\n for cases where the servers/locators are behind a NAT firewall (AWS for example) where \nclient-bind-address\n needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in \nclient-bind-address\n resolves to the internal IP address from inside and to the public IP address from outside, but for other cases, this property is required\n\n\n\n\n\n\n-J-Dsnappydata.enable-rls\n\n\nEnables the system for row level security when set to true.  By default, this is off. If this property is set to true,  then the Smart Connector access to SnappyData fails.\n\n\n\n\n\n\n-J-Dsnappydata.RESTRICT_TABLE_CREATION\n\n\nApplicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false.\n\n\n\n\n\n\n-locators\n\n\nList of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. \nThe list must include all locators in use and must be configured consistently for every member of the distributed system.\n\n\n\n\n\n\n-log-file\n\n\nPath of the file to which this member writes log messages. The default is \nsnappyserver.log\n in the working directory. In case logging is set via log4j, the default log file is \nsnappydata.log\n.\n\n\n\n\n\n\n-member-timeout\n\n\nUses the \nmember-timeout\n server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:\n 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.\n 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. \nValid values are in the range 1000..600000.\n\n\n\n\n\n\n-rebalance\n\n\nCauses the new member to trigger a rebalancing operation for all partitioned tables in the system. \nThe system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option. Usually rebalancing is triggered when the overall capacity is increased or reduced through member startup, shut down, or failure.\n\n\n\n\n\n\n-spark.local.dir\n\n\nDirectory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks.\n\n\n\n\n\n\nProperties for SSL Encryption\n\n\nssl-enabled\n, \nssl-ciphers\n, \nssl-protocols\n, \nssl-require-authentication\n.\n\n\n\n\n\n\n-thrift-ssl\n\n\nSpecifies if you want to enable or disable SSL. Values: true or false\n\n\n\n\n\n\n-thrift-ssl-properties\n\n\nComma-separated SSL properties including:\nprotocol\n: default \"TLS\",\nenabled-protocols\n: enabled protocols separated by \":\"\ncipher-suites\n: enabled cipher suites separated by \":\"\nclient-auth\n=(true or false): if client also needs to be authenticated \nkeystore\n: path to key store file \nkeystore-type\n: the type of key-store (default \"JKS\") \nkeystore-password\n: password for the key store file\nkeymanager-type\n: the type of key manager factory \ntruststore\n: path to trust store file\ntruststore-type\n: the type of trust-store (default \"JKS\")\ntruststore-password\n: password for the trust store file \ntrustmanager-type\n: the type of trust manager factory \n\n\n\n\n\n\n\n\nExample\n: To start a two servers (node-c and node-c), update the configuration file as follows:\n\n\n$ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999\n\n\n\n\nSpecifying Configuration Properties using Environment Variables\n\n\nSnappyData configuration properties can be specified using environment variables LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, and LEAD_STARTUP_OPTIONS respectively for locators, leads and servers.  These environment variables are useful to specify common properties for locators, servers, and leads.  These startup environment variables can be specified in \nconf/spark-env.sh\n file. This file is sourced when SnappyData system is started. A template file \nconf/spark-env.sh.template\n is provided in \nconf\n directory for reference. You can copy this file and use it to configure properties. \n\n\nFor example:\n\n\n# create a spark-env.sh from the template file\n$cp conf/spark-env.sh.template conf/spark-env.sh \n\n# Following example configuration can be added to spark-env.sh, \n# it shows how to add security configuration using the environment variables\n\nSECURITY_ARGS=\n-auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=password123 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=password123\n\n\n#applies the configuration specified by SECURITY_ARGS to all locators\nLOCATOR_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\n#applies the configuration specified by SECURITY_ARGS to all servers\nSERVER_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\n#applies the configuration specified by SECURITY_ARGS to all leads\nLEAD_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\n\n\n\n\n\n\n\n\nConfiguring SnappyData Smart Connector\n\n\nSpark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). In Smart connector mode, a Spark application connects to SnappyData cluster to store and process data. SnappyData currently works with Spark version 2.1.1. To work with SnappyData cluster, a Spark application must set the \nsnappydata.connection\n property while starting.   \n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsnappydata.connection\n\n\nSnappyData cluster's locator host and JDBC client port on which locator listens for connections. Has to be specified while starting a Spark application.\n\n\n\n\n\n\n\n\nExample\n:\n\n\n$ ./bin/spark-submit --deploy-mode cluster --class somePackage.someClass  \n    --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 \n    --packages 'SnappyDataInc:snappydata:1.0.2.1-s_2.11'\n\n\n\n\n\n\nEnvironment Settings\n\n\nAny Spark or SnappyData specific environment settings can be done by creating a \nsnappy-env.sh\n or \nspark-env.sh\n in \nSNAPPY_HOME/conf\n.\n\n\n\n\nHadoop Provided Settings\n\n\nIf you want to run SnappyData with an already existing custom Hadoop cluster like MapR or Cloudera you should download Snappy without Hadoop from the download link. This allows you to provide Hadoop at runtime.\n\n\nTo do this, you need to put an entry in $SNAPPY-HOME/conf/spark-env.sh as below:\n\n\nexport SPARK_DIST_CLASSPATH=$($OTHER_HADOOP_HOME/bin/hadoop classpath)\n\n\n\n\n\n\nLogging\n\n\nCurrently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property \n-log-file\n as the path of the directory. \n\nThe logging levels can be modified by adding a \nconf/log4j.properties\n file in the product directory. \n\n\n$ cat conf/log4j.properties \nlog4j.logger.org.apache.spark.scheduler.DAGScheduler=DEBUG\nlog4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG\n\n\n\n\n\n\nNote\n\n\nFor a set of applicable class names and default values see the file \nconf/log4j.properties.template\n, which can be used as a starting point. Consult the \nlog4j 1.2.x documentation\n for more details on the configuration file.\n\n\n\n\n\n\nAuto-Configuring Off-Heap Memory Size\n\n\nOff-Heap memory size is auto-configured by default in the following scenarios:\n\n\n\n\n\n\nWhen the lead, locator, and server are setup on different host machines:\n\n    In this case, off-heap memory size is configured by default for the host machines with the server setup. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. For example, if the RAM is greater than 8GB, the heap memory is between 4-8 GB and the remaining becomes the off-heap memory.\n\n\n\n\n\n\nWhen leads and one of the server node are on the same host:\n\nIn this case,  off-heap memory size is configured by default and is adjusted based on the number of leads that are present. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. However, here the heap memory is the total heap size of the server as well as that of the lead. \n\n\n\n\n\n\n\n\nNote\n\n\nThe off-heap memory size is not auto-configured when the heap memory and the off-heap memory are explicitly configured through properties or when multiple servers are on the same host machine.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuration", 
            "text": "Configuration files for locator, lead, and server should be created in the  conf  folder located in the SnappyData home directory with names  locators ,  leads , and  servers .  To do so, you can copy the existing template files  servers.template ,  locators.template ,  leads.template , and rename them to  servers ,  locators ,  leads .\nThese files should contain the hostnames of the nodes (one per line) where you intend to start the member. You can modify the properties to configure individual members.   Tip    For system properties (set in the conf/lead, conf/servers and conf/locators file), -D and -XX: can be used. All other JVM properties need the  -J  prefix.    Instead of starting the SnappyData cluster, you can  start  and  stop  individual components on a system locally.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-locators", 
            "text": "Locators provide discovery service for the cluster. Clients (for example, JDBC) connect to the locator and discover the lead and data servers in the cluster. The clients automatically connect to the data servers upon discovery (upon initial connection). Cluster members (Data servers, Lead nodes) also discover each other using the locator. Refer to the  Architecture  section for more information on the core components.  It is recommended to configure two locators (for HA) in production using the  conf/locators  file located in the  SnappyData_home /conf  directory.   In this file, you can specify:    The hostname on which a SnappyData locator is started.    The startup directory where the logs and configuration files for that locator instance are located.    SnappyData specific properties that can be passed.    You can refer to the  conf/locators.template  file for some examples.", 
            "title": "Configuring Locators"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#list-of-locator-properties", 
            "text": "Refer to the  SnappyData properties  for the complete list of SnappyData properties.     Property  Description      -bind-address  IP address on which the locator is bound. The default behavior is to bind to all local addresses.    -classpath  Location of user classes required by the SnappyData Server. This path is appended to the current classpath.    -client-port  The port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527.    -dir  The working directory of the server that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory).    -heap-size   Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings.  For example, -heap-size=1024m.  If you use the  -heap-size  option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the  eviction-heap-percentage  to 85.5% of the  critical-heap-percentage .  SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them.    -J  JVM option passed to the spawned SnappyData server JVM.  For example, use -J-Xmx1024m to set the JVM heap to 1GB.    -J-Dsnappydata.enable-rls  Enables the system for row level security when set to true.  By default, this is off. If this property is set to true, then the Smart Connector access to SnappyData fails.    -locators  List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system.  The list must include all locators in use and must be configured consistently for every member of the distributed system.    -log-file  Path of the file to which this member writes log messages. The default is  snappylocator.log  in the working directory. In case logging is set via log4j, the default log file is  snappydata.log .    -member-timeout  Uses the  member-timeout  server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:  1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.  2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure.  Valid values are in the range 1000..600000.    -peer-discovery-address  Use this as value for the port in the \"host:port\" value of \"-locators\" property    -peer-discovery-port  The port on which the locator listens for peer discovery (includes servers as well as other locators).   Valid values are in the range 1-65535, with a default of 10334.    Properties for SSL Encryption  ssl-enabled ,  ssl-ciphers ,  ssl-protocols ,  ssl-require-authentication .      Example : To start two locators on node-a:9999 and node-b:8888, update the configuration file as follows:  $ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999", 
            "title": "List of Locator Properties"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-leads", 
            "text": "Lead Nodes primarily runs the SnappyData managed Spark driver. There is one primary lead node at any given instance, but there can be multiple secondary lead node instances on standby for fault tolerance. Applications can run Jobs using the REST service provided by the Lead node. Most of the SQL queries are automatically routed to the Lead to be planned and executed through a scheduler. You can refer to the  conf/leads.template  file for some examples.   Create the configuration file ( leads ) for leads in the  SnappyData_home /conf  directory.   Note  In the  conf/spark-env.sh  file set the  SPARK_PUBLIC_DNS  property to the public DNS name of the lead node. This enables the Member Logs to be displayed correctly to users accessing SnappyData Pulse from outside the network.", 
            "title": "Configuring Leads"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#list-of-lead-properties", 
            "text": "Refer to the  SnappyData properties  for the complete list of SnappyData properties.     Property  Description      -bind-address  IP address on which the lead is bound. The default behavior is to bind to all local addresses.    -classpath  Location of user classes required by the SnappyData Server. This path is appended to the current classpath.    -critical-heap-percentage  Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100.  If you set  -heap-size , the default value for  critical-heap-percentage  is set to 95% of the heap size.  Use this switch to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.    -dir  The working directory of the lead that contains the SnappyData Lead status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory).    -eviction-heap-percentage  Sets the memory usage percentage threshold (0-100) that the Resource Manager uses to evict data from the heap. By default, the eviction threshold is 85.5% of whatever is set for  -critical-heap-percentage . Use this switch to override the default.    -heap-size   Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings.  For example,  -heap-size=8g    It is recommended to allocate minimum 6-8 GB of heap size per lead node. If you use the  -heap-size  option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the  eviction-heap-percentage  to 85.5% of the  critical-heap-percentage .  SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them.    -J  JVM option passed to the spawned SnappyData Lead JVM.  For example, use -J-Xmx1024m to set the JVM heap to 1GB.    -J-Dsnappydata.enable-rls  Enables the system for row level security when set to true.  By default, this is off. If this property is set to true,  then the Smart Connector access to SnappyData fails.    -J-Dsnappydata.RESTRICT_TABLE_CREATION  Applicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false.    jobserver.waitForInitialization  When this property is set to true, the cluster startup waits for the Spark jobserver to be fully initialized before marking the lead node as  RUNNING . The default is false.    -locators  List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system.  The list must include all locators in use and must be configured consistently for every member of the distributed system.    -log-file  Path of the file to which this member writes log messages. The default  snappyleader.log  in the working directory. In case logging is set via log4j, the default log file is  snappydata.log .    -member-timeout  Uses the  member-timeout  configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:  1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.  2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure.  Valid values are in the range 1000..600000.    -memory-size  Specifies the total memory that can be used by the node for column storage and execution in off-heap. However, lead member do not need off-heap memory.  You can configure the off-heap memory for leads only when you are planning to increase the broadcast limit to a large value. This is generally not recommended and you must preferably limit the broadcast to a smaller value. The default off-heap size for leads is 0.    -snappydata.column.batchSize  The default size of blocks to use for storage in the SnappyData column store. The default value is 24M.    spark.context-settings.num-cpu-cores  The number of cores that can be allocated. The default is 4.    spark.context-settings.memory-per-node  The executor memory per node (-Xmx style. For example: 512m, 1G). The default is 512m.    spark.context-settings.streaming.batch_interval  The batch interval for Spark Streaming contexts in milliseconds. The default is 1000.    spark.context-settings.streaming.stopGracefully  If set to true, the streaming stops gracefully by waiting for the completion of processing of all the received data. The default is true.    spark.context-settings.streaming.stopSparkContext  if set to true, the SparkContext is stopped along with the StreamingContext. The default is true.    -spark.driver.maxResultSize  Limit of the total size of serialized results of all partitions for each action (for example, collect). The value should be at least 1MB or 0 for unlimited. Jobs are aborted if the total size of the results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB    -spark.executor.cores  The number of cores to use on each server.    -spark.jobserver.port  The port on which to run the jobserver. Default port is 8090.    -spark.jobserver.bind-address  The address on which the jobserver listens. Default address is 0.0.0.    -spark.jobserver.job-result-cache-size  The number of job results to keep per JobResultActor/context. The default is 5000.    -spark.jobserver.max-jobs-per-context  The number of jobs that can be run simultaneously in the context. The default is 8.    -spark.local.dir  Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks.    -spark.network.timeout  The default timeout for all network interactions while running queries.    -spark.sql.codegen.cacheSize  Size of the generated code cache that is used by Spark, in the  SnappyData Spark distribution, and by SnappyData. The default is 2000.    -spark.ssl.enabled  Enables or disables Spark layer encryption. The default is false.    -spark.ssl.keyPassword  The password to the private key in the key store.    -spark.ssl.keyStore  Path to the key store file. The path can be absolute or relative to the directory in which the process is started.    -spark.ssl.keyStorePassword  The password used to access the keystore.    -spark.ssl.trustStore  Path to the trust store file. The path can be absolute or relative to the directory in which the process is started.    -spark.ssl.trustStorePassword  The password used to access the truststore.    -spark.ssl.protocol  The protocol that must be supported by JVM. For example, TLS.    -spark.ui.port  Port for your SnappyData Pulse, which shows tables, memory and workload data. The default is 5050.    Properties for SSL Encryption  ssl-enabled ,  ssl-ciphers ,  ssl-protocols ,  ssl-require-authentication .   These properties need not be added to  the Lead members in case of a client-server connection.     Example : To start a lead (node-l), set  spark.executor.cores  as 10 on all servers, and change the Spark UI port from 5050 to 9090, update the configuration file as follows:  $ cat conf/leads\nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10", 
            "title": "List of Lead Properties"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-secondary-lead", 
            "text": "To configure secondary leads, you must add the required number of entries in the  conf/leads  file.   For example:  $ cat conf/leads\nnode-l1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-l2 -heap-size=4096m -locators=node-b:8888,node-a:9999  In this example, two leads (one on node-l1 and another on node-l2) are configured. Using  sbin/snappy-start-all.sh , when you launch the cluster, one of them becomes the primary lead and the other becomes the secondary lead.", 
            "title": "Configuring Secondary Lead"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-data-servers", 
            "text": "Data Servers hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than the Spark engine. Data servers use intelligent query routing to either execute the query directly on the node or to pass it to the lead node for execution by Spark SQL. You can refer to the  conf/servers.template  file for some examples.   Create the configuration file ( servers ) for data servers in the  SnappyData_home /conf  directory.", 
            "title": "Configuring Data Servers"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#list-of-server-properties", 
            "text": "Refer to the  SnappyData properties  for the complete list of SnappyData properties.     Property  Description      -bind-address  IP address on which the server is bound. The default behavior is to bind to all local addresses.    -classpath  Location of user classes required by the SnappyData Server. This path is appended to the current classpath.    -client-port  The port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527.    -critical-heap-percentage  Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100.  If you set  -heap-size , the default value for  critical-heap-percentage  is set to 95% of the heap size.  Use this switch to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.    -critical-off-heap-percentage  Sets the critical threshold for off-heap memory usage in percentage, 0-100.  When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory.    -dir  The working directory of the server that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory).  work  is the default current working directory.    -eviction-heap-percentage  Sets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 85.5% of whatever is set for  -critical-heap-percentage . Use this switch to override the default.    -eviction-off-heap-percentage  Sets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory.  By default, the eviction threshold is 85.5% of the value that is set for  -critical-off-heap-percentage .  Use this switch to override the default.    -heap-size   Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings.  For example, -heap-size=1024m.  If you use the  -heap-size  option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the  eviction-heap-percentage  to 85.5% of the  critical-heap-percentage .  SnappyData also sets resource management properties for eviction and garbage collection if the JVM supports them.    -memory-size  Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in  specific scenarios .    -J  JVM option passed to the spawned SnappyData server JVM.  For example, use  -J-XX:+PrintGCDetails  to print the GC details in JVM logs.    -J-Dgemfirexd.hostname-for-clients  The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection. The default value causes the  client-bind-address  to be given to clients.   This value can be different from  client-bind-address  for cases where the servers/locators are behind a NAT firewall (AWS for example) where  client-bind-address  needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases, this is handled by the hostname translation itself, that is, the hostname used in  client-bind-address  resolves to the internal IP address from inside and to the public IP address from outside, but for other cases, this property is required    -J-Dsnappydata.enable-rls  Enables the system for row level security when set to true.  By default, this is off. If this property is set to true,  then the Smart Connector access to SnappyData fails.    -J-Dsnappydata.RESTRICT_TABLE_CREATION  Applicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false.    -locators  List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system.  The list must include all locators in use and must be configured consistently for every member of the distributed system.    -log-file  Path of the file to which this member writes log messages. The default is  snappyserver.log  in the working directory. In case logging is set via log4j, the default log file is  snappydata.log .    -member-timeout  Uses the  member-timeout  server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:  1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.  2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the are you alive datagram within the period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure.  Valid values are in the range 1000..600000.    -rebalance  Causes the new member to trigger a rebalancing operation for all partitioned tables in the system.  The system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option. Usually rebalancing is triggered when the overall capacity is increased or reduced through member startup, shut down, or failure.    -spark.local.dir  Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks.    Properties for SSL Encryption  ssl-enabled ,  ssl-ciphers ,  ssl-protocols ,  ssl-require-authentication .    -thrift-ssl  Specifies if you want to enable or disable SSL. Values: true or false    -thrift-ssl-properties  Comma-separated SSL properties including: protocol : default \"TLS\", enabled-protocols : enabled protocols separated by \":\" cipher-suites : enabled cipher suites separated by \":\" client-auth =(true or false): if client also needs to be authenticated  keystore : path to key store file  keystore-type : the type of key-store (default \"JKS\")  keystore-password : password for the key store file keymanager-type : the type of key manager factory  truststore : path to trust store file truststore-type : the type of trust-store (default \"JKS\") truststore-password : password for the trust store file  trustmanager-type : the type of trust manager factory      Example : To start a two servers (node-c and node-c), update the configuration file as follows:  $ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -memory-size=16g -locators=node-b:8888,node-a:9999", 
            "title": "List of Server Properties"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#specifying-configuration-properties-using-environment-variables", 
            "text": "SnappyData configuration properties can be specified using environment variables LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, and LEAD_STARTUP_OPTIONS respectively for locators, leads and servers.  These environment variables are useful to specify common properties for locators, servers, and leads.  These startup environment variables can be specified in  conf/spark-env.sh  file. This file is sourced when SnappyData system is started. A template file  conf/spark-env.sh.template  is provided in  conf  directory for reference. You can copy this file and use it to configure properties.   For example:  # create a spark-env.sh from the template file\n$cp conf/spark-env.sh.template conf/spark-env.sh \n\n# Following example configuration can be added to spark-env.sh, \n# it shows how to add security configuration using the environment variables\n\nSECURITY_ARGS= -auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=password123 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=password123 \n\n#applies the configuration specified by SECURITY_ARGS to all locators\nLOCATOR_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\n#applies the configuration specified by SECURITY_ARGS to all servers\nSERVER_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\n#applies the configuration specified by SECURITY_ARGS to all leads\nLEAD_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d", 
            "title": "Specifying Configuration Properties using Environment Variables"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#configuring-snappydata-smart-connector", 
            "text": "Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). In Smart connector mode, a Spark application connects to SnappyData cluster to store and process data. SnappyData currently works with Spark version 2.1.1. To work with SnappyData cluster, a Spark application must set the  snappydata.connection  property while starting.        Property  Description      snappydata.connection  SnappyData cluster's locator host and JDBC client port on which locator listens for connections. Has to be specified while starting a Spark application.     Example :  $ ./bin/spark-submit --deploy-mode cluster --class somePackage.someClass  \n    --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 \n    --packages 'SnappyDataInc:snappydata:1.0.2.1-s_2.11'", 
            "title": "Configuring SnappyData Smart Connector"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#environment-settings", 
            "text": "Any Spark or SnappyData specific environment settings can be done by creating a  snappy-env.sh  or  spark-env.sh  in  SNAPPY_HOME/conf .", 
            "title": "Environment Settings"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#hadoop-provided-settings", 
            "text": "If you want to run SnappyData with an already existing custom Hadoop cluster like MapR or Cloudera you should download Snappy without Hadoop from the download link. This allows you to provide Hadoop at runtime.  To do this, you need to put an entry in $SNAPPY-HOME/conf/spark-env.sh as below:  export SPARK_DIST_CLASSPATH=$($OTHER_HADOOP_HOME/bin/hadoop classpath)", 
            "title": "Hadoop Provided Settings"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#logging", 
            "text": "Currently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property  -log-file  as the path of the directory.  \nThe logging levels can be modified by adding a  conf/log4j.properties  file in the product directory.   $ cat conf/log4j.properties \nlog4j.logger.org.apache.spark.scheduler.DAGScheduler=DEBUG\nlog4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG   Note  For a set of applicable class names and default values see the file  conf/log4j.properties.template , which can be used as a starting point. Consult the  log4j 1.2.x documentation  for more details on the configuration file.", 
            "title": "Logging"
        }, 
        {
            "location": "/configuring_cluster/configuring_cluster/#auto-configuring-off-heap-memory-size", 
            "text": "Off-Heap memory size is auto-configured by default in the following scenarios:    When the lead, locator, and server are setup on different host machines: \n    In this case, off-heap memory size is configured by default for the host machines with the server setup. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. For example, if the RAM is greater than 8GB, the heap memory is between 4-8 GB and the remaining becomes the off-heap memory.    When leads and one of the server node are on the same host: \nIn this case,  off-heap memory size is configured by default and is adjusted based on the number of leads that are present. The total size of heap and off-heap memory does not exceed more than 75% of the total RAM. However, here the heap memory is the total heap size of the server as well as that of the lead.      Note  The off-heap memory size is not auto-configured when the heap memory and the off-heap memory are explicitly configured through properties or when multiple servers are on the same host machine.", 
            "title": "Auto-Configuring Off-Heap Memory Size"
        }, 
        {
            "location": "/configuring_cluster/property_description/", 
            "text": "List of Properties\n\n\nThe following list of commonly used properties can be set to configure the cluster.  These properties can be set in the \nconf/servers\n, \nconf/leads\n or \nconf/locators\n configuration files.\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nComponents\n\n\n\n\n\n\n\n\n\n\n-bind-address\n\n\nIP address on which the member is bound. The default behavior is to bind to all local addresses.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-classpath\n\n\nLocation of user classes required by the SnappyData Server.\nThis path is appended to the current classpath.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-client-port\n\n\nThe port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527.\n\n\nLocator\nServer\n\n\n\n\n\n\n-critical-heap-percentage\n\n\nSets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100. \nIf you set \n-heap-size\n, the default value for \ncritical-heap-percentage\n is set to 95% of the heap size. \nUse this switch to override the default.\nWhen this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.\n\n\nServer\nLead\n\n\n\n\n\n\n-critical-off-heap-percentage\n\n\nSets the critical threshold for off-heap memory usage in percentage, 0-100. \nWhen this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory.\n\n\nServer\n\n\n\n\n\n\n-dir\n\n\nWorking directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory).\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-eviction-heap-percentage\n\n\nSets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 85.5% of whatever is set for \n-critical-heap-percentage\n.\nUse this switch to override the default.\n\n\nServer\nLead\n\n\n\n\n\n\n-eviction-off-heap-percentage\n\n\nSets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory. \nBy default, the eviction threshold is 85.5% of whatever is set for \n-critical-off-heap-percentage\n. \nUse this switch to override the default.\n\n\nServer\n\n\n\n\n\n\n-heap-size\n\n\n Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings. \nFor example, -heap-size=1GB. \nIf you use the \n-heap-size\n option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the \neviction-heap-percentage\n to 85.5% of the \ncritical-heap-percentage\n. \nSnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-J\n\n\nJVM option passed to the spawned SnappyData server JVM. \nFor example, use -J-Xmx1GB to set the JVM heap to 1GB.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-J-Dgemfirexd.hostname-for-clients\n\n\nSet the IP address or host name that this server/locator sends to JDBC/ODBC/thrift clients to use for connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where locators, servers are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases this is handled by hostname translation itself, i.e. hostname used in client-bind-address resolves to internal IP address from inside but to public IP address from outside, but for other cases this property will be required.\n\n\nServer\n\n\n\n\n\n\n-J-Dsnappydata.enable-rls\n\n\nEnables the system for row level security when set to true.  By default this is off. If this property is set to true,  then the Smart Connector access to SnappyData fails.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-J-Dsnappydata.RESTRICT_TABLE_CREATION\n\n\nApplicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\njobserver.waitForInitialization\n\n\nWhen this property is set to true, the cluster startup waits for the Spark jobserver to be fully initialized before marking the lead node as \nRUNNING\n. The default is false.\n\n\nLead\n\n\n\n\n\n\n-locators\n\n\nList of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system. \nThe list must include all locators in use and must be configured consistently for every member of the distributed system. This property should be configured for all the nodes in the respective configuration files, if there are multiple locators.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-log-file\n\n\nPath of the file to which this member writes log messages (default is snappy[member].log in the working directory. For example, \nsnappylocator.log\n, \nsnappyleader.log\n,\nsnappyserver.log\n. In case logging is set via log4j, the default log file is \nsnappydata.log\n for each of the SnappyData member.)\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-memory-size\n\n\nSpecifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in \nspecific scenarios\n.\n\n\nServer\nLead\n\n\n\n\n\n\n-member-timeout\n\n\nUses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:\n 1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.\n 2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the \nare you alive\n datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure. \nValid values are in the range 1000-600000 milliseconds. For more information, refer to \nBest Practices\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-peer-discovery-address\n\n\nUse this as value for the port in the \"host:port\" value of \"-locators\" property\n\n\nLocator\n\n\n\n\n\n\n-peer-discovery-port\n\n\nPort on which the locator listens for peer discovery (includes servers as well as other locators).  \nValid values are in the range 1-65535, with a default of 10334.\n\n\nLocator\n\n\n\n\n\n\n-rebalance\n\n\nTriggers a rebalancing operation for all partitioned tables in the system. \nThe system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option.\n\n\nServer\n\n\n\n\n\n\n-spark.driver.maxResultSize\n\n\nLimit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1MB or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB.\n\n\nLead\n\n\n\n\n\n\n-spark.executor.cores\n\n\nThe number of cores to use on each server.\n\n\nLead\n\n\n\n\n\n\n-spark.context-settings.num-cpu-cores\n\n\nThe number of cores that can be allocated. The default is 4.\n\n\nLead\n\n\n\n\n\n\n-spark.context-settings.memory-per-node\n\n\nThe executor memory per node (-Xmx style. For example: 512m, 1G). The default is 512m.\n\n\nLead\n\n\n\n\n\n\n-spark.context-settings.streaming.batch_interval\n\n\nThe batch interval for Spark Streaming contexts in milliseconds. The default is 1000.\n\n\nLead\n\n\n\n\n\n\n-spark.context-settings.streaming.stopGracefully\n\n\nIf set to true, the streaming stops gracefully by waiting for the completion of processing of all the received data. The default is true.\n\n\nLead\n\n\n\n\n\n\n-spark.context-settings.streaming.stopSparkContext\n\n\nif set to true, the SparkContext is stopped along with the StreamingContext. The default is true.\n\n\nLead\n\n\n\n\n\n\n-spark.jobserver.bind-address\n\n\nThe address on which the jobserver listens. Default address is 0.0.0.\n\n\nLead\n\n\n\n\n\n\n-spark.jobserver.job-result-cache-size\n\n\nThe number of job results to keep per JobResultActor/context. The default is 5000.\n\n\nLead\n\n\n\n\n\n\n-spark.jobserver.max-jobs-per-context\n\n\nThe number of jobs that can be run simultaneously in the context. The default is 8.\n\n\nLead\n\n\n\n\n\n\n-spark.jobserver.port\n\n\nThe port on which to run the jobserver. Default port is 8090.\n\n\nLead\n\n\n\n\n\n\n-spark.local.dir\n\n\nDirectory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. For more information, refer to \nBest Practices\n.\n\n\nLead\nServer\n\n\n\n\n\n\n-spark.network.timeout\n\n\nThe default timeout for all network interactions while running queries.\n\n\nLead\n\n\n\n\n\n\n-spark.sql.codegen.cacheSize\n\n\nSize of the generated code cache. This effectively controls the maximum number of query plans whose generated code (Classes) is cached. The default is 2000.\n\n\nLead\n\n\n\n\n\n\n-spark.ssl.enabled\n\n\nEnables or disables Spark layer encryption. The default is false.\n\n\nLead\n\n\n\n\n\n\n-spark.ssl.keyPassword\n\n\nThe password to the private key in the key store.\n\n\nLead\n\n\n\n\n\n\n-spark.ssl.keyStore\n\n\nPath to the key store file. The path can be absolute or relative to the directory in which the process is started.\n\n\nLead\n\n\n\n\n\n\n-spark.ssl.keyStorePassword\n\n\nThe password used to access the keystore.\n\n\nLead\n\n\n\n\n\n\n-spark.ssl.trustStore\n\n\nPath to the trust store file. The path can be absolute or relative to the directory in which the process is started.\n\n\nLead\n\n\n\n\n\n\n-spark.ssl.trustStorePassword\n\n\nThe password used to access the truststore.\n\n\nLead\n\n\n\n\n\n\n-spark.ssl.protocol\n\n\nThe protocol that must be supported by JVM. For example, TLS.\n\n\nLead\n\n\n\n\n\n\n-spark.ui.port\n\n\nPort for your SnappyData Pulse, which shows tables, memory and workload data. The default is 5050\n\n\nLead\n\n\n\n\n\n\nProperties for SSL Encryption\n\n\nssl-enabled\n, \nssl-ciphers\n, \nssl-protocols\n, \nssl-require-authentication\n. \n These properties need not be added to  the Lead members in case of a client-server connection.\n\n\nServer\nLead\nLocator\n\n\n\n\n\n\n-thrift-ssl\n\n\nSpecifies if you want to enable or disable SSL. Values are true or false.\n\n\n\n\n\n\n\n\n-thrift-ssl-properties\n\n\nComma-separated SSL properties including:\nprotocol\n: default \"TLS\",\nenabled-protocols\n: enabled protocols separated by \":\"\ncipher-suites\n: enabled cipher suites separated by \":\"\nclient-auth\n=(true or false): if client also needs to be authenticated \nkeystore\n: Path to key store file \nkeystore-type\n: The type of key-store (default \"JKS\") \nkeystore-password\n: Password for the key store file\nkeymanager-type\n: The type of key manager factory \ntruststore\n: Path to trust store file\ntruststore-type\n: The type of trust-store (default \"JKS\")\ntruststore-password\n: Password for the trust store file \ntrustmanager-type\n: The type of trust manager factory \n\n\nServer\n\n\n\n\n\n\n\n\nOther than the above properties, you can also refer the \nConfiguration Parameters section\n for properties that are used in special cases.\n\n\n\n\nSQL Properties\n\n\nThese properties can be set using a \nSET SQL\n command or using the configuration properties in the \nconf/leads\n file. The \nSET SQL\n command sets the property for the current SnappySession while setting it in \nconf/leads\n file sets the property for all SnappySession.\n\n\nFor example: Set in the snappy SQL shell\n\n\nsnappy\n connect client 'localhost:1527';\nsnappy\n set snappydata.column.batchSize=100k;\n\n\n\n\nThis sets the property for the snappy SQL shell's session.\n\n\nSet in the \nconf/leads\n file\n\n\n$ cat conf/leads\nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k\n\n\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-snappydata.column.batchSize\n\n\nThe default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes or k/m/g suffixes for unit) that is used to split the data into chunks for efficient storage and retrieval. \n This property can also be set for each table in the \ncreate table\n DDL. Maximum allowed size is 2GB. The default is 24m.\n\n\n\n\n\n\n-snappydata.column.maxDeltaRows\n\n\nThe maximum number of rows that can be in the delta buffer of a column table. The size of the delta buffer is already limited by \nColumnBatchSize\n property, but this allows a lower limit on the number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of \nColumnBatchSize\n and this property is hit first. It can also be set for each table in the \ncreate table\n DDL, else this setting is used for the \ncreate table\n\n\n\n\n\n\n-snappydata.sql.hashJoinSize\n\n\nThe join would be converted into a hash join if the table is of size less than the \nhashJoinSize\n.  The limit specifies an estimate on the input data size (in bytes or k/m/g/t suffixes for unit). The default value is 100MB.\n\n\n\n\n\n\n-snappydata.sql.hashAggregateSize\n\n\nAggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (in bytes or k/m/g/t suffixes for unit) and not the output size. Set this only if there are queries that can return a large number of rows in aggregation results. The default value is set to 0 which means, no limit is set on the size, so the optimized hash aggregation is always used.\n\n\n\n\n\n\n-snappydata.sql.planCacheSize\n\n\nNumber of query plans that will be cached.\n\n\n\n\n\n\n-spark.sql.autoBroadcastJoinThreshold\n\n\nConfigures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join.  By setting this value to \n-1\n broadcasting can be disabled.\n\n\n\n\n\n\n-snappydata.linkPartitionsToBuckets\n\n\nWhen this property is set to true, each bucket is always treated as a separate partition in column/row table scans. When this is set to false, SnappyData creates only as many partitions as executor cores by clubbing multiple buckets into each partition when possible. The default is false.\n\n\n\n\n\n\n-snappydata.preferPrimaries\n\n\nUse this property to configure your preference to use primary buckets in queries. This reduces the scalability of queries in the interest of reduced memory usage for secondary buckets. The default is false.\n\n\n\n\n\n\n-snappydata.sql.partitionPruning\n\n\nUse this property to set/unset the partition pruning of queries.\n\n\n\n\n\n\n-snappydata.sql.tokenize\n\n\nUse this property to enable/disable tokenization.\n\n\n\n\n\n\nsnappydata.cache.putIntoInnerJoinResultSize\n\n\nUse this property with extreme limits such as 1K and 10GB. The default is 100 MB.\n\n\n\n\n\n\n-snappydata.scheduler.pool\n\n\nUse this property to define scheduler pool to either default or low latency. You can also assign queries to different pools.\n\n\n\n\n\n\n-snappydata.enable-experimental-features\n\n\nUse this property to enable and disable experimental features. You can call out in case some features are completely broken and need to be removed from the product.\n\n\n\n\n\n\n\n\n\n\nSDE Properties\n\n\nThe \nSDE\n properties can be set using a Snappy SQL shell (snappy-sql) command or using the configuration properties in the \nconf/leads\n file. \n\nThe command sets the property for the current SnappySession while setting it in \nconf/leads\n file sets the property for all SnappySession. \n\n\nFor example: Set in the  Snappy SQL shell (snappy-sql)\n\n\nsnappy\n connect client 'localhost:1527';\nsnappy\n set snappydata.flushReservoirThreshold=20000;\n\n\n\n\nSet in the \nconf/leads\n file\n\n\n$ cat conf/leads\nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k -spark.sql.aqp.error=0.5\n\n\n\n\nThis sets the property for the snappy SQL shell's session.\n\n\n\n\n\n\n\n\nProperties\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-snappydata.flushReservoirThreshold\n\n\nReservoirs of sample table will be flushed and stored in columnar format if sampling is done on the base table of size more than flushReservoirThreshold. The default value is10,000.\n This property must be set in the \nconf/servers\n and \nconf/leads\n file.\n\n\n\n\n\n\n-spark.sql.aqp.numBootStrapTrials\n\n\nNumber of bootstrap trials to do for calculating error bounds. The default value is100. \nThis property must be set in the \nconf/leads\n file.\n\n\n\n\n\n\n-spark.sql.aqp.error\n\n\nMaximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2. \nThis property can be set as connection property in the Snappy SQL shell.\n\n\n\n\n\n\n-spark.sql.aqp.confidence\n\n\nConfidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1. \n The default value is0.95. \nThis property can be set as connection property in the Snappy SQL shell.\n\n\n\n\n\n\n-sparksql.aqp.behavior\n\n\nThe action to be taken if the error computed goes outside the error tolerance limit. The default value is\nDO_NOTHING\n. \nThis property can be set as connection property in the Snappy SQL shell.", 
            "title": "List of Properties"
        }, 
        {
            "location": "/configuring_cluster/property_description/#list-of-properties", 
            "text": "The following list of commonly used properties can be set to configure the cluster.  These properties can be set in the  conf/servers ,  conf/leads  or  conf/locators  configuration files.     Property  Description  Components      -bind-address  IP address on which the member is bound. The default behavior is to bind to all local addresses.  Server Lead Locator    -classpath  Location of user classes required by the SnappyData Server. This path is appended to the current classpath.  Server Lead Locator    -client-port  The port that the network controller listens for client connections in the range of 1 to 65535. The default value is 1527.  Locator Server    -critical-heap-percentage  Sets the Resource Manager's critical heap threshold in percentage of the old generation heap, 0-100.  If you set  -heap-size , the default value for  critical-heap-percentage  is set to 95% of the heap size.  Use this switch to override the default. When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of memory.  Server Lead    -critical-off-heap-percentage  Sets the critical threshold for off-heap memory usage in percentage, 0-100.  When this limit is breached, the system starts canceling memory-intensive queries, throws low memory exceptions for new SQL statements, and so forth, to avoid running out of off-heap memory.  Server    -dir  Working directory of the member that contains the SnappyData Server status file and the default location for the log file, persistent files, data dictionary, and so forth (defaults to the current directory).  Server Lead Locator    -eviction-heap-percentage  Sets the memory usage percentage threshold (0-100) that the Resource Manager will use to start evicting data from the heap. By default, the eviction threshold is 85.5% of whatever is set for  -critical-heap-percentage . Use this switch to override the default.  Server Lead    -eviction-off-heap-percentage  Sets the off-heap memory usage percentage threshold, 0-100, that the Resource Manager uses to start evicting data from off-heap memory.  By default, the eviction threshold is 85.5% of whatever is set for  -critical-off-heap-percentage .  Use this switch to override the default.  Server    -heap-size   Sets the maximum heap size for the Java VM, using SnappyData default resource manager settings.  For example, -heap-size=1GB.  If you use the  -heap-size  option, by default SnappyData sets the critical-heap-percentage to 95% of the heap size, and the  eviction-heap-percentage  to 85.5% of the  critical-heap-percentage .  SnappyData also sets resource management properties for eviction and garbage collection if they are supported by the JVM.  Server Lead Locator    -J  JVM option passed to the spawned SnappyData server JVM.  For example, use -J-Xmx1GB to set the JVM heap to 1GB.  Server Lead Locator    -J-Dgemfirexd.hostname-for-clients  Set the IP address or host name that this server/locator sends to JDBC/ODBC/thrift clients to use for connection. The default value causes the client-bind-address to be given to clients. This value can be different from client-bind-address for cases where locators, servers are behind a NAT firewall (AWS for example) where client-bind-address needs to be a private one that gets exposed to clients outside the firewall as a different public address specified by this property. In many cases this is handled by hostname translation itself, i.e. hostname used in client-bind-address resolves to internal IP address from inside but to public IP address from outside, but for other cases this property will be required.  Server    -J-Dsnappydata.enable-rls  Enables the system for row level security when set to true.  By default this is off. If this property is set to true,  then the Smart Connector access to SnappyData fails.  Server Lead Locator    -J-Dsnappydata.RESTRICT_TABLE_CREATION  Applicable when security is enabled in the cluster. If true, users cannot execute queries (including DDLs and DMLs) even in their default or own schema unless cluster admin explicitly grants them the required permissions using GRANT command. The default is false.  Server Lead Locator    jobserver.waitForInitialization  When this property is set to true, the cluster startup waits for the Spark jobserver to be fully initialized before marking the lead node as  RUNNING . The default is false.  Lead    -locators  List of locators as comma-separated host:port values used to communicate with running locators in the system and thus discover other peers of the distributed system.  The list must include all locators in use and must be configured consistently for every member of the distributed system. This property should be configured for all the nodes in the respective configuration files, if there are multiple locators.  Server Lead Locator    -log-file  Path of the file to which this member writes log messages (default is snappy[member].log in the working directory. For example,  snappylocator.log ,  snappyleader.log , snappyserver.log . In case logging is set via log4j, the default log file is  snappydata.log  for each of the SnappyData member.)  Server Lead Locator    -memory-size  Specifies the total memory that can be used by the node for column storage and execution in off-heap. The default value is either 0 or it gets auto-configured in  specific scenarios .  Server Lead    -member-timeout  Uses the member-timeout server configuration, specified in milliseconds, to detect the abnormal termination of members. The configuration setting is used in two ways:  1) First, it is used during the UDP heartbeat detection process. When a member detects that a heartbeat datagram is missing from the member that it is monitoring after the time interval of 2 * the value of member-timeout, the detecting member attempts to form a TCP/IP stream-socket connection with the monitored member as described in the next case.  2) The property is then used again during the TCP/IP stream-socket connection. If the suspected process does not respond to the  are you alive  datagram within the time period specified in member-timeout, the membership coordinator sends out a new membership view that notes the member's failure.  Valid values are in the range 1000-600000 milliseconds. For more information, refer to  Best Practices  Server Lead Locator    -peer-discovery-address  Use this as value for the port in the \"host:port\" value of \"-locators\" property  Locator    -peer-discovery-port  Port on which the locator listens for peer discovery (includes servers as well as other locators).   Valid values are in the range 1-65535, with a default of 10334.  Locator    -rebalance  Triggers a rebalancing operation for all partitioned tables in the system.  The system always tries to satisfy the redundancy of all partitioned tables on new member startup regardless of this option.  Server    -spark.driver.maxResultSize  Limit of the total size of serialized results of all partitions for each action (e.g. collect). The value should be at least 1MB or 0 for unlimited. Jobs will be aborted if the total size of results is above this limit. Having a high limit may cause out-of-memory errors in the lead. The default max size is 1GB.  Lead    -spark.executor.cores  The number of cores to use on each server.  Lead    -spark.context-settings.num-cpu-cores  The number of cores that can be allocated. The default is 4.  Lead    -spark.context-settings.memory-per-node  The executor memory per node (-Xmx style. For example: 512m, 1G). The default is 512m.  Lead    -spark.context-settings.streaming.batch_interval  The batch interval for Spark Streaming contexts in milliseconds. The default is 1000.  Lead    -spark.context-settings.streaming.stopGracefully  If set to true, the streaming stops gracefully by waiting for the completion of processing of all the received data. The default is true.  Lead    -spark.context-settings.streaming.stopSparkContext  if set to true, the SparkContext is stopped along with the StreamingContext. The default is true.  Lead    -spark.jobserver.bind-address  The address on which the jobserver listens. Default address is 0.0.0.  Lead    -spark.jobserver.job-result-cache-size  The number of job results to keep per JobResultActor/context. The default is 5000.  Lead    -spark.jobserver.max-jobs-per-context  The number of jobs that can be run simultaneously in the context. The default is 8.  Lead    -spark.jobserver.port  The port on which to run the jobserver. Default port is 8090.  Lead    -spark.local.dir  Directory to use for \"scratch\" space in SnappyData, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. For more information, refer to  Best Practices .  Lead Server    -spark.network.timeout  The default timeout for all network interactions while running queries.  Lead    -spark.sql.codegen.cacheSize  Size of the generated code cache. This effectively controls the maximum number of query plans whose generated code (Classes) is cached. The default is 2000.  Lead    -spark.ssl.enabled  Enables or disables Spark layer encryption. The default is false.  Lead    -spark.ssl.keyPassword  The password to the private key in the key store.  Lead    -spark.ssl.keyStore  Path to the key store file. The path can be absolute or relative to the directory in which the process is started.  Lead    -spark.ssl.keyStorePassword  The password used to access the keystore.  Lead    -spark.ssl.trustStore  Path to the trust store file. The path can be absolute or relative to the directory in which the process is started.  Lead    -spark.ssl.trustStorePassword  The password used to access the truststore.  Lead    -spark.ssl.protocol  The protocol that must be supported by JVM. For example, TLS.  Lead    -spark.ui.port  Port for your SnappyData Pulse, which shows tables, memory and workload data. The default is 5050  Lead    Properties for SSL Encryption  ssl-enabled ,  ssl-ciphers ,  ssl-protocols ,  ssl-require-authentication .   These properties need not be added to  the Lead members in case of a client-server connection.  Server Lead Locator    -thrift-ssl  Specifies if you want to enable or disable SSL. Values are true or false.     -thrift-ssl-properties  Comma-separated SSL properties including: protocol : default \"TLS\", enabled-protocols : enabled protocols separated by \":\" cipher-suites : enabled cipher suites separated by \":\" client-auth =(true or false): if client also needs to be authenticated  keystore : Path to key store file  keystore-type : The type of key-store (default \"JKS\")  keystore-password : Password for the key store file keymanager-type : The type of key manager factory  truststore : Path to trust store file truststore-type : The type of trust-store (default \"JKS\") truststore-password : Password for the trust store file  trustmanager-type : The type of trust manager factory   Server     Other than the above properties, you can also refer the  Configuration Parameters section  for properties that are used in special cases.", 
            "title": "List of Properties"
        }, 
        {
            "location": "/configuring_cluster/property_description/#sql-properties", 
            "text": "These properties can be set using a  SET SQL  command or using the configuration properties in the  conf/leads  file. The  SET SQL  command sets the property for the current SnappySession while setting it in  conf/leads  file sets the property for all SnappySession.  For example: Set in the snappy SQL shell  snappy  connect client 'localhost:1527';\nsnappy  set snappydata.column.batchSize=100k;  This sets the property for the snappy SQL shell's session.  Set in the  conf/leads  file  $ cat conf/leads\nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k     Property  Description      -snappydata.column.batchSize  The default size of blocks to use for storage in SnappyData column and store. When inserting data into the column storage this is the unit (in bytes or k/m/g suffixes for unit) that is used to split the data into chunks for efficient storage and retrieval.   This property can also be set for each table in the  create table  DDL. Maximum allowed size is 2GB. The default is 24m.    -snappydata.column.maxDeltaRows  The maximum number of rows that can be in the delta buffer of a column table. The size of the delta buffer is already limited by  ColumnBatchSize  property, but this allows a lower limit on the number of rows for better scan performance. So the delta buffer is rolled into the column store whichever of  ColumnBatchSize  and this property is hit first. It can also be set for each table in the  create table  DDL, else this setting is used for the  create table    -snappydata.sql.hashJoinSize  The join would be converted into a hash join if the table is of size less than the  hashJoinSize .  The limit specifies an estimate on the input data size (in bytes or k/m/g/t suffixes for unit). The default value is 100MB.    -snappydata.sql.hashAggregateSize  Aggregation uses optimized hash aggregation plan but one that does not overflow to disk and can cause OOME if the result of aggregation is large. The limit specifies the input data size (in bytes or k/m/g/t suffixes for unit) and not the output size. Set this only if there are queries that can return a large number of rows in aggregation results. The default value is set to 0 which means, no limit is set on the size, so the optimized hash aggregation is always used.    -snappydata.sql.planCacheSize  Number of query plans that will be cached.    -spark.sql.autoBroadcastJoinThreshold  Configures the maximum size in bytes for a table that is broadcast to all server nodes when performing a join.  By setting this value to  -1  broadcasting can be disabled.    -snappydata.linkPartitionsToBuckets  When this property is set to true, each bucket is always treated as a separate partition in column/row table scans. When this is set to false, SnappyData creates only as many partitions as executor cores by clubbing multiple buckets into each partition when possible. The default is false.    -snappydata.preferPrimaries  Use this property to configure your preference to use primary buckets in queries. This reduces the scalability of queries in the interest of reduced memory usage for secondary buckets. The default is false.    -snappydata.sql.partitionPruning  Use this property to set/unset the partition pruning of queries.    -snappydata.sql.tokenize  Use this property to enable/disable tokenization.    snappydata.cache.putIntoInnerJoinResultSize  Use this property with extreme limits such as 1K and 10GB. The default is 100 MB.    -snappydata.scheduler.pool  Use this property to define scheduler pool to either default or low latency. You can also assign queries to different pools.    -snappydata.enable-experimental-features  Use this property to enable and disable experimental features. You can call out in case some features are completely broken and need to be removed from the product.", 
            "title": "SQL Properties"
        }, 
        {
            "location": "/configuring_cluster/property_description/#sde-properties", 
            "text": "The  SDE  properties can be set using a Snappy SQL shell (snappy-sql) command or using the configuration properties in the  conf/leads  file.  \nThe command sets the property for the current SnappySession while setting it in  conf/leads  file sets the property for all SnappySession.   For example: Set in the  Snappy SQL shell (snappy-sql)  snappy  connect client 'localhost:1527';\nsnappy  set snappydata.flushReservoirThreshold=20000;  Set in the  conf/leads  file  $ cat conf/leads\nnode-l -heap-size=4096m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10 -snappydata.column.batchSize=100k -spark.sql.aqp.error=0.5  This sets the property for the snappy SQL shell's session.     Properties  Description      -snappydata.flushReservoirThreshold  Reservoirs of sample table will be flushed and stored in columnar format if sampling is done on the base table of size more than flushReservoirThreshold. The default value is10,000.  This property must be set in the  conf/servers  and  conf/leads  file.    -spark.sql.aqp.numBootStrapTrials  Number of bootstrap trials to do for calculating error bounds. The default value is100.  This property must be set in the  conf/leads  file.    -spark.sql.aqp.error  Maximum relative error tolerable in the approximate value calculation. It should be a fractional value not exceeding 1. The default value is0.2.  This property can be set as connection property in the Snappy SQL shell.    -spark.sql.aqp.confidence  Confidence with which the error bounds are calculated for the approximate value. It should be a fractional value not exceeding 1.   The default value is0.95.  This property can be set as connection property in the Snappy SQL shell.    -sparksql.aqp.behavior  The action to be taken if the error computed goes outside the error tolerance limit. The default value is DO_NOTHING .  This property can be set as connection property in the Snappy SQL shell.", 
            "title": "SDE Properties"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/", 
            "text": "Firewalls and Connections\n\n\nYou may face possible connection problems that can result from running a firewall on your machine.\n\n\nSnappyData is a network-centric distributed system, so if you have a firewall running on your machine it could cause connection problems. For example, your connections may fail if your firewall places restrictions on inbound or outbound permissions for Java-based sockets. You may need to modify your firewall configuration to permit traffic to Java applications running on your machine. The specific configuration depends on the firewall you are using.\n\n\nAs one example, firewalls may close connections to SnappyData due to timeout settings. If a firewall senses no activity in a certain time period, it may close a connection and open a new connection when activity resumes, which can cause some confusion about which connections you have.\n\n\nFirewall and Port Considerations\n\n\nYou can configure and limit port usage for situations that involve firewalls, for example, between client-server or server-server connections.\n\n\n\nMake sure your port settings are configured correctly for firewalls. For each SnappyData member, there are two different port settings you may need to be concerned with regarding firewalls:\n\n\n\n\n\n\nThe port that the server or locator listens on for client connections. This is configurable using the \n-client-port\n option to the snappy server or snappy locator command.\n\n\n\n\n\n\nThe peer discovery port. SnappyData members connect to the locator for peer-to-peer messaging. The locator port is configurable using the \n-peer-discovery-port\n option to the snappy server or snappy locator command.\n\n\nBy default, SnappyData servers and locators discover each other on a pre-defined port (10334) on the localhost.\n\n\n\n\n\n\nLimiting Ephemeral Ports for Peer-to-Peer Membership\n\n\nBy default, SnappyData utilizes \nephemeral\n ports for UDP messaging and TCP failure detection. Ephemeral ports are temporary ports assigned from a designated range, which can encompass a large number of possible ports. When a firewall is present, the ephemeral port range usually must be limited to a much smaller number, for example six. If you are configuring P2P communications through a firewall, you must also set each the tcp port for each process and ensure that UDP traffic is allowed through the firewall.\n\n\nProperties for Firewall and Port Configuration\n\n\nStore Layer\n\n\nThis following tables contain properties potentially involved in firewall behavior, with a brief description of each property. The \nConfiguration Properties\n section contains detailed information for each property.\n\n\n\n\n\n\n\n\nConfiguration Area\n\n\nProperty or Setting\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\npeer-to-peer config\n\n\nlocators\n\n\nThe list of locators used by system members. The list must be configured consistently for every member of the distributed system.\n\n\n\n\n\n\npeer-to-peer config\n\n\nmembership-port-range\n\n\nThe range of ephemeral ports available for unicast UDP messaging and for TCP failure detection in the peer-to-peer distributed system.\n\n\n\n\n\n\nmember config\n\n\n-J-Dgemfirexd.hostname-for-clients\n\n\nThe IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection.\n\n\n\n\n\n\nmember config\n\n\nclient-port\n\u00a0option to the\u00a0\nsnappy server\n\u00a0and\u00a0\nsnappy locator\n\u00a0commands\n\n\nPort that the member listens on for client communication.\n\n\n\n\n\n\nLocator\n\n\nlocator\u00a0command\n\n\n10334\n\n\n\n\n\n\n\n\nSpark Layer\n\n\nThe following table lists the Spark properties you can set to configure the ports required for Spark infrastructure.\nRefer to \nSpark Configuration\n in the official documentation for detailed information.\n\n\n\n\n\n\n\n\nProperty\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nspark.blockManager.port\n\n\nrandom\n\n\nPort for all block managers to listen on. These exist on both the driver and the executors.\n\n\n\n\n\n\nspark.driver.blockManager.port\n\n\n(value of spark.blockManager.port)\n\n\nDriver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors.\n\n\n\n\n\n\nspark.driver.port\n\n\nrandom\n\n\nPort for the driver to listen on. This is used for communicating with the executors and the standalone Master.\n\n\n\n\n\n\nspark.port.maxRetries\n\n\n16\n\n\nMaximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries.\n\n\n\n\n\n\nspark.shuffle.service.port\n\n\n7337\n\n\nPort on which the external shuffle service will run.\n\n\n\n\n\n\nspark.ui.port\n\n\n4040\n\n\nPort for your application's dashboard, which shows memory and workload data.\n\n\n\n\n\n\nspark.ssl.[namespace].port\n\n\nNone\n\n\nThe port where the SSL service will listen on.\nThe port must be defined within a namespace configuration; see SSL Configuration for the available namespaces.\n When not set, the SSL port will be derived from the non-SSL port for the same service. A value of \"0\" will make the service bind to an ephemeral port.\n\n\n\n\n\n\nspark.history.ui.port\n\n\nThe port to which the web interface of the history server binds.\n\n\n18080\n\n\n\n\n\n\nSPARK_MASTER_PORT\n\n\nStart the master on a different port.\n\n\nDefault: 7077\n\n\n\n\n\n\nSPARK_WORKER_PORT\n\n\nStart the Spark worker on a specific port.\n\n\n(Default: random\n\n\n\n\n\n\n\n\nLocators and Ports\n\n\nThe ephemeral port range and TCP port range for locators must be accessible to members through the firewall.\n\n\nLocators are used in the peer-to-peer cache to discover other processes. They can be used by clients to locate servers as an alternative to configuring clients with a collection of server addresses and ports.\n\n\nLocators have a TCP/IP port that all members must be able to connect to. They also start a distributed system and so need to have their ephemeral port range and TCP port accessible to other members through the firewall.\n\n\nClients need only be able to connect to the locator's locator port. They don't interact with the locator's distributed system; clients get server names and ports from the locator and use these to connect to the servers. For more information, see \nUsing Locators\n.", 
            "title": "Firewalls and Connections"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/#firewalls-and-connections", 
            "text": "You may face possible connection problems that can result from running a firewall on your machine.  SnappyData is a network-centric distributed system, so if you have a firewall running on your machine it could cause connection problems. For example, your connections may fail if your firewall places restrictions on inbound or outbound permissions for Java-based sockets. You may need to modify your firewall configuration to permit traffic to Java applications running on your machine. The specific configuration depends on the firewall you are using.  As one example, firewalls may close connections to SnappyData due to timeout settings. If a firewall senses no activity in a certain time period, it may close a connection and open a new connection when activity resumes, which can cause some confusion about which connections you have.", 
            "title": "Firewalls and Connections"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/#firewall-and-port-considerations", 
            "text": "You can configure and limit port usage for situations that involve firewalls, for example, between client-server or server-server connections.  \nMake sure your port settings are configured correctly for firewalls. For each SnappyData member, there are two different port settings you may need to be concerned with regarding firewalls:    The port that the server or locator listens on for client connections. This is configurable using the  -client-port  option to the snappy server or snappy locator command.    The peer discovery port. SnappyData members connect to the locator for peer-to-peer messaging. The locator port is configurable using the  -peer-discovery-port  option to the snappy server or snappy locator command.  By default, SnappyData servers and locators discover each other on a pre-defined port (10334) on the localhost.", 
            "title": "Firewall and Port Considerations"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/#limiting-ephemeral-ports-for-peer-to-peer-membership", 
            "text": "By default, SnappyData utilizes  ephemeral  ports for UDP messaging and TCP failure detection. Ephemeral ports are temporary ports assigned from a designated range, which can encompass a large number of possible ports. When a firewall is present, the ephemeral port range usually must be limited to a much smaller number, for example six. If you are configuring P2P communications through a firewall, you must also set each the tcp port for each process and ensure that UDP traffic is allowed through the firewall.", 
            "title": "Limiting Ephemeral Ports for Peer-to-Peer Membership"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/#properties-for-firewall-and-port-configuration", 
            "text": "", 
            "title": "Properties for Firewall and Port Configuration"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/#store-layer", 
            "text": "This following tables contain properties potentially involved in firewall behavior, with a brief description of each property. The  Configuration Properties  section contains detailed information for each property.     Configuration Area  Property or Setting  Definition      peer-to-peer config  locators  The list of locators used by system members. The list must be configured consistently for every member of the distributed system.    peer-to-peer config  membership-port-range  The range of ephemeral ports available for unicast UDP messaging and for TCP failure detection in the peer-to-peer distributed system.    member config  -J-Dgemfirexd.hostname-for-clients  The IP address or host name that this server/locator sends to the JDBC/ODBC/thrift clients to use for the connection.    member config  client-port \u00a0option to the\u00a0 snappy server \u00a0and\u00a0 snappy locator \u00a0commands  Port that the member listens on for client communication.    Locator  locator\u00a0command  10334", 
            "title": "Store Layer"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/#spark-layer", 
            "text": "The following table lists the Spark properties you can set to configure the ports required for Spark infrastructure. Refer to  Spark Configuration  in the official documentation for detailed information.     Property  Default  Description      spark.blockManager.port  random  Port for all block managers to listen on. These exist on both the driver and the executors.    spark.driver.blockManager.port  (value of spark.blockManager.port)  Driver-specific port for the block manager to listen on, for cases where it cannot use the same configuration as executors.    spark.driver.port  random  Port for the driver to listen on. This is used for communicating with the executors and the standalone Master.    spark.port.maxRetries  16  Maximum number of retries when binding to a port before giving up. When a port is given a specific value (non 0), each subsequent retry will increment the port used in the previous attempt by 1 before retrying. This essentially allows it to try a range of ports from the start port specified to port + maxRetries.    spark.shuffle.service.port  7337  Port on which the external shuffle service will run.    spark.ui.port  4040  Port for your application's dashboard, which shows memory and workload data.    spark.ssl.[namespace].port  None  The port where the SSL service will listen on. The port must be defined within a namespace configuration; see SSL Configuration for the available namespaces.  When not set, the SSL port will be derived from the non-SSL port for the same service. A value of \"0\" will make the service bind to an ephemeral port.    spark.history.ui.port  The port to which the web interface of the history server binds.  18080    SPARK_MASTER_PORT  Start the master on a different port.  Default: 7077    SPARK_WORKER_PORT  Start the Spark worker on a specific port.  (Default: random", 
            "title": "Spark Layer"
        }, 
        {
            "location": "/configuring_cluster/firewalls_connections/#locators-and-ports", 
            "text": "The ephemeral port range and TCP port range for locators must be accessible to members through the firewall.  Locators are used in the peer-to-peer cache to discover other processes. They can be used by clients to locate servers as an alternative to configuring clients with a collection of server addresses and ports.  Locators have a TCP/IP port that all members must be able to connect to. They also start a distributed system and so need to have their ephemeral port range and TCP port accessible to other members through the firewall.  Clients need only be able to connect to the locator's locator port. They don't interact with the locator's distributed system; clients get server names and ports from the locator and use these to connect to the servers. For more information, see  Using Locators .", 
            "title": "Locators and Ports"
        }, 
        {
            "location": "/programming_guide/", 
            "text": "Programming Guide\n\n\nSnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). \nAll SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames.\n\nIt is therefore recommended that you understand the \nconcepts in SparkSQL\n \nand the \nDataFrame API\n. You can also store and manage arbitrary RDDs (or even Spark DataSets) through the implicit or explicit transformation to a DataFrame. While the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced \nhere\n.\n\n\nIn Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters.\nData in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but it can also be reliably managed on disk.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nSparkSession, SnappySession and SnappyStreamingContext\n\n\n\n\n\n\nSnappyData Jobs\n\n\n\n\n\n\nManaging JAR Files\n\n\n\n\n\n\nUsing SnappyData Shell\n\n\n\n\n\n\nUsing the Spark Shell and spark-submit\n\n\n\n\n\n\nWorking with Hadoop YARN Cluster Manager\n\n\n\n\n\n\nUsing JDBC with SnappyData\n\n\n\n\n\n\nMultiple Language Binding using Thrift Protocol\n\n\n\n\n\n\nBuilding SnappyData Applications using Spark API\n\n\n\n\n\n\nTables in SnappyData\n\n\n\n\n\n\nStream Processing using SQL\n\n\n\n\n\n\nUser Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)", 
            "title": "Programming Guide"
        }, 
        {
            "location": "/programming_guide/#programming-guide", 
            "text": "SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). \nAll SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames. \nIt is therefore recommended that you understand the  concepts in SparkSQL  \nand the  DataFrame API . You can also store and manage arbitrary RDDs (or even Spark DataSets) through the implicit or explicit transformation to a DataFrame. While the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced  here .  In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters.\nData in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but it can also be reliably managed on disk.  The following topics are covered in this section:    SparkSession, SnappySession and SnappyStreamingContext    SnappyData Jobs    Managing JAR Files    Using SnappyData Shell    Using the Spark Shell and spark-submit    Working with Hadoop YARN Cluster Manager    Using JDBC with SnappyData    Multiple Language Binding using Thrift Protocol    Building SnappyData Applications using Spark API    Tables in SnappyData    Stream Processing using SQL    User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)", 
            "title": "Programming Guide"
        }, 
        {
            "location": "/programming_guide/sparksession_snappysession_and_snappystreamingcontext/", 
            "text": "SparkSession, SnappySession and SnappyStreamingContext\n\n\nCreate a SparkSession\n\n\nSpark Context\n is the main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n\n\nSpark Session\n is the entry point to programming Spark with the Dataset and DataFrame API.\nSparkSession object can be created by using SparkSession.Builder used as below.\n\n\nSparkSession.builder()\n     .master(\nlocal\n)\n     .appName(\nWord Count\n)\n     .config(\nspark.some.config.option\n, \nsome-value\n)\n     .getOrCreate()\n\n\n\n\nIn environments where SparkSession has been created up front (e.g. REPL, notebooks), use the builder to get an existing session:\n\n\nSparkSession.builder().getOrCreate()\n\n\n\n\nCreate a SnappySession\n\n\nSnappySession\n is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's \nSparkSession\n to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame.\n\n\nTo create a SnappySession:\n\n\nScala\n\n\n val spark: SparkSession = SparkSession\n         .builder\n         .appName(\nSparkApp\n)\n         .master(\nmaster_url\n)\n         .getOrCreate\n\n val snappy = new SnappySession(spark.sparkContext)\n\n\n\n\nJava\n\n\n SparkSession spark = SparkSession\n       .builder()\n       .appName(\nSparkApp\n)\n       .master(\nmaster_url\n)\n       .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n SnappySession snappy = new SnappySession(spark.sparkContext());\n\n\n\n\nPython\n\n\n from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)\n\n\n\n\nCreate a SnappyStreamingContext\n\n\nSnappyStreamingContext\n is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's\n\nStreaming Context\n.\n\n\nTo create a SnappyStreamingContext:\n\n\nScala\n\n\n val spark: SparkSession = SparkSession\n         .builder\n         .appName(\nSparkApp\n)\n         .master(\nmaster_url\n)\n         .getOrCreate\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n\n\n\n\nJava\n\n\n SparkSession spark = SparkSession\n     .builder()\n     .appName(\nSparkApp\n)\n     .master(\nmaster_url\n)\n     .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n Duration batchDuration = Milliseconds.apply(500);\n JavaSnappyStreamingContext jsnsc = new JavaSnappyStreamingContext(jsc, batchDuration);\n\n\n\n\nPython\n\n\n from pyspark.streaming.snappy.context import SnappyStreamingContext\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n duration = .5\n snsc = SnappyStreamingContext(sc, duration)\n\n\n\n\nAlso, SnappyData can be run in three different modes, Local Mode, Embedded Mode and SnappyData Connector mode. Before proceeding, it is important that you understand these modes. For more information, see \nAffinity modes\n.\n\n\nIf you are using SnappyData in LocalMode or Connector mode, it is the responsibility of the user to create a SnappySession.\nIf you are in the Embedded Mode, applications typically submit jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext.\nJobs are the primary mechanism to interact with SnappyData using the Spark API in embedded mode.\nA job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.", 
            "title": "SparkSession, SnappySession and SnappyStreamingContext"
        }, 
        {
            "location": "/programming_guide/sparksession_snappysession_and_snappystreamingcontext/#sparksession-snappysession-and-snappystreamingcontext", 
            "text": "", 
            "title": "SparkSession, SnappySession and SnappyStreamingContext"
        }, 
        {
            "location": "/programming_guide/sparksession_snappysession_and_snappystreamingcontext/#create-a-sparksession", 
            "text": "Spark Context  is the main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster and can be used to create RDDs, accumulators and broadcast variables on that cluster.  Spark Session  is the entry point to programming Spark with the Dataset and DataFrame API.\nSparkSession object can be created by using SparkSession.Builder used as below.  SparkSession.builder()\n     .master( local )\n     .appName( Word Count )\n     .config( spark.some.config.option ,  some-value )\n     .getOrCreate()  In environments where SparkSession has been created up front (e.g. REPL, notebooks), use the builder to get an existing session:  SparkSession.builder().getOrCreate()", 
            "title": "Create a SparkSession"
        }, 
        {
            "location": "/programming_guide/sparksession_snappysession_and_snappystreamingcontext/#create-a-snappysession", 
            "text": "SnappySession  is the main entry point for SnappyData extensions to Spark. A SnappySession extends Spark's  SparkSession  to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame.  To create a SnappySession:  Scala   val spark: SparkSession = SparkSession\n         .builder\n         .appName( SparkApp )\n         .master( master_url )\n         .getOrCreate\n\n val snappy = new SnappySession(spark.sparkContext)  Java   SparkSession spark = SparkSession\n       .builder()\n       .appName( SparkApp )\n       .master( master_url )\n       .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n SnappySession snappy = new SnappySession(spark.sparkContext());  Python   from pyspark.sql.snappy import SnappySession\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n snappy = SnappySession(sc)", 
            "title": "Create a SnappySession"
        }, 
        {
            "location": "/programming_guide/sparksession_snappysession_and_snappystreamingcontext/#create-a-snappystreamingcontext", 
            "text": "SnappyStreamingContext  is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's Streaming Context .  To create a SnappyStreamingContext:  Scala   val spark: SparkSession = SparkSession\n         .builder\n         .appName( SparkApp )\n         .master( master_url )\n         .getOrCreate\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))  Java   SparkSession spark = SparkSession\n     .builder()\n     .appName( SparkApp )\n     .master( master_url )\n     .getOrCreate();\n\n JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n Duration batchDuration = Milliseconds.apply(500);\n JavaSnappyStreamingContext jsnsc = new JavaSnappyStreamingContext(jsc, batchDuration);  Python   from pyspark.streaming.snappy.context import SnappyStreamingContext\n from pyspark import SparkContext, SparkConf\n\n conf = SparkConf().setAppName(appName).setMaster(master)\n sc = SparkContext(conf=conf)\n duration = .5\n snsc = SnappyStreamingContext(sc, duration)  Also, SnappyData can be run in three different modes, Local Mode, Embedded Mode and SnappyData Connector mode. Before proceeding, it is important that you understand these modes. For more information, see  Affinity modes .  If you are using SnappyData in LocalMode or Connector mode, it is the responsibility of the user to create a SnappySession.\nIf you are in the Embedded Mode, applications typically submit jobs to SnappyData and do not explicitly create a SnappySession or SnappyStreamingContext.\nJobs are the primary mechanism to interact with SnappyData using the Spark API in embedded mode.\nA job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.", 
            "title": "Create a SnappyStreamingContext"
        }, 
        {
            "location": "/programming_guide/snappydata_jobs/", 
            "text": "SnappyData Jobs\n\n\nTo create a job that can be submitted through the job server, the job must implement the \nSnappySQLJob\n or \nSnappyStreamingJob\n trait. The structure of a job looks as below:\n\n\nScala\n\n\nobject SnappySampleJob extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  override def runSnappyJob(snSession: SnappySession, jobConfig: Config): Any = {\n}\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  override def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation = SnappyJobValid()\n\n}\n\n\n\n\nJava\n\n\nclass SnappySampleJob extends JavaSnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(SnappySession snappy, Config jobConfig) {//Implementation}\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(SnappySession snappy, Config config) {//validate}\n}\n\n\n\n\n\nScala\n\n\nobject SnappyStreamingSampleJob extends SnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  override def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any = {\n}\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  override def isValidJob(sc: SnappyStreamingContext, config: Config): SnappyJobValidation = SnappyJobValid()\n}\n\n\n\n\nJava\n\n\nclass SnappyStreamingSampleJob extends JavaSnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)\n  {//validate}\n}\n\n\n\n\n\n\nNote\n\n\nThe \nJob\n traits are simply extensions of the \nSparkJob\n implemented by \nSpark JobServer\n. \n\n\n\n\n\n\n\n\nrunSnappyJob\n contains the implementation of the Job.\nThe \nSnappySession\n/\nSnappyStreamingContext\n is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and is provided to the job through this method. This relieves the developer from configuration management that comes with the creation of a Spark job and allows the Job Server to manage and reuse contexts.\n\n\n\n\n\n\nisValidJob\n allows for an initial validation of the context and any provided configuration.\n    If the context and configuration can run the job, returning \nspark.jobserver.SnappyJobValid\n allows the job to execute, otherwise returning \nspark.jobserver.SnappyJobInvalid\nreason\n prevents the job from running and provides means to convey the reason for failure. In this case, the call immediately returns an \"HTTP/1.1 400 Bad Request\" status code. Validate helps you prevent running jobs that eventually fail due to a  missing or wrong configuration, and saves both time and resources.\n\n\n\n\n\n\nSee \nexamples\n for Spark and Spark streaming jobs. \n\n\nSnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SnappySession per job. Similarly, SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs.\n\n\nSubmitting Jobs\n\n\nThe following command submits \nCreateAndLoadAirlineDataJob\n. This job creates DataFrames from parquet files, loads the data from DataFrame into column tables and row tables, and creates sample table on column table in its \nrunJob\n method.\n\n\n\n\nNote\n\n\nWhen submitting concurrent jobs user must ensure that the \n--app-name\n parameter is different for each concurrent job. If two applications with the same name are submitted concurrently, the job fails and an error is reported, as the job server maintains a map of the application names and jar files used for that application.\n\n\n\n\nThe program must be compiled and bundled as a jar file and submitted to jobs server as shown below:\n\n\n$ ./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar\n\n\n\n\nThe utility \nsnappy-job.sh\n submits the job and returns a JSON that has a Job Id of this job.\n\n\n\n\n\n\n--lead\n: Specifies the host name of the lead node along with the port on which it accepts jobs (8090)\n\n\n\n\n\n\n--app-name\n: Specifies the name given to the submitted application\n\n\n\n\n\n\n--class\n: Specifies the name of the class that contains implementation of the Spark job to be run\n\n\n\n\n\n\n--app-jar\n: Specifies the jar file that packages the code for Spark job\n\n\n\n\n\n\n--packages\n: Specifies the packages names, which must be comma separated. These package names can be used to inform Spark about all the dependencies of a job. For more details, refer to \nDeploying Dependency Jars\n.\n\n\n\n\n\n\nThe status returned by the utility is displayed below:\n\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  }\n}\n\n\n\n\nThis Job ID can be used to query the status of the running job. \n\n\n$ ./bin/snappy-job.sh status  \\\n    --lead localhost:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{\n  \nduration\n: \n17.53 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.CreateAndLoadAirlineDataJob\n,\n  \nstartTime\n: \n2016-01-12T16:59:14.746+05:30\n,\n  \ncontext\n: \nsnappyContext1452598154529305363\n,\n  \nresult\n: \nSee /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n}\n\n\n\n\nOnce the tables are created, they can be queried by running another job. Please refer to \nAirlineDataJob\n for implementing the job. \n\n\n$ ./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar\n\n\n\n\nThe status of this job can be queried in the same manner as shown above. The result of the job returns a file path that has the query results.\n\n\nJar Dependencies for Jobs\n\n\nFor writing jobs users need to include \nMaven/SBT dependencies for the latest released version of SnappyData\n to their project dependencies. In case the project already includes dependency on Apache Spark and the user does not want to include snappy-spark dependencies, then, it is possible to explicitly exclude the snappy-spark dependencies.\n\n\nFor example, gradle can be configured as:\n\n\ncompile('io.snappydata:snappydata-cluster_2.11:1.0.2.1') {\n        exclude(group: 'io.snappydata', module: 'snappy-spark-unsafe_2.11')\n        exclude(group: 'io.snappydata', module: 'snappy-spark-core_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-yarn_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-hive-thriftserver_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-streaming-kafka-0.10_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-repl_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-sql_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-mllib_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-streaming_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-catalyst_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-hive_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-graphx_2.11')\n    }\n\n\n\n\nRunning Python Applications\n\n\nPython users can submit a Python application using \n./bin/spark-submit\n in the SnappyData Connector mode. Run the following command to submit a Python application:\n\n\n./bin/spark-submit \\\n    --master local[*]  \\\n    --conf snappydata.connection=localhost:1527 \\\n    --conf spark.ui.port=4042 ./quickstart/python/CreateTable.py\n\n\n\n\nsnappydata.connection\n property is a combination of locator host and JDBC client port on which the locator listens for connections (default 1527). It is used to connect to the SnappyData cluster.\n\n\n\n\nNote\n\n\nFor running ML/MLlib applications you need to install appropriate python packages(if your application uses any).\n\nKMeans uses numpy hence you need to install numpy package before using Spark KMeans.\n\nFor example \nsudo apt-get install python-numpy\n\n\n\n\nStreaming Jobs\n\n\nAn implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying \n--stream\n as an option to the submit command. This option creates a new SnappyStreamingContext before the job is submitted. \nAlternatively, you can specify the name of an existing/pre-created streaming context as \n--context \ncontext-name\n with the \nsubmit\n command.\n\n\nFor example, \nTwitterPopularTagsJob\n can be submitted as follows. \nThis job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, and top 10 popular tweets.\n\n\n$ ./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --conf streaming.batch_interval=5000 \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\\n    --stream\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n982ac142-3550-41e1-aace-6987cb39fec8\n,\n    \ncontext\n: \nsnappyStreamingContext1463987084945028747\n\n  }\n}\n\n\n\n\nTo start another streaming job with a new streaming context, you need to first stop the currently running streaming job, followed by its streaming context.\n\n\n$ ./bin/snappy-job.sh stop  \\\n    --lead localhost:8090  \\\n    --job-id 982ac142-3550-41e1-aace-6987cb39fec8\n\n$ ./bin/snappy-job.sh listcontexts  \\\n    --lead localhost:8090\n[\nsnappyContext1452598154529305363\n, \nsnappyStreamingContext1463987084945028747\n, \nsnappyStreamingContext\n]\n\n$ ./bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \\\n    --lead localhost:8090\n\n\n\n\nRelated Topic\n:\n\n\n\n\nHow to use Python to Create Tables and Run Queries", 
            "title": "SnappyData Jobs"
        }, 
        {
            "location": "/programming_guide/snappydata_jobs/#snappydata-jobs", 
            "text": "To create a job that can be submitted through the job server, the job must implement the  SnappySQLJob  or  SnappyStreamingJob  trait. The structure of a job looks as below:  Scala  object SnappySampleJob extends SnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  override def runSnappyJob(snSession: SnappySession, jobConfig: Config): Any = {\n}\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  override def isValidJob(sc: SnappySession, config: Config): SnappyJobValidation = SnappyJobValid()\n\n}  Java  class SnappySampleJob extends JavaSnappySQLJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(SnappySession snappy, Config jobConfig) {//Implementation}\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(SnappySession snappy, Config config) {//validate}\n}  Scala  object SnappyStreamingSampleJob extends SnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  override def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any = {\n}\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  override def isValidJob(sc: SnappyStreamingContext, config: Config): SnappyJobValidation = SnappyJobValid()\n}  Java  class SnappyStreamingSampleJob extends JavaSnappyStreamingJob {\n  /** SnappyData uses this as an entry point to execute SnappyData jobs. **/\n  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)\n  {//validate}\n}   Note  The  Job  traits are simply extensions of the  SparkJob  implemented by  Spark JobServer .      runSnappyJob  contains the implementation of the Job.\nThe  SnappySession / SnappyStreamingContext  is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and is provided to the job through this method. This relieves the developer from configuration management that comes with the creation of a Spark job and allows the Job Server to manage and reuse contexts.    isValidJob  allows for an initial validation of the context and any provided configuration.\n    If the context and configuration can run the job, returning  spark.jobserver.SnappyJobValid  allows the job to execute, otherwise returning  spark.jobserver.SnappyJobInvalid reason  prevents the job from running and provides means to convey the reason for failure. In this case, the call immediately returns an \"HTTP/1.1 400 Bad Request\" status code. Validate helps you prevent running jobs that eventually fail due to a  missing or wrong configuration, and saves both time and resources.    See  examples  for Spark and Spark streaming jobs.   SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SnappySession per job. Similarly, SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs.", 
            "title": "SnappyData Jobs"
        }, 
        {
            "location": "/programming_guide/snappydata_jobs/#submitting-jobs", 
            "text": "The following command submits  CreateAndLoadAirlineDataJob . This job creates DataFrames from parquet files, loads the data from DataFrame into column tables and row tables, and creates sample table on column table in its  runJob  method.   Note  When submitting concurrent jobs user must ensure that the  --app-name  parameter is different for each concurrent job. If two applications with the same name are submitted concurrently, the job fails and an error is reported, as the job server maintains a map of the application names and jar files used for that application.   The program must be compiled and bundled as a jar file and submitted to jobs server as shown below:  $ ./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar  The utility  snappy-job.sh  submits the job and returns a JSON that has a Job Id of this job.    --lead : Specifies the host name of the lead node along with the port on which it accepts jobs (8090)    --app-name : Specifies the name given to the submitted application    --class : Specifies the name of the class that contains implementation of the Spark job to be run    --app-jar : Specifies the jar file that packages the code for Spark job    --packages : Specifies the packages names, which must be comma separated. These package names can be used to inform Spark about all the dependencies of a job. For more details, refer to  Deploying Dependency Jars .    The status returned by the utility is displayed below:  {\n   status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  }\n}  This Job ID can be used to query the status of the running job.   $ ./bin/snappy-job.sh status  \\\n    --lead localhost:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{\n   duration :  17.53 secs ,\n   classPath :  io.snappydata.examples.CreateAndLoadAirlineDataJob ,\n   startTime :  2016-01-12T16:59:14.746+05:30 ,\n   context :  snappyContext1452598154529305363 ,\n   result :  See /home/user1/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n}  Once the tables are created, they can be queried by running another job. Please refer to  AirlineDataJob  for implementing the job.   $ ./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar  The status of this job can be queried in the same manner as shown above. The result of the job returns a file path that has the query results.", 
            "title": "Submitting Jobs"
        }, 
        {
            "location": "/programming_guide/snappydata_jobs/#jar-dependencies-for-jobs", 
            "text": "For writing jobs users need to include  Maven/SBT dependencies for the latest released version of SnappyData  to their project dependencies. In case the project already includes dependency on Apache Spark and the user does not want to include snappy-spark dependencies, then, it is possible to explicitly exclude the snappy-spark dependencies.  For example, gradle can be configured as:  compile('io.snappydata:snappydata-cluster_2.11:1.0.2.1') {\n        exclude(group: 'io.snappydata', module: 'snappy-spark-unsafe_2.11')\n        exclude(group: 'io.snappydata', module: 'snappy-spark-core_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-yarn_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-hive-thriftserver_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-streaming-kafka-0.10_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-repl_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-sql_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-mllib_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-streaming_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-catalyst_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-hive_2.11')\n        exclude(group: 'io.snappydata',module: 'snappy-spark-graphx_2.11')\n    }", 
            "title": "Jar Dependencies for Jobs"
        }, 
        {
            "location": "/programming_guide/snappydata_jobs/#running-python-applications", 
            "text": "Python users can submit a Python application using  ./bin/spark-submit  in the SnappyData Connector mode. Run the following command to submit a Python application:  ./bin/spark-submit \\\n    --master local[*]  \\\n    --conf snappydata.connection=localhost:1527 \\\n    --conf spark.ui.port=4042 ./quickstart/python/CreateTable.py  snappydata.connection  property is a combination of locator host and JDBC client port on which the locator listens for connections (default 1527). It is used to connect to the SnappyData cluster.   Note  For running ML/MLlib applications you need to install appropriate python packages(if your application uses any). \nKMeans uses numpy hence you need to install numpy package before using Spark KMeans. \nFor example  sudo apt-get install python-numpy", 
            "title": "Running Python Applications"
        }, 
        {
            "location": "/programming_guide/snappydata_jobs/#streaming-jobs", 
            "text": "An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying  --stream  as an option to the submit command. This option creates a new SnappyStreamingContext before the job is submitted. \nAlternatively, you can specify the name of an existing/pre-created streaming context as  --context  context-name  with the  submit  command.  For example,  TwitterPopularTagsJob  can be submitted as follows. \nThis job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, and top 10 popular tweets.  $ ./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --conf streaming.batch_interval=5000 \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\\n    --stream\n\n{\n   status :  STARTED ,\n   result : {\n     jobId :  982ac142-3550-41e1-aace-6987cb39fec8 ,\n     context :  snappyStreamingContext1463987084945028747 \n  }\n}  To start another streaming job with a new streaming context, you need to first stop the currently running streaming job, followed by its streaming context.  $ ./bin/snappy-job.sh stop  \\\n    --lead localhost:8090  \\\n    --job-id 982ac142-3550-41e1-aace-6987cb39fec8\n\n$ ./bin/snappy-job.sh listcontexts  \\\n    --lead localhost:8090\n[ snappyContext1452598154529305363 ,  snappyStreamingContext1463987084945028747 ,  snappyStreamingContext ]\n\n$ ./bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \\\n    --lead localhost:8090  Related Topic :   How to use Python to Create Tables and Run Queries", 
            "title": "Streaming Jobs"
        }, 
        {
            "location": "/programming_guide/managing_jar_files/", 
            "text": "Managing JAR Files\n\n\nSnappyData provides system procedures that you can use to install and manage JAR files from a client connection. These can be used to install your custom code (for example code shared across multiple jobs) in SnappyData cluster.\n\n\nInstalling a JAR\n\n\nRelated jobs may require some common libraries. These libraries can be made available to jobs by installing them. Use the SQLJ.INSTALL_JAR procedure to install a JAR file as mentioned below:\n\n\nSyntax:\n\n\nSQLJ.INSTALL_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672), IN DEPLOY INTEGER)\n\n\n\n\n\n\n\n\nJAR_FILE_PATH  is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using a locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers\n\n\n\n\n\n\nQUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.\n\n\n\n\n\n\nDEPLOY: This argument is currently ignored.\n\n\n\n\n\n\nExample:\n\n\nsnappy\n call sqlj.install_jar('/path_to_jar/procs.jar', 'APP.custom_procs', 0);\n\n\n\n\nReplacing a JAR\n\n\nUse  SQLJ.REPLACE_JAR procedure to replace an installed JAR file\n\n\nSyntax:\n\n\nSQLJ.REPLACE_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672))\n\n\n\n\n\n\n\n\nJAR_FILE_PATH  is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using the locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers.\n\n\n\n\n\n\nQUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.\n\n\n\n\n\n\nExample:\n\n\nCALL sqlj.replace_jar('/path_to_jar/newprocs.jar', 'APP.custom_procs')\n\n\n\n\nRemoving a JAR\n\n\nUse SQLJ.REMOVE_JAR  procedure to remove a JAR file\n\n\nSyntax:\n\n\nSQLJ.REMOVE_JAR(IN QUALIFIED_JAR_NAME VARCHAR(32672), IN UNDEPLOY INTEGER)\n\n\n\n\n\n\n\n\nQUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.\n\n\n\n\n\n\nUNDEPLOY: This argument is currently ignored.\n\n\n\n\n\n\nExample:\n\n\nCALL SQLJ.REMOVE_JAR('APP.custom_procs', 0)", 
            "title": "Managing JAR Files"
        }, 
        {
            "location": "/programming_guide/managing_jar_files/#managing-jar-files", 
            "text": "SnappyData provides system procedures that you can use to install and manage JAR files from a client connection. These can be used to install your custom code (for example code shared across multiple jobs) in SnappyData cluster.", 
            "title": "Managing JAR Files"
        }, 
        {
            "location": "/programming_guide/managing_jar_files/#installing-a-jar", 
            "text": "Related jobs may require some common libraries. These libraries can be made available to jobs by installing them. Use the SQLJ.INSTALL_JAR procedure to install a JAR file as mentioned below:  Syntax:  SQLJ.INSTALL_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672), IN DEPLOY INTEGER)    JAR_FILE_PATH  is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using a locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers    QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.    DEPLOY: This argument is currently ignored.    Example:  snappy  call sqlj.install_jar('/path_to_jar/procs.jar', 'APP.custom_procs', 0);", 
            "title": "Installing a JAR"
        }, 
        {
            "location": "/programming_guide/managing_jar_files/#replacing-a-jar", 
            "text": "Use  SQLJ.REPLACE_JAR procedure to replace an installed JAR file  Syntax:  SQLJ.REPLACE_JAR(IN JAR_FILE_PATH VARCHAR(32672), IN QUALIFIED_JAR_NAME VARCHAR(32672))    JAR_FILE_PATH  is the full path to the JAR file. This path must be accessible to the server on which the INSTALL_JAR procedure is being executed. If the JDBC client connection on which this procedure is being executed is using the locator to connect to the cluster, then actual client connection could be with any available servers. In this case, the JAR file path should be available to all servers.    QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.    Example:  CALL sqlj.replace_jar('/path_to_jar/newprocs.jar', 'APP.custom_procs')", 
            "title": "Replacing a JAR"
        }, 
        {
            "location": "/programming_guide/managing_jar_files/#removing-a-jar", 
            "text": "Use SQLJ.REMOVE_JAR  procedure to remove a JAR file  Syntax:  SQLJ.REMOVE_JAR(IN QUALIFIED_JAR_NAME VARCHAR(32672), IN UNDEPLOY INTEGER)    QUALIFIED_JAR_NAME: The SnappyData name of the JAR file, qualified by a valid schema name.    UNDEPLOY: This argument is currently ignored.    Example:  CALL SQLJ.REMOVE_JAR('APP.custom_procs', 0)", 
            "title": "Removing a JAR"
        }, 
        {
            "location": "/programming_guide/using_snappydata_shell/", 
            "text": "Using SnappyData Shell\n\n\nThe SnappyData SQL Shell (\nsnappy-sql\n) provides a simple command line interface to the SnappyData cluster.\nIt allows you to run interactive queries on the row and column stores, run administrative operations and run status commands on the cluster. \nInternally, it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.\n\n\nStart the SnappyData cluster\n and enter the following:\n\n\n// From the SnappyData base directory  \n$ ./bin/snappy-sql\nSnappyData version 1.0.2.1\nsnappy-sql\n \n\n//Connect to the cluster as a client  \nsnappy-sql\n connect client 'localhost:1527'; //It connects to the locator which is running in localhost with client port configured as 1527.\n\n//Show active connections  \nsnappy-sql\n show connections;\n\n//Display cluster members by querying a system table  \nsnappy-sql\n select id, kind, status, host, port from sys.members;\n\n//or\nsnappy-sql\n show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy-sql\n run './quickstart/scripts/create_and_load_column_table.sql';\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy-sql\n run './quickstart/scripts/create_and_load_row_table.sql';\n\n\n\n\nThe complete list of commands available through \nsnappy_shell\n can be found \nhere\n.", 
            "title": "Using SnappyData Shell"
        }, 
        {
            "location": "/programming_guide/using_snappydata_shell/#using-snappydata-shell", 
            "text": "The SnappyData SQL Shell ( snappy-sql ) provides a simple command line interface to the SnappyData cluster.\nIt allows you to run interactive queries on the row and column stores, run administrative operations and run status commands on the cluster. \nInternally, it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.  Start the SnappyData cluster  and enter the following:  // From the SnappyData base directory  \n$ ./bin/snappy-sql\nSnappyData version 1.0.2.1\nsnappy-sql  \n\n//Connect to the cluster as a client  \nsnappy-sql  connect client 'localhost:1527'; //It connects to the locator which is running in localhost with client port configured as 1527.\n\n//Show active connections  \nsnappy-sql  show connections;\n\n//Display cluster members by querying a system table  \nsnappy-sql  select id, kind, status, host, port from sys.members;\n\n//or\nsnappy-sql  show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy-sql  run './quickstart/scripts/create_and_load_column_table.sql';\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy-sql  run './quickstart/scripts/create_and_load_row_table.sql';  The complete list of commands available through  snappy_shell  can be found  here .", 
            "title": "Using SnappyData Shell"
        }, 
        {
            "location": "/programming_guide/using_the_spark_shell_and_spark-submit/", 
            "text": "Using spark-shell and spark-submit\n\n\nSnappyData, out-of-the-box, colocates Spark executors and the SnappyData store for efficient data intensive computations. \nYou, however, may need to isolate the computational cluster for other reasons. For instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly.\nRefer to \nSnappyData Smart Connector Mode\n for examples.\n\n\nTo support such cases it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store the \nspark.snappydata.connection\n property should be provided while starting the Spark-shell. \n\n\nTo run all SnappyData functionalities, you need to create a \nSnappySession\n.\n\n\n// from the SnappyData base directory  \n// Start the Spark shell in local mode. Pass SnappyData's locators host:clientPort as a conf parameter.\n$ ./bin/spark-shell  --master local[*] --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041\nscala\n\n // Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql\nscala\n val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\nscala\n val airlineDF = snappy.table(\nairline\n).show\nscala\n val resultset = snappy.sql(\nselect * from airline\n)\n\n\n\n\nAny Spark application can also use the SnappyData as store and Spark as a computational engine by providing the \nspark.snappydata.connection\n property as mentioned below:\n\n\n// Start the Spark standalone cluster from SnappyData base directory\n$ ./sbin/start-all.sh \n// Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ ./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort $SNAPPY_HOME/examples/jars/quickstart.jar\n\n// The results can be seen on the command line.", 
            "title": "Using the Spark Shell and spark-submit"
        }, 
        {
            "location": "/programming_guide/using_the_spark_shell_and_spark-submit/#using-spark-shell-and-spark-submit", 
            "text": "SnappyData, out-of-the-box, colocates Spark executors and the SnappyData store for efficient data intensive computations. \nYou, however, may need to isolate the computational cluster for other reasons. For instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly.\nRefer to  SnappyData Smart Connector Mode  for examples.  To support such cases it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store the  spark.snappydata.connection  property should be provided while starting the Spark-shell.   To run all SnappyData functionalities, you need to create a  SnappySession .  // from the SnappyData base directory  \n// Start the Spark shell in local mode. Pass SnappyData's locators host:clientPort as a conf parameter.\n$ ./bin/spark-shell  --master local[*] --conf spark.snappydata.connection=locatorhost:clientPort --conf spark.ui.port=4041\nscala \n // Try few commands on the spark-shell. Following command shows the tables created using the snappy-sql\nscala  val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)\nscala  val airlineDF = snappy.table( airline ).show\nscala  val resultset = snappy.sql( select * from airline )  Any Spark application can also use the SnappyData as store and Spark as a computational engine by providing the  spark.snappydata.connection  property as mentioned below:  // Start the Spark standalone cluster from SnappyData base directory\n$ ./sbin/start-all.sh \n// Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ ./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf spark.snappydata.connection=locatorhost:clientPort $SNAPPY_HOME/examples/jars/quickstart.jar\n\n// The results can be seen on the command line.", 
            "title": "Using spark-shell and spark-submit"
        }, 
        {
            "location": "/programming_guide/working_with_hadoop_yarn_cluster_manager/", 
            "text": "Working with Hadoop YARN Cluster Manager\n\n\nThe SnappyData embedded cluster uses its own cluster manager and as such cannot be managed using the YARN cluster manager. However, you can start the Spark cluster with the YARN cluster manager, which can interact with the SnappyData cluster in the \nSmart Connector Mode\n.\n\n\n\n\nNote\n\n\nWe assume that Apache Hadoop and YARN are already installed, and you want to bring in SnappyData cluster to work with YARN.\n\n\n\n\nYou need to set the following environment variables:\n\n\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop\n\n\n\n\n\nLaunching spark-shell with YARN\n\n\nStart a SnappyData default cluster using the \n./sbin/snappy-start-all.sh\n command\n\n\nTo run SnappyData quickstart example using YARN, do the following:\n\n\n./bin/spark-shell --master yarn  --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 -i $SNAPPY_HOME/quickstart/scripts/Quickstart.scala\n\n\n\n\n\n\nNote\n\n\nYARN is mentioned as a master url.\n\n\n\n\nSubmitting spark-jobs using YARN\n\n\n\n\n\n\nCreate the required tables in the SnappyData cluster\n\n\n./bin/snappy-job.sh submit --lead localhost:8090 --app-name CreateAndLoadAirlineDataJob --class io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar\n\n\n\n\n\n\n\nRun queries on the tables created from CreateAndLoadAirlineDataJob.\n\n\n./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master yarn --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar", 
            "title": "Working with Hadoop YARN cluster Manager"
        }, 
        {
            "location": "/programming_guide/working_with_hadoop_yarn_cluster_manager/#working-with-hadoop-yarn-cluster-manager", 
            "text": "The SnappyData embedded cluster uses its own cluster manager and as such cannot be managed using the YARN cluster manager. However, you can start the Spark cluster with the YARN cluster manager, which can interact with the SnappyData cluster in the  Smart Connector Mode .   Note  We assume that Apache Hadoop and YARN are already installed, and you want to bring in SnappyData cluster to work with YARN.   You need to set the following environment variables:  export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop", 
            "title": "Working with Hadoop YARN Cluster Manager"
        }, 
        {
            "location": "/programming_guide/working_with_hadoop_yarn_cluster_manager/#launching-spark-shell-with-yarn", 
            "text": "Start a SnappyData default cluster using the  ./sbin/snappy-start-all.sh  command  To run SnappyData quickstart example using YARN, do the following:  ./bin/spark-shell --master yarn  --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 -i $SNAPPY_HOME/quickstart/scripts/Quickstart.scala   Note  YARN is mentioned as a master url.", 
            "title": "Launching spark-shell with YARN"
        }, 
        {
            "location": "/programming_guide/working_with_hadoop_yarn_cluster_manager/#submitting-spark-jobs-using-yarn", 
            "text": "Create the required tables in the SnappyData cluster  ./bin/snappy-job.sh submit --lead localhost:8090 --app-name CreateAndLoadAirlineDataJob --class io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar    Run queries on the tables created from CreateAndLoadAirlineDataJob.  ./bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master yarn --conf spark.snappydata.connection=localhost:1527 --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart.jar", 
            "title": "Submitting spark-jobs using YARN"
        }, 
        {
            "location": "/programming_guide/using_jdbc_with_snappydata/", 
            "text": "Using JDBC with SnappyData\n\n\nSnappyData is shipped with few JDBC drivers. The connection URL typically points to one of the locators. In the background, the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically adjusting underlying physical connections in case the servers fail. \n\n\n// 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection (\njdbc:snappydata://locatorHostName:1527/\n);\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint\n\n\n\n\n\n\nNote\n\n\nIf you are using a third part tool that connects to the database using JDBC, and if the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the \nio.snappydata.jdbc.ClientDriver\n class.\n\n\n\n\nFor more information, see \nHow to connect using JDBC driver\n.", 
            "title": "Using JDBC with SnappyData"
        }, 
        {
            "location": "/programming_guide/using_jdbc_with_snappydata/#using-jdbc-with-snappydata", 
            "text": "SnappyData is shipped with few JDBC drivers. The connection URL typically points to one of the locators. In the background, the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically adjusting underlying physical connections in case the servers fail.   // 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection ( jdbc:snappydata://locatorHostName:1527/ );\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint   Note  If you are using a third part tool that connects to the database using JDBC, and if the tool does not automatically select a driver class, you may have the option of selecting a class from within the JAR file. In this case, select the  io.snappydata.jdbc.ClientDriver  class.   For more information, see  How to connect using JDBC driver .", 
            "title": "Using JDBC with SnappyData"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/", 
            "text": "Accessing SnappyData Tables from any Spark (2.1+) Cluster\n\n\nSpark applications can be run embedded inside the SnappyData cluster by submitting Jobs using \nSnappy-Job.sh\n or it can be run using the native \nSmart Connector\n. However, from SnappyData 1.0.2 release the connector can only be used from a Spark 2.1 compatible cluster.\nIf you are using a Spark version or distribution that is based on a version higher than 2.1 then, you can use the \nSnappyData JDBC Extension Connector\n as described below. \n\n\nHow can Spark Applications Connect to SnappyData using Spark JDBC?\n\n\nSpark SQL supports reading and writing to databases using a built-in \nJDBC data source\n. Applications can configure and use JDBC like any other Spark data source queries return data frames and can be efficiently processed in Spark SQL or joined with other data sources. The JDBC data source is also easy to use from Java or Python.\nAll you need is a JDBC driver from the database vendor. Likewise, applications can use the Spark \nDataFrameWriter\n to insert, append, or replace a dataset in the database.\n\n\n\n\nNote\n\n\nThe usage model for the Spark JDBC data source is described \nhere\n. We strongly recommend you to go through this section in case you are not familiar with how Spark works with data sources.\n\n\n\n\nPushing Entire Query into the Database\n\n\nWhen Spark queries are executed against external data sources, the current Spark model can only push down filters and projections in the query down to the database. If you are running an expensive aggregation on a large data set, then the entire data set is fetched into the Spark partitions, and the query is executed inside your Spark cluster. \nHowever, when you use a JDBC data source, you can pass entire queries or portions of the query entirely to the database such as shown in the following sample:\n\n\nval pushdownQuery = \n(select x, sum(y), max(z) from largeTable group by x order by x) t1\n;\nspark.read.jdbc(jdbcUrl, pushDownQuery, connectionProperties);\n\n\n\n\nDeficiencies in the Spark JDBC Connector \n\n\nUnfortunately, there are following limitations with Spark JDBC Connector which we address in the SnappyData JDBC Extension Connector. \n\n\n\n\n\n\nPerformance\nWhen an entire query is pushed down, Spark runs two queries:\n\n\n\n\nFirst it runs the query that is supplied to fetch the result set metadata so that it knows the structure of the data frame that is returned to the application.\n\n\nSecondly it runs the actual query.\nThe SnappyData connector internally figures out the structure of the result set without having to run multiple queries.\n\n\n\n\n\n\n\n\nLack of connection pooling\n With no built-in support for pooling connections, every time a query is executed against a JDBC database, each of the partition in Spark has to set up a new connection which can be expensive. SnappyData internally uses an efficient pooled implementation with sensible defaults.\n\n\n\n\n\n\nData manipulation\n While the Spark DataFrameWriter API can be used to append/insert a full dataset (dataframe) into the database, it is not simple to run the ad-hoc updates on the database including mass updates. The SnappyData JDBC Extension Connector makes this much simpler. \n\n\n\n\nUsability\n With the SnappyData JDBC Extension Connector, it is easier to deal with all the connection properties. You need not impair your application with sensitive properties dispersed in your app code.\n\n\n\n\nConnecting to SnappyData using the JDBC Extension Connector\n\n\nFollowing is a sample of Spark JDBC Extension setup and usage:\n\n\n\n\n\n\nInclude the \nsnappydata-jdbc\n package in the Spark job with spark-submit or spark-shell: \n\n\n    $SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2\n\n\n\n\n\n\n\nSet the session properties.\n The SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in \nspark-defaults.conf\n along with all the other Spark properties.  You can also set any of these properties in your app code. \nOverloads of the above methods accepting \nuser+password\n and \nhost+port \nis also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the \nDataFrameReader.jdbc \nmethod.\n Following is a sample code of configuring the properties in \nSparkConf\n:\n\n\n    $SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2 --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.user=\nuser\n --conf spark.snappydata.password=\npassword\n\n\n\n\n\n\n\n\nImport the required implicits in the job/shell code as follows:\n\n\n    import io.snappydata.sql.implicits._\n\n\n\n\n\n\n\nRunning Queries\n\n\nYour application must import the SnappyData SQL implicits when using Scala. \n\n\nimport io.snappydata.sql.implicits._\n\n\n\n\nOnce the required session properties are set (connection and user/password as shown above), then one can run the required queries/DMLs without any other configuration.\n\n\nScala Query Example\n\n\nval spark = \ncreate/access your spark session instance here \n;\nval dataset = spark.snappyQuery(\nselect x, sum(y), max(z) from largeTable group by x order by x\n) // query pushed down to SnappyData data cluster\n\n\n\n\nJava Query Example\n\n\nimport org.apache.spark.sql.*;\n\nJdbcExecute exec = new JdbcExecute(\nyour SparkSession instance\n);\nDataFrame df = exec.snappyQuery(\nselect x, sum(y), max(z) from largeTable group by x order by x\n) // query pushed down to SnappyData data cluster\n\n\n\n\n\n\nNote\n\n\nOverloads of the above methods of accepting \nuser+password \nand \nhost+port\n is also provided in case those properties are not set in the session or need to be overridden. You can optionally pass additional connection properties such as in \nDataFrameReader.jdbc\n method.\n\n\n\n\n\n\n\nUpdating/Writing Data in SnappyData Tables\n\n\nYour application can use the Spark \nDataFrameWriter\n API to either insert or append data.\n\n\nAlso, for convenience, the connector provides an implicit in scala that is \nimport io.snappydata.sql.implicits._\n for the DataFrameWriter to simplify writing to SnappyData. Hence, there is no need to explicitly set the connection properties. \n\n\nAfter the required session properties are set (connection and user/password as shown above), then you can fire the required queries/DMLs without any other configuration.\n\n\nInserting a dataset from the job can also use the \nsnappy\n extension to avoid passing in the URL and credentials explicitly:\n\n\ndf.write.snappy(\u201ctestTable1\u201d)  // You can use all the Spark writer APIs when using the snappy implicit. \nOr using explicit wrapper in Java: new JdbcWriter(spark.write).snappy(\u201ctestTable\u201d)\n\n\n\n\nUsing SQL DML to Execute Ad-hoc SQL\n\n\nYou can also use the \nsnappyExecute\n method (see below) to run the arbitrary SQL DML statements directly on the database. You need not acquire/manage explicit JDBC connections or set properties.\n\n\n\n// execute DDL\nspark.snappyExecute(\ncreate table testTable1 (id long, data string) using column\n)\n// DML\nspark.snappyExecute(\ninsert into testTable1 values (1, \u2018data1\u2019)\n)\n// bulk insert from external table in embedded mode\nspark.snappyExecute(\ninsert into testTable1 select * from externalTable1\n)\n\n\n\n\nWhen using Java, the wrapper has to be created explicitly as shown below:\n\n\nimport org.apache.spark.sql.*;\n\nJdbcExecute exec = new JdbcExecute(spark);\nexec.snappyExecute(\u201ccreate table testTable1 (id long, data string) using column\u201d);\nexec.snappyExecute(\ninsert into testTable1 values (1, \u2018data1\u2019)\n);\n...\n\n\n\n\nComparison with Current Spark APIs\n\n\nThere is no equivalent of \nsnappyExecute\n and one has to explicitly use JDBC API. For \nsnappyQuery\n, if you were to use Spark\u2019s JDBC connector directly, then the equivalent code would appear as follows (assuming snappyExecute equivalent was done beforehand using JDBC API or otherwise):\n\n\nval jdbcUrl = \njdbc:snappydata:pool://localhost:1527\n\nval connProps = new java.util.Properties()\nconnProps.setProperty(\ndriver\n, \nio.snappydata.jdbc.ClientPoolDriver\n)\nconnProps.setProperty(\nuser\n, userName)\nconnProps.setProperty(\npassword\n, password)\n\nval df = spark.read.jdbc(jdbcUrl, \u201c(select count(*) from testTable1) q\u201d, connProps)\n\n\n\n\n\nBesides being more verbose, this suffers from the problem of double query execution (first query to fetch query result metadata, followed by the actual query).\n\n\nAPI Reference\n\n\nThe following extensions are used to implement the Spark JDBC Connector:\n\n\n\n\nSparkSession.snappyQuery()\nThis method creates a wrapper over SparkSession and is similar to \nSparkSession.sql() API\n. The only difference between the both is that the entire query is pushed down to the SnappyData cluster. Hence, a query cannot have any temporary or external tables/views that are not visible in the SnappyData cluster.\n\n\nSparkSession.snappyExecute()\n Similarly, with this method, the SQL (assuming it to be a DML) is pushed to SnappyData, using the JDBC API, and returns the update count.\n\n\nsnappy\n An implicit for DataFrameWriter named snappy simplifies the bulk writes. Therefore a write operation such as \nsession.write.jdbc()\n becomes \nsession.write.snappy()\n with the difference that JDBC URL, driver, and connection properties are auto-configured using session properties, if possible.\n\n\n\n\nPerformance Considerations\n\n\nIt should be noted that using the Spark Data Source Writer or the Snappy Implicit are both much slower as compared to SnappyData embedded job or Smart Connector. If the DataFrame that is to be written is medium or large sized, then it is better to ingest directly in an embedded mode. In case writing an embedded job is not an option, the incoming DataFrame can be dumped to an external table in a location accessible to both Spark and SnappyData clusters. After this, it can be ingested in an embedded mode using \nsnappyExecute\n.", 
            "title": "Accessing SnappyData Tables from any Spark (2.1+) Cluster"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#accessing-snappydata-tables-from-any-spark-21-cluster", 
            "text": "Spark applications can be run embedded inside the SnappyData cluster by submitting Jobs using  Snappy-Job.sh  or it can be run using the native  Smart Connector . However, from SnappyData 1.0.2 release the connector can only be used from a Spark 2.1 compatible cluster.\nIf you are using a Spark version or distribution that is based on a version higher than 2.1 then, you can use the  SnappyData JDBC Extension Connector  as described below.", 
            "title": "Accessing SnappyData Tables from any Spark (2.1+) Cluster"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#how-can-spark-applications-connect-to-snappydata-using-spark-jdbc", 
            "text": "Spark SQL supports reading and writing to databases using a built-in  JDBC data source . Applications can configure and use JDBC like any other Spark data source queries return data frames and can be efficiently processed in Spark SQL or joined with other data sources. The JDBC data source is also easy to use from Java or Python.\nAll you need is a JDBC driver from the database vendor. Likewise, applications can use the Spark  DataFrameWriter  to insert, append, or replace a dataset in the database.   Note  The usage model for the Spark JDBC data source is described  here . We strongly recommend you to go through this section in case you are not familiar with how Spark works with data sources.", 
            "title": "How can Spark Applications Connect to SnappyData using Spark JDBC?"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#pushing-entire-query-into-the-database", 
            "text": "When Spark queries are executed against external data sources, the current Spark model can only push down filters and projections in the query down to the database. If you are running an expensive aggregation on a large data set, then the entire data set is fetched into the Spark partitions, and the query is executed inside your Spark cluster. \nHowever, when you use a JDBC data source, you can pass entire queries or portions of the query entirely to the database such as shown in the following sample:  val pushdownQuery =  (select x, sum(y), max(z) from largeTable group by x order by x) t1 ;\nspark.read.jdbc(jdbcUrl, pushDownQuery, connectionProperties);", 
            "title": "Pushing Entire Query into the Database"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#deficiencies-in-the-spark-jdbc-connector", 
            "text": "Unfortunately, there are following limitations with Spark JDBC Connector which we address in the SnappyData JDBC Extension Connector.     Performance When an entire query is pushed down, Spark runs two queries:   First it runs the query that is supplied to fetch the result set metadata so that it knows the structure of the data frame that is returned to the application.  Secondly it runs the actual query.\nThe SnappyData connector internally figures out the structure of the result set without having to run multiple queries.     Lack of connection pooling  With no built-in support for pooling connections, every time a query is executed against a JDBC database, each of the partition in Spark has to set up a new connection which can be expensive. SnappyData internally uses an efficient pooled implementation with sensible defaults.    Data manipulation  While the Spark DataFrameWriter API can be used to append/insert a full dataset (dataframe) into the database, it is not simple to run the ad-hoc updates on the database including mass updates. The SnappyData JDBC Extension Connector makes this much simpler.    Usability  With the SnappyData JDBC Extension Connector, it is easier to deal with all the connection properties. You need not impair your application with sensitive properties dispersed in your app code.", 
            "title": "Deficiencies in the Spark JDBC Connector "
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#connecting-to-snappydata-using-the-jdbc-extension-connector", 
            "text": "Following is a sample of Spark JDBC Extension setup and usage:    Include the  snappydata-jdbc  package in the Spark job with spark-submit or spark-shell:       $SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2    Set the session properties.  The SnappyData connection properties (to enable auto-configuration of JDBC URL) and credentials can be provided in Spark configuration itself, or set later in SparkSession to avoid passing them in all the method calls. These properties can also be provided in  spark-defaults.conf  along with all the other Spark properties.  You can also set any of these properties in your app code.  Overloads of the above methods accepting  user+password  and  host+port  is also provided in case those properties are not set in the session or needs to be overridden. You can optionally pass additional connection properties similarly as in the  DataFrameReader.jdbc  method.  Following is a sample code of configuring the properties in  SparkConf :      $SPARK_HOME/bin/spark-shell --packages io.snappydata:snappydata-jdbc_2.11:1.0.2.2 --conf spark.snappydata.connection=localhost:1527 --conf spark.snappydata.user= user  --conf spark.snappydata.password= password     Import the required implicits in the job/shell code as follows:      import io.snappydata.sql.implicits._", 
            "title": "Connecting to SnappyData using the JDBC Extension Connector"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#running-queries", 
            "text": "Your application must import the SnappyData SQL implicits when using Scala.   import io.snappydata.sql.implicits._  Once the required session properties are set (connection and user/password as shown above), then one can run the required queries/DMLs without any other configuration.", 
            "title": "Running Queries"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#scala-query-example", 
            "text": "val spark =  create/access your spark session instance here  ;\nval dataset = spark.snappyQuery( select x, sum(y), max(z) from largeTable group by x order by x ) // query pushed down to SnappyData data cluster", 
            "title": "Scala Query Example"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#java-query-example", 
            "text": "import org.apache.spark.sql.*;\n\nJdbcExecute exec = new JdbcExecute( your SparkSession instance );\nDataFrame df = exec.snappyQuery( select x, sum(y), max(z) from largeTable group by x order by x ) // query pushed down to SnappyData data cluster   Note  Overloads of the above methods of accepting  user+password  and  host+port  is also provided in case those properties are not set in the session or need to be overridden. You can optionally pass additional connection properties such as in  DataFrameReader.jdbc  method.", 
            "title": "Java Query Example"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#updatingwriting-data-in-snappydata-tables", 
            "text": "Your application can use the Spark  DataFrameWriter  API to either insert or append data.  Also, for convenience, the connector provides an implicit in scala that is  import io.snappydata.sql.implicits._  for the DataFrameWriter to simplify writing to SnappyData. Hence, there is no need to explicitly set the connection properties.   After the required session properties are set (connection and user/password as shown above), then you can fire the required queries/DMLs without any other configuration.  Inserting a dataset from the job can also use the  snappy  extension to avoid passing in the URL and credentials explicitly:  df.write.snappy(\u201ctestTable1\u201d)  // You can use all the Spark writer APIs when using the snappy implicit. \nOr using explicit wrapper in Java: new JdbcWriter(spark.write).snappy(\u201ctestTable\u201d)", 
            "title": "Updating/Writing Data in SnappyData Tables"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#using-sql-dml-to-execute-ad-hoc-sql", 
            "text": "You can also use the  snappyExecute  method (see below) to run the arbitrary SQL DML statements directly on the database. You need not acquire/manage explicit JDBC connections or set properties.  \n// execute DDL\nspark.snappyExecute( create table testTable1 (id long, data string) using column )\n// DML\nspark.snappyExecute( insert into testTable1 values (1, \u2018data1\u2019) )\n// bulk insert from external table in embedded mode\nspark.snappyExecute( insert into testTable1 select * from externalTable1 )  When using Java, the wrapper has to be created explicitly as shown below:  import org.apache.spark.sql.*;\n\nJdbcExecute exec = new JdbcExecute(spark);\nexec.snappyExecute(\u201ccreate table testTable1 (id long, data string) using column\u201d);\nexec.snappyExecute( insert into testTable1 values (1, \u2018data1\u2019) );\n...", 
            "title": "Using SQL DML to Execute Ad-hoc SQL"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#comparison-with-current-spark-apis", 
            "text": "There is no equivalent of  snappyExecute  and one has to explicitly use JDBC API. For  snappyQuery , if you were to use Spark\u2019s JDBC connector directly, then the equivalent code would appear as follows (assuming snappyExecute equivalent was done beforehand using JDBC API or otherwise):  val jdbcUrl =  jdbc:snappydata:pool://localhost:1527 \nval connProps = new java.util.Properties()\nconnProps.setProperty( driver ,  io.snappydata.jdbc.ClientPoolDriver )\nconnProps.setProperty( user , userName)\nconnProps.setProperty( password , password)\n\nval df = spark.read.jdbc(jdbcUrl, \u201c(select count(*) from testTable1) q\u201d, connProps)  Besides being more verbose, this suffers from the problem of double query execution (first query to fetch query result metadata, followed by the actual query).", 
            "title": "Comparison with Current Spark APIs"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#api-reference", 
            "text": "The following extensions are used to implement the Spark JDBC Connector:   SparkSession.snappyQuery() This method creates a wrapper over SparkSession and is similar to  SparkSession.sql() API . The only difference between the both is that the entire query is pushed down to the SnappyData cluster. Hence, a query cannot have any temporary or external tables/views that are not visible in the SnappyData cluster.  SparkSession.snappyExecute()  Similarly, with this method, the SQL (assuming it to be a DML) is pushed to SnappyData, using the JDBC API, and returns the update count.  snappy  An implicit for DataFrameWriter named snappy simplifies the bulk writes. Therefore a write operation such as  session.write.jdbc()  becomes  session.write.snappy()  with the difference that JDBC URL, driver, and connection properties are auto-configured using session properties, if possible.", 
            "title": "API Reference"
        }, 
        {
            "location": "/programming_guide/spark_jdbc_connector/#performance-considerations", 
            "text": "It should be noted that using the Spark Data Source Writer or the Snappy Implicit are both much slower as compared to SnappyData embedded job or Smart Connector. If the DataFrame that is to be written is medium or large sized, then it is better to ingest directly in an embedded mode. In case writing an embedded job is not an option, the incoming DataFrame can be dumped to an external table in a location accessible to both Spark and SnappyData clusters. After this, it can be ingested in an embedded mode using  snappyExecute .", 
            "title": "Performance Considerations"
        }, 
        {
            "location": "/programming_guide/multiple_language_binding_using_thrift_protocol/", 
            "text": "Multiple Language Binding using Thrift Protocol\n\n\nSnappyData provides support for Apache Thrift protocol which enables users to access the cluster from other languages that are not supported directly by SnappyData.\nThrift allows efficient and reliable communication across programming languages like Java, Python, PHP, Ruby, Elixir, Perl and other languages. For more information on Thrift, refer to the \nApache Thrift documentation\n.\n\n\nThe JDBC driver for SnappyData that uses the \njdbc:snappydata://\n URL schema, now uses Thrift for underlying protocol. The older URL scheme for RowStore \njdbc:gemfirexd://\n continues to use the deprecated DRDA protocol.\n\n\nLikewise, locators and servers in SnappyData now default to starting up thrift servers and when started in RowStore mode (\nsnappy-start-all.sh rowstore\n) the DRDA servers are started as before.\n\n\nTo explicitly start a DRDA server in SnappyData, you can use the \n-drda-server-address\n and \n-drda-server-port\n options for the \nbind address\n and \nport\n respectively. Likewise, to explicitly start a Thrift server in RowStore mode, you can use the \n-thrift-server-address\n and \n-thrift-server-port\n options.\n\n\nRefer to the following documents for information on support provided by SnappyData:\n\n\n\n\n\n\nAbout SnappyData Thrift\n: Contains detailed information about the feature and its capabilities.\n\n\n\n\n\n\nThe Thrift Interface Definition Language (IDL)\n: This is a Thrift interface definition file for the SnappyData service.\n\n\n\n\n\n\nExample\n:\n Example of the Thrift definitions using the SnappyData Thrift IDL.", 
            "title": "Multiple Language Binding using Thrift Protocol"
        }, 
        {
            "location": "/programming_guide/multiple_language_binding_using_thrift_protocol/#multiple-language-binding-using-thrift-protocol", 
            "text": "SnappyData provides support for Apache Thrift protocol which enables users to access the cluster from other languages that are not supported directly by SnappyData.\nThrift allows efficient and reliable communication across programming languages like Java, Python, PHP, Ruby, Elixir, Perl and other languages. For more information on Thrift, refer to the  Apache Thrift documentation .  The JDBC driver for SnappyData that uses the  jdbc:snappydata://  URL schema, now uses Thrift for underlying protocol. The older URL scheme for RowStore  jdbc:gemfirexd://  continues to use the deprecated DRDA protocol.  Likewise, locators and servers in SnappyData now default to starting up thrift servers and when started in RowStore mode ( snappy-start-all.sh rowstore ) the DRDA servers are started as before.  To explicitly start a DRDA server in SnappyData, you can use the  -drda-server-address  and  -drda-server-port  options for the  bind address  and  port  respectively. Likewise, to explicitly start a Thrift server in RowStore mode, you can use the  -thrift-server-address  and  -thrift-server-port  options.  Refer to the following documents for information on support provided by SnappyData:    About SnappyData Thrift : Contains detailed information about the feature and its capabilities.    The Thrift Interface Definition Language (IDL) : This is a Thrift interface definition file for the SnappyData service.    Example :\n Example of the Thrift definitions using the SnappyData Thrift IDL.", 
            "title": "Multiple Language Binding using Thrift Protocol"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/", 
            "text": "Building SnappyData Applications using Spark API\n\n\nSnappySession Usage\n\n\nCreate Columnar Tables using API\n\n\nOther than \ncreate\n and \ndrop\n table, rest are all based on the Spark SQL Data Source APIs.\n\n\nScala\n\n\n val props = Map(\nBUCKETS\n -\n \n8\n)// Number of partitions to use in the SnappyStore\n\n case class Data(COL1: Int, COL2: Int, COL3: Int)\n\n val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n val rdd = spark.sparkContext.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\n\n val df = snappy.createDataFrame(rdd)\n\n // create a column table\n snappy.dropTable(\nCOLUMN_TABLE\n, ifExists = true)\n\n // \ncolumn\n is the table format (that is row or column)\n // dataDF.schema provides the schema for table\n snappy.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, df.schema, props)\n // append dataDF into the table\n df.write.insertInto(\nCOLUMN_TABLE\n)\n\n val results = snappy.sql(\nSELECT * FROM COLUMN_TABLE\n)\n println(\ncontents of column table are:\n)\n results.foreach(r =\n println(r))\n\n\n\n\nJava\n\n\n Map\nString, String\n props1 = new HashMap\n();\n props1.put(\nbuckets\n, \n16\n);\n\n JavaRDD\nRow\n jrdd = jsc.parallelize(Arrays.asList(\n  RowFactory.create(1, 2, 3),\n  RowFactory.create(7, 8, 9),\n  RowFactory.create(9, 2, 3),\n  RowFactory.create(4, 2, 3),\n  RowFactory.create(5, 6, 7)\n ));\n\n StructType schema = new StructType(new StructField[]{\n  new StructField(\ncol1\n, DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField(\ncol2\n, DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField(\ncol3\n, DataTypes.IntegerType, false, Metadata.empty()),\n });\n\n Dataset\nRow\n df = snappy.createDataFrame(jrdd, schema);\n\n// create a column table\n snappy.dropTable(\nCOLUMN_TABLE\n, true);\n\n// \ncolumn\n is the table format (that is row or column)\n// dataDF.schema provides the schema for table\n snappy.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, df.schema(), props1, false);\n// append dataDF into the table\n df.write().insertInto(\nCOLUMN_TABLE\n);\n\n Dataset\nRow\n  results = snappy.sql(\nSELECT * FROM COLUMN_TABLE\n);\n System.out.println(\ncontents of column table are:\n);\n for (Row r : results.select(\ncol1\n, \ncol2\n, \ncol3\n). collectAsList()) {\n   System.out.println(r);\n }\n\n\n\n\nPython\n\n\nfrom pyspark.sql.types import *\n\ndata = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]\nrdd = sc.parallelize(data)\nschema=StructType([StructField(\ncol1\n, IntegerType()),\n                   StructField(\ncol2\n, IntegerType()),\n                   StructField(\ncol3\n, IntegerType())])\n\ndataDF = snappy.createDataFrame(rdd, schema)\n\n# create a column table\nsnappy.dropTable(\nCOLUMN_TABLE\n, True)\n#\ncolumn\n is the table format (that is row or column)\n#dataDF.schema provides the schema for table\nsnappy.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, dataDF.schema, True, buckets=\n16\n)\n\n#append dataDF into the table\ndataDF.write.insertInto(\nCOLUMN_TABLE\n)\nresults1 = snappy.sql(\nSELECT * FROM COLUMN_TABLE\n)\n\nprint(\ncontents of column table are:\n)\nresults1.select(\ncol1\n, \ncol2\n, \ncol3\n). show()\n\n\n\n\nThe optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster is expanded) a bucket is the smallest unit that can be moved around. \nFor more details about the properties ('props1' map in above example) and \ncreateTable\n API refer to the documentation for \nrow and column tables\n.\n\n\nCreate Row Tables using API, Update the Contents of Row Table\n\n\n// create a row format table called ROW_TABLE\nsnappy.dropTable(\nROW_TABLE\n, ifExists = true)\n// \nrow\n is the table format\n// dataDF.schema provides the schema for table\nval props2 = Map.empty[String, String]\nsnappy.createTable(\nROW_TABLE\n, \nrow\n, dataDF.schema, props2)\n\n// append dataDF into the data\ndataDF.write.insertInto(\nROW_TABLE\n)\n\nval results2 = snappy.sql(\nselect * from ROW_TABLE\n)\nprintln(\ncontents of row table are:\n)\nresults2.foreach(println)\n\n// row tables can be mutated\n// for example update \nROW_TABLE\n and set col3 to 99 where\n// criteria \ncol3 = 3\n is true using update API\nsnappy.update(\nROW_TABLE\n, \nCOL3 = 3\n, org.apache.spark.sql.Row(99), \nCOL3\n )\n\nval results3 = snappy.sql(\nSELECT * FROM ROW_TABLE\n)\nprintln(\ncontents of row table are after setting col3 = 99 are:\n)\nresults3.foreach(println)\n\n// update rows using sql update statement\nsnappy.sql(\nUPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99\n)\nval results4 = snappy.sql(\nSELECT * FROM ROW_TABLE\n)\nprintln(\ncontents of row table are after setting col1 = 100 are:\n)\nresults4.foreach(println)\n\n\n\n\nSnappyStreamingContext Usage\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.\n\n\nScala\n\n\n import org.apache.spark.sql._\n import org.apache.spark.streaming._\n import scala.collection.mutable\n import org.apache.spark.rdd._\n import org.apache.spark.sql.types._\n import scala.collection.immutable.Map\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n val schema = StructType(List(StructField(\nid\n, IntegerType) ,StructField(\ntext\n, StringType)))\n\n case class ShowCaseSchemaStream (loc:Int, text:String)\n\n snsc.snappyContext.dropTable(\nstreamingExample\n, ifExists = true)\n snsc.snappyContext.createTable(\nstreamingExample\n, \ncolumn\n,  schema, Map.empty[String, String] , false)\n\n def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =\n ShowCaseSchemaStream( i, s\nText$i\n))\n\n val dstream = snsc.queueStream[ShowCaseSchemaStream](\n                 mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))\n\n val schemaDStream = snsc.createSchemaDStream(dstream )\n\n schemaDStream.foreachDataFrame(df =\n {\n     df.write.format(\ncolumn\n).\n     mode(SaveMode.Append).\n     options(Map.empty[String, String]).\n     saveAsTable(\nstreamingExample\n)    })\n\n snsc.start()\n snsc.sql(\nselect count(*) from streamingExample\n).show\n\n\n\n\nJava\n\n\n StructType schema = new StructType(new StructField[]{\n     new StructField(\nid\n, DataTypes.IntegerType, false, Metadata.empty()),\n     new StructField(\ntext\n, DataTypes.StringType, false, Metadata.empty())\n });\n\n Map\nString, String\n props = Collections.emptyMap();\n jsnsc.snappySession().dropTable(\nstreamingExample\n, true);\n jsnsc.snappySession().createTable(\nstreamingExample\n, \ncolumn\n, schema, props, false);\n\n Queue\nJavaRDD\nShowCaseSchemaStream\n rddQueue = new LinkedList\n();// Define a JavaBean named ShowCaseSchemaStream\n rddQueue.add(rddList(jsc, 1, 10));\n rddQueue.add(rddList(jsc, 10, 20));\n rddQueue.add(rddList(jsc, 20, 30));\n\n //rddList methods is defined as\n/* private static JavaRDD\nShowCaseSchemaStream\n rddList(JavaSparkContext jsc, int start, int end){\n    List\nShowCaseSchemaStream\n objs = new ArrayList\n();\n      for(int i= start; i\n=end; i++){\n        objs.add(new ShowCaseSchemaStream(i, String.format(\nText %d\n,i)));\n      }\n    return jsc.parallelize(objs);\n }*/\n\n JavaDStream\nShowCaseSchemaStream\n dStream = jsnsc.queueStream(rddQueue);\n SchemaDStream schemaDStream = jsnsc.createSchemaDStream(dStream, ShowCaseSchemaStream.class);\n\n schemaDStream.foreachDataFrame(new VoidFunction\nDataset\nRow\n() {\n   @Override\n   public void call(Dataset\nRow\n df) {\n     df.write().insertInto(\nstreamingExample\n);\n   }\n });\n\n jsnsc.start();\n\n jsnsc.sql(\nselect count(*) from streamingExample\n).show();\n\n\n\n\nPython\n\n\nfrom pyspark.streaming.snappy.context import SnappyStreamingContext\nfrom pyspark.sql.types import *\n\ndef  rddList(start, end):\n  return sc.parallelize(range(start,  end)).map(lambda i : ( i, \nText\n + str(i)))\n\ndef saveFunction(df):\n   df.write.format(\ncolumn\n).mode(\nappend\n).saveAsTable(\nstreamingExample\n)\n\nschema=StructType([StructField(\nloc\n, IntegerType()),\n                   StructField(\ntext\n, StringType())])\n\nsnsc = SnappyStreamingContext(sc, 1)\n\ndstream = snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])\n\nsnsc._snappycontext.dropTable(\nstreamingExample\n , True)\nsnsc._snappycontext.createTable(\nstreamingExample\n, \ncolumn\n, schema)\n\nschemadstream = snsc.createSchemaDStream(dstream, schema)\nschemadstream.foreachDataFrame(lambda df: saveFunction(df))\nsnsc.start()\ntime.sleep(1)\nsnsc.sql(\nselect count(*) from streamingExample\n).show()", 
            "title": "Building SnappyData Applications using Spark API"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#building-snappydata-applications-using-spark-api", 
            "text": "", 
            "title": "Building SnappyData Applications using Spark API"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#snappysession-usage", 
            "text": "", 
            "title": "SnappySession Usage"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#create-columnar-tables-using-api", 
            "text": "Other than  create  and  drop  table, rest are all based on the Spark SQL Data Source APIs.", 
            "title": "Create Columnar Tables using API"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#scala", 
            "text": "val props = Map( BUCKETS  -   8 )// Number of partitions to use in the SnappyStore\n\n case class Data(COL1: Int, COL2: Int, COL3: Int)\n\n val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n val rdd = spark.sparkContext.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\n\n val df = snappy.createDataFrame(rdd)\n\n // create a column table\n snappy.dropTable( COLUMN_TABLE , ifExists = true)\n\n //  column  is the table format (that is row or column)\n // dataDF.schema provides the schema for table\n snappy.createTable( COLUMN_TABLE ,  column , df.schema, props)\n // append dataDF into the table\n df.write.insertInto( COLUMN_TABLE )\n\n val results = snappy.sql( SELECT * FROM COLUMN_TABLE )\n println( contents of column table are: )\n results.foreach(r =  println(r))", 
            "title": "Scala"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#java", 
            "text": "Map String, String  props1 = new HashMap ();\n props1.put( buckets ,  16 );\n\n JavaRDD Row  jrdd = jsc.parallelize(Arrays.asList(\n  RowFactory.create(1, 2, 3),\n  RowFactory.create(7, 8, 9),\n  RowFactory.create(9, 2, 3),\n  RowFactory.create(4, 2, 3),\n  RowFactory.create(5, 6, 7)\n ));\n\n StructType schema = new StructType(new StructField[]{\n  new StructField( col1 , DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField( col2 , DataTypes.IntegerType, false, Metadata.empty()),\n  new StructField( col3 , DataTypes.IntegerType, false, Metadata.empty()),\n });\n\n Dataset Row  df = snappy.createDataFrame(jrdd, schema);\n\n// create a column table\n snappy.dropTable( COLUMN_TABLE , true);\n\n//  column  is the table format (that is row or column)\n// dataDF.schema provides the schema for table\n snappy.createTable( COLUMN_TABLE ,  column , df.schema(), props1, false);\n// append dataDF into the table\n df.write().insertInto( COLUMN_TABLE );\n\n Dataset Row   results = snappy.sql( SELECT * FROM COLUMN_TABLE );\n System.out.println( contents of column table are: );\n for (Row r : results.select( col1 ,  col2 ,  col3 ). collectAsList()) {\n   System.out.println(r);\n }", 
            "title": "Java"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#python", 
            "text": "from pyspark.sql.types import *\n\ndata = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]\nrdd = sc.parallelize(data)\nschema=StructType([StructField( col1 , IntegerType()),\n                   StructField( col2 , IntegerType()),\n                   StructField( col3 , IntegerType())])\n\ndataDF = snappy.createDataFrame(rdd, schema)\n\n# create a column table\nsnappy.dropTable( COLUMN_TABLE , True)\n# column  is the table format (that is row or column)\n#dataDF.schema provides the schema for table\nsnappy.createTable( COLUMN_TABLE ,  column , dataDF.schema, True, buckets= 16 )\n\n#append dataDF into the table\ndataDF.write.insertInto( COLUMN_TABLE )\nresults1 = snappy.sql( SELECT * FROM COLUMN_TABLE )\n\nprint( contents of column table are: )\nresults1.select( col1 ,  col2 ,  col3 ). show()  The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster is expanded) a bucket is the smallest unit that can be moved around. \nFor more details about the properties ('props1' map in above example) and  createTable  API refer to the documentation for  row and column tables .", 
            "title": "Python"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#create-row-tables-using-api-update-the-contents-of-row-table", 
            "text": "// create a row format table called ROW_TABLE\nsnappy.dropTable( ROW_TABLE , ifExists = true)\n//  row  is the table format\n// dataDF.schema provides the schema for table\nval props2 = Map.empty[String, String]\nsnappy.createTable( ROW_TABLE ,  row , dataDF.schema, props2)\n\n// append dataDF into the data\ndataDF.write.insertInto( ROW_TABLE )\n\nval results2 = snappy.sql( select * from ROW_TABLE )\nprintln( contents of row table are: )\nresults2.foreach(println)\n\n// row tables can be mutated\n// for example update  ROW_TABLE  and set col3 to 99 where\n// criteria  col3 = 3  is true using update API\nsnappy.update( ROW_TABLE ,  COL3 = 3 , org.apache.spark.sql.Row(99),  COL3  )\n\nval results3 = snappy.sql( SELECT * FROM ROW_TABLE )\nprintln( contents of row table are after setting col3 = 99 are: )\nresults3.foreach(println)\n\n// update rows using sql update statement\nsnappy.sql( UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99 )\nval results4 = snappy.sql( SELECT * FROM ROW_TABLE )\nprintln( contents of row table are after setting col1 = 100 are: )\nresults4.foreach(println)", 
            "title": "Create Row Tables using API, Update the Contents of Row Table"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#snappystreamingcontext-usage", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.", 
            "title": "SnappyStreamingContext Usage"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#scala_1", 
            "text": "import org.apache.spark.sql._\n import org.apache.spark.streaming._\n import scala.collection.mutable\n import org.apache.spark.rdd._\n import org.apache.spark.sql.types._\n import scala.collection.immutable.Map\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n val schema = StructType(List(StructField( id , IntegerType) ,StructField( text , StringType)))\n\n case class ShowCaseSchemaStream (loc:Int, text:String)\n\n snsc.snappyContext.dropTable( streamingExample , ifExists = true)\n snsc.snappyContext.createTable( streamingExample ,  column ,  schema, Map.empty[String, String] , false)\n\n def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =  ShowCaseSchemaStream( i, s Text$i ))\n\n val dstream = snsc.queueStream[ShowCaseSchemaStream](\n                 mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))\n\n val schemaDStream = snsc.createSchemaDStream(dstream )\n\n schemaDStream.foreachDataFrame(df =  {\n     df.write.format( column ).\n     mode(SaveMode.Append).\n     options(Map.empty[String, String]).\n     saveAsTable( streamingExample )    })\n\n snsc.start()\n snsc.sql( select count(*) from streamingExample ).show", 
            "title": "Scala"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#java_1", 
            "text": "StructType schema = new StructType(new StructField[]{\n     new StructField( id , DataTypes.IntegerType, false, Metadata.empty()),\n     new StructField( text , DataTypes.StringType, false, Metadata.empty())\n });\n\n Map String, String  props = Collections.emptyMap();\n jsnsc.snappySession().dropTable( streamingExample , true);\n jsnsc.snappySession().createTable( streamingExample ,  column , schema, props, false);\n\n Queue JavaRDD ShowCaseSchemaStream  rddQueue = new LinkedList ();// Define a JavaBean named ShowCaseSchemaStream\n rddQueue.add(rddList(jsc, 1, 10));\n rddQueue.add(rddList(jsc, 10, 20));\n rddQueue.add(rddList(jsc, 20, 30));\n\n //rddList methods is defined as\n/* private static JavaRDD ShowCaseSchemaStream  rddList(JavaSparkContext jsc, int start, int end){\n    List ShowCaseSchemaStream  objs = new ArrayList ();\n      for(int i= start; i =end; i++){\n        objs.add(new ShowCaseSchemaStream(i, String.format( Text %d ,i)));\n      }\n    return jsc.parallelize(objs);\n }*/\n\n JavaDStream ShowCaseSchemaStream  dStream = jsnsc.queueStream(rddQueue);\n SchemaDStream schemaDStream = jsnsc.createSchemaDStream(dStream, ShowCaseSchemaStream.class);\n\n schemaDStream.foreachDataFrame(new VoidFunction Dataset Row () {\n   @Override\n   public void call(Dataset Row  df) {\n     df.write().insertInto( streamingExample );\n   }\n });\n\n jsnsc.start();\n\n jsnsc.sql( select count(*) from streamingExample ).show();", 
            "title": "Java"
        }, 
        {
            "location": "/programming_guide/building_snappydata_applications_using_spark_api/#python_1", 
            "text": "from pyspark.streaming.snappy.context import SnappyStreamingContext\nfrom pyspark.sql.types import *\n\ndef  rddList(start, end):\n  return sc.parallelize(range(start,  end)).map(lambda i : ( i,  Text  + str(i)))\n\ndef saveFunction(df):\n   df.write.format( column ).mode( append ).saveAsTable( streamingExample )\n\nschema=StructType([StructField( loc , IntegerType()),\n                   StructField( text , StringType())])\n\nsnsc = SnappyStreamingContext(sc, 1)\n\ndstream = snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])\n\nsnsc._snappycontext.dropTable( streamingExample  , True)\nsnsc._snappycontext.createTable( streamingExample ,  column , schema)\n\nschemadstream = snsc.createSchemaDStream(dstream, schema)\nschemadstream.foreachDataFrame(lambda df: saveFunction(df))\nsnsc.start()\ntime.sleep(1)\nsnsc.sql( select count(*) from streamingExample ).show()", 
            "title": "Python"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/", 
            "text": "Tables in SnappyData\n\n\nRow and Column Tables\n\n\nColumn tables organize and manage data in memory in a compressed columnar form such that, modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\n\n\nRow tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location is determined by a hash function and hence is fast for point lookups or updates.\n\n\n\nCreate table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.\n\n\nDDL and DML Syntax for Tables\n\n\nCREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )    \n    USING [row | column]\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables.\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\n    DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set.\n    EXPIRE 'time_to_live_in_seconds',\n    COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table.\n    KEY_COLUMNS  'column_name,..', // Only for column table if putInto support is required\n    COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer \n 0 and \n 2GB. Only for column table.\n    )\n    [AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name\n\n\n\n\nRefer to the \nBest Practices\n section for more information on partitioning and colocating data and \nCREATE TABLE\n for information on creating a row/column table.\n\nDDL extensions are required to \nconfigure a table\n based on user requirements. \n\n\nYou can also define complex types (Map, Array and StructType) as columns for column tables. \n\n\nsnappy.sql(\nCREATE TABLE tableName (\ncol1 INT , \ncol2 Array\nDecimal\n, \ncol3 Map\nTimestamp, Struct\nx: Int, y: String, z: Decimal(10,5)\n, \ncol6 Struct\na: Int, b: String, c: Decimal(10,5)\n\n) USING column options(BUCKETS '8')\n )\n\n\n\n\nTo access the complex data from JDBC you can see \nJDBCWithComplexTypes\n for examples.\n\n\n\n\nNote\n\n\nClauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.\n\n\n\n\nSpark API for Managing Tables\n\n\nGet a reference to \nSnappySession\n:\n\n\nval snappy: SnappySession = new SnappySession(spark.sparkContext)\n\n\n\nCreate a SnappyStore table using Spark APIs\n\n\n    val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section\n    case class Data(col1: Int, col2: Int, col3: Int)\n    val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n    val rdd = sparkContext.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\n    val dataDF = snappy.createDataFrame(rdd)\n    snappy.createTable(\ncolumn_table\n, \ncolumn\n, dataDF.schema, props)\n    //or create a row format table\n    snappy.createTable(\nrow_table\n, \nrow\n, dataDF.schema, props)\n\n\n\n\nDrop a SnappyStore table using Spark APIs\n:\n\n\nsnappy.dropTable(tableName, ifExists = true)\n\n\n\nRestrictions on Column Tables\n\n\n\n\n\n\nColumn tables cannot specify any primary key, unique key constraints\n\n\n\n\n\n\nIndex on column table is not supported\n\n\n\n\n\n\nOption EXPIRE is not applicable for column tables\n\n\n\n\n\n\nOption EVICTION_BY with value LRUCOUNT is not applicable for column tables\n\n\n\n\n\n\nREAD_COMMITTED and REPEATABLE_READ isolation levels are not supported for column tables.\n\n\n\n\n\n\nDML Operations on Tables\n\n\nINSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\nUPDATE tablename SET column = value [, column = value ...] [WHERE expression]\nPUT INTO tableName (column, ...) VALUES (value, ...)\nDELETE FROM tablename1 [WHERE expression]\nTRUNCATE TABLE tablename1;\n\n\n\n\nAPI Extensions Provided in SnappyContext\n\n\nSeveral APIs have been added in \nSnappySession\n to manipulate data stored in row and column format. Apart from SQL, these APIs can be used to manipulate tables.\n\n\n//  Applicable for both row and column tables\ndef insert(tableName: String, rows: Row*): Int .\n\n// Only for row tables\ndef put(tableName: String, rows: Row*): Int\ndef update(tableName: String, filterExpr: String, newColumnValues: Row, \n           updateColumns: String*): Int\ndef delete(tableName: String, filterExpr: String): Int\n\n\n\n\nUsage SnappySession.insert()\n: Insert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r =\n\n  snappy.insert(\ntableName\n, Row.fromSeq(r))\n}\n\n\n\n\nUsage SnappySession.put()\n: Upsert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r =\n\n  snappy.put(tableName, Row.fromSeq(r))\n}\n\n\n\n\nUsage SnappySession.update(): Update all rows in table that match passed filter expression\n\n\nsnappy.update(tableName, \nITEMREF = 3\n , Row(99) , \nITEMREF\n )\n\n\n\n\nUsage SnappySession.delete()\n: Delete all rows in table that match passed filter expression\n\n\nsnappy.delete(tableName, \nITEMREF = 3\n)\n\n\n\n\nRow Buffers for Column Tables\n\n\nGenerally, the column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored.\n\n\nIn SnappyData, the column table consists of two components, delta row buffer and column store. SnappyData tries to support individual insert of a single row, as it is stored in a delta row buffer which is write optimized and highly available.\n\n\nOnce the size of buffer reaches the COLUMN_BATCH_SIZE set by the user, the delta row buffer is compressed column wise and stored in the column store.\nAny query on column table also takes into account the row cached buffer. By doing this, it ensures that the query does not miss any data.\n\n\nSQL Reference to the Syntax\n\n\nRefer to the \nSQL Reference Guide\n for information on the syntax.", 
            "title": "Tables in SnappyData"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#tables-in-snappydata", 
            "text": "", 
            "title": "Tables in SnappyData"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#row-and-column-tables", 
            "text": "Column tables organize and manage data in memory in a compressed columnar form such that, modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.   Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location is determined by a hash function and hence is fast for point lookups or updates.  Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.", 
            "title": "Row and Column Tables"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#ddl-and-dml-syntax-for-tables", 
            "text": "CREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )    \n    USING [row | column]\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables.\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\n    DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set.\n    EXPIRE 'time_to_live_in_seconds',\n    COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table.\n    KEY_COLUMNS  'column_name,..', // Only for column table if putInto support is required\n    COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer   0 and   2GB. Only for column table.\n    )\n    [AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name  Refer to the  Best Practices  section for more information on partitioning and colocating data and  CREATE TABLE  for information on creating a row/column table. \nDDL extensions are required to  configure a table  based on user requirements.   You can also define complex types (Map, Array and StructType) as columns for column tables.   snappy.sql( CREATE TABLE tableName (\ncol1 INT , \ncol2 Array Decimal , \ncol3 Map Timestamp, Struct x: Int, y: String, z: Decimal(10,5) , \ncol6 Struct a: Int, b: String, c: Decimal(10,5) \n) USING column options(BUCKETS '8')  )  To access the complex data from JDBC you can see  JDBCWithComplexTypes  for examples.   Note  Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.", 
            "title": "DDL and DML Syntax for Tables"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#spark-api-for-managing-tables", 
            "text": "Get a reference to  SnappySession :  val snappy: SnappySession = new SnappySession(spark.sparkContext)  Create a SnappyStore table using Spark APIs      val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section\n    case class Data(col1: Int, col2: Int, col3: Int)\n    val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n    val rdd = sparkContext.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\n    val dataDF = snappy.createDataFrame(rdd)\n    snappy.createTable( column_table ,  column , dataDF.schema, props)\n    //or create a row format table\n    snappy.createTable( row_table ,  row , dataDF.schema, props)  Drop a SnappyStore table using Spark APIs :  snappy.dropTable(tableName, ifExists = true)", 
            "title": "Spark API for Managing Tables"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#restrictions-on-column-tables", 
            "text": "Column tables cannot specify any primary key, unique key constraints    Index on column table is not supported    Option EXPIRE is not applicable for column tables    Option EVICTION_BY with value LRUCOUNT is not applicable for column tables    READ_COMMITTED and REPEATABLE_READ isolation levels are not supported for column tables.", 
            "title": "Restrictions on Column Tables"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#dml-operations-on-tables", 
            "text": "INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\nUPDATE tablename SET column = value [, column = value ...] [WHERE expression]\nPUT INTO tableName (column, ...) VALUES (value, ...)\nDELETE FROM tablename1 [WHERE expression]\nTRUNCATE TABLE tablename1;", 
            "title": "DML Operations on Tables"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#api-extensions-provided-in-snappycontext", 
            "text": "Several APIs have been added in  SnappySession  to manipulate data stored in row and column format. Apart from SQL, these APIs can be used to manipulate tables.  //  Applicable for both row and column tables\ndef insert(tableName: String, rows: Row*): Int .\n\n// Only for row tables\ndef put(tableName: String, rows: Row*): Int\ndef update(tableName: String, filterExpr: String, newColumnValues: Row, \n           updateColumns: String*): Int\ndef delete(tableName: String, filterExpr: String): Int  Usage SnappySession.insert() : Insert one or more [[org.apache.spark.sql.Row]] into an existing table  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r = \n  snappy.insert( tableName , Row.fromSeq(r))\n}  Usage SnappySession.put() : Upsert one or more [[org.apache.spark.sql.Row]] into an existing table  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r = \n  snappy.put(tableName, Row.fromSeq(r))\n}  Usage SnappySession.update(): Update all rows in table that match passed filter expression  snappy.update(tableName,  ITEMREF = 3  , Row(99) ,  ITEMREF  )  Usage SnappySession.delete() : Delete all rows in table that match passed filter expression  snappy.delete(tableName,  ITEMREF = 3 )", 
            "title": "API Extensions Provided in SnappyContext"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#row-buffers-for-column-tables", 
            "text": "Generally, the column table is used for analytical purpose. To this end, most of the operations (read or write) on it are bulk operations. Taking advantage of this fact the rows are compressed column wise and stored.  In SnappyData, the column table consists of two components, delta row buffer and column store. SnappyData tries to support individual insert of a single row, as it is stored in a delta row buffer which is write optimized and highly available.  Once the size of buffer reaches the COLUMN_BATCH_SIZE set by the user, the delta row buffer is compressed column wise and stored in the column store.\nAny query on column table also takes into account the row cached buffer. By doing this, it ensures that the query does not miss any data.", 
            "title": "Row Buffers for Column Tables"
        }, 
        {
            "location": "/programming_guide/tables_in_snappydata/#sql-reference-to-the-syntax", 
            "text": "Refer to the  SQL Reference Guide  for information on the syntax.", 
            "title": "SQL Reference to the Syntax"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/", 
            "text": "Stream Processing using SQL\n\n\nSnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. \nHere is a brief overview of \nSpark streaming\n from the Spark Streaming guide. \n\n\nSpark Streaming Overview\n\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like \nmap\n, \nreduce\n, \njoin\n and \nwindow\n.\n\n\nFinally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's \nmachine learning\n and \ngraph processing\n algorithms on data streams.\n\n\n\n\nInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n\n\n\nSpark Streaming provides a high-level abstraction called \ndiscretized stream\n or \nDStream\n, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of \nRDDs\n.\u2028\n\n\nAdditional details on the Spark Streaming concepts and programming is covered \nhere\n.\n\n\nSnappyData Streaming Extensions over Spark\n\n\nThe following enhancements over Spark Streaming are provided: \n\n\n\n\n\n\nManage Streams declaratively\n: Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. \n\n\n\n\n\n\nSQL based stream processing\n: With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. \n\n\n\n\n\n\nContinuous queries and time windows\n: Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, Spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, the standard SQL is extended to be able to specify the time window for the query. \n\n\n\n\n\n\nOLAP optimizations\n: By integrating and colocating stream processing with the hybrid in-memory storage engine, the product leverages the optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with RowStore.\n\n\n\n\n\n\nApproximate stream analytics\n: When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.\n\n\n\n\n\n\nWorking with Stream Tables\n\n\nSnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.\n\n\n// DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFINITION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream'\nOPTIONS (\n// multiple stream source specific options\n  storagelevel '',\n  rowConverter '',\n  subscribe '',\n  kafkaParams '',\n  consumerKey '',\n  consumerSecret '',\n  accessToken '',\n  accessTokenSecret '',\n  hostname '',\n  port '',\n  directory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT \nbatchInterval\n [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE]\n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP\n\n\n\n\nFor example to create a stream table using kafka source : \n\n\n val spark: SparkSession = SparkSession\n     .builder\n     .appName(\nSparkApp\n)\n     .master(\nlocal[4]\n)\n     .getOrCreate\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n\n snsc.sql(\ncreate stream table streamTable (userId string, clickStreamLog string) \n +\n     \nusing kafka_stream options (\n +\n     \nstoragelevel 'MEMORY_AND_DISK_SER_2', \n +\n     \nrowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \n +\n     \nkafkaParams 'zookeeper.connect-\nlocalhost:2181;auto.offset.reset-\nsmallest;group.id-\nmyGroupId', \n +\n     \nsubscribe 'streamTopic:01')\n)\n\n // You can get a handle of underlying DStream of the table\n val dStream = snsc.getSchemaDStream(\nstreamTable\n)\n\n // You can also save the DataFrames to an external table\n dStream.foreachDataFrame(_.write.insertInto(tableName))\n\n\n\n\nThe streamTable created in the above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries.\n\n\nStream SQL through snappy-sql\n\n\nStart a SnappyData cluster and connect through snappy-sql :\n\n\n//create a connection\nsnappy\n connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy\n streaming init 2secs;\n\n// Create a stream table\nsnappy\n create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy\n streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy\n select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy\n drop table streamTable;\n\n// Stop the streaming\nsnappy\n streaming stop;\n\n\n\n\nSchemaDStream\n\n\nSchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provides foreachDataFrame API. SchemaDStream can be registered as a table.\nSome of these ideas (especially naming our abstractions) were borrowed from \nIntel's Streaming SQL project\n.\n\n\nRegistering Continuous Queries\n\n\n//You can join two stream tables and produce a result stream.\nval resultStream = snsc.registerCQ(\nSELECT s1.id, s1.text FROM stream1 window (duration\n    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\n)\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto(\nyourTableName\n))\n\n\n\n\nDynamic (ad-hoc) Continuous Queries\n\n\nUnlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The continuous queries can be registered even after the \nSnappyStreamingContext\n has started.", 
            "title": "Stream Processing using SQL"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#stream-processing-using-sql", 
            "text": "SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. \nHere is a brief overview of  Spark streaming  from the Spark Streaming guide.", 
            "title": "Stream Processing using SQL"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#spark-streaming-overview", 
            "text": "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like  map ,  reduce ,  join  and  window .  Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's  machine learning  and  graph processing  algorithms on data streams.   Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.   Spark Streaming provides a high-level abstraction called  discretized stream  or  DStream , which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of  RDDs .\u2028  Additional details on the Spark Streaming concepts and programming is covered  here .", 
            "title": "Spark Streaming Overview"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#snappydata-streaming-extensions-over-spark", 
            "text": "The following enhancements over Spark Streaming are provided:     Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions.     SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams.     Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, Spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, the standard SQL is extended to be able to specify the time window for the query.     OLAP optimizations : By integrating and colocating stream processing with the hybrid in-memory storage engine, the product leverages the optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with RowStore.    Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.", 
            "title": "SnappyData Streaming Extensions over Spark"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#working-with-stream-tables", 
            "text": "SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.  // DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFINITION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream'\nOPTIONS (\n// multiple stream source specific options\n  storagelevel '',\n  rowConverter '',\n  subscribe '',\n  kafkaParams '',\n  consumerKey '',\n  consumerSecret '',\n  accessToken '',\n  accessTokenSecret '',\n  hostname '',\n  port '',\n  directory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT  batchInterval  [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE]\n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP  For example to create a stream table using kafka source :    val spark: SparkSession = SparkSession\n     .builder\n     .appName( SparkApp )\n     .master( local[4] )\n     .getOrCreate\n\n val snsc = new SnappyStreamingContext(spark.sparkContext, Duration(1))\n\n snsc.sql( create stream table streamTable (userId string, clickStreamLog string)   +\n      using kafka_stream options (  +\n      storagelevel 'MEMORY_AND_DISK_SER_2',   +\n      rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter',   +\n      kafkaParams 'zookeeper.connect- localhost:2181;auto.offset.reset- smallest;group.id- myGroupId',   +\n      subscribe 'streamTopic:01') )\n\n // You can get a handle of underlying DStream of the table\n val dStream = snsc.getSchemaDStream( streamTable )\n\n // You can also save the DataFrames to an external table\n dStream.foreachDataFrame(_.write.insertInto(tableName))  The streamTable created in the above example can be accessed from snappy-sql and can be queried using ad-hoc SQL queries.", 
            "title": "Working with Stream Tables"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#stream-sql-through-snappy-sql", 
            "text": "Start a SnappyData cluster and connect through snappy-sql :  //create a connection\nsnappy  connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy  streaming init 2secs;\n\n// Create a stream table\nsnappy  create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy  streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy  select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy  drop table streamTable;\n\n// Stop the streaming\nsnappy  streaming stop;", 
            "title": "Stream SQL through snappy-sql"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#schemadstream", 
            "text": "SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provides foreachDataFrame API. SchemaDStream can be registered as a table.\nSome of these ideas (especially naming our abstractions) were borrowed from  Intel's Streaming SQL project .", 
            "title": "SchemaDStream"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#registering-continuous-queries", 
            "text": "//You can join two stream tables and produce a result stream.\nval resultStream = snsc.registerCQ( SELECT s1.id, s1.text FROM stream1 window (duration\n    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id )\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto( yourTableName ))", 
            "title": "Registering Continuous Queries"
        }, 
        {
            "location": "/programming_guide/stream_processing_using_sql/#dynamic-ad-hoc-continuous-queries", 
            "text": "Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The continuous queries can be registered even after the  SnappyStreamingContext  has started.", 
            "title": "Dynamic (ad-hoc) Continuous Queries"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/", 
            "text": "User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)\n\n\nUsers can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. \nThe definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.\n\n\n\n\nNote\n\n\nSupport for UDF is available from SnappyData version release 0.8 onwards.\n\n\n\n\nCreate User Defined Function\n\n\nYou can simply extend any one of the interfaces in the package \norg.apache.spark.sql.api.java\n. \nThese interfaces can be included in your client application by adding \nsnappy-spark-sql_2.11-2.0.3-2.jar\n to your classpath.\n\n\nDefine a User Defined Function class\n\n\nThe number of the interfaces (UDF1 to UDF22) signifies the number of parameters a UDF can take.\n\n\n\n\nNote\n\n\nCurrently, any UDF which can take more than 22 parameters is not supported.\n\n\n\n\npackage some.package\nimport org.apache.spark.sql.api.java.UDF1\n\nclass StringLengthUDF extends UDF1[String, Int] {\n override def call(t1: String): Int = t1.length\n}\n\n\n\n\n \n\n\nCreate a User Defined Function\n\n\n\n\nNote\n\n\nPlace the jars used for creating persistent UDFs in a shared location (NFS, HDFS etc.) if you are configuring multiple leads for high availability. The same jar is used for DDL replay while the standby lead becomes the active lead.\n\n\n\n\nAfter defining a UDF you can bundle the UDF class in a JAR file and create the function by using \n./bin/snappy-sql\n of SnappyData. This creates a persistent entry in the catalog after which, you use the UDF.\n\n\nCREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'\n\n\n\n\nFor example:\n\n\nCREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'\n\n\n\n\nYou can write a JAVA or SCALA class to write a UDF implementation. \n\n\n\n\nNote\n\n\nFor input/output types: \n\nThe framework always returns the Java types to the UDFs. So, if you are writing \nscala.math.BigDecimal\n as an input type or output type, an exception is reported. You can use \njava.math.BigDecimal\n in the SCALA code. \n\n\n\n\nReturn Types to UDF program type mapping\n\n\n\n\n\n\n\n\nSnappyData Type\n\n\nUDF Type\n\n\n\n\n\n\n\n\n\n\nSTRING\n\n\njava.lang.String\n\n\n\n\n\n\nINTEGER\n\n\njava.lang.Integer\n\n\n\n\n\n\nLONG\n\n\njava.lang.Long\n\n\n\n\n\n\nDOUBLE\n\n\njava.lang.Double\n\n\n\n\n\n\nDECIMAL\n\n\njava.math.BigDecimal\n\n\n\n\n\n\nDATE\n\n\njava.sql.Date\n\n\n\n\n\n\nTIMESTAMP\n\n\njava.sql.Timestamp\n\n\n\n\n\n\nFLOAT\n\n\njava.lang.Float\n\n\n\n\n\n\nBOOLEAN\n\n\njava.lang.Boolean\n\n\n\n\n\n\nSHORT\n\n\njava.lang.Short\n\n\n\n\n\n\nBYTE\n\n\njava.lang.Byte\n\n\n\n\n\n\n\n\nUse a User Defined Function\n\n\nselect strnglen(string_column) from \ntable\n\n\n\n\n\nIf you try to use a UDF on a different type of column, for example, an \nInt\n column an exception is reported.\n\n\nDrop the Function\n\n\nDROP FUNCTION IF EXISTS udf_name\n\n\n\n\nFor example:\n\n\nDROP FUNCTION IF EXISTS app.strnglen\n\n\n\n\nModify an Existing User Defined Function\n\n\n1) Drop the existing UDF\n\n\n2) Modify the UDF code and \ncreate a new UDF\n. You can create the UDF with the same name as that of the dropped UDF.\n\n\nCreate User Defined Aggregate Functions\n\n\nSnappyData uses the same interface as that of Spark to define a User Defined Aggregate Function  \norg.apache.spark.sql.expressions.UserDefinedAggregateFunction\n. For more information refer to this \ndocument\n.\n\n\nKnown Limitations\n\n\n\n\n\n\nIn the current version of the product, setting schema over a JDBC connection (using the \nset schema\n command) or SnappySession (using \nSnappySession.setSchema\n API) does not work in all scenarios. Even if the schema is set, the operations are occasionally performed in the default \nAPP\n schema. \nAs a workaround, you can qualify the schemaname with tablename. \n \nFor example, to select all rows from table 't1' in schema 'schema1', use query \nselect * from schema1.t1\n\n\n\n\n\n\nIn the current version of the product, user defined functions are not displayed when you run the SHOW FUNCTIONS command in SnappyData shell. This will be available in the future releases.", 
            "title": "User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#user-defined-functions-udf-and-user-defined-aggregate-functions-udaf", 
            "text": "Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. \nThe definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.   Note  Support for UDF is available from SnappyData version release 0.8 onwards.", 
            "title": "User Defined Functions (UDF) and User Defined Aggregate Functions (UDAF)"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#create-user-defined-function", 
            "text": "You can simply extend any one of the interfaces in the package  org.apache.spark.sql.api.java . \nThese interfaces can be included in your client application by adding  snappy-spark-sql_2.11-2.0.3-2.jar  to your classpath.", 
            "title": "Create User Defined Function"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#define-a-user-defined-function-class", 
            "text": "The number of the interfaces (UDF1 to UDF22) signifies the number of parameters a UDF can take.   Note  Currently, any UDF which can take more than 22 parameters is not supported.   package some.package\nimport org.apache.spark.sql.api.java.UDF1\n\nclass StringLengthUDF extends UDF1[String, Int] {\n override def call(t1: String): Int = t1.length\n}", 
            "title": "Define a User Defined Function class"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#create-a-user-defined-function", 
            "text": "Note  Place the jars used for creating persistent UDFs in a shared location (NFS, HDFS etc.) if you are configuring multiple leads for high availability. The same jar is used for DDL replay while the standby lead becomes the active lead.   After defining a UDF you can bundle the UDF class in a JAR file and create the function by using  ./bin/snappy-sql  of SnappyData. This creates a persistent entry in the catalog after which, you use the UDF.  CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'  For example:  CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'  You can write a JAVA or SCALA class to write a UDF implementation.    Note  For input/output types:  \nThe framework always returns the Java types to the UDFs. So, if you are writing  scala.math.BigDecimal  as an input type or output type, an exception is reported. You can use  java.math.BigDecimal  in the SCALA code.    Return Types to UDF program type mapping     SnappyData Type  UDF Type      STRING  java.lang.String    INTEGER  java.lang.Integer    LONG  java.lang.Long    DOUBLE  java.lang.Double    DECIMAL  java.math.BigDecimal    DATE  java.sql.Date    TIMESTAMP  java.sql.Timestamp    FLOAT  java.lang.Float    BOOLEAN  java.lang.Boolean    SHORT  java.lang.Short    BYTE  java.lang.Byte", 
            "title": "Create a User Defined Function"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#use-a-user-defined-function", 
            "text": "select strnglen(string_column) from  table   If you try to use a UDF on a different type of column, for example, an  Int  column an exception is reported.", 
            "title": "Use a User Defined Function"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#drop-the-function", 
            "text": "DROP FUNCTION IF EXISTS udf_name  For example:  DROP FUNCTION IF EXISTS app.strnglen", 
            "title": "Drop the Function"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#modify-an-existing-user-defined-function", 
            "text": "1) Drop the existing UDF  2) Modify the UDF code and  create a new UDF . You can create the UDF with the same name as that of the dropped UDF.", 
            "title": "Modify an Existing User Defined Function"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#create-user-defined-aggregate-functions", 
            "text": "SnappyData uses the same interface as that of Spark to define a User Defined Aggregate Function   org.apache.spark.sql.expressions.UserDefinedAggregateFunction . For more information refer to this  document .", 
            "title": "Create User Defined Aggregate Functions"
        }, 
        {
            "location": "/programming_guide/udf_and_udaf/#known-limitations", 
            "text": "In the current version of the product, setting schema over a JDBC connection (using the  set schema  command) or SnappySession (using  SnappySession.setSchema  API) does not work in all scenarios. Even if the schema is set, the operations are occasionally performed in the default  APP  schema. \nAs a workaround, you can qualify the schemaname with tablename.   \nFor example, to select all rows from table 't1' in schema 'schema1', use query  select * from schema1.t1    In the current version of the product, user defined functions are not displayed when you run the SHOW FUNCTIONS command in SnappyData shell. This will be available in the future releases.", 
            "title": "Known Limitations"
        }, 
        {
            "location": "/consistency/transactions_about/", 
            "text": "Overview of SnappyData Distributed Transactions\n\n\nSnappyData supports transaction characteristics of isolation and atomicity. Transactions are supported using JDBC/ODBC through statements such as SET \nautocommit\n, \nSET Isolation\n, \nCOMMIT\n, and \nROLLBACK\n.  \n\n\n\n\nNote\n\n\n\n\n\n\nFull distributed transactions (that is, multiple update SQL statements in one logical transaction) is currently supported only for row tables.\n\n\n\n\n\n\nColumn tables only support single statement implicit transactions. That is, every DML (insert/update/delete) statement is executed in a implicit transaction. The DML statement can in-fact be a multi-row statement and is executed with \"all or nothing\" semantics.\n\n\n\n\n\n\nTransactions execution do not depend on a central locking facility and is highly scalable.\n\n\n\n\n\n\nSnappyData supports high concurrency for transactions. Readers (queries) do not acquire locks and isolated from concurrent transactions using an MVCC implementation.\n\n\n\n\n\n\nCurrently, demarcated transactions (Commit, rollback) is only supported through the JDBC and ODBC API. Support for commit/rollback will be added to the Spark API will be added in a later release.\n\n\n\n\n\n\n\n\nAdditional Information\n\n\n\n\n\n\nHow to use Transactions Isolation Levels\n\n\n\n\n\n\nBest Practices for using Distributed Transactions and Snapshot Isolation", 
            "title": "Distributed Transactions"
        }, 
        {
            "location": "/consistency/transactions_about/#overview-of-snappydata-distributed-transactions", 
            "text": "SnappyData supports transaction characteristics of isolation and atomicity. Transactions are supported using JDBC/ODBC through statements such as SET  autocommit ,  SET Isolation ,  COMMIT , and  ROLLBACK .     Note    Full distributed transactions (that is, multiple update SQL statements in one logical transaction) is currently supported only for row tables.    Column tables only support single statement implicit transactions. That is, every DML (insert/update/delete) statement is executed in a implicit transaction. The DML statement can in-fact be a multi-row statement and is executed with \"all or nothing\" semantics.    Transactions execution do not depend on a central locking facility and is highly scalable.    SnappyData supports high concurrency for transactions. Readers (queries) do not acquire locks and isolated from concurrent transactions using an MVCC implementation.    Currently, demarcated transactions (Commit, rollback) is only supported through the JDBC and ODBC API. Support for commit/rollback will be added to the Spark API will be added in a later release.     Additional Information    How to use Transactions Isolation Levels    Best Practices for using Distributed Transactions and Snapshot Isolation", 
            "title": "Overview of SnappyData Distributed Transactions"
        }, 
        {
            "location": "/consistency/using_transactions_row/", 
            "text": "How Transactions Work for Row Tables\n\n\n\n\nNote\n\n\nDistributed transaction is supported only for row tables.\n\n\n\n\nThere is no centralized transaction coordinator in SnappyData. Instead, the member on which a transaction was started acts as the coordinator for the duration of the transaction. If the application updates one or more rows, the transaction coordinator determines which owning members are involved, and acquires local \"write\" locks on all of the copies of the rows. At commit time, all changes are applied to the local store and any redundant copies. If another concurrent transaction attempts to change one of the rows, the local \"write\" acquisition fails for the row, and that transaction is automatically rolled back.\n\n\nUnlike traditional distributed databases, SnappyData does not use write-ahead logging for transaction recovery in case the commit fails during replication or redundant updates to one or more members. The most likely failure scenario is one where the member is unhealthy and gets forced out of the distributed system, guaranteeing the consistency of the data. When the failed member comes back online, it automatically recovers the replicated/redundant data set and establishes coherency with the other members. If all copies of some data go down before the commit is issued, then this condition is detected using the group membership system, and the transaction is rolled back automatically on all members.\n\n\n\n\nNote\n\n\nSnappyData does not support transactions is new data store members are added while in progress. If you add a new member to the cluster in the middle of a transaction and the new member is involved in the transaction (e.g. owns a partition of the data or is a replica), SnappyData implicitly rolls back the transaction and throws an SQLException (SQLState: \"X0Z05\").\n\n\n\n\nThe following images represent the functioning of read and write operations in the transaction model:\n\n\n\n\n\n\n\n\nUsing Transactions for Row Tables\n\n\nTransactions specify an \nisolation level\n that defines the degree to which one transaction must be isolated from resource or data modifications made by other transactions. The transaction isolation levels define the type of locks acquired on read operations. Only one of the isolation level options can be set at a time, and it remains set for that connection until it is explicitly changed.\n\n\n\n\nNote\n\n\n\n\n\n\nIf you set the isolation level to \nREAD_COMMITTED\n or \nREPEATABLE_READ\n, queries on column table report an error if \nautocommit\n is set to off (false). \nQueries on column tables are supported when isolation level is set to \nREAD_COMMITTED\n or \nREPEATABLE_READ\n and autocommit is set to \ntrue\n.\n\n\n\n\n\n\nDDL execution (for example \nCREATE TABLE\n /\nDROP TABLE\n) is not allowed when \nautocommit\n is set to \nfalse\n  and transaction isolation level is \nREAD_COMMITTED\n or \nREPEATABLE_READ\n.  DDL commands reports syntax error in such cases. DDL execution is allowed if \nautocommit\n is \ntrue\n for \nREAD_COMMITTED\n or \nREPEATABLE_READ\n isolation levels.\n\n\n\n\n\n\n\n\nThe following isolation levels are supported for row tables:\n\n\n\n\n\n\n\n\nIsolation level\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNONE\n\n\nDefault isolation level. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads.\n\n\n\n\n\n\nREAD_COMMITTED\n\n\nSnappyData ensures that ongoing transactional as well as non-transactional (isolation-level NONE) operations never read uncommitted (dirty) data. SnappyData accomplishes this by maintaining transactional changes in a separate transaction state that are applied to the actual data-store for the table only at commit time. SnappyData detects only Write-Write conflicts while in READ_COMMITTED isolation level. \nIn READ COMMITTED, a read view is created at the start of each statement and lasts only as long as each statement execution.\n\n\n\n\n\n\nREPEATABLE_READ\n\n\nIn this isolation level, a lock-based concurrency control DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction. In REPEATABLE READ every lock acquired during a transaction is held for the duration of the transaction.\n\n\n\n\n\n\n\n\nFor more information, see, \nSET ISOLATION\n\n\n\n\nRollback Behavior and Member Failures\n\n\nWithin the scope of a transaction, SnappyData automatically initiates a rollback if it encounters a constraint violation.\n\n\nAny errors that occur while parsing queries or while binding parameters in a SQL statement \ndo not\n cause a rollback. For example, a syntax error that occurs while executing a SQL statement does not cause previous statements in the transaction to rollback. However, a column constraint violation would cause all previous SQL operations in the transaction to roll back.\n\n\n\n\nHandling Member Failures\n\n\nThe following steps describe specific events that can occur depending on which member fails and when the failure occurs during a transaction:\n\n\n\n\n\n\nIf the transaction coordinator member fails before a commit is fired, then each of the cohort members aborts the ongoing transaction.\n\n\n\n\n\n\nIf a participating member fails before a commit is fired, then it is simply ignored. If the copies/replicas go to zero for certain keys, then any subsequent update operations on those keys throw an exception as in the case of non-transactional updates. If a commit is fired in this state, then the whole transaction is aborted.\n\n\n\n\n\n\nIf the transaction coordinator fails before completing the commit process (with or without sending the commit message to all cohorts), the surviving cohorts determine the outcome of the transaction.\n\n\nIf all of the cohorts are in the PREPARED state and successfully apply changes to the cache without any unique constraint violations, the transaction is committed on all cohorts. Otherwise, if any member reports failure or the last copy the associated rows go down during the PREPARED state, the transaction is rolled back on all cohorts.\n\n\n\n\n\n\nIf a participating member fails before acknowledging to the client, then the transaction continues on other members without any interruption. However, if that member contains the last copy of a table or bucket, then the transaction is rolled back.\n\n\n\n\n\n\nThe transaction coordinator might also fail while executing a rollback operation. In this case, the client would see such a failure as an SQLState error. If the client was performing a SELECT statement in a transaction, the member failure would result in SQLState error X0Z01::\n\n\nERROR X0Z01: Node 'node-name' went down or data no longer available while iterating the results (method 'rollback()'). Please retry the operation.\n\n\nClients that were performing a DML statement in the context of a transaction would fail with one of the SQLState errors: X0Z05, X0Z16, 40XD2, or 40XD0.\n\n\n\n\nNote\n\n\nOutside the scope of a transaction, a DML statement would not see an exception due to a member failure. Instead, the statement would be automatically retried on another SnappyData member. However, SELECT statements would receive the X0Z01 statement even outside of a transaction.\n\n\n\n\n\n\n\n\nIf this type of failure occurs, the remaining members of the SnappyData distributed system clean-up the open transactions for the failed node, and no additional steps are needed to recover from the failure.\n\n\n\n\nNote\n\n\nIn this release of SnappyData, a transaction fails if any of the cohorts depart abnormally. \n\n\n\n\n\n\nOther Rollback Scenarios\n\n\nSnappyData may cancel an executing statement due to low memory, a timeout, or a manual request to cancel the statement.\n\n\nIf a statement that is being executed within the context of a transaction is canceled due to low memory or a manual cancellation request, then SnappyData rolls back the associated transaction. \n\n\n\n\nNote\n\n\nSnappyData does not roll back a transaction if a statement is canceled due to a timeout.\n\n\n\n\n\n\nTransaction Functionality and Limitations\n\n\nIn this release of SnappyData, the scope for transactional functionality is:\n\n\n\n\n\n\nThe result set that is obtained from executing a query should either be completely consumed, or the result set is explicitly closed. Otherwise, DDL operations wait until the ResultSet is garbage-collected.\n\n\n\n\n\n\nTransactions for persistent tables are enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a row is always available (redundant members are available) in the event of member failures.\n\n\n\n\n\n\nSQL statements that implicitly place locks, such as \nselect for update\n, are not supported outside of transactions (default isolation level).\n\n\n\n\n\n\nThe supported isolation levels are 'READ COMMITTED' and 'READ UNCOMMITTED' where both behave as 'READ COMMITTED.' Autocommit is OFF by default in SnappyData, unlike in other JDBC drivers.\n\n\n\n\n\n\nTransactions always do \"write-write\" conflict detection at operation or commit time. Applications do not need to use \nselect for update\n or explicit locking to get this behavior, as compared to other databases. (\nselect for update\n is not supported outside of a transaction.)\n\n\n\n\n\n\nNested transactions and savepoints are not supported.\n\n\n\n\n\n\nSnappyData does not support transactions on partitioned tables that are configured with the DESTROY evict action. This restriction exists because the requirements of ACID transactions can conflict with the semantics of destroying evicted entries. For example, a transaction may need to update a number of entries that is greater than the amount allowed by the eviction setting. Transactions are supported with the OVERFLOW evict action because the required entries can be loaded into memory as necessary to support transaction semantics.\n\n\n\n\n\n\nSnappyData does not restrict concurrent non-transactional clients from updating tables that may be involved in transactions. This is by design, to maintain very high performance when no transactions are in use. If an application uses transactions on a table, make sure the application consistently uses transactions when updating that table.\n\n\n\n\n\n\nAll DML on a single row is atomic in nature inside or outside of transactions.\n\n\n\n\n\n\nThere is a small window during a commit when the committed set is being applied to the underlying table and concurrent readers, which do not consult any transactional state, have visibility to the partially-committed state. The larger the transaction, the larger the window. Also, transaction state is maintained in a memory-based buffer. The shorter and smaller the transaction, the less likely the transaction manager will run short on memory.\n\n\n\n\n\n\n\n\nTransactions with SELECT FOR UPDATE\n\n\nThe \nSELECT FOR UPDATE\n statement and other statements that implicitly place locks are not supported outside of a transaction (default isolation level).\n\n\nA SELECT FOR UPDATE begins by obtaining a read lock, which allows other transactions to possibly obtain read locks on the same data. A transaction's read lock is immediately upgraded to an exclusive write lock after a row is qualified for the SELECT FOR UPDATE statement. At this point, any other transactions that obtained a read lock on the data receive a conflict exception and can roll back and release their locks.\n\n\nThe transaction that has the exclusive lock can successfully commit only after all other read locks on the table have been released. In some cases, it is possible for one transaction to obtain an exclusive lock for data on one SnappyData member, while another transaction obtains an exclusive lock on a different member. In this case, both transactions will fail during the commit.", 
            "title": "How Transactions Work for Row Tables"
        }, 
        {
            "location": "/consistency/using_transactions_row/#how-transactions-work-for-row-tables", 
            "text": "Note  Distributed transaction is supported only for row tables.   There is no centralized transaction coordinator in SnappyData. Instead, the member on which a transaction was started acts as the coordinator for the duration of the transaction. If the application updates one or more rows, the transaction coordinator determines which owning members are involved, and acquires local \"write\" locks on all of the copies of the rows. At commit time, all changes are applied to the local store and any redundant copies. If another concurrent transaction attempts to change one of the rows, the local \"write\" acquisition fails for the row, and that transaction is automatically rolled back.  Unlike traditional distributed databases, SnappyData does not use write-ahead logging for transaction recovery in case the commit fails during replication or redundant updates to one or more members. The most likely failure scenario is one where the member is unhealthy and gets forced out of the distributed system, guaranteeing the consistency of the data. When the failed member comes back online, it automatically recovers the replicated/redundant data set and establishes coherency with the other members. If all copies of some data go down before the commit is issued, then this condition is detected using the group membership system, and the transaction is rolled back automatically on all members.   Note  SnappyData does not support transactions is new data store members are added while in progress. If you add a new member to the cluster in the middle of a transaction and the new member is involved in the transaction (e.g. owns a partition of the data or is a replica), SnappyData implicitly rolls back the transaction and throws an SQLException (SQLState: \"X0Z05\").   The following images represent the functioning of read and write operations in the transaction model:", 
            "title": "How Transactions Work for Row Tables"
        }, 
        {
            "location": "/consistency/using_transactions_row/#using-transactions-for-row-tables", 
            "text": "Transactions specify an  isolation level  that defines the degree to which one transaction must be isolated from resource or data modifications made by other transactions. The transaction isolation levels define the type of locks acquired on read operations. Only one of the isolation level options can be set at a time, and it remains set for that connection until it is explicitly changed.   Note    If you set the isolation level to  READ_COMMITTED  or  REPEATABLE_READ , queries on column table report an error if  autocommit  is set to off (false).  Queries on column tables are supported when isolation level is set to  READ_COMMITTED  or  REPEATABLE_READ  and autocommit is set to  true .    DDL execution (for example  CREATE TABLE  / DROP TABLE ) is not allowed when  autocommit  is set to  false   and transaction isolation level is  READ_COMMITTED  or  REPEATABLE_READ .  DDL commands reports syntax error in such cases. DDL execution is allowed if  autocommit  is  true  for  READ_COMMITTED  or  REPEATABLE_READ  isolation levels.     The following isolation levels are supported for row tables:     Isolation level  Description      NONE  Default isolation level. This corresponds to the JDBC TRANSACTION_NONE isolation level. At this level writes performed by a single thread are seen by all other threads in the order in which they were issued, but writes from different threads may be seen in a different order by other threads.    READ_COMMITTED  SnappyData ensures that ongoing transactional as well as non-transactional (isolation-level NONE) operations never read uncommitted (dirty) data. SnappyData accomplishes this by maintaining transactional changes in a separate transaction state that are applied to the actual data-store for the table only at commit time. SnappyData detects only Write-Write conflicts while in READ_COMMITTED isolation level.  In READ COMMITTED, a read view is created at the start of each statement and lasts only as long as each statement execution.    REPEATABLE_READ  In this isolation level, a lock-based concurrency control DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction. In REPEATABLE READ every lock acquired during a transaction is held for the duration of the transaction.     For more information, see,  SET ISOLATION", 
            "title": "Using Transactions for Row Tables"
        }, 
        {
            "location": "/consistency/using_transactions_row/#rollback-behavior-and-member-failures", 
            "text": "Within the scope of a transaction, SnappyData automatically initiates a rollback if it encounters a constraint violation.  Any errors that occur while parsing queries or while binding parameters in a SQL statement  do not  cause a rollback. For example, a syntax error that occurs while executing a SQL statement does not cause previous statements in the transaction to rollback. However, a column constraint violation would cause all previous SQL operations in the transaction to roll back.", 
            "title": "Rollback Behavior and Member Failures"
        }, 
        {
            "location": "/consistency/using_transactions_row/#handling-member-failures", 
            "text": "The following steps describe specific events that can occur depending on which member fails and when the failure occurs during a transaction:    If the transaction coordinator member fails before a commit is fired, then each of the cohort members aborts the ongoing transaction.    If a participating member fails before a commit is fired, then it is simply ignored. If the copies/replicas go to zero for certain keys, then any subsequent update operations on those keys throw an exception as in the case of non-transactional updates. If a commit is fired in this state, then the whole transaction is aborted.    If the transaction coordinator fails before completing the commit process (with or without sending the commit message to all cohorts), the surviving cohorts determine the outcome of the transaction.  If all of the cohorts are in the PREPARED state and successfully apply changes to the cache without any unique constraint violations, the transaction is committed on all cohorts. Otherwise, if any member reports failure or the last copy the associated rows go down during the PREPARED state, the transaction is rolled back on all cohorts.    If a participating member fails before acknowledging to the client, then the transaction continues on other members without any interruption. However, if that member contains the last copy of a table or bucket, then the transaction is rolled back.    The transaction coordinator might also fail while executing a rollback operation. In this case, the client would see such a failure as an SQLState error. If the client was performing a SELECT statement in a transaction, the member failure would result in SQLState error X0Z01::  ERROR X0Z01: Node 'node-name' went down or data no longer available while iterating the results (method 'rollback()'). Please retry the operation.  Clients that were performing a DML statement in the context of a transaction would fail with one of the SQLState errors: X0Z05, X0Z16, 40XD2, or 40XD0.   Note  Outside the scope of a transaction, a DML statement would not see an exception due to a member failure. Instead, the statement would be automatically retried on another SnappyData member. However, SELECT statements would receive the X0Z01 statement even outside of a transaction.     If this type of failure occurs, the remaining members of the SnappyData distributed system clean-up the open transactions for the failed node, and no additional steps are needed to recover from the failure.   Note  In this release of SnappyData, a transaction fails if any of the cohorts depart abnormally.", 
            "title": "Handling Member Failures"
        }, 
        {
            "location": "/consistency/using_transactions_row/#other-rollback-scenarios", 
            "text": "SnappyData may cancel an executing statement due to low memory, a timeout, or a manual request to cancel the statement.  If a statement that is being executed within the context of a transaction is canceled due to low memory or a manual cancellation request, then SnappyData rolls back the associated transaction.    Note  SnappyData does not roll back a transaction if a statement is canceled due to a timeout.", 
            "title": "Other Rollback Scenarios"
        }, 
        {
            "location": "/consistency/using_transactions_row/#transaction-functionality-and-limitations", 
            "text": "In this release of SnappyData, the scope for transactional functionality is:    The result set that is obtained from executing a query should either be completely consumed, or the result set is explicitly closed. Otherwise, DDL operations wait until the ResultSet is garbage-collected.    Transactions for persistent tables are enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a row is always available (redundant members are available) in the event of member failures.    SQL statements that implicitly place locks, such as  select for update , are not supported outside of transactions (default isolation level).    The supported isolation levels are 'READ COMMITTED' and 'READ UNCOMMITTED' where both behave as 'READ COMMITTED.' Autocommit is OFF by default in SnappyData, unlike in other JDBC drivers.    Transactions always do \"write-write\" conflict detection at operation or commit time. Applications do not need to use  select for update  or explicit locking to get this behavior, as compared to other databases. ( select for update  is not supported outside of a transaction.)    Nested transactions and savepoints are not supported.    SnappyData does not support transactions on partitioned tables that are configured with the DESTROY evict action. This restriction exists because the requirements of ACID transactions can conflict with the semantics of destroying evicted entries. For example, a transaction may need to update a number of entries that is greater than the amount allowed by the eviction setting. Transactions are supported with the OVERFLOW evict action because the required entries can be loaded into memory as necessary to support transaction semantics.    SnappyData does not restrict concurrent non-transactional clients from updating tables that may be involved in transactions. This is by design, to maintain very high performance when no transactions are in use. If an application uses transactions on a table, make sure the application consistently uses transactions when updating that table.    All DML on a single row is atomic in nature inside or outside of transactions.    There is a small window during a commit when the committed set is being applied to the underlying table and concurrent readers, which do not consult any transactional state, have visibility to the partially-committed state. The larger the transaction, the larger the window. Also, transaction state is maintained in a memory-based buffer. The shorter and smaller the transaction, the less likely the transaction manager will run short on memory.", 
            "title": "Transaction Functionality and Limitations"
        }, 
        {
            "location": "/consistency/using_transactions_row/#transactions-with-select-for-update", 
            "text": "The  SELECT FOR UPDATE  statement and other statements that implicitly place locks are not supported outside of a transaction (default isolation level).  A SELECT FOR UPDATE begins by obtaining a read lock, which allows other transactions to possibly obtain read locks on the same data. A transaction's read lock is immediately upgraded to an exclusive write lock after a row is qualified for the SELECT FOR UPDATE statement. At this point, any other transactions that obtained a read lock on the data receive a conflict exception and can roll back and release their locks.  The transaction that has the exclusive lock can successfully commit only after all other read locks on the table have been released. In some cases, it is possible for one transaction to obtain an exclusive lock for data on one SnappyData member, while another transaction obtains an exclusive lock on a different member. In this case, both transactions will fail during the commit.", 
            "title": "Transactions with SELECT FOR UPDATE"
        }, 
        {
            "location": "/consistency/using_snapshot_isolation_column/", 
            "text": "Lock-free Queries using MVCC (multi-version concurrency control) and Snapshot Isolation for Column Tables\n\n\n\n\nNote\n\n\nSnapshot isolation is supported only for column tables.\n\n\n\n\nAs the term suggests, all queries in the system operate on a snapshot view of the database. This is, even if concurrent updates are in progress, the querying system gets a non-changing view of the state of the database at the moment in time when the query is executed. The snapshot is partition wise. The snapshot of the partition is taken the moment the query accesses the partition. This behavior is set by default for column tables and cannot be modified.\n\n\n\n\nHow the Snapshot Model Works\n\n\nSnappyData maintains a version vector for each of the table on every node. The version information for each row of the table is also maintained.\n\n\nWhen a user query is executed, a snapshot is taken of the version vector of all the tables on the node on which the query is executed. The write operation modifies the row, increments its version while still maintaining a reference to the older version.\n\n\nAt the time of commit, the version information is published under a lock so that all the changes of an operation is published atomically. Older rows are cleaned periodically once it is made sure that there are no operations that require these older rows.\n\n\nThe read operations compare the version of each row to the ones in its snapshot and return the row whose version is same as the snapshot.\n\n\nIn case of failure, the versions are not published, which makes the rows invisible to any future operations. A new node joining the cluster copies all the committed rows from the existing node making sure that any snapshot will see only committed data.\n\n\nThe following image represents the functioning of read and write operations in the Snapshot isolation model:\n\n\n\n\nBy default, all individual operations (read/write) on column table have snapshot isolation with \nautocommit\n set to \nON\n. This means, in case of a failure the user operation fails and \nrollback\n is triggered. \n\nYou cannot set \nautocommit\n to \nOff\n. Snapshot isolation ensures that changes made, after the ongoing operation has taken a snapshot is not visible partially or totally.\n\nIf there are concurrent updates in a row, then the last commit is used.\n\n\n\n\nNote\n\n\nTo get per statement transactional behavior, all the write operations can span only one partition.\n\n\nHowever, if you have operations that span multiple partitions, then, ensure that:\n\n\n\n\n\n\nIn case of failure on one partition, the operation is retried on another copy of the same partition. Set \nredundancy\n to more than 0, if transactional behavior with operations spanning more than one partition is required.\n\n\n\n\n\n\nIf the operation fails on all the redundant copies of a partition and the same operation succeeds on some of the other partitions, then, partial rollback is initiated.\n\nIn this case, you can retry the operation at the application level.\n\n\n\n\n\n\n\n\n\n\nRollback Behavior and Member Failures\n\n\nIn column tables, roll back is performed in case of low memory. If the operation fails due to low memory, automatic roll back is initiated.\n\n\n\n\nSnapshot Limitations\n\n\nThe following limitations have been reported:\n\n\n\n\n\n\nFor column tables, snapshot isolation is enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a partition is always available (redundant members are available) in the event of member failures.\n\n\n\n\n\n\nWrite-write conflict is not detected. The last write option is applied.\n\n\n\n\n\n\nMulti-statement is not supported.\n\n\n\n\n\n\n\n\nSnapshot Isolation with SELECT FOR UPDATE\n\n\nThe \nSELECT FOR UPDATE\n statement and other statements that implicitly place locks are not supported for column tables, and snapshot isolation is applied by default for updates. In case of multiple concurrent updates, the last update is applied.", 
            "title": "Lock-free Queries using MVCC and Snapshot Isolation for Column Tables"
        }, 
        {
            "location": "/consistency/using_snapshot_isolation_column/#lock-free-queries-using-mvcc-multi-version-concurrency-control-and-snapshot-isolation-for-column-tables", 
            "text": "Note  Snapshot isolation is supported only for column tables.   As the term suggests, all queries in the system operate on a snapshot view of the database. This is, even if concurrent updates are in progress, the querying system gets a non-changing view of the state of the database at the moment in time when the query is executed. The snapshot is partition wise. The snapshot of the partition is taken the moment the query accesses the partition. This behavior is set by default for column tables and cannot be modified.", 
            "title": "Lock-free Queries using MVCC (multi-version concurrency control) and Snapshot Isolation for Column Tables"
        }, 
        {
            "location": "/consistency/using_snapshot_isolation_column/#how-the-snapshot-model-works", 
            "text": "SnappyData maintains a version vector for each of the table on every node. The version information for each row of the table is also maintained.  When a user query is executed, a snapshot is taken of the version vector of all the tables on the node on which the query is executed. The write operation modifies the row, increments its version while still maintaining a reference to the older version.  At the time of commit, the version information is published under a lock so that all the changes of an operation is published atomically. Older rows are cleaned periodically once it is made sure that there are no operations that require these older rows.  The read operations compare the version of each row to the ones in its snapshot and return the row whose version is same as the snapshot.  In case of failure, the versions are not published, which makes the rows invisible to any future operations. A new node joining the cluster copies all the committed rows from the existing node making sure that any snapshot will see only committed data.  The following image represents the functioning of read and write operations in the Snapshot isolation model:   By default, all individual operations (read/write) on column table have snapshot isolation with  autocommit  set to  ON . This means, in case of a failure the user operation fails and  rollback  is triggered.  \nYou cannot set  autocommit  to  Off . Snapshot isolation ensures that changes made, after the ongoing operation has taken a snapshot is not visible partially or totally. \nIf there are concurrent updates in a row, then the last commit is used.   Note  To get per statement transactional behavior, all the write operations can span only one partition.  However, if you have operations that span multiple partitions, then, ensure that:    In case of failure on one partition, the operation is retried on another copy of the same partition. Set  redundancy  to more than 0, if transactional behavior with operations spanning more than one partition is required.    If the operation fails on all the redundant copies of a partition and the same operation succeeds on some of the other partitions, then, partial rollback is initiated. \nIn this case, you can retry the operation at the application level.", 
            "title": "How the Snapshot Model Works"
        }, 
        {
            "location": "/consistency/using_snapshot_isolation_column/#rollback-behavior-and-member-failures", 
            "text": "In column tables, roll back is performed in case of low memory. If the operation fails due to low memory, automatic roll back is initiated.", 
            "title": "Rollback Behavior and Member Failures"
        }, 
        {
            "location": "/consistency/using_snapshot_isolation_column/#snapshot-limitations", 
            "text": "The following limitations have been reported:    For column tables, snapshot isolation is enabled by default, but the full range of fault tolerance is not yet implemented. It is assumed that at least one copy of a partition is always available (redundant members are available) in the event of member failures.    Write-write conflict is not detected. The last write option is applied.    Multi-statement is not supported.", 
            "title": "Snapshot Limitations"
        }, 
        {
            "location": "/consistency/using_snapshot_isolation_column/#snapshot-isolation-with-select-for-update", 
            "text": "The  SELECT FOR UPDATE  statement and other statements that implicitly place locks are not supported for column tables, and snapshot isolation is applied by default for updates. In case of multiple concurrent updates, the last update is applied.", 
            "title": "Snapshot Isolation with SELECT FOR UPDATE"
        }, 
        {
            "location": "/deployment/", 
            "text": "Affinity Modes\n\n\nIn this section, the various modes available for colocation of related data and computation is discussed.\n\n\nYou can run the SnappyData store in the following modes:\n\n\n\n\n\n\nLocal Mode\n: Used mainly for development, where the client application, the executors, and data store are all running in the same JVM\n\n\n\n\n\n\nEmbedded SnappyData Store Mode\n: The Spark computations and in-memory data store run colocated in the same JVM\n\n\n\n\n\n\nSnappyData Smart Connector Mode\n: Allows you to work with the SnappyData store cluster from any compatible Spark distribution", 
            "title": "Affinity Modes"
        }, 
        {
            "location": "/deployment/#affinity-modes", 
            "text": "In this section, the various modes available for colocation of related data and computation is discussed.  You can run the SnappyData store in the following modes:    Local Mode : Used mainly for development, where the client application, the executors, and data store are all running in the same JVM    Embedded SnappyData Store Mode : The Spark computations and in-memory data store run colocated in the same JVM    SnappyData Smart Connector Mode : Allows you to work with the SnappyData store cluster from any compatible Spark distribution", 
            "title": "Affinity Modes"
        }, 
        {
            "location": "/affinity_modes/local_mode/", 
            "text": "Local Mode\n\n\nIn this mode, you can execute all the components (client application, executors, and data store) locally in the application's JVM. It is the simplest way to start testing and using SnappyData, as you do not require a cluster, and the  executor threads are launched locally for processing.\n\n\nKey Points\n\n\n\n\n\n\nNo cluster required\n\n\n\n\n\n\nLaunch Single JVM (Single-node Cluster)\n\n\n\n\n\n\nLaunches executor threads locally for processing\n\n\n\n\n\n\nEmbeds the SnappyData in-memory store in-process\n\n\n\n\n\n\nFor development purposes only\n\n\n\n\n\n\n\n\nExample: Using the Local mode for developing SnappyData programs\n\n\nYou can use an IDE of your choice, and provide the below dependency to get SnappyData binaries:\n\n\nExample: Maven dependency\n\n\n!-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --\n\n\ndependency\n\n    \ngroupId\nio.snappydata\n/groupId\n\n    \nartifactId\nsnappydata-cluster_2.11\n/artifactId\n\n    \nversion\n1.0.2.1\n/version\n\n\n/dependency\n\n\n\n\n\nExample: SBT dependency\n\n\n// https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies += \nio.snappydata\n % \nsnappydata-cluster_2.11\n % \n1.0.2.1\n\n\n\n\n\nNote\n:\n\nIf your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file. \nAs a workaround, add the below code to the \nbuild.sbt\n:\n\n\nval workaround = {\n  sys.props += \npackaging.type\n -\n \njar\n\n  ()\n}\n\n\n\n\nFor more details, refer \nhttps://github.com/sbt/sbt/issues/3618\n.\n\n\nCreate SnappySession\n:\n\n\nTo start SnappyData store you need to create a SnappySession in your program:\n\n\n val spark: SparkSession = SparkSession\n         .builder\n         .appName(\nSparkApp\n)\n         .master(\nlocal[*]\n)\n         .getOrCreate\n val snappy = new SnappySession(spark.sparkContext)\n\n\n\n\nExample\n: \nLaunch Apache Spark shell and provide SnappyData dependency as a Spark package\n:\n\n\nIf you already have Spark2.0 installed in your local machine you can directly use \n--packages\n option to download the SnappyData binaries.\n\n\n./bin/spark-shell --packages \nSnappyDataInc:snappydata:1.0.2.1-s_2.11", 
            "title": "Local Mode"
        }, 
        {
            "location": "/affinity_modes/local_mode/#local-mode", 
            "text": "In this mode, you can execute all the components (client application, executors, and data store) locally in the application's JVM. It is the simplest way to start testing and using SnappyData, as you do not require a cluster, and the  executor threads are launched locally for processing.  Key Points    No cluster required    Launch Single JVM (Single-node Cluster)    Launches executor threads locally for processing    Embeds the SnappyData in-memory store in-process    For development purposes only     Example: Using the Local mode for developing SnappyData programs  You can use an IDE of your choice, and provide the below dependency to get SnappyData binaries:  Example: Maven dependency  !-- https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11 --  dependency \n     groupId io.snappydata /groupId \n     artifactId snappydata-cluster_2.11 /artifactId \n     version 1.0.2.1 /version  /dependency   Example: SBT dependency  // https://mvnrepository.com/artifact/io.snappydata/snappydata-cluster_2.11\nlibraryDependencies +=  io.snappydata  %  snappydata-cluster_2.11  %  1.0.2.1   Note : \nIf your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file.  As a workaround, add the below code to the  build.sbt :  val workaround = {\n  sys.props +=  packaging.type  -   jar \n  ()\n}  For more details, refer  https://github.com/sbt/sbt/issues/3618 .  Create SnappySession :  To start SnappyData store you need to create a SnappySession in your program:   val spark: SparkSession = SparkSession\n         .builder\n         .appName( SparkApp )\n         .master( local[*] )\n         .getOrCreate\n val snappy = new SnappySession(spark.sparkContext)  Example :  Launch Apache Spark shell and provide SnappyData dependency as a Spark package :  If you already have Spark2.0 installed in your local machine you can directly use  --packages  option to download the SnappyData binaries.  ./bin/spark-shell --packages  SnappyDataInc:snappydata:1.0.2.1-s_2.11", 
            "title": "Local Mode"
        }, 
        {
            "location": "/affinity_modes/embedded_mode/", 
            "text": "Embedded SnappyData Store Mode\n\n\nIn this mode, the Spark computations and in-memory data store run colocated in the same JVM. This is our out of the box configuration and suitable for most SnappyData real-time production environments. You launch SnappyData servers to bootstrap any data from disk, replicas or from external data sources.\nSpark executors are dynamically launched when the first Spark Job arrives.\n\n\nSome of the advantages of this mode are:\n\n\n\n\n\n\nHigh performance\n: All your Spark applications access the table data locally, in-process. The query engine accesses all the data locally by reference and avoids copying (which can be very expensive when working with large volumes).\n\n\n\n\n\n\nDriver High Availability\n: When Spark jobs are submitted, they can now run in an HA configuration. The submitted job becomes visible to a redundant \u201clead\u201d node that prevents the executors to go down when the Spark driver fails. Any submitted Spark job continues to run as long as there is at least one \u201clead\u201d node running.\n\n\n\n\n\n\nLess complex\n: There is only a single cluster to start, monitor, debug and tune.\n\n\n\n\n\n\n\n\nIn this mode, one can write Spark programs using jobs. For more details, refer to the \nSnappyData Jobs\n section.\n\n\nExample: Submit a Spark Job to the SnappyData Cluster\n\n\n./bin/snappy-job.sh submit --app-name JsonApp --class org.apache.spark.examples.snappydata.WorkingWithJson --app-jar examples/jars/quickstart.jar --lead [leadHost:port] --conf json_resource_folder=../../quickstart/src/main/resources\n\n\n\n\nAlso, you can use \nSnappySQL\n to create and query tables.\n\n\nYou can either \nstart SnappyData members\n using the \nsnappy-start-all.sh\n script or you can start them individually.\n\n\nHaving the Spark computation embedded in the same JVM allows us to do a number of optimization at query planning level. For example:\n\n\n\n\n\n\nIf the join expression matches the partitioning scheme of tables, a partition to partition join instead of a shuffle based join is done. \n Moreover, if two tables are colocated (while defining the tables) costly data movement can be avoided.\n\n\n\n\n\n\nFor replicated tables, that are present in all the data nodes, a simple local join (local look up)  is done instead of a broadcast join.\n\n\n\n\n\n\nSimilarly inserts to tables groups rows according to table partitioning keys, and route to the JVM hosting the partition. This results in higher ingestion rate.", 
            "title": "Embedded SnappyData Store Mode"
        }, 
        {
            "location": "/affinity_modes/embedded_mode/#embedded-snappydata-store-mode", 
            "text": "In this mode, the Spark computations and in-memory data store run colocated in the same JVM. This is our out of the box configuration and suitable for most SnappyData real-time production environments. You launch SnappyData servers to bootstrap any data from disk, replicas or from external data sources.\nSpark executors are dynamically launched when the first Spark Job arrives.  Some of the advantages of this mode are:    High performance : All your Spark applications access the table data locally, in-process. The query engine accesses all the data locally by reference and avoids copying (which can be very expensive when working with large volumes).    Driver High Availability : When Spark jobs are submitted, they can now run in an HA configuration. The submitted job becomes visible to a redundant \u201clead\u201d node that prevents the executors to go down when the Spark driver fails. Any submitted Spark job continues to run as long as there is at least one \u201clead\u201d node running.    Less complex : There is only a single cluster to start, monitor, debug and tune.     In this mode, one can write Spark programs using jobs. For more details, refer to the  SnappyData Jobs  section.  Example: Submit a Spark Job to the SnappyData Cluster  ./bin/snappy-job.sh submit --app-name JsonApp --class org.apache.spark.examples.snappydata.WorkingWithJson --app-jar examples/jars/quickstart.jar --lead [leadHost:port] --conf json_resource_folder=../../quickstart/src/main/resources  Also, you can use  SnappySQL  to create and query tables.  You can either  start SnappyData members  using the  snappy-start-all.sh  script or you can start them individually.  Having the Spark computation embedded in the same JVM allows us to do a number of optimization at query planning level. For example:    If the join expression matches the partitioning scheme of tables, a partition to partition join instead of a shuffle based join is done.   Moreover, if two tables are colocated (while defining the tables) costly data movement can be avoided.    For replicated tables, that are present in all the data nodes, a simple local join (local look up)  is done instead of a broadcast join.    Similarly inserts to tables groups rows according to table partitioning keys, and route to the JVM hosting the partition. This results in higher ingestion rate.", 
            "title": "Embedded SnappyData Store Mode"
        }, 
        {
            "location": "/affinity_modes/connector_mode/", 
            "text": "SnappyData Smart Connector Mode\n\n\nIn this mode, the Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. Conceptually, this is similar to how Spark applications work with stores like Cassandra, Redis etc. The Smart connector mode also implements several performance optimizations as described in this section.\n\n\nSpark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).\n\n\nSpecifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.\n\n\n\n\nKey Points:\n\n\n\n\n\n\nCan work with SnappyData store from a compatible Spark distribution (2.1.1)\n\n\n\n\n\n\nSpark application executes in its own independent JVM processes\n\n\n\n\n\n\nThe Spark application connects to SnappyData as a Spark Data source\n\n\n\n\n\n\nSupports any of the Spark supported resource managers (for example, Spark Standalone Manager, YARN or Mesos)\n\n\n\n\n\n\nSome of the advantages of this mode are:\n\n\nPerformance\n\nWhen Spark partitions store data in \ncolumn tables\n, the connector automatically attempts to localize the partitions into SnappyData store buckets on the local node. The connector uses the same column store format as well as compression techniques in Spark avoiding all data formatting related inefficiencies or unnecessary serialization costs. This is the fastest way to ingest data when Spark and the SnappyData cluster are operating as independent clusters.\n\n\nWhen storing to \nRow tables\n or when the partitioning in Spark is different than the partitioning configured on the table, data batches could be shuffled across nodes. Whenever Spark applications are writing to SnappyData tables, the data is always batched for the highest possible throughput.\n\n\nWhen queries are executed, while the entire query planning and execution is coordinated by the Spark engine (Catalyst), the smart connector still carries out a number of optimizations, which are listed below:\n\n\n\n\n\n\nRoute jobs to same machines as SnappyData data nodes if the executor nodes are co-hosted on the same machines as the data nodes. Job for each partition tries to fetch only from same machine data store where possible.\n\n\n\n\n\n\nColocated joins: If the underlying tables are colocated partition-wise, and executor nodes are co-hosting SnappyData data nodes, then the column batches are fetched from local machines and the join itself is partition-wise and does not require any exchange.\n\n\n\n\n\n\nOptimized column batch inserts like in the Embedded mode with job routing to same machines as data stores if possible.\n\n\n\n\n\n\n\n\nExample: Launch a Spark local mode cluster and use Smart Connector to access SnappyData cluster\n\n\nStep 1: Start the SnappyData cluster\n:\nYou can either start SnappyData members using the \nsnappy_start_all\n script or you can start them individually.\n\n\nStep 2: Launch the Apache Spark program\n\n\nIn the Local mode\n\n\n\n./bin/spark-shell  --master local[*] --conf spark.snappydata.connection=localhost:1527 --packages \nSnappyDataInc:snappydata:1.0.2.1-s_2.11\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThe \nspark.snappydata.connection\n property points to the locator of a running SnappyData cluster. The value of this property is a combination of locator host and JDBC client port on which the locator listens for connections (default is 1527).\n\n\n\n\n\n\nIn the Smart Connector mode, all \nsnappydata.*\n SQL configuration properties should be prefixed with \nspark\n. For example, \nspark.snappydata.column.batchSize\n.\n\n\n\n\n\n\n\n\nThis opens a Scala Shell.\n\n\nStep 3: Import any or all of the following:\n \n\n\n\n\nSQL Context\n\n\nSnappyContext\n\n\nSnappySession\n\n\n\n\nimport org.apache.spark.sql.{SQLContext,SnappyContext,SnappySession}\n\n\n\n\nThis starts the SnappyData cluster with Smart Connector mode. Create a SnappySession to interact with the SnappyData store.\n\n\n    // Create a SnappySession to work with SnappyData store\n    $scala \n val snSession = new SnappySession(spark.sparkContext)\n\n\n\n\nThe code example for writing a Smart Connector application program is located in \nSmartConnectorExample\n\n\nUsing External Cluster Manager\n\n\nCluster mode\n\n\n./bin/spark-submit --deploy-mode cluster --class somePackage.someClass  --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 --packages \nSnappyDataInc:snappydata:1.0.2.1-s_2.11\n\n\n\n\n\nClient mode\n\n\n./bin/spark-submit --class somePackage.someClass  --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 --packages \nSnappyDataInc:snappydata:1.0.2.1-s_2.11\n\n\n\n\n\nUsing YARN as a Cluster Manager\n\n\nCluster mode\n\n\n./spark-submit --master yarn  --deploy-mode cluster --conf spark.driver.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --conf spark.executor.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --class MainClass SampleProjectYarn.jar\n\n\n\n\nClient mode\n\n\n./spark-submit --master yarn  --deploy-mode client --conf spark.driver.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --conf spark.executor.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --class MainClass SampleProjectYarn.jar", 
            "title": "SnappyData Smart Connector Mode"
        }, 
        {
            "location": "/affinity_modes/connector_mode/#snappydata-smart-connector-mode", 
            "text": "In this mode, the Spark cluster executes in its own independent JVM processes and connects to SnappyData as a Spark data source. Conceptually, this is similar to how Spark applications work with stores like Cassandra, Redis etc. The Smart connector mode also implements several performance optimizations as described in this section.  Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).  Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.   Key Points:    Can work with SnappyData store from a compatible Spark distribution (2.1.1)    Spark application executes in its own independent JVM processes    The Spark application connects to SnappyData as a Spark Data source    Supports any of the Spark supported resource managers (for example, Spark Standalone Manager, YARN or Mesos)    Some of the advantages of this mode are:  Performance \nWhen Spark partitions store data in  column tables , the connector automatically attempts to localize the partitions into SnappyData store buckets on the local node. The connector uses the same column store format as well as compression techniques in Spark avoiding all data formatting related inefficiencies or unnecessary serialization costs. This is the fastest way to ingest data when Spark and the SnappyData cluster are operating as independent clusters.  When storing to  Row tables  or when the partitioning in Spark is different than the partitioning configured on the table, data batches could be shuffled across nodes. Whenever Spark applications are writing to SnappyData tables, the data is always batched for the highest possible throughput.  When queries are executed, while the entire query planning and execution is coordinated by the Spark engine (Catalyst), the smart connector still carries out a number of optimizations, which are listed below:    Route jobs to same machines as SnappyData data nodes if the executor nodes are co-hosted on the same machines as the data nodes. Job for each partition tries to fetch only from same machine data store where possible.    Colocated joins: If the underlying tables are colocated partition-wise, and executor nodes are co-hosting SnappyData data nodes, then the column batches are fetched from local machines and the join itself is partition-wise and does not require any exchange.    Optimized column batch inserts like in the Embedded mode with job routing to same machines as data stores if possible.     Example: Launch a Spark local mode cluster and use Smart Connector to access SnappyData cluster  Step 1: Start the SnappyData cluster :\nYou can either start SnappyData members using the  snappy_start_all  script or you can start them individually.  Step 2: Launch the Apache Spark program  In the Local mode  \n./bin/spark-shell  --master local[*] --conf spark.snappydata.connection=localhost:1527 --packages  SnappyDataInc:snappydata:1.0.2.1-s_2.11    Note    The  spark.snappydata.connection  property points to the locator of a running SnappyData cluster. The value of this property is a combination of locator host and JDBC client port on which the locator listens for connections (default is 1527).    In the Smart Connector mode, all  snappydata.*  SQL configuration properties should be prefixed with  spark . For example,  spark.snappydata.column.batchSize .     This opens a Scala Shell.  Step 3: Import any or all of the following:     SQL Context  SnappyContext  SnappySession   import org.apache.spark.sql.{SQLContext,SnappyContext,SnappySession}  This starts the SnappyData cluster with Smart Connector mode. Create a SnappySession to interact with the SnappyData store.      // Create a SnappySession to work with SnappyData store\n    $scala   val snSession = new SnappySession(spark.sparkContext)  The code example for writing a Smart Connector application program is located in  SmartConnectorExample  Using External Cluster Manager  Cluster mode  ./bin/spark-submit --deploy-mode cluster --class somePackage.someClass  --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 --packages  SnappyDataInc:snappydata:1.0.2.1-s_2.11   Client mode  ./bin/spark-submit --class somePackage.someClass  --master spark://localhost:7077 --conf spark.snappydata.connection=localhost:1527 --packages  SnappyDataInc:snappydata:1.0.2.1-s_2.11   Using YARN as a Cluster Manager  Cluster mode  ./spark-submit --master yarn  --deploy-mode cluster --conf spark.driver.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --conf spark.executor.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --class MainClass SampleProjectYarn.jar  Client mode  ./spark-submit --master yarn  --deploy-mode client --conf spark.driver.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --conf spark.executor.extraClassPath=/home/snappyuser/snappydata-0.6-SNAPSHOT-bin/jars/* --class MainClass SampleProjectYarn.jar", 
            "title": "SnappyData Smart Connector Mode"
        }, 
        {
            "location": "/best_practices/", 
            "text": "Best\u00a0Practices\n\n\nThe best practices section provides you guidelines for setting up your cluster and designing your database and the schema to use.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nTuning for Concurrency and Computation\n\n\n\n\n\n\nDesigning your Database and Schema\n\n\n\n\n\n\nMemory Management\n\n\n\n\n\n\nHA Considerations\n\n\n\n\n\n\nImportant Settings\n\n\n\n\n\n\n\n\nTip\n\n\nSnappyData Pulse is a web UI that displays information that can be used to analyse your query plan. For more details refer to \nSnappy Pulse\n.", 
            "title": "Best Practices"
        }, 
        {
            "location": "/best_practices/#best-practices", 
            "text": "The best practices section provides you guidelines for setting up your cluster and designing your database and the schema to use.  The following topics are covered in this section:    Tuning for Concurrency and Computation    Designing your Database and Schema    Memory Management    HA Considerations    Important Settings     Tip  SnappyData Pulse is a web UI that displays information that can be used to analyse your query plan. For more details refer to  Snappy Pulse .", 
            "title": "Best\u00a0Practices"
        }, 
        {
            "location": "/best_practices/setup_cluster/", 
            "text": "Tuning for Concurrency and Computation\n\n\nHandling Low Latency vs Analytic Jobs\n\n\nUnlike Spark, SnappyData can distinguish requests that are cheap (low latency) vs requests that require a lot of computational resources (high latency). This is done by a resource scheduler that can balance the needs of many contending users/threads.\n\n\nFor instance, when a SQL client executes a \u2018fetch by primary key\u2019 query, there is no need to involve any scheduler or spawn many tasks for such a simple request. The request is immediately delegated to the data node (single thread) and the response is directly sent to the requesting client (probably within a few milliseconds). \n\nIn the current version of the product, all query requests that filter on a primary key, a set of keys, or can directly filter using an index are executed without routing to the SnappyData scheduler. Only Row tables can have primary keys or indexes. \n\n\nWhen the above conditions are not met, the request is routed to the \u2018Lead\u2019 node where the Spark plan is generated and \u2018jobs\u2019 are scheduled for execution. The scheduler uses a FAIR scheduling algorithm for higher concurrency, that is, all concurrent jobs are executed in a round-robin manner.\n\n\nEach job is made up of one or more stages and the planning phase computes the number of  parallel tasks for the stage. Tasks from scheduled jobs are then allocated to the logical cores available until all cores are allocated. \nA round-robin algorithm picks a task from Job1, a task from Job2 and so on. If more cores are available, the second task from Job1 is picked and the cycle continues. But, there are circumstances a single job can completely consume all cores.\n\nFor instance, when all cores are available, if a large loading job is scheduled it receives all available cores of which, each of the tasks can be long running. During this time, if other concurrent jobs are assigned, none of the executing tasks is preempted.\n\n\n\n\nNote\n\n\nThis above scheduling logic is applicable only when queries are fully managed by SnappyData cluster. When running your application using the smart connector, each task running in the Spark cluster directly accesses the store partitions.\n\n\n\n\nComputing the Number of Cores for a Job\n\n\nExecuting queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job has multiple tasks. The number of tasks is determined by the number of partitions of the underlying data.\n\nConcurrency in SnappyData is tightly bound with the capacity of the cluster, which means, the number of cores available in the cluster determines the number of concurrent tasks that can be run.\n\n\nThe default setting is \nCORES = 2 X number of cores on a machine\n.\n\n\nIt is recommended to use 2 X number of cores on a machine. If more than one server is running on a machine, the cores should be divided accordingly and specified using the \nspark.executor.cores\n property.\n\nspark.executor.cores\n is used to override the number of cores per server.\n\n\nFor example, for a cluster with 2 servers running on two different machines with  4 CPU cores each, a maximum number of tasks that can run concurrently is 16. \n \nIf a table has 16 partitions (buckets, for the row or column tables), a scan query on this table creates 16 tasks. This means, 16 tasks run concurrently and the last task will run when one of these 16 tasks has finished execution.\n\n\nSnappyData uses an optimization method which clubs multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions. \n\n\nIn SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get a fair distribution of resources. \n\n\nFor example, In the image below, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently.\n\n\nPending tasks have to wait for completion of the current tasks and are assigned to the core that is first available.\n\n\nWhen you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute.\n\n\n\n\nConfiguring the Scheduler Pools for Concurrency\n\n\nSnappyData out of the box comes configured with two execution pools:\n\n\n\n\n\n\nLow-latency pool\n: This pool is automatically used when SnappyData determines that a request is of low latency, that is, the queries that are partition pruned to two or fewer partitions. \n\n\n\n\n\n\nDefault pool\n: This is the pool that is used for the remaining requests.\n\n\n\n\n\n\nTwo cores are statically assigned to the low latency pool. Also, the low latency pool has weight twice that of the default pool. Thus, if there are 30 cores available to an executor for a query that has 30 partitions, only 28 would be assigned to it and two cores would be reserved to not starve the low latency queries. When the system has both low latency and normal queries, 20 cores are used for the low latency queries as it has higher priority (weight=2).\nIf a query requires all 30 partitions and no low latency queries are running at that time, all 30 cores are assigned to the first query. However, when a low latency query is assigned, the scheduler does its best to allocate cores as soon as tasks from the earlier query finish.\n\n\n\nApplications can explicitly configure to use a particular pool for the current session using a SQL configuration property, \nsnappydata.scheduler.pool\n. For example, the \nset snappydata.scheduler.pool=lowlatency\n command sets the pool as low latency pool for the current session. \n\n\nNew pools can be added and properties of the existing pools can be configured by modifying the \nconf/fairscheduler.xml\n file. We do not recommend changing the pool names (\ndefault\n and \nlowlatency\n).\n\n\nControlling CPU Usage for User Jobs\n\n\nYou can control the CPU usage for user jobs by configuring separate pools for different kinds of jobs. \n See configuration \nhere\n. \n\nThe product is configured with two out-of-the-box pools, that is the \nDefault pool\n and the \nLow-latency pool\n. The \nDefault pool\n has higher priority and also has a \nminShare\n, so that some minimum cores are reserved for those jobs if possible. \nThe \nStages\n tab on the SnappyData Pulse UI shows the available pools.  When you track a job for an SQL query on the \nSQL\n tab, it shows the pool that is used in the \nPool Name\n column. In-built tasks such as ingestion can show lower priority pools by default to give priority to foreground queries. To configure such priority, do the following: \n\n\n\n\nDefine the pools in \nconf/fairscheduler.xml\n \n\n\nSet a pool for a job using Spark API  or use \nset snappydata.scheduler.pool\n property in a SnappySession.\n\n\n\n\nTo configure the priority based on specific requirements, you can also either permit the users to set the priority for queries or add some pool allocation logic in the application as per client requirements.\n\n\nUsing a Partitioning Strategy to Increase Concurrency\n\n\nThe best way to increasing concurrency is to design your schema such that you minimize the need to run your queries across many partitions. The common strategy is to understand your application patterns and choose a partitioning strategy such that queries often target a specific partition. Such queries will be pruned to a single node and SnappyData automatically optimises such queries to use a single task. \nFor more information see, \nHow to design your schema\n.\n\n\nUsing Smart Connector for Expanding Capacity of the Cluster\n\n\nOne of the instances, when \nSnappyData Smart connector mode\n is useful, is when the computations is separate from the data. This allows  you to increase the computational capacity without adding more servers to the SnappyData cluster. Thus, more executors can be provisioned for a Smart Connector application than the number of SnappyData servers. \n\nAlso, expensive batch jobs can be run in a separate Smart Connector application and it does not impact the performance of the SnappyData cluster. See, \nHow to Access SnappyData store from an Existing Spark Installation using Smart Connector\n.", 
            "title": "Tuning for Concurrency and Computation"
        }, 
        {
            "location": "/best_practices/setup_cluster/#tuning-for-concurrency-and-computation", 
            "text": "", 
            "title": "Tuning for Concurrency and Computation"
        }, 
        {
            "location": "/best_practices/setup_cluster/#handling-low-latency-vs-analytic-jobs", 
            "text": "Unlike Spark, SnappyData can distinguish requests that are cheap (low latency) vs requests that require a lot of computational resources (high latency). This is done by a resource scheduler that can balance the needs of many contending users/threads.  For instance, when a SQL client executes a \u2018fetch by primary key\u2019 query, there is no need to involve any scheduler or spawn many tasks for such a simple request. The request is immediately delegated to the data node (single thread) and the response is directly sent to the requesting client (probably within a few milliseconds).  \nIn the current version of the product, all query requests that filter on a primary key, a set of keys, or can directly filter using an index are executed without routing to the SnappyData scheduler. Only Row tables can have primary keys or indexes.   When the above conditions are not met, the request is routed to the \u2018Lead\u2019 node where the Spark plan is generated and \u2018jobs\u2019 are scheduled for execution. The scheduler uses a FAIR scheduling algorithm for higher concurrency, that is, all concurrent jobs are executed in a round-robin manner.  Each job is made up of one or more stages and the planning phase computes the number of  parallel tasks for the stage. Tasks from scheduled jobs are then allocated to the logical cores available until all cores are allocated. \nA round-robin algorithm picks a task from Job1, a task from Job2 and so on. If more cores are available, the second task from Job1 is picked and the cycle continues. But, there are circumstances a single job can completely consume all cores. \nFor instance, when all cores are available, if a large loading job is scheduled it receives all available cores of which, each of the tasks can be long running. During this time, if other concurrent jobs are assigned, none of the executing tasks is preempted.   Note  This above scheduling logic is applicable only when queries are fully managed by SnappyData cluster. When running your application using the smart connector, each task running in the Spark cluster directly accesses the store partitions.", 
            "title": "Handling Low Latency vs Analytic Jobs"
        }, 
        {
            "location": "/best_practices/setup_cluster/#computing-the-number-of-cores-for-a-job", 
            "text": "Executing queries or code in SnappyData results in the creation of one or more Spark jobs. Each Spark job has multiple tasks. The number of tasks is determined by the number of partitions of the underlying data. \nConcurrency in SnappyData is tightly bound with the capacity of the cluster, which means, the number of cores available in the cluster determines the number of concurrent tasks that can be run.  The default setting is  CORES = 2 X number of cores on a machine .  It is recommended to use 2 X number of cores on a machine. If more than one server is running on a machine, the cores should be divided accordingly and specified using the  spark.executor.cores  property. spark.executor.cores  is used to override the number of cores per server.  For example, for a cluster with 2 servers running on two different machines with  4 CPU cores each, a maximum number of tasks that can run concurrently is 16.   \nIf a table has 16 partitions (buckets, for the row or column tables), a scan query on this table creates 16 tasks. This means, 16 tasks run concurrently and the last task will run when one of these 16 tasks has finished execution.  SnappyData uses an optimization method which clubs multiple partitions on a single machine to form a single partition when there are fewer cores available. This reduces the overhead of scheduling partitions.   In SnappyData, multiple queries can be executed concurrently, if they are submitted by different threads or different jobs. For concurrent queries, SnappyData uses fair scheduling to manage the available resources such that all the queries get a fair distribution of resources.   For example, In the image below, 6 cores are available on 3 systems, and 2 jobs have 4 tasks each. Because of fair scheduling, both jobs get 3 cores and hence three tasks per job execute concurrently.  Pending tasks have to wait for completion of the current tasks and are assigned to the core that is first available.  When you add more servers to SnappyData, the processing capacity of the system increases in terms of available cores. Thus, more cores are available so more tasks can concurrently execute.", 
            "title": "Computing the Number of Cores for a Job"
        }, 
        {
            "location": "/best_practices/setup_cluster/#configuring-the-scheduler-pools-for-concurrency", 
            "text": "SnappyData out of the box comes configured with two execution pools:    Low-latency pool : This pool is automatically used when SnappyData determines that a request is of low latency, that is, the queries that are partition pruned to two or fewer partitions.     Default pool : This is the pool that is used for the remaining requests.    Two cores are statically assigned to the low latency pool. Also, the low latency pool has weight twice that of the default pool. Thus, if there are 30 cores available to an executor for a query that has 30 partitions, only 28 would be assigned to it and two cores would be reserved to not starve the low latency queries. When the system has both low latency and normal queries, 20 cores are used for the low latency queries as it has higher priority (weight=2).\nIf a query requires all 30 partitions and no low latency queries are running at that time, all 30 cores are assigned to the first query. However, when a low latency query is assigned, the scheduler does its best to allocate cores as soon as tasks from the earlier query finish.  Applications can explicitly configure to use a particular pool for the current session using a SQL configuration property,  snappydata.scheduler.pool . For example, the  set snappydata.scheduler.pool=lowlatency  command sets the pool as low latency pool for the current session.   New pools can be added and properties of the existing pools can be configured by modifying the  conf/fairscheduler.xml  file. We do not recommend changing the pool names ( default  and  lowlatency ).", 
            "title": "Configuring the Scheduler Pools for Concurrency"
        }, 
        {
            "location": "/best_practices/setup_cluster/#controlling-cpu-usage-for-user-jobs", 
            "text": "You can control the CPU usage for user jobs by configuring separate pools for different kinds of jobs.   See configuration  here .  \nThe product is configured with two out-of-the-box pools, that is the  Default pool  and the  Low-latency pool . The  Default pool  has higher priority and also has a  minShare , so that some minimum cores are reserved for those jobs if possible. \nThe  Stages  tab on the SnappyData Pulse UI shows the available pools.  When you track a job for an SQL query on the  SQL  tab, it shows the pool that is used in the  Pool Name  column. In-built tasks such as ingestion can show lower priority pools by default to give priority to foreground queries. To configure such priority, do the following:    Define the pools in  conf/fairscheduler.xml    Set a pool for a job using Spark API  or use  set snappydata.scheduler.pool  property in a SnappySession.   To configure the priority based on specific requirements, you can also either permit the users to set the priority for queries or add some pool allocation logic in the application as per client requirements.", 
            "title": "Controlling CPU Usage for User Jobs"
        }, 
        {
            "location": "/best_practices/setup_cluster/#using-a-partitioning-strategy-to-increase-concurrency", 
            "text": "The best way to increasing concurrency is to design your schema such that you minimize the need to run your queries across many partitions. The common strategy is to understand your application patterns and choose a partitioning strategy such that queries often target a specific partition. Such queries will be pruned to a single node and SnappyData automatically optimises such queries to use a single task. \nFor more information see,  How to design your schema .", 
            "title": "Using a Partitioning Strategy to Increase Concurrency"
        }, 
        {
            "location": "/best_practices/setup_cluster/#using-smart-connector-for-expanding-capacity-of-the-cluster", 
            "text": "One of the instances, when  SnappyData Smart connector mode  is useful, is when the computations is separate from the data. This allows  you to increase the computational capacity without adding more servers to the SnappyData cluster. Thus, more executors can be provisioned for a Smart Connector application than the number of SnappyData servers.  Also, expensive batch jobs can be run in a separate Smart Connector application and it does not impact the performance of the SnappyData cluster. See,  How to Access SnappyData store from an Existing Spark Installation using Smart Connector .", 
            "title": "Using Smart Connector for Expanding Capacity of the Cluster"
        }, 
        {
            "location": "/best_practices/design_schema/", 
            "text": "Designing your Database and Schema\n\n\nThe key design goal for achieving linear scaling is to use a partitioning strategy that allows most data access (queries) to be pruned to a single partition. This avoids expensive locking operations across multiple partitions during query execution.\n\n\nIn a highly concurrent system that has thousands of connections, multiple queries are generally spread uniformly across the entire data set (and therefore across all partitions). Therefore, increasing the number of data stores enables linear scalability. Given sufficient network performance, additional connections can be supported without degrading the response time for queries.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nDesign Principles of Scalable, Partition-Aware Databases\n\n\n\n\n\n\nOptimizing Query Latency: Partitioning and Replication Strategies", 
            "title": "Designing your Database and Schema"
        }, 
        {
            "location": "/best_practices/design_schema/#designing-your-database-and-schema", 
            "text": "The key design goal for achieving linear scaling is to use a partitioning strategy that allows most data access (queries) to be pruned to a single partition. This avoids expensive locking operations across multiple partitions during query execution.  In a highly concurrent system that has thousands of connections, multiple queries are generally spread uniformly across the entire data set (and therefore across all partitions). Therefore, increasing the number of data stores enables linear scalability. Given sufficient network performance, additional connections can be supported without degrading the response time for queries.  The following topics are covered in this section:    Design Principles of Scalable, Partition-Aware Databases    Optimizing Query Latency: Partitioning and Replication Strategies", 
            "title": "Designing your Database and Schema"
        }, 
        {
            "location": "/best_practices/design_principles/", 
            "text": "Design Principles of Scalable, Partition-Aware Databases\n\n\n\n\nThe general strategy for designing a SnappyData database is to identify the tables to partition or replicate in the SnappyData cluster, and then to determine the correct partitioning key(s) for partitioned tables. This usually requires an iterative process to produce the optimal design:\n\n\n\n\n\n\nRead \nIdentify Entity Groups and Partitioning Keys\n and \nReplicate Dimension Tables\n to understand the basic rules for defining partitioned or replicated tables.\n\n\n\n\n\n\nEvaluate your data access patterns to define those entity groups that are candidates for partitioning. Focus your efforts on commonly-joined entities. Colocating commonly joined tables will improve the performance of join queries by avoiding shuffle of data when the join is on partitioning keys.\n\n\n\n\n\n\nIdentify all of the tables in the entity groups.\n\n\n\n\n\n\nIdentify the \u201cpartitioning key\u201d for each partitioned table. The partitioning key is the column or set of columns that are common across a set of related tables. Look for parent-child relationships in the joined tables.\n\n\n\n\n\n\nIdentify all of the tables that are candidates for replication. You can replicate table data for high availability, or to colocate table data that is necessary to execute joins.\n\n\n\n\n\n\n\n\nIdentify Entity Groups and Partitioning Keys\n\n\nIn relational database terms, an entity group corresponds to rows that are related to one another through foreign key relationships. Members of an entity group are typically related by parent-child relationships and can be managed in a single partition. To design a SnappyData database for data partitioning, begin by identifying \u201centity groups\u201d and their associated partitioning keys.\n\n\nFor example:\n\n\n\n\n\n\nIn a customer order management system, most transactions operate on data related to a single customer at a time. Queries frequently join a customer\u2019s billing information with their orders and shipping information. For this type of application, you partition related tables using the customer identity. Any customer row along with their \u201corder\u201d and \u201cshipping\u201d rows forms a single entity group that has the customer ID as the entity group identity (the partitioning key). You would partition related tables using the customer identity, which would enable you to scale the system linearly by adding more members to support additional customers.\n\n\n\n\n\n\nIn a system that manages a product catalog (product categories, product specifications, customer reviews, rebates, related products, and so forth) most data access focuses on a single product at a time. In this type of system, you would partition data on the product key and add members as needed to manage additional products.\n\n\n\n\n\n\nIn an online auction application, you may need to stream incoming auction bids to hundreds of clients with very low latency. To do so, you would manage selected \u201chot\u201d auctions on a single partition, so that they receive sufficient processing power. As the processing demand increases, you would add more partitions and route the application logic that matches bids to clients to the data store itself.\n\n\n\n\n\n\nIn a financial trading engine that constantly matches bid prices to asking prices for thousands of securities, you would partition data using the ID of the security. To ensure low-latency execution when a security\u2019s market data changes, you would colocate all of the related reference data with the matching algorithm.\n\n\n\n\n\n\nBy identifying entity groups and using those groups as the basis for SnappyData partitioning and colocation, you can realize these benefits:\n\n\n\n\n\n\nRebalancing\n: SnappyData rebalances the data automatically by making sure that related rows are migrated together and without any integrity loss. This enables you to add capacity as needed.\n\n\n\n\n\n\nParallel scatter-gather\n: Queries that cannot be pruned to a single partition are automatically executed in parallel on data stores. \n\n\n\n\n\n\nSub-queries on remote partitions\n: Even when a query is pruned to a single partition, the query can execute sub-queries that operate on data that is stored on remote partitions.\n\n\n\n\n\n\n\n\nAdapting a Database Schema\n\n\nIf you have an existing database design that you want to deploy to SnappyData, translate the entity-relationship model into a physical design that is optimized for SnappyData design principles.\n\n\nGuidelines for Adapting a Database to SnappyData\n\n\nThis example shows tables from the Microsoft \nNorthwind Traders sample database\n.\n\n\nIn order to adapt this schema for use in SnappyData, follow the basic steps outlined in \nDesign Principles of Scalable, Partition-Aware Databases\n:\n\n\n\n\n\n\nDetermine the entity groups.\n\n\nEntity groups are generally course-grained entities that have children, grand children, and so forth, and they are commonly used in queries. This example chooses these entity groups:\n\n\n\n\n\n\n\n\nEntity Group\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCustomer\n\n\nThis group uses the customer identity along with orders and order details as the children\n\n\n\n\n\n\nProduct\n\n\nThis group uses product details along with the associated supplier information\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the tables in each entity group.\n    Identify the tables that belong to each entity group. In this example, entity groups use the following tables.\n\n\n\n\n\n\n\n\nEntity Group\n\n\nTables\n\n\n\n\n\n\n\n\n\n\nCustomer\n\n\nCustomers \nOrders\nShippers\nOrder Details\n\n\n\n\n\n\nProduct\n\n\nProduct\nSuppliers\nCategory\n\n\n\n\n\n\n\n\n\n\n\n\nDefine the partitioning key for each group.\n\n    In this example, the partitioning keys are:\n\n\n\n\n\n\n\n\nEntity Group\n\n\nPartitioning key\n\n\n\n\n\n\n\n\n\n\nCustomer\n\n\nCustomerID\n\n\n\n\n\n\nProduct\n\n\nProductID\n\n\n\n\n\n\n\n\nThis example uses customerID as the partitioning key for the Customer group. The customer row and all associated orders will be colocated into a single partition. To explicitly colocate Orders with its parent customer row, use the \ncolocate with\n clause in the create table statement:\n\n\ncreate table orders (\ncolumn definitions\n) \nusing column options ('partition_by' customerID, \n'colocate_with' customers);\n\n\n\nIn this way, SnappyData supports any queries that join any the Customers and Orders tables. This join query would be distributed to all partitions and executed in parallel, with the results streamed back to the client:\n\n\nselect * from customer c , orders o where c.customerID = o.customerID;\n\n\n\nA query such as this would be pruned to the single partition that stores \u201ccustomer100\u201d and executed only on that SnappyData member:\n\n\nselect * from customer c, orders o where c.customerID = o.customerID  and c.customerID = 'customer100';\n\n\n\nThe optimization provided when queries are highly selective comes from engaging the query processor and indexing on a single member rather than on all partitions. With all customer data managed in memory, query response times are very fast. \n\nFinally, consider a case where an application needs to access customer order data for several customers:\n\n\nselect * from customer c, orders o \nwhere c.customerID = o.customerID and c.customerID IN ('cust1', 'cust2', 'cust3');\n\n\n\nHere, SnappyData prunes the query execution to only those partitions that host \u2018cust1\u2019, 'cust2\u2019, and 'cust3\u2019. The union of the results is then returned to the caller.\nNote that the selection of customerID as the partitioning key means that the OrderDetails and Shippers tables cannot be partitioned and colocated with Customers and Orders (because OrderDetails and Shippers do not contain the customerID value for partitioning). \n\n\n\n\n\n\nIdentify replicated tables.\n\n    If we assume that the number of categories and suppliers rarely changes, those tables can be replicated in the SnappyData cluster (replicated to all of the SnappyData members that host the entity group). If we assume that the Products table does change often and can be relatively large in size, then partitioning is a better strategy for that table.\n    So for the product entity group, table Products is partitioned by ProductID, and the Suppliers and Categories tables are replicated to all of the members where Products is partitioned.\n    Applications can now join Products, Suppliers and categories. For example:\n\n\nselect * from Products p , Suppliers s, Categories c \nwhere c.categoryID = p.categoryID and p.supplierID = s.supplierID \nand p.productID IN ('someProductKey1', ' someProductKey2', ' someProductKey3');\n\n\n\nIn the above query, SnappyData prunes the query execution to only those partitions that host 'someProductKey1\u2019, \u2019 someProductKey2\u2019, and \u2019 someProductKey3.\u2019", 
            "title": "Design Principles of Scalable, Partition-Aware Databases"
        }, 
        {
            "location": "/best_practices/design_principles/#design-principles-of-scalable-partition-aware-databases", 
            "text": "The general strategy for designing a SnappyData database is to identify the tables to partition or replicate in the SnappyData cluster, and then to determine the correct partitioning key(s) for partitioned tables. This usually requires an iterative process to produce the optimal design:    Read  Identify Entity Groups and Partitioning Keys  and  Replicate Dimension Tables  to understand the basic rules for defining partitioned or replicated tables.    Evaluate your data access patterns to define those entity groups that are candidates for partitioning. Focus your efforts on commonly-joined entities. Colocating commonly joined tables will improve the performance of join queries by avoiding shuffle of data when the join is on partitioning keys.    Identify all of the tables in the entity groups.    Identify the \u201cpartitioning key\u201d for each partitioned table. The partitioning key is the column or set of columns that are common across a set of related tables. Look for parent-child relationships in the joined tables.    Identify all of the tables that are candidates for replication. You can replicate table data for high availability, or to colocate table data that is necessary to execute joins.", 
            "title": "Design Principles of Scalable, Partition-Aware Databases"
        }, 
        {
            "location": "/best_practices/design_principles/#identify-entity-groups-and-partitioning-keys", 
            "text": "In relational database terms, an entity group corresponds to rows that are related to one another through foreign key relationships. Members of an entity group are typically related by parent-child relationships and can be managed in a single partition. To design a SnappyData database for data partitioning, begin by identifying \u201centity groups\u201d and their associated partitioning keys.  For example:    In a customer order management system, most transactions operate on data related to a single customer at a time. Queries frequently join a customer\u2019s billing information with their orders and shipping information. For this type of application, you partition related tables using the customer identity. Any customer row along with their \u201corder\u201d and \u201cshipping\u201d rows forms a single entity group that has the customer ID as the entity group identity (the partitioning key). You would partition related tables using the customer identity, which would enable you to scale the system linearly by adding more members to support additional customers.    In a system that manages a product catalog (product categories, product specifications, customer reviews, rebates, related products, and so forth) most data access focuses on a single product at a time. In this type of system, you would partition data on the product key and add members as needed to manage additional products.    In an online auction application, you may need to stream incoming auction bids to hundreds of clients with very low latency. To do so, you would manage selected \u201chot\u201d auctions on a single partition, so that they receive sufficient processing power. As the processing demand increases, you would add more partitions and route the application logic that matches bids to clients to the data store itself.    In a financial trading engine that constantly matches bid prices to asking prices for thousands of securities, you would partition data using the ID of the security. To ensure low-latency execution when a security\u2019s market data changes, you would colocate all of the related reference data with the matching algorithm.    By identifying entity groups and using those groups as the basis for SnappyData partitioning and colocation, you can realize these benefits:    Rebalancing : SnappyData rebalances the data automatically by making sure that related rows are migrated together and without any integrity loss. This enables you to add capacity as needed.    Parallel scatter-gather : Queries that cannot be pruned to a single partition are automatically executed in parallel on data stores.     Sub-queries on remote partitions : Even when a query is pruned to a single partition, the query can execute sub-queries that operate on data that is stored on remote partitions.", 
            "title": "Identify Entity Groups and Partitioning Keys"
        }, 
        {
            "location": "/best_practices/design_principles/#adapting-a-database-schema", 
            "text": "If you have an existing database design that you want to deploy to SnappyData, translate the entity-relationship model into a physical design that is optimized for SnappyData design principles.", 
            "title": "Adapting a Database Schema"
        }, 
        {
            "location": "/best_practices/design_principles/#guidelines-for-adapting-a-database-to-snappydata", 
            "text": "This example shows tables from the Microsoft  Northwind Traders sample database .  In order to adapt this schema for use in SnappyData, follow the basic steps outlined in  Design Principles of Scalable, Partition-Aware Databases :    Determine the entity groups.  Entity groups are generally course-grained entities that have children, grand children, and so forth, and they are commonly used in queries. This example chooses these entity groups:     Entity Group  Description      Customer  This group uses the customer identity along with orders and order details as the children    Product  This group uses product details along with the associated supplier information       Identify the tables in each entity group.\n    Identify the tables that belong to each entity group. In this example, entity groups use the following tables.     Entity Group  Tables      Customer  Customers  Orders Shippers Order Details    Product  Product Suppliers Category       Define the partitioning key for each group. \n    In this example, the partitioning keys are:     Entity Group  Partitioning key      Customer  CustomerID    Product  ProductID     This example uses customerID as the partitioning key for the Customer group. The customer row and all associated orders will be colocated into a single partition. To explicitly colocate Orders with its parent customer row, use the  colocate with  clause in the create table statement:  create table orders ( column definitions ) \nusing column options ('partition_by' customerID, \n'colocate_with' customers);  In this way, SnappyData supports any queries that join any the Customers and Orders tables. This join query would be distributed to all partitions and executed in parallel, with the results streamed back to the client:  select * from customer c , orders o where c.customerID = o.customerID;  A query such as this would be pruned to the single partition that stores \u201ccustomer100\u201d and executed only on that SnappyData member:  select * from customer c, orders o where c.customerID = o.customerID  and c.customerID = 'customer100';  The optimization provided when queries are highly selective comes from engaging the query processor and indexing on a single member rather than on all partitions. With all customer data managed in memory, query response times are very fast.  \nFinally, consider a case where an application needs to access customer order data for several customers:  select * from customer c, orders o \nwhere c.customerID = o.customerID and c.customerID IN ('cust1', 'cust2', 'cust3');  Here, SnappyData prunes the query execution to only those partitions that host \u2018cust1\u2019, 'cust2\u2019, and 'cust3\u2019. The union of the results is then returned to the caller.\nNote that the selection of customerID as the partitioning key means that the OrderDetails and Shippers tables cannot be partitioned and colocated with Customers and Orders (because OrderDetails and Shippers do not contain the customerID value for partitioning).     Identify replicated tables. \n    If we assume that the number of categories and suppliers rarely changes, those tables can be replicated in the SnappyData cluster (replicated to all of the SnappyData members that host the entity group). If we assume that the Products table does change often and can be relatively large in size, then partitioning is a better strategy for that table.\n    So for the product entity group, table Products is partitioned by ProductID, and the Suppliers and Categories tables are replicated to all of the members where Products is partitioned.\n    Applications can now join Products, Suppliers and categories. For example:  select * from Products p , Suppliers s, Categories c \nwhere c.categoryID = p.categoryID and p.supplierID = s.supplierID \nand p.productID IN ('someProductKey1', ' someProductKey2', ' someProductKey3');  In the above query, SnappyData prunes the query execution to only those partitions that host 'someProductKey1\u2019, \u2019 someProductKey2\u2019, and \u2019 someProductKey3.\u2019", 
            "title": "Guidelines for Adapting a Database to SnappyData"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/", 
            "text": "Optimizing Query Latency: Partitioning and Replication Strategies\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nUsing Column vs Row Table\n\n\n\n\n\n\nUsing Partitioned vs Replicated Row Table\n\n\n\n\n\n\nApplying Partitioning Scheme\n\n\n\n\n\n\nUsing Redundancy\n\n\n\n\n\n\nOverflow Configuration\n\n\n\n\n\n\n\n\nUsing Column vs Row Table\n\n\nA columnar table data is stored in a sequence of columns, whereas, in a row table it stores table records in a sequence of rows.\n\n\n\n\nUsing Column Tables\n\n\nAnalytical Queries\n: A column table has distinct advantages for OLAP queries and therefore large tables involved in such queries are recommended to be created as columnar tables. These tables are rarely mutated (deleted/updated).\nFor a given query on a column table, only the required columns are read (since only the required subset columns are to be scanned), which gives a better scan performance. Thus, aggregation queries execute faster on a column table compared  to a  row table.\n\n\nCompression of Data\n: Another advantage that the column table offers is it allows highly efficient compression of data which reduces the total storage footprint for large tables.\n\n\nColumn tables are not suitable for OLTP scenarios. In this case, row tables are recommended.\n\n\n\n\nUsing Row Tables\n\n\nOLTP Queries\n: Row tables are designed to return the entire row efficiently and are suited for OLTP scenarios when the tables are required to be mutated frequently (when the table rows need to be updated/deleted based on some conditions). In these cases, row tables offer distinct advantages over the column tables.\n\n\nPoint queries\n: Row tables are also suitable for point queries (for example, queries that select only a few records based on certain where clause conditions). \n\n\nSmall Dimension Tables\n: Row tables are also suitable to create small dimension tables as these can be created as replicated tables (table data replicated on all data servers).\n\n\nCreate Index\n: Row tables also allow the creation of an index on certain columns of the table which improves  performance.\n\n\n\n\nUsing Partitioned vs Replicated Row Table\n\n\nIn SnappyData, row tables can be either partitioned across all servers or replicated on every server. For row tables, large fact tables should be partitioned whereas, dimension tables can be replicated.\n\n\nThe SnappyData architecture encourages you to denormalize \u201cdimension\u201d tables into fact tables when possible, and then replicate remaining dimension tables to all datastores in the distributed system.\n\n\nMost databases follow the \nstar schema\n design pattern where large \u201cfact\u201d tables store key information about the events in a system or a business process. For example, a fact table would store rows for events like product sales or bank transactions. Each fact table generally has foreign key relationships to multiple \u201cdimension\u201d tables, which describe further aspects of each row in the fact table.\n\n\nWhen designing a database schema for SnappyData, the main goal with a typical star schema database is to partition the entities in fact tables. Slow-changing dimension tables should then be replicated on each data store that hosts a partitioned fact table. In this way, a join between the fact table and any number of its dimension tables can be executed concurrently on each partition, without requiring multiple network hops to other members in the distributed system.\n\n\n\n\nApplying Partitioning Scheme\n\n\n\n\nColocated Joins\n\n\nIn SQL, a JOIN clause is used to combine data from two or more tables, based on a related column between them. JOINS have traditionally been expensive in distributed systems because the data for the tables involved in the JOIN may reside on different physical nodes and the operation has to first move/shuffle the relevant data to one node and perform the operation. \n\nSnappyData offers a way to declaratively \"co-locate\" tables to prevent or reduce shuffling to execute JOINS. When two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData member. Therefore, in a join query, the join operation is performed on each node's local data. Eliminating data shuffling improves performance significantly.\n\nFor examples refer to, \nHow to colocate tables for doing a colocated join\n.\n\n\n\n\nBuckets\n\n\nA bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. For more information on BUCKETS, refer to \nBUCKETS\n.\n\n\nThe default number of buckets in SnappyData cluster mode is 128. In the local mode it is cores*2, subject to maximum of 64 buckets and minumum of 8 buckets.\n\n\nThe number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow.\n\n\nWhen a new server joins or an existing server leaves the cluster, buckets are moved around in order to ensure that data is balanced across the nodes where the table is defined.\n\n\nThe  \n-rebalance\n option on the startup command-line triggers bucket rebalancing and the new server becomes the primary for some of the buckets (and secondary for some if REDUNDANCY\n0 has been specified). \n\nYou can also set the system procedure \ncall sys.rebalance_all_buckets()\n to trigger rebalance.\n\n\n\n\nCriteria for Column Partitioning\n\n\nIt is recommended to use a relevant dimension for partitioning so that all partitions are active and the query is executed concurrently.\n\nIf only a single partition is active and is used largely by queries (especially concurrent queries) it means a significant bottleneck where only a single partition is active all the time, while others are idle. This serializes execution into a single thread handling that partition. Therefore, it is not recommended to use DATE/TIMESTAMP as partitioning.\n\n\n\n\nUsing Redundancy\n\n\nREDUNDANCY clause of \nCREATE TABLE\n specifies the number of secondary copies you want to maintain for your partitioned table. This allows the table data to be highly available even if one of the SnappyData members fails or shuts down. \n\n\nA REDUNDANCY value of 1 is recommended to maintain a secondary copy of the table data. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage.\n\n\nFor an example on the REDUNDANCY clause refer to \nTables in SnappyData\n.\n\n\n\n\nOverflow Configuration\n\n\nIn SnappyData, row and column tables by default overflow to disk (which is equivalent to setting OVERFLOW to 'true'), based on EVICTION_BY criteria. Users cannot set OVERFLOW to 'false', except when EVICTION_BY is not set, in which case it disables the eviction.\n\n\nFor example, setting EVICTION_BY to \nLRUHEAPPERCENT\n allows table data to be evicted to disk based on the current memory consumption of the server.\n\n\nRefer to \nCREATE TABLE\n link to understand how to configure \nOVERFLOW\n and \nEVICTION_BY\n clauses.\n\n\n\n\nTip\n\n\nBy default eviction is set to \noverflow-to-disk\n.\n\n\n\n\nKnown Limitation\n\n\nChange NOT IN queries to use NOT EXISTS if possible\n\n\nCurrently, all \nNOT IN\n queries use an unoptimized plan and lead to a nested-loop-join which can take long if both sides are large (\nhttps://issues.apache.org/jira/browse/SPARK-16951\n). Change your queries to use \nNOT EXISTS\n which uses an optimized anti-join plan. \n\n\nFor example a query like:\n\n\nselect count(*) from T1 where id not in (select id from T2)\n\n\n\n\ncan be changed to:\n\n\nselect count(*) from T1 where not exists (select 1 from T2 where T1.id = T2.id)\n\n\n\n\nBe aware of the different null value semantics of the two operators as noted here for \nSpark\n.\n\n\nIn a nutshell, the \nNOT IN\n operator is null-aware and skips the row if the sub-query has a null value, while the \nNOT EXISTS\n operator ignores the null values in the sub-query. In other words, the following two are equivalent when dealing with null values:\n\n\nselect count(*) from T1 where id not in (select id from T2 where id is not null)\nselect count(*) from T1 where not exists (select 1 from T2 where T1.id = T2.id)", 
            "title": "Optimizing Query Latency: Partitioning and Replication Strategies"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#optimizing-query-latency-partitioning-and-replication-strategies", 
            "text": "The following topics are covered in this section:    Using Column vs Row Table    Using Partitioned vs Replicated Row Table    Applying Partitioning Scheme    Using Redundancy    Overflow Configuration", 
            "title": "Optimizing Query Latency: Partitioning and Replication Strategies"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#using-column-vs-row-table", 
            "text": "A columnar table data is stored in a sequence of columns, whereas, in a row table it stores table records in a sequence of rows.", 
            "title": "Using Column vs Row Table"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#using-column-tables", 
            "text": "Analytical Queries : A column table has distinct advantages for OLAP queries and therefore large tables involved in such queries are recommended to be created as columnar tables. These tables are rarely mutated (deleted/updated).\nFor a given query on a column table, only the required columns are read (since only the required subset columns are to be scanned), which gives a better scan performance. Thus, aggregation queries execute faster on a column table compared  to a  row table.  Compression of Data : Another advantage that the column table offers is it allows highly efficient compression of data which reduces the total storage footprint for large tables.  Column tables are not suitable for OLTP scenarios. In this case, row tables are recommended.", 
            "title": "Using Column Tables"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#using-row-tables", 
            "text": "OLTP Queries : Row tables are designed to return the entire row efficiently and are suited for OLTP scenarios when the tables are required to be mutated frequently (when the table rows need to be updated/deleted based on some conditions). In these cases, row tables offer distinct advantages over the column tables.  Point queries : Row tables are also suitable for point queries (for example, queries that select only a few records based on certain where clause conditions).   Small Dimension Tables : Row tables are also suitable to create small dimension tables as these can be created as replicated tables (table data replicated on all data servers).  Create Index : Row tables also allow the creation of an index on certain columns of the table which improves  performance.", 
            "title": "Using Row Tables"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#using-partitioned-vs-replicated-row-table", 
            "text": "In SnappyData, row tables can be either partitioned across all servers or replicated on every server. For row tables, large fact tables should be partitioned whereas, dimension tables can be replicated.  The SnappyData architecture encourages you to denormalize \u201cdimension\u201d tables into fact tables when possible, and then replicate remaining dimension tables to all datastores in the distributed system.  Most databases follow the  star schema  design pattern where large \u201cfact\u201d tables store key information about the events in a system or a business process. For example, a fact table would store rows for events like product sales or bank transactions. Each fact table generally has foreign key relationships to multiple \u201cdimension\u201d tables, which describe further aspects of each row in the fact table.  When designing a database schema for SnappyData, the main goal with a typical star schema database is to partition the entities in fact tables. Slow-changing dimension tables should then be replicated on each data store that hosts a partitioned fact table. In this way, a join between the fact table and any number of its dimension tables can be executed concurrently on each partition, without requiring multiple network hops to other members in the distributed system.", 
            "title": "Using Partitioned vs Replicated Row Table"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#applying-partitioning-scheme", 
            "text": "", 
            "title": "Applying Partitioning Scheme"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#colocated-joins", 
            "text": "In SQL, a JOIN clause is used to combine data from two or more tables, based on a related column between them. JOINS have traditionally been expensive in distributed systems because the data for the tables involved in the JOIN may reside on different physical nodes and the operation has to first move/shuffle the relevant data to one node and perform the operation.  \nSnappyData offers a way to declaratively \"co-locate\" tables to prevent or reduce shuffling to execute JOINS. When two tables are partitioned on columns and colocated, it forces partitions having the same values for those columns in both tables to be located on the same SnappyData member. Therefore, in a join query, the join operation is performed on each node's local data. Eliminating data shuffling improves performance significantly. \nFor examples refer to,  How to colocate tables for doing a colocated join .", 
            "title": "Colocated Joins"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#buckets", 
            "text": "A bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. For more information on BUCKETS, refer to  BUCKETS .  The default number of buckets in SnappyData cluster mode is 128. In the local mode it is cores*2, subject to maximum of 64 buckets and minumum of 8 buckets.  The number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow.  When a new server joins or an existing server leaves the cluster, buckets are moved around in order to ensure that data is balanced across the nodes where the table is defined.  The   -rebalance  option on the startup command-line triggers bucket rebalancing and the new server becomes the primary for some of the buckets (and secondary for some if REDUNDANCY 0 has been specified).  \nYou can also set the system procedure  call sys.rebalance_all_buckets()  to trigger rebalance.", 
            "title": "Buckets"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#criteria-for-column-partitioning", 
            "text": "It is recommended to use a relevant dimension for partitioning so that all partitions are active and the query is executed concurrently. \nIf only a single partition is active and is used largely by queries (especially concurrent queries) it means a significant bottleneck where only a single partition is active all the time, while others are idle. This serializes execution into a single thread handling that partition. Therefore, it is not recommended to use DATE/TIMESTAMP as partitioning.", 
            "title": "Criteria for Column Partitioning"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#using-redundancy", 
            "text": "REDUNDANCY clause of  CREATE TABLE  specifies the number of secondary copies you want to maintain for your partitioned table. This allows the table data to be highly available even if one of the SnappyData members fails or shuts down.   A REDUNDANCY value of 1 is recommended to maintain a secondary copy of the table data. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage.  For an example on the REDUNDANCY clause refer to  Tables in SnappyData .", 
            "title": "Using Redundancy"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#overflow-configuration", 
            "text": "In SnappyData, row and column tables by default overflow to disk (which is equivalent to setting OVERFLOW to 'true'), based on EVICTION_BY criteria. Users cannot set OVERFLOW to 'false', except when EVICTION_BY is not set, in which case it disables the eviction.  For example, setting EVICTION_BY to  LRUHEAPPERCENT  allows table data to be evicted to disk based on the current memory consumption of the server.  Refer to  CREATE TABLE  link to understand how to configure  OVERFLOW  and  EVICTION_BY  clauses.   Tip  By default eviction is set to  overflow-to-disk .", 
            "title": "Overflow Configuration"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#known-limitation", 
            "text": "", 
            "title": "Known Limitation"
        }, 
        {
            "location": "/best_practices/optimizing_query_latency/#change-not-in-queries-to-use-not-exists-if-possible", 
            "text": "Currently, all  NOT IN  queries use an unoptimized plan and lead to a nested-loop-join which can take long if both sides are large ( https://issues.apache.org/jira/browse/SPARK-16951 ). Change your queries to use  NOT EXISTS  which uses an optimized anti-join plan.   For example a query like:  select count(*) from T1 where id not in (select id from T2)  can be changed to:  select count(*) from T1 where not exists (select 1 from T2 where T1.id = T2.id)  Be aware of the different null value semantics of the two operators as noted here for  Spark .  In a nutshell, the  NOT IN  operator is null-aware and skips the row if the sub-query has a null value, while the  NOT EXISTS  operator ignores the null values in the sub-query. In other words, the following two are equivalent when dealing with null values:  select count(*) from T1 where id not in (select id from T2 where id is not null)\nselect count(*) from T1 where not exists (select 1 from T2 where T1.id = T2.id)", 
            "title": "Change NOT IN queries to use NOT EXISTS if possible"
        }, 
        {
            "location": "/best_practices/memory_management/", 
            "text": "Memory Management\n\n\nSpark executors and SnappyData in-memory store share the same memory space. SnappyData extends the Spark's memory manager providing a unified space for spark storage, execution and SnappyData column and row tables. This Unified MemoryManager smartly keeps track of memory allocations across Spark execution and the Store, elastically expanding into the other if the room is available. Rather than a pre-allocation strategy where Spark memory is independent of the store, SnappyData uses a unified strategy where all allocations come from a common pool. Essentially, it optimizes the memory utilization to the extent possible.\n\n\nSnappyData also monitors the JVM memory pools and avoids running into out-of-memory conditions in most cases. You can configure the threshold for when data evicts to disk and the critical threshold for heap utilization. When the usage exceeds this critical threshold, memory allocations within SnappyData fail, and a LowMemoryException error is reported. This, however, safeguards the server from crashing due to OutOfMemoryException.\n\n\n\n\nEstimating Memory Size for Column and Row Tables\n\n\nColumn tables use compression by default and the amount of compression is dependent on the data itself. While we commonly see compression of 50%, it is also possible to achieve much higher compression ratios when the data has many repeated strings or text.\n\nRow tables, on the other hand, consume more space than the original data size. There is a per row overhead in SnappyData. While this overhead varies and is dependent on the options configured on the Row table, as a simple guideline we suggest you assume 100 bytes per row as overhead. Thus, it is clear that it is not straightforward to compute the memory requirements.\n \nIt is recommended that you take a sample of the data set (as close as possible to your production data) and populate each of the tables. Ensure that you create the required indexes and note down the size estimates (in bytes) in the SnappyData Pulse dashboard. You can then extrapolate this number given the total number of records you anticipate to load or grow into, for the memory requirements for your table.\n\n\nDisk and Memory Sizing\n\n\nFor efficient use of the disk, the best alternative is to load some sample data and extrapolate for both memory and disk requirements. The disk usage is the sum of all the \nTotal size\n of the tables. You can check the value of \nTotal Size\n on the SnappyData Pulse UI. \nFor total disk requirement, the rule of thumb is ~4X data size which accounts for temporary space required for the compactor and the space required for \nspark.local.dir\n. In case of concurrent thread execution,the requirement will differ as mentioned in \nspark.local.dir\n.\nIf the data and the temporary storage set with \nspark.local.dir\n are in separate locations, then the disk for data storage can be 2X of the total estimated data size while temporary storage can be 2X. The temporary storage is used to shuffle the output of large joins, and a query can potentially shuffle the entire data. Likewise, a massive import can also shuffle data before inserting into partitioned tables.\n\n\n\n\nTable Memory Requirements\n\n\nSnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (off-heap is enabled), the entire column table is stored in off-heap memory.\n\n\nSnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk.\n\n\n\n\n\n\n\n\nTABLE IS PERSISTED?\n\n\nOVERFLOW IS CONFIGURED?\n\n\nAPPROXIMATE HEAP OVERHEAD\n\n\n\n\n\n\n\n\n\n\nNo\n\n\nNo\n\n\n64 bytes\n\n\n\n\n\n\nYes\n\n\nNo\n\n\n120 bytes\n\n\n\n\n\n\nYes\n\n\nYes\n\n\n152 bytes\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nFor a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.)\n\n\n\n\n\n\n\n\n\n\nTYPE OF INDEX ENTRY\n\n\nAPPROXIMATE HEAP OVERHEAD\n\n\n\n\n\n\n\n\n\n\nNew index entry\n\n\n80 bytes\n\n\n\n\n\n\nFirst non-unique index entry\n\n\n24 bytes\n\n\n\n\n\n\nSubsequent non-unique index entry\n\n\n8 bytes to 24 bytes*\n\n\n\n\n\n\n\n\nIf there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes.\n\n\n\n\nEstimating Memory Size for Execution\n\n\nSpark and SnappyData also need room for execution. This includes memory for sorting, joining data sets, Spark execution, application managed objects (for example, a UDF allocating memory), etc. Most of these allocations automatically overflow to disk.  But it is strongly recommended to allocate minimum 6-8 GB of heap per data server/lead node for production systems that run large scale analytic queries.\n\n\nSnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (for example, columns stored as byte arrays).\n\nIt is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The \nmemory-size\n and \nheap-size\n properties control the off-heap and on-heap sizes of the SnappyData server process.\n\n\n\n\nSnappyData uses JVM heap memory for most of its allocations. Only column tables can use off-heap storage (if configured). We suggest going through the following options and configuring them appropriately based on the suggested sizing estimates.\n\n\n\n\nSnappyData Heap Memory\n\n\nHeap is provided for row tables and working/temp object memory. For large imports most external connectors still do not use off-heap for temporary buffers.\nSnappyData heap memory regions are divided into two parts called \nHeap Storage Pool\n and \nHeap Execution Pool\n. Sizes of each pool are determined by the configuration parameters provided at boot time to each server. These two regions are only tentative demarcation and can grow into each other based on some conditions.\n\n\nHeap Storage Pool\n\n\nThe heap objects which belong to SnappyData storage of Spark storage are accounted here. For example, when a row is inserted into a table or deleted, this pool accounts the memory size of that row. Objects that are temporary and die young are not considered here. As it is difficult and costly to do a precise estimation, this pool is an approximation of heap memory for objects that are going to be long-lived. Since precise estimation of heap memory is difficult, there is a heap monitor thread running in the background. \n\nIf the total heap as seen by JVM (and not SnappyUnifiedMemoryManager) exceeds \ncritical-heap-percentage\n the database engine starts canceling jobs and queries and a LowMemoryException is reported. This is also an indication of heap pressure on the system.\n\n\nHeap Execution Pool:\n\n\nDuring query execution or while running a Spark job, all temporary object allocations are done from this pool. For instance, queries like HashJoin and aggregate queries creates expensive in-memory maps. This pool is used to allocate such memory.\n\n\nYou can set the following configuration parameters to control the pools:\n\n\n\n\n\n\n\n\nParameter Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nheap-size\n\n\n4GB in SnappyData Embedded mode cluster\n\n\nMax heap size which can be used by the JVM\n\n\n\n\n\n\nspark.memory.storageFraction\n\n\n50\n\n\nFraction of workable memory allocated for storage pool and the remaining memory is allocated to the execution pool. It is recommended that you do not change this setting.\n\n\n\n\n\n\ncritical-heap-percentage\n\n\n95\n\n\nThe heap percent beyond which the system considers itself in a critical state. This is to safeguard the system from crashing due to an OutOfMemoryException. Beyond this point, SnappyData starts canceling all jobs and queries and a LowMemoryException is reported.\n This means (100 minus \ncritical-heap-percent\n) memory is not allocated to any pool and is unused.\n\n\n\n\n\n\neviction-heap-percentage\n\n\n85.5\n\n\nInitially, the amount of memory that is available for storage pool is 50% of the total workable memory. This can however grow up to \neviction-heap-percentage\n (default 85.5%). On reaching this threshold it starts evicting table data as per the eviction clause that was specified when creating the table.\n\n\n\n\n\n\nspark.memory.fraction\n\n\n0.97\n\n\nTotal workable memory for execution and storage. This fraction is applied after removing reserved memory (100 minus \ncritical-heap-percentage\n). This gives a cushion before the system reaches a critical state. It is recommended that you do not change this setting.\n\n\n\n\n\n\n\n\nAt the start, each of the two pools is assigned a portion of the available memory. This is driven by \nspark.memory.storageFraction\n property (default 50%). However, SnappyData allows each pool to \"balloon\" into the other if capacity is available subject to following rules:\n\n\n\n\n\n\nThe storage pool can grow to the execution pool if the execution pool has some capacity, but not beyond the \neviction-heap-percentage\n.\n\n\n\n\n\n\nIf the storage pool cannot borrow from the executor pool, it evicts some of its own blocks to make space for incoming blocks.\n\n\n\n\n\n\nIf the storage pool has already grown into the execution pool, the execution pool evicts block from the storage pool until the earlier limit (that is, 50% demarcation) is reached. Beyond that, the executor threads cannot evict blocks from the storage pool. If sufficient memory is not available, it can either fall back to disk overflow or wait until sufficient memory is available.\n\n\n\n\n\n\nIf the storage pool has some free memory, the execution pool can borrow that memory from the storage pool during execution. The borrowed memory is returned back once execution is over. \n\n\n\n\n\n\n\n\nExample\n: Configuration for memory (typically configured in \nconf/leads\n or \nconf/servers\n) \n\n\n-heap-size=20g -critical-heap-percentage=95 -eviction-heap-percentage=85.5\n\n\n\n\nExample\n: Depicts how SnappyData derives different memory region sizes.\n\n\nReserved_Heap_Memory =\n 20g * (1 - 0.95) = 1g ( 0.95 being derived from critical_heap_percentage)\nHeap_Memory_Fraction =\n (20g - Reserved_Memory) *(0.97) = 17.4 ( 0.97 being derived from spark.memory.fraction)\nHeap_Storage_Pool_Size =\n 17.4 * (0.5) = 8.73 ( 0.5 being derived from spark.memory.storageFraction)\nHeap_Execution_Pool_Size =\n 17.4 * (0.5) = 8.73\nHeap_Max_Storage_pool_Size =\n 17.4 * 0.85 = 14.7 ( 0.85 derived from eviction_heap_percentage)\n\n\n\n\n\n\nSnappyData Off-Heap Memory\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nIn addition to heap memory, SnappyData can also be configured with off-heap memory. If configured, column table data, as well as many of the execution structures use off-heap memory. For a serious installation, the off-heap setting is recommended. However, several artifacts in the product need heap memory, so some minimum heap size is also required for this.\n\n\n\n\n\n\n\n\nParameter Name\n\n\nDefault Value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmemory-size\n\n\nThe default value is either 0 or it gets auto-configured in \nspecific scenarios\n.\n\n\nTotal off-heap memory size\n\n\n\n\n\n\n\n\nSimilar to heap pools, off-heap pools are also divided between off-heap storage pool and off-heap execution pool. The rules of borrowing memory from each other also remains same.\n\n\n\n\nExample\n: Off-heap configuration: \n\n\n-heap-size = 4g -memory-size=16g -critical-heap-percentage=95 -eviction-heap-percentage=85.5\n\n\n\n\nExample\n: How SnappyData derives different memory region sizes.\n\n\nReserved_Memory ( Heap Memory) =\n 4g * (1 - 0.95) = 200m ( 0.95 being derived from critical_heap_percentage)\nMemory_Fraction ( Heap Memory) =\n (4g - Reserved_Memory) *(0.97) = 3.5g\nHeap Storage_Pool_Size =\n 3.5 * (0.5) = 1.75\nHeap Execution_Pool_Size =\n 3.5 * (0.5) = 1.75\nMax_Heap_Storage_pool_Size =\n 3.5g * 0.85 = 2.9 ( 0.85 derived from eviction_heap_percentage)\n\n\nOff-Heap Storage_Pool_Size =\n 16g * (0.5) = 8g\nOff-Heap Execution_Pool_Size =\n 16g * (0.5) = 8g\nMax_Off_Heap_Storage_pool_Size =\n 16g * 0.9 = 14.4 ( 0.9 System default)\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nFor row tables: According to the requirements of your row table size, configure the heap size. Row tables in SnappyData do not use off-heap memory.\n\n\n\n\n\n\nTo read write Parquet and CSV: Parquet and CSV read-write are memory consuming activities for which heap memory is used. Ensure that you provision sufficient heap memory for this.\n\n\n\n\n\n\nWhen most of your data reside in column tables: Use off-heap memory, as they are faster and puts less pressure on garbage collection threads.\n\n\n\n\n\n\nWhen configuring eviction: The tables are evicted to disk by default. This impacts performance to some degree and hence it is recommended to size your VM before you begin.", 
            "title": "Memory Management"
        }, 
        {
            "location": "/best_practices/memory_management/#memory-management", 
            "text": "Spark executors and SnappyData in-memory store share the same memory space. SnappyData extends the Spark's memory manager providing a unified space for spark storage, execution and SnappyData column and row tables. This Unified MemoryManager smartly keeps track of memory allocations across Spark execution and the Store, elastically expanding into the other if the room is available. Rather than a pre-allocation strategy where Spark memory is independent of the store, SnappyData uses a unified strategy where all allocations come from a common pool. Essentially, it optimizes the memory utilization to the extent possible.  SnappyData also monitors the JVM memory pools and avoids running into out-of-memory conditions in most cases. You can configure the threshold for when data evicts to disk and the critical threshold for heap utilization. When the usage exceeds this critical threshold, memory allocations within SnappyData fail, and a LowMemoryException error is reported. This, however, safeguards the server from crashing due to OutOfMemoryException.", 
            "title": "Memory Management"
        }, 
        {
            "location": "/best_practices/memory_management/#estimating-memory-size-for-column-and-row-tables", 
            "text": "Column tables use compression by default and the amount of compression is dependent on the data itself. While we commonly see compression of 50%, it is also possible to achieve much higher compression ratios when the data has many repeated strings or text. \nRow tables, on the other hand, consume more space than the original data size. There is a per row overhead in SnappyData. While this overhead varies and is dependent on the options configured on the Row table, as a simple guideline we suggest you assume 100 bytes per row as overhead. Thus, it is clear that it is not straightforward to compute the memory requirements.  \nIt is recommended that you take a sample of the data set (as close as possible to your production data) and populate each of the tables. Ensure that you create the required indexes and note down the size estimates (in bytes) in the SnappyData Pulse dashboard. You can then extrapolate this number given the total number of records you anticipate to load or grow into, for the memory requirements for your table.", 
            "title": "Estimating Memory Size for Column and Row Tables"
        }, 
        {
            "location": "/best_practices/memory_management/#disk-and-memory-sizing", 
            "text": "For efficient use of the disk, the best alternative is to load some sample data and extrapolate for both memory and disk requirements. The disk usage is the sum of all the  Total size  of the tables. You can check the value of  Total Size  on the SnappyData Pulse UI. \nFor total disk requirement, the rule of thumb is ~4X data size which accounts for temporary space required for the compactor and the space required for  spark.local.dir . In case of concurrent thread execution,the requirement will differ as mentioned in  spark.local.dir .\nIf the data and the temporary storage set with  spark.local.dir  are in separate locations, then the disk for data storage can be 2X of the total estimated data size while temporary storage can be 2X. The temporary storage is used to shuffle the output of large joins, and a query can potentially shuffle the entire data. Likewise, a massive import can also shuffle data before inserting into partitioned tables.", 
            "title": "Disk and Memory Sizing"
        }, 
        {
            "location": "/best_practices/memory_management/#table-memory-requirements", 
            "text": "SnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. If the memory-size is configured (off-heap is enabled), the entire column table is stored in off-heap memory.  SnappyData row tables memory requirements have to be calculated by taking into account row overheads. Row tables have different amounts of heap memory overhead per table and index entry, which depends on whether you persist table data or configure tables for overflow to disk.     TABLE IS PERSISTED?  OVERFLOW IS CONFIGURED?  APPROXIMATE HEAP OVERHEAD      No  No  64 bytes    Yes  No  120 bytes    Yes  Yes  152 bytes      Note  For a persistent, partitioned row table, SnappyData uses an additional 16 bytes per entry used to improve the speed of recovering data from disk. When an entry is deleted, a tombstone entry of approximately 13 bytes is created and maintained until the tombstone expires or is garbage-collected in the member that hosts the table. (When an entry is destroyed, the member temporarily retains the entry to detect possible conflicts with operations that have occurred. This retained entry is referred to as a tombstone.)      TYPE OF INDEX ENTRY  APPROXIMATE HEAP OVERHEAD      New index entry  80 bytes    First non-unique index entry  24 bytes    Subsequent non-unique index entry  8 bytes to 24 bytes*     If there are more than 100 entries for a single index entry, the heap overhead per entry increases from 8 bytes to approximately 24 bytes.", 
            "title": "Table Memory Requirements"
        }, 
        {
            "location": "/best_practices/memory_management/#estimating-memory-size-for-execution", 
            "text": "Spark and SnappyData also need room for execution. This includes memory for sorting, joining data sets, Spark execution, application managed objects (for example, a UDF allocating memory), etc. Most of these allocations automatically overflow to disk.  But it is strongly recommended to allocate minimum 6-8 GB of heap per data server/lead node for production systems that run large scale analytic queries.  SnappyData is a Java application and by default supports on-heap storage. It also supports off-heap storage, to improve the performance for large blocks of data (for example, columns stored as byte arrays). It is recommended to use off-heap storage for column tables. Row tables are always stored on on-heap. The  memory-size  and  heap-size  properties control the off-heap and on-heap sizes of the SnappyData server process.   SnappyData uses JVM heap memory for most of its allocations. Only column tables can use off-heap storage (if configured). We suggest going through the following options and configuring them appropriately based on the suggested sizing estimates.", 
            "title": "Estimating Memory Size for Execution"
        }, 
        {
            "location": "/best_practices/memory_management/#snappydata-heap-memory", 
            "text": "Heap is provided for row tables and working/temp object memory. For large imports most external connectors still do not use off-heap for temporary buffers.\nSnappyData heap memory regions are divided into two parts called  Heap Storage Pool  and  Heap Execution Pool . Sizes of each pool are determined by the configuration parameters provided at boot time to each server. These two regions are only tentative demarcation and can grow into each other based on some conditions.", 
            "title": "SnappyData Heap Memory"
        }, 
        {
            "location": "/best_practices/memory_management/#heap-storage-pool", 
            "text": "The heap objects which belong to SnappyData storage of Spark storage are accounted here. For example, when a row is inserted into a table or deleted, this pool accounts the memory size of that row. Objects that are temporary and die young are not considered here. As it is difficult and costly to do a precise estimation, this pool is an approximation of heap memory for objects that are going to be long-lived. Since precise estimation of heap memory is difficult, there is a heap monitor thread running in the background.  \nIf the total heap as seen by JVM (and not SnappyUnifiedMemoryManager) exceeds  critical-heap-percentage  the database engine starts canceling jobs and queries and a LowMemoryException is reported. This is also an indication of heap pressure on the system.", 
            "title": "Heap Storage Pool"
        }, 
        {
            "location": "/best_practices/memory_management/#heap-execution-pool", 
            "text": "During query execution or while running a Spark job, all temporary object allocations are done from this pool. For instance, queries like HashJoin and aggregate queries creates expensive in-memory maps. This pool is used to allocate such memory.  You can set the following configuration parameters to control the pools:     Parameter Name  Default Value  Description      heap-size  4GB in SnappyData Embedded mode cluster  Max heap size which can be used by the JVM    spark.memory.storageFraction  50  Fraction of workable memory allocated for storage pool and the remaining memory is allocated to the execution pool. It is recommended that you do not change this setting.    critical-heap-percentage  95  The heap percent beyond which the system considers itself in a critical state. This is to safeguard the system from crashing due to an OutOfMemoryException. Beyond this point, SnappyData starts canceling all jobs and queries and a LowMemoryException is reported.  This means (100 minus  critical-heap-percent ) memory is not allocated to any pool and is unused.    eviction-heap-percentage  85.5  Initially, the amount of memory that is available for storage pool is 50% of the total workable memory. This can however grow up to  eviction-heap-percentage  (default 85.5%). On reaching this threshold it starts evicting table data as per the eviction clause that was specified when creating the table.    spark.memory.fraction  0.97  Total workable memory for execution and storage. This fraction is applied after removing reserved memory (100 minus  critical-heap-percentage ). This gives a cushion before the system reaches a critical state. It is recommended that you do not change this setting.     At the start, each of the two pools is assigned a portion of the available memory. This is driven by  spark.memory.storageFraction  property (default 50%). However, SnappyData allows each pool to \"balloon\" into the other if capacity is available subject to following rules:    The storage pool can grow to the execution pool if the execution pool has some capacity, but not beyond the  eviction-heap-percentage .    If the storage pool cannot borrow from the executor pool, it evicts some of its own blocks to make space for incoming blocks.    If the storage pool has already grown into the execution pool, the execution pool evicts block from the storage pool until the earlier limit (that is, 50% demarcation) is reached. Beyond that, the executor threads cannot evict blocks from the storage pool. If sufficient memory is not available, it can either fall back to disk overflow or wait until sufficient memory is available.    If the storage pool has some free memory, the execution pool can borrow that memory from the storage pool during execution. The borrowed memory is returned back once execution is over.      Example : Configuration for memory (typically configured in  conf/leads  or  conf/servers )   -heap-size=20g -critical-heap-percentage=95 -eviction-heap-percentage=85.5  Example : Depicts how SnappyData derives different memory region sizes.  Reserved_Heap_Memory =  20g * (1 - 0.95) = 1g ( 0.95 being derived from critical_heap_percentage)\nHeap_Memory_Fraction =  (20g - Reserved_Memory) *(0.97) = 17.4 ( 0.97 being derived from spark.memory.fraction)\nHeap_Storage_Pool_Size =  17.4 * (0.5) = 8.73 ( 0.5 being derived from spark.memory.storageFraction)\nHeap_Execution_Pool_Size =  17.4 * (0.5) = 8.73\nHeap_Max_Storage_pool_Size =  17.4 * 0.85 = 14.7 ( 0.85 derived from eviction_heap_percentage)", 
            "title": "Heap Execution Pool:"
        }, 
        {
            "location": "/best_practices/memory_management/#snappydata-off-heap-memory", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   In addition to heap memory, SnappyData can also be configured with off-heap memory. If configured, column table data, as well as many of the execution structures use off-heap memory. For a serious installation, the off-heap setting is recommended. However, several artifacts in the product need heap memory, so some minimum heap size is also required for this.     Parameter Name  Default Value  Description      memory-size  The default value is either 0 or it gets auto-configured in  specific scenarios .  Total off-heap memory size     Similar to heap pools, off-heap pools are also divided between off-heap storage pool and off-heap execution pool. The rules of borrowing memory from each other also remains same.   Example : Off-heap configuration:   -heap-size = 4g -memory-size=16g -critical-heap-percentage=95 -eviction-heap-percentage=85.5  Example : How SnappyData derives different memory region sizes.  Reserved_Memory ( Heap Memory) =  4g * (1 - 0.95) = 200m ( 0.95 being derived from critical_heap_percentage)\nMemory_Fraction ( Heap Memory) =  (4g - Reserved_Memory) *(0.97) = 3.5g\nHeap Storage_Pool_Size =  3.5 * (0.5) = 1.75\nHeap Execution_Pool_Size =  3.5 * (0.5) = 1.75\nMax_Heap_Storage_pool_Size =  3.5g * 0.85 = 2.9 ( 0.85 derived from eviction_heap_percentage)\n\n\nOff-Heap Storage_Pool_Size =  16g * (0.5) = 8g\nOff-Heap Execution_Pool_Size =  16g * (0.5) = 8g\nMax_Off_Heap_Storage_pool_Size =  16g * 0.9 = 14.4 ( 0.9 System default)   Note    For row tables: According to the requirements of your row table size, configure the heap size. Row tables in SnappyData do not use off-heap memory.    To read write Parquet and CSV: Parquet and CSV read-write are memory consuming activities for which heap memory is used. Ensure that you provision sufficient heap memory for this.    When most of your data reside in column tables: Use off-heap memory, as they are faster and puts less pressure on garbage collection threads.    When configuring eviction: The tables are evicted to disk by default. This impacts performance to some degree and hence it is recommended to size your VM before you begin.", 
            "title": "SnappyData Off-Heap Memory"
        }, 
        {
            "location": "/best_practices/ha_considerations/", 
            "text": "High Availability (HA) Considerations\n\n\n \n\n\nHigh availability options are available for all the SnappyData components.\n\n\nLead\n \n\nSnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of  the secondary lead nodes takes over immediately. \n\nSetting up the secondary lead node\n is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries and jobs that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted.\n\n\nLocator\n\nSnappyData supports multiple locators in the cluster for high availability. \nIt is recommended to set up multiple locators (ideally two). If a locator becomes unavailable, the cluster continues to be available. However, new members cannot join the cluster.\n\nWith multiple locators, there are no impact on the clients and the fail over recovery is completely transparent.\n\n\nDataServer\n\nSnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data.  So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. \nHowever, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again. \n\n\nKnown Limitation\n\n\nIn case of lead HA, the new primary lead node creates a new Snappy session for the JDBC clients. This means session specific properties (for example, \nspark.sql.autoBroadcastJoinThreshold\n, \nsnappydata.sql.hashJoinSize\n) need to be set again after a lead fail over.", 
            "title": "HA Considerations"
        }, 
        {
            "location": "/best_practices/ha_considerations/#high-availability-ha-considerations", 
            "text": "High availability options are available for all the SnappyData components.  Lead   \nSnappyData supports secondary lead nodes. If the primary lead becomes unavailable, one of  the secondary lead nodes takes over immediately.  Setting up the secondary lead node  is highly recommended because the system cannot function if the lead node is unavailable. Currently, the queries and jobs that are executing when the primary lead becomes unavailable, are not re-tried and have to be resubmitted.  Locator \nSnappyData supports multiple locators in the cluster for high availability. \nIt is recommended to set up multiple locators (ideally two). If a locator becomes unavailable, the cluster continues to be available. However, new members cannot join the cluster. \nWith multiple locators, there are no impact on the clients and the fail over recovery is completely transparent.  DataServer \nSnappyData supports redundant copies of data for fault tolerance. A table can be configured to store redundant copies of the data.  So, if a server is unavailable, and if there is a redundant copy available on some other server, the tasks are automatically retried on those servers. This is totally transparent to the user. \nHowever, the redundant copies double the memory requirements. If there are no redundant copies and a server with some data goes down, the execution of the queries fail and PartitionOfflineException is reported. The execution does not begin until that server is available again.", 
            "title": "High Availability (HA) Considerations"
        }, 
        {
            "location": "/best_practices/ha_considerations/#known-limitation", 
            "text": "In case of lead HA, the new primary lead node creates a new Snappy session for the JDBC clients. This means session specific properties (for example,  spark.sql.autoBroadcastJoinThreshold ,  snappydata.sql.hashJoinSize ) need to be set again after a lead fail over.", 
            "title": "Known Limitation"
        }, 
        {
            "location": "/best_practices/important_settings/", 
            "text": "Important Settings \n\n\nResource allocation is important for the execution of any job. If not configured correctly, the job can consume the entire clusters resources and cause execution failure because of memory and other related problems.\n\n\nThis section provides guidelines for configuring the following important settings:\n\n\n\n\n\n\nBuckets\n\n\n\n\n\n\nmember-timeout\n\n\n\n\n\n\nspark.local.dir\n\n\n\n\n\n\nOperating System Settings\n\n\n\n\n\n\nSnappyData Smart Connector mode and Local mode Settings\n\n\n\n\n\n\nCode Generation and Tokenization\n\n\n\n\n\n\n\n\nBuckets\n\n\nA bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, buckets are moved around to ensure that the data is balanced across the nodes where the table is defined.\n\n\nThe default number of buckets in the SnappyData cluster mode is 128. In the local mode, it is cores*2, subject to a maximum of 64 buckets and a minimum of 8 buckets.\n\n\nThe number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow.\n\n\nIf there are more buckets in a table than required, it means there is less data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. Similarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently. Also, if the cluster is scaled at a later point of time rebalancing may not be optimal.\n\n\nFor column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data. This attribute is set when \ncreating a table\n.\n\n\nmember-timeout\n\n\n\n\nThe default \nmember-timeout\n in SnappyData cluster is 30 seconds. The default \nspark.network.timeout\n is 120 seconds and \nspark.executor.heartbeatInterval\n is 10 seconds as noted in the \nSpark documents\n. \n \nIf applications require node failure detection to be faster, then these properties should be reduced accordingly (\nspark.executor.heartbeatInterval\n but must always be much lower than \nspark.network.timeout\n as specified in the Spark Documents). \n\nHowever, note that this can cause spurious node failures to be reported due to GC pauses. For example, the applications with reduced settings need to be resistant to job failures due to GC settings.\n\n\nThis attribute is set in the \nconfiguration files\n \nconf/locators\n, \nconf/servers\n and \nconf/leads\n files. \n\n\n\n\nspark.local.dir\n\n\nSnappyData writes table data on disk.  By default, the disk location that SnappyData uses is the directory specified using \n-dir\n option, while starting the member. \nSnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size. \n\nTo achieve better performance, it is recommended to store temporary data on a different disk (preferably using SSD storage) than the table data. This can be done by setting the \nspark.local.dir\n property to a location with enough space. For example, ~2X of the data size, in case of single thread execution. In case of concurrent thread execution, the requirement for temp space is approximately data size * number of threads. For example, if the data size in the cluster is 100 GB and three threads are executing concurrent ad hoc analytical queries in the cluster, then the temp space should be ~3X of the data size. This property is set in \nconf/leads\n as follows:\n\n\nlocalhost -spark.local.dir=/path/to/local-directory \n\n\n\n\nThe path specified is inherited by all servers. The temporary data defaults to \n/tmp\n. In case different paths are required on each of the servers, then remove the property from \nconf/leads\n and instead set as system property in each of the \nconf/servers\n file as follows:\n\n\nlocalhost ... -J-Dspark.local.dir=/path/to/local-directory1\n\n\n\n\n\n\nOperating System Settings\n\n\nFor best performance, the following operating system settings are recommended on the lead and server nodes.\n\n\nUlimit\n \n \nSpark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes. \n\n\nA minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K. \n\nTo change the limits of these settings for a user, the \n/etc/security/limits.conf\n file needs to be updated. A typical \nlimits.conf\n used for SnappyData servers and leads appears as follows: \n\n\nec2-user          hard    nofile      32768\nec2-user          soft    nofile      32768\nec2-user          hard    nproc       unlimited\nec2-user          soft    nproc       524288\nec2-user          hard    sigpending  unlimited\nec2-user          soft    sigpending  524288\n\n\n\n\n\n\nec2-user\n is the user running SnappyData.\n\n\n\n\nRecent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) need the NOFILE limit to be increased in systemd configuration too. Edit \n/etc/systemd/system.conf \n as root, search for \n#DefaultLimitNOFILE\n under the \n[Manager] \nsection. Uncomment and change it to \nDefaultLimitNOFILE=32768\n. \nReboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with \n\"ulimit -a -S\"\n (soft limits) and \n\"ulimit -a -H\"\n (hard limits).\n\n\nOS Cache Size\n \nWhen there is a lot of disk activity especially during table joins and during an eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages.\n \n\n\nAdd the following to \n/etc/sysctl.conf\n using the command \nsudo vim /etc/sysctl.conf\n or \nsudo gedit /etc/sysctl.conf\n or by using an editor of your choice:\n\n\nvm.dirty_background_ratio=2\nvm.dirty_ratio=4\nvm.dirty_expire_centisecs=2000\nvm.dirty_writeback_centisecs=300\n\n\n\n\nThen apply to current session using the command \nsudo sysctl -p\n\n\nThese settings lower the OS cache buffer sizes which reduce the long GC pauses during disk flush but can decrease overall disk write throughput. This is especially true for slower magnetic disks where the bulk insert throughput can see a noticeable drop (such as 20%), while the duration of GC pauses should reduce significantly (such as 50% or more). If long GC pauses, for example in the range of 10s of seconds, during bulk inserts, updates, or deletes is not a problem then these settings can be skipped.\n\n\nSwap File\n \n \nSince modern operating systems perform lazy allocation, it has been observed that despite setting \n-Xmx\n and \n-Xms\n settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in the process going down.\n\nIt is recommended to set swap space on your system using the following commands:\n\n\n# sets a swap space of 32 GB\n\n## If fallocate is available, run the following command: \nsudo sh -c \nfallocate -l 32G /var/swapfile \n chmod 0600 /var/swapfile \n mkswap /var/swapfile \n swapon /var/swapfile\n\n## fallocate is recommended since it is much faster, although not supported by some filesystems such as ext3 and zfs.\n## In case fallocate is not available, use dd:\nsudo dd if=/dev/zero of=/var/swapfile bs=1M count=32768\nsudo chmod 600 /var/swapfile\nsudo mkswap /var/swapfile\nsudo swapon /var/swapfile\n\n\n\n\n\n\nSnappyData Smart Connector Mode and Local Mode Settings\n\n\nManaging Executor Memory\n\n\nFor efficient loading of data from a Smart Connector application or a Local Mode application, all the partitions of the input data are processed in parallel by making use of all the available cores. Further, to have better ingestion speed, small internal columnar storage structures are created in the Spark application's cluster itself, which is then directly inserted into the required buckets of the column table in the SnappyData cluster.\nThese internal structures are in encoded form, and for efficient encoding, some memory space is acquired upfront which is independent of the amount of data to be loaded into the tables. \n\nFor example, if there are 32 cores for the Smart Connector application and the number of buckets of the column table is equal or more than that, then, each of the 32 executor threads can take around 32MB of memory. This indicates that 32MB * 32MB (1 GB) of memory is required. Thus, the default of 1GB for executor memory is not sufficient, and therefore a default of at least 2 GB is recommended in this case.\n\n\nYou can modify this setting in the \nspark.executor.memory\n property. For more information, refer to the \nSpark documentation\n.\n\n\nJVM settings for optimal performance\n\n\nThe following JVM settings are set by default on the server nodes of SnappyData cluster. You can use these as guidelines for smart connector and local modes:\n\n\n\n\n-XX:+UseParNewGC\n\n\n-XX:+UseConcMarkSweepGC\n\n\n-XX:CMSInitiatingOccupancyFraction=50\n\n\n-XX:+CMSClassUnloadingEnabled\n\n\n-XX:-DontCompileHugeMethods\n\n\n-XX:CompileThreshold=2000\n\n\n-XX:+UnlockDiagnosticVMOptions\n\n\n-XX:ParGCCardsPerStrideChunk=4k\n\n\n-Djdk.nio.maxCachedBufferSize=131072\n\n\n\n\nExample\n:\n\n\n-XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k\n\n\n\n\nCMS collector with ParNew is used by default as above and recommended. GC settings set above have been seen to work best in representative workloads and can be tuned further as per application. For enterprise users \noff-heap\n is recommended for best performance.\n\n\nSet in the \nconf/locators\n, \nconf/leads\n, and \nconf/servers\n file.\n\n\n \n\n\nHandling Out-of-Memory Error in SnappyData Cluster\n\n\nWhen the SnappyData cluster faces an Out-Of-Memory (OOM) situation, it may not function appropriately, and the JVM cannot create a new process to execute the kill command upon OOM. See \nJDK-8027434\n.\n However, JVM uses the \nfork()\n system call to execute the kill command. This system call can fail for large JVMs due to memory overcommit limits in the operating system. Therefore, to solve such issues in SnappyData, \njvmkill\n is used which has much smaller memory requirements.\n\n\njvmkill\n is a simple JVMTI agent that forcibly terminates the JVM when it is unable to allocate memory or create a thread. It is also essential for reliability purposes because an OOM error can often leave the JVM in an inconsistent state. Whereas, terminating the JVM allows it to be restarted by an external process manager. \nA common alternative to this agent is to use the \n-XX:OnOutOfMemoryError\n JVM argument to execute a \nkill -9\n command. \njvmkill\n is applied by default to all the nodes in a SnappyData cluster, that is the server, lead, and locator nodes. The \njvmkill\n agent is useful in a smart connector as well as in a local mode too.\n\n\nOptionally when using the \n-XX:+HeapDumpOnOutOfMemoryError\n option, you can specify the timeout period for scenarios when the heap dump takes an unusually long time or hangs up. This option can be specified in the configuration file for leads, locators, or servers respectively. For example:\n-snappydata.onCriticalHeapDumpTimeoutSeconds=10\n\n\njvmkill\n agent issues a \nSIGTERM\n signal initially and waits for a default period of 30 seconds. Thereby allowing for graceful shutdown before issuing a \nSIGKILL\n if the PID is still running. You can also set the environment variable JVMKILL_SLEEP_SECONDS to set the timeout period. For example: \nexport JVMKILL_SLEEP_SECONDS=10\n\n\njvmkill\n is verified on centos6 and Mac OSX versions. For running SnappyData on any other versions, you can recompile the \nlib\n files by running the \nsnappyHome/aqp/src/main/cpp/io/snappydata/build.sh\n script. This script replaces the \nlib\n file located at the following path:\n\n\n\n\n\n\nFor Linux \n\n    \nagentPath snappyHome/jars/libgemfirexd.so\n\n\n\n\n\n\nFor Mac\n\n    \nagentPath snappyHome/jars/libgemfirexd.dylib\n\n\n\n\n\n\n\n\nCode Generation and Tokenization\n\n\nSnappyData uses generated code for best performance for most of the queries and internal operations. This is done for both Spark-side whole-stage code generation for queries, for example,\nTechnical Preview of Apache Spark 2.0 blog\n, and internally by SnappyData for many operations. For example, rolling over data from row buffer to column store or merging batches among others. \nThe point key lookup queries on row tables, and JDBC inserts bypass this and perform direct operations. However, for all other operations, the product uses code generation for best performance.\n\n\nIn many cases, the first query execution is slightly slower than subsequent query executions. This is primarily due to the overhead of compilation of generated code for the query plan and optimized machine code generation by JVM's hotspot JIT.\nEach distinct piece of generated code is a separate class which is loaded using its own ClassLoader. To reduce these overheads in multiple runs, this class is reused using a cache whose size is controlled by \nspark.sql.codegen.cacheSize\n property (default is 2000). Thus when the size limit of the cache is breached, the older classes that are used for a while gets removed from the cache.\n\n\nFurther to minimize the generated plans, SnappyData performs tokenization of the values that are most constant in queries by default. Therefore the queries that differ only in constants can still create the same generated code plan.\nThus if an application has a fixed number of query patterns that are used repeatedly, then the effect of the slack during the first execution, due to compilation and JIT, is minimized.\n\n\n\n\nNote\n\n\nA single query pattern constitutes of queries that differ only in constant values that are embedded in the query string.\n\n\n\n\nFor cases where the application has many query patterns, you can increase the value of \nspark.sql.codegen.cacheSize\n property from the default size of \n2000\n. \n\n\nYou can also increase the value for JVM's \nReservedCodeCacheSize\n property and add additional RAM capacity accordingly. \n\n\n\n\nNote\n\n\nIn the smart connector mode, Apache Spark has the default cache size as 100 which cannot be changed while the same property works if you are using SnappyData's Spark distribution.", 
            "title": "Important Settings"
        }, 
        {
            "location": "/best_practices/important_settings/#important-settings", 
            "text": "Resource allocation is important for the execution of any job. If not configured correctly, the job can consume the entire clusters resources and cause execution failure because of memory and other related problems.  This section provides guidelines for configuring the following important settings:    Buckets    member-timeout    spark.local.dir    Operating System Settings    SnappyData Smart Connector mode and Local mode Settings    Code Generation and Tokenization", 
            "title": "Important Settings "
        }, 
        {
            "location": "/best_practices/important_settings/#buckets", 
            "text": "A bucket is the smallest unit of in-memory storage for SnappyData tables. Data in a table is distributed evenly across all the buckets. When a new server joins or an existing server leaves the cluster, buckets are moved around to ensure that the data is balanced across the nodes where the table is defined.  The default number of buckets in the SnappyData cluster mode is 128. In the local mode, it is cores*2, subject to a maximum of 64 buckets and a minimum of 8 buckets.  The number of buckets has an impact on query performance, storage density, and ability to scale the system as data volumes grow.  If there are more buckets in a table than required, it means there is less data per bucket. For column tables, this may result in reduced compression that SnappyData achieves with various encodings. Similarly, if there are not enough buckets in a table, not enough partitions are created while running a query and hence cluster resources are not used efficiently. Also, if the cluster is scaled at a later point of time rebalancing may not be optimal.  For column tables, it is recommended to set a number of buckets such that each bucket has at least 100-150 MB of data. This attribute is set when  creating a table .", 
            "title": "Buckets"
        }, 
        {
            "location": "/best_practices/important_settings/#member-timeout", 
            "text": "The default  member-timeout  in SnappyData cluster is 30 seconds. The default  spark.network.timeout  is 120 seconds and  spark.executor.heartbeatInterval  is 10 seconds as noted in the  Spark documents .   \nIf applications require node failure detection to be faster, then these properties should be reduced accordingly ( spark.executor.heartbeatInterval  but must always be much lower than  spark.network.timeout  as specified in the Spark Documents).  \nHowever, note that this can cause spurious node failures to be reported due to GC pauses. For example, the applications with reduced settings need to be resistant to job failures due to GC settings.  This attribute is set in the  configuration files   conf/locators ,  conf/servers  and  conf/leads  files.", 
            "title": "member-timeout"
        }, 
        {
            "location": "/best_practices/important_settings/#sparklocaldir", 
            "text": "SnappyData writes table data on disk.  By default, the disk location that SnappyData uses is the directory specified using  -dir  option, while starting the member. \nSnappyData also uses temporary storage for storing intermediate data. The amount of intermediate data depends on the type of query and can be in the range of the actual data size.  \nTo achieve better performance, it is recommended to store temporary data on a different disk (preferably using SSD storage) than the table data. This can be done by setting the  spark.local.dir  property to a location with enough space. For example, ~2X of the data size, in case of single thread execution. In case of concurrent thread execution, the requirement for temp space is approximately data size * number of threads. For example, if the data size in the cluster is 100 GB and three threads are executing concurrent ad hoc analytical queries in the cluster, then the temp space should be ~3X of the data size. This property is set in  conf/leads  as follows:  localhost -spark.local.dir=/path/to/local-directory   The path specified is inherited by all servers. The temporary data defaults to  /tmp . In case different paths are required on each of the servers, then remove the property from  conf/leads  and instead set as system property in each of the  conf/servers  file as follows:  localhost ... -J-Dspark.local.dir=/path/to/local-directory1", 
            "title": "spark.local.dir"
        }, 
        {
            "location": "/best_practices/important_settings/#operating-system-settings", 
            "text": "For best performance, the following operating system settings are recommended on the lead and server nodes.  Ulimit    \nSpark and SnappyData spawn a number of threads and sockets for concurrent/parallel processing so the server and lead node machines may need to be configured for higher limits of open files and threads/processes.   A minimum of 8192 is recommended for open file descriptors limit and nproc limit to be greater than 128K.  To change the limits of these settings for a user, the  /etc/security/limits.conf  file needs to be updated. A typical  limits.conf  used for SnappyData servers and leads appears as follows:   ec2-user          hard    nofile      32768\nec2-user          soft    nofile      32768\nec2-user          hard    nproc       unlimited\nec2-user          soft    nproc       524288\nec2-user          hard    sigpending  unlimited\nec2-user          soft    sigpending  524288   ec2-user  is the user running SnappyData.   Recent linux distributions using systemd (like RHEL/CentOS 7, Ubuntu 18.04) need the NOFILE limit to be increased in systemd configuration too. Edit  /etc/systemd/system.conf   as root, search for  #DefaultLimitNOFILE  under the  [Manager]  section. Uncomment and change it to  DefaultLimitNOFILE=32768 . \nReboot for the above changes to be applied. Confirm that the new limits have been applied in a terminal/ssh window with  \"ulimit -a -S\"  (soft limits) and  \"ulimit -a -H\"  (hard limits).  OS Cache Size  \nWhen there is a lot of disk activity especially during table joins and during an eviction, the process may experience GC pauses. To avoid such situations, it is recommended to reduce the OS cache size by specifying a lower dirty ratio and less expiry time of the dirty pages.    Add the following to  /etc/sysctl.conf  using the command  sudo vim /etc/sysctl.conf  or  sudo gedit /etc/sysctl.conf  or by using an editor of your choice:  vm.dirty_background_ratio=2\nvm.dirty_ratio=4\nvm.dirty_expire_centisecs=2000\nvm.dirty_writeback_centisecs=300  Then apply to current session using the command  sudo sysctl -p  These settings lower the OS cache buffer sizes which reduce the long GC pauses during disk flush but can decrease overall disk write throughput. This is especially true for slower magnetic disks where the bulk insert throughput can see a noticeable drop (such as 20%), while the duration of GC pauses should reduce significantly (such as 50% or more). If long GC pauses, for example in the range of 10s of seconds, during bulk inserts, updates, or deletes is not a problem then these settings can be skipped.  Swap File    \nSince modern operating systems perform lazy allocation, it has been observed that despite setting  -Xmx  and  -Xms  settings, at runtime, the operating system may fail to allocate new pages to the JVM. This can result in the process going down. \nIt is recommended to set swap space on your system using the following commands:  # sets a swap space of 32 GB\n\n## If fallocate is available, run the following command: \nsudo sh -c  fallocate -l 32G /var/swapfile   chmod 0600 /var/swapfile   mkswap /var/swapfile   swapon /var/swapfile \n## fallocate is recommended since it is much faster, although not supported by some filesystems such as ext3 and zfs.\n## In case fallocate is not available, use dd:\nsudo dd if=/dev/zero of=/var/swapfile bs=1M count=32768\nsudo chmod 600 /var/swapfile\nsudo mkswap /var/swapfile\nsudo swapon /var/swapfile", 
            "title": "Operating System Settings"
        }, 
        {
            "location": "/best_practices/important_settings/#snappydata-smart-connector-mode-and-local-mode-settings", 
            "text": "", 
            "title": "SnappyData Smart Connector Mode and Local Mode Settings"
        }, 
        {
            "location": "/best_practices/important_settings/#managing-executor-memory", 
            "text": "For efficient loading of data from a Smart Connector application or a Local Mode application, all the partitions of the input data are processed in parallel by making use of all the available cores. Further, to have better ingestion speed, small internal columnar storage structures are created in the Spark application's cluster itself, which is then directly inserted into the required buckets of the column table in the SnappyData cluster.\nThese internal structures are in encoded form, and for efficient encoding, some memory space is acquired upfront which is independent of the amount of data to be loaded into the tables.  \nFor example, if there are 32 cores for the Smart Connector application and the number of buckets of the column table is equal or more than that, then, each of the 32 executor threads can take around 32MB of memory. This indicates that 32MB * 32MB (1 GB) of memory is required. Thus, the default of 1GB for executor memory is not sufficient, and therefore a default of at least 2 GB is recommended in this case.  You can modify this setting in the  spark.executor.memory  property. For more information, refer to the  Spark documentation .", 
            "title": "Managing Executor Memory"
        }, 
        {
            "location": "/best_practices/important_settings/#jvm-settings-for-optimal-performance", 
            "text": "The following JVM settings are set by default on the server nodes of SnappyData cluster. You can use these as guidelines for smart connector and local modes:   -XX:+UseParNewGC  -XX:+UseConcMarkSweepGC  -XX:CMSInitiatingOccupancyFraction=50  -XX:+CMSClassUnloadingEnabled  -XX:-DontCompileHugeMethods  -XX:CompileThreshold=2000  -XX:+UnlockDiagnosticVMOptions  -XX:ParGCCardsPerStrideChunk=4k  -Djdk.nio.maxCachedBufferSize=131072   Example :  -XX:-DontCompileHugeMethods -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=4k  CMS collector with ParNew is used by default as above and recommended. GC settings set above have been seen to work best in representative workloads and can be tuned further as per application. For enterprise users  off-heap  is recommended for best performance.  Set in the  conf/locators ,  conf/leads , and  conf/servers  file.", 
            "title": "JVM settings for optimal performance"
        }, 
        {
            "location": "/best_practices/important_settings/#handling-out-of-memory-error-in-snappydata-cluster", 
            "text": "When the SnappyData cluster faces an Out-Of-Memory (OOM) situation, it may not function appropriately, and the JVM cannot create a new process to execute the kill command upon OOM. See  JDK-8027434 .  However, JVM uses the  fork()  system call to execute the kill command. This system call can fail for large JVMs due to memory overcommit limits in the operating system. Therefore, to solve such issues in SnappyData,  jvmkill  is used which has much smaller memory requirements.  jvmkill  is a simple JVMTI agent that forcibly terminates the JVM when it is unable to allocate memory or create a thread. It is also essential for reliability purposes because an OOM error can often leave the JVM in an inconsistent state. Whereas, terminating the JVM allows it to be restarted by an external process manager.  A common alternative to this agent is to use the  -XX:OnOutOfMemoryError  JVM argument to execute a  kill -9  command.  jvmkill  is applied by default to all the nodes in a SnappyData cluster, that is the server, lead, and locator nodes. The  jvmkill  agent is useful in a smart connector as well as in a local mode too.  Optionally when using the  -XX:+HeapDumpOnOutOfMemoryError  option, you can specify the timeout period for scenarios when the heap dump takes an unusually long time or hangs up. This option can be specified in the configuration file for leads, locators, or servers respectively. For example: -snappydata.onCriticalHeapDumpTimeoutSeconds=10  jvmkill  agent issues a  SIGTERM  signal initially and waits for a default period of 30 seconds. Thereby allowing for graceful shutdown before issuing a  SIGKILL  if the PID is still running. You can also set the environment variable JVMKILL_SLEEP_SECONDS to set the timeout period. For example:  export JVMKILL_SLEEP_SECONDS=10  jvmkill  is verified on centos6 and Mac OSX versions. For running SnappyData on any other versions, you can recompile the  lib  files by running the  snappyHome/aqp/src/main/cpp/io/snappydata/build.sh  script. This script replaces the  lib  file located at the following path:    For Linux  \n     agentPath snappyHome/jars/libgemfirexd.so    For Mac \n     agentPath snappyHome/jars/libgemfirexd.dylib", 
            "title": "Handling Out-of-Memory Error in SnappyData Cluster"
        }, 
        {
            "location": "/best_practices/important_settings/#code-generation-and-tokenization", 
            "text": "SnappyData uses generated code for best performance for most of the queries and internal operations. This is done for both Spark-side whole-stage code generation for queries, for example, Technical Preview of Apache Spark 2.0 blog , and internally by SnappyData for many operations. For example, rolling over data from row buffer to column store or merging batches among others.  The point key lookup queries on row tables, and JDBC inserts bypass this and perform direct operations. However, for all other operations, the product uses code generation for best performance.  In many cases, the first query execution is slightly slower than subsequent query executions. This is primarily due to the overhead of compilation of generated code for the query plan and optimized machine code generation by JVM's hotspot JIT.\nEach distinct piece of generated code is a separate class which is loaded using its own ClassLoader. To reduce these overheads in multiple runs, this class is reused using a cache whose size is controlled by  spark.sql.codegen.cacheSize  property (default is 2000). Thus when the size limit of the cache is breached, the older classes that are used for a while gets removed from the cache.  Further to minimize the generated plans, SnappyData performs tokenization of the values that are most constant in queries by default. Therefore the queries that differ only in constants can still create the same generated code plan.\nThus if an application has a fixed number of query patterns that are used repeatedly, then the effect of the slack during the first execution, due to compilation and JIT, is minimized.   Note  A single query pattern constitutes of queries that differ only in constant values that are embedded in the query string.   For cases where the application has many query patterns, you can increase the value of  spark.sql.codegen.cacheSize  property from the default size of  2000 .   You can also increase the value for JVM's  ReservedCodeCacheSize  property and add additional RAM capacity accordingly.    Note  In the smart connector mode, Apache Spark has the default cache size as 100 which cannot be changed while the same property works if you are using SnappyData's Spark distribution.", 
            "title": "Code Generation and Tokenization"
        }, 
        {
            "location": "/best_practices/transactions_best_practices/", 
            "text": "SnappyData Distributed Transactions\n\n\n\n\nUsing Transactions\n\n\n\n\n\n\nFor high performance, mimimize the duration of transactions to avoid conflicts with other concurrent transactions. If atomicity for only single row updates is required, then completely avoid using transactions because SnappyData provides atomicity and isolation for single rows without transactions.\n\n\n\n\n\n\nWhen using transactions, keep the number of rows involved in the transaction as low as possible. SnappyData acquires locks eagerly, and long-lasting transactions increase the probability of conflicts and transaction failures. Avoid transactions for large batch update statements or statements that effect a lot of rows. \n\n\n\n\n\n\nUnlike in traditional databases, SnappyData transactions can fail with a conflict exception on writes instead of on commit. This choice makes sense given that the outcome of the transaction has been determined to fail.\n\n\n\n\n\n\nTo the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided.\n\n\n\n\n\n\nDDL Statements in a transaction\n    SnappyData permits schema and data manipulation statements (DML) within a single transaction. A data definition statement (DDL) is not automatically committed when it is performed, but participates in the transaction within which it is issued.\n\n\nAlthough the table itself becomes visible in the system immediately, it acquires exclusive locks on the system tables and the affected tables on all the members in the cluster, so that any DML operations in other transactions will block and wait for the table's locks.\n\n\nFor example, if a new index is created on a table in a transaction, then all other transactions that refer to that table wait for the transaction to commit or roll back. Because of this behavior, as a best practice you should keep transactions that involve DDL statements short (preferably in a single transaction by itself).\n\n\n\n\n\n\n\n\nUsing Snapshot Isolation\n\n\nTo the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided. In case of failure, the rollback is complete and not partial.\n\n\nMore information\n\n\n\n\n\n\nOverview of SnappyData Distributed Transactions\n\n\n\n\n\n\nHow to use Transactions Isolation Levels", 
            "title": "SnappyData Distributed Transactions"
        }, 
        {
            "location": "/best_practices/transactions_best_practices/#snappydata-distributed-transactions", 
            "text": "", 
            "title": "SnappyData Distributed Transactions"
        }, 
        {
            "location": "/best_practices/transactions_best_practices/#using-transactions", 
            "text": "For high performance, mimimize the duration of transactions to avoid conflicts with other concurrent transactions. If atomicity for only single row updates is required, then completely avoid using transactions because SnappyData provides atomicity and isolation for single rows without transactions.    When using transactions, keep the number of rows involved in the transaction as low as possible. SnappyData acquires locks eagerly, and long-lasting transactions increase the probability of conflicts and transaction failures. Avoid transactions for large batch update statements or statements that effect a lot of rows.     Unlike in traditional databases, SnappyData transactions can fail with a conflict exception on writes instead of on commit. This choice makes sense given that the outcome of the transaction has been determined to fail.    To the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided.    DDL Statements in a transaction\n    SnappyData permits schema and data manipulation statements (DML) within a single transaction. A data definition statement (DDL) is not automatically committed when it is performed, but participates in the transaction within which it is issued.  Although the table itself becomes visible in the system immediately, it acquires exclusive locks on the system tables and the affected tables on all the members in the cluster, so that any DML operations in other transactions will block and wait for the table's locks.  For example, if a new index is created on a table in a transaction, then all other transactions that refer to that table wait for the transaction to commit or roll back. Because of this behavior, as a best practice you should keep transactions that involve DDL statements short (preferably in a single transaction by itself).", 
            "title": "Using Transactions"
        }, 
        {
            "location": "/best_practices/transactions_best_practices/#using-snapshot-isolation", 
            "text": "To the extent possible, model your database so that most transactions operate on colocated data. When all transactional data is on a single member, then stricter isolation guarantees are provided. In case of failure, the rollback is complete and not partial.  More information    Overview of SnappyData Distributed Transactions    How to use Transactions Isolation Levels", 
            "title": "Using Snapshot Isolation"
        }, 
        {
            "location": "/aqp/", 
            "text": "Overview of Synopsis Data Engine (SDE)\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\n\n\nNote\n\n\nThis is the beta version of the SDE feature which is still undergoing final testing before its official release. \n\n\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nKey Concepts\n\n\n\n\n\n\nWorking with Stratified Samples\n\n\n\n\n\n\nRunning Queries\n\n\n\n\n\n\nMore Examples\n\n\n\n\n\n\nSample Selection\n\n\n\n\n\n\nHigh-level Accuracy Contracts (HAC)\n\n\n\n\n\n\nSketching\n\n\n\n\n\n\nThe SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large datasets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time. \n\n\nFor instance, in exploratory analytics, a data analyst might be slicing and dicing large datasets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering), while the engineer continues to slice and dice the datasets without any interruptions. \n\n\nWhen accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters.\n\n\nWhile in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records. \n\n\nUnlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a prior known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query.\n\n\nHow does it work?\n\n\nThe following diagram provides a simplified view of how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can, however, be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample. \n\n\n\n\nUsing SDE\n\n\nIn the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.", 
            "title": "Synopsis Data Engine (SDE)"
        }, 
        {
            "location": "/aqp/#overview-of-synopsis-data-engine-sde", 
            "text": "This feature is available only in the Enterprise version of SnappyData.    Note  This is the beta version of the SDE feature which is still undergoing final testing before its official release.    The following topics are covered in this section:    Key Concepts    Working with Stratified Samples    Running Queries    More Examples    Sample Selection    High-level Accuracy Contracts (HAC)    Sketching    The SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large datasets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire dataset. The approach trades off query accuracy for fast response time.   For instance, in exploratory analytics, a data analyst might be slicing and dicing large datasets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering), while the engineer continues to slice and dice the datasets without any interruptions.   When accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters.  While in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records.   Unlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a prior known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query.  How does it work?  The following diagram provides a simplified view of how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can, however, be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample.    Using SDE  In the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.", 
            "title": "Overview of Synopsis Data Engine (SDE)"
        }, 
        {
            "location": "/sde/key_concepts/", 
            "text": "Key Concepts\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nSnappyData SDE relies on two methods for approximations - \nStratified Sampling\n and \nSketching\n. A brief introduction to these concepts is provided below.\n\n\nStratified Sampling\n\n\nSampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying.\n\n\nTake this simple example table that manages AdImpressions. If random sample is created that is a third of the original size two records is picked in random. \nThis is depicted in the following figure:\n\n\n\n\nIf a query is executed, like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample. \n\n\n\n\nBut, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries. \n\n\nStratified sampling, on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata have enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer. \n\n\n\n\nTo understand these concepts in further detail, refer to the \nhandbook\n. It explains different sampling strategies, error estimation mechanisms, and various types of data synopses.\n\n\nOnline Sampling\n\n\nSDE also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark DataFrame APIs to create a uniform random sample over static RDDs. For online sampling, SDE first does \nreservoir sampling\n for each startum in a write-optimized store before flushing it into a read-optimized store for stratified samples. \nThere is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, SnappyData can ensure having enough samples over each 5-minute time window, while still ensuring that all GEOs have good representation in the sample.\n\n\nSketching\n\n\nWhile stratified sampling ensures that data dimensions with low representation are captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, other data structures like a Count-min-sketch are relied on to capture data frequencies in a stream. This is a data structure that requires that it captures how often an element is seen in a stream for the top-N such elements. \nWhile a \nCount-min-sketch\n is well described, SDE extends this with support for providing top-K estimates over time series data.", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/sde/key_concepts/#key-concepts", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   SnappyData SDE relies on two methods for approximations -  Stratified Sampling  and  Sketching . A brief introduction to these concepts is provided below.", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/sde/key_concepts/#stratified-sampling", 
            "text": "Sampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying.  Take this simple example table that manages AdImpressions. If random sample is created that is a third of the original size two records is picked in random. \nThis is depicted in the following figure:   If a query is executed, like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample.    But, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries.   Stratified sampling, on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata have enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer.    To understand these concepts in further detail, refer to the  handbook . It explains different sampling strategies, error estimation mechanisms, and various types of data synopses.", 
            "title": "Stratified Sampling"
        }, 
        {
            "location": "/sde/key_concepts/#online-sampling", 
            "text": "SDE also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark DataFrame APIs to create a uniform random sample over static RDDs. For online sampling, SDE first does  reservoir sampling  for each startum in a write-optimized store before flushing it into a read-optimized store for stratified samples. \nThere is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, SnappyData can ensure having enough samples over each 5-minute time window, while still ensuring that all GEOs have good representation in the sample.", 
            "title": "Online Sampling"
        }, 
        {
            "location": "/sde/key_concepts/#sketching", 
            "text": "While stratified sampling ensures that data dimensions with low representation are captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, other data structures like a Count-min-sketch are relied on to capture data frequencies in a stream. This is a data structure that requires that it captures how often an element is seen in a stream for the top-N such elements. \nWhile a  Count-min-sketch  is well described, SDE extends this with support for providing top-K estimates over time series data.", 
            "title": "Sketching"
        }, 
        {
            "location": "/sde/working_with_stratified_samples/", 
            "text": "Working with Stratified Samples\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nCreate Sample Tables\n\n\nYou can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark DataFrames, or sourced from an external data source such as S3 or HDFS. \n\n\nHere is an SQL based example to create a sample on tables locally available in the SnappyData cluster. \n\n\nCREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI \n  OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01') \n  AS (SELECT * FROM NYCTAXI);\n\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  OPTIONS (qcs 'hack_license', fraction '0.01') \n  AS (SELECT * FROM TAXIFARE);\n\n\n\n\nOften your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. \nIn this example below, a sample table is created for an S3 (external) dataset:\n\n\nCREATE EXTERNAL TABLE TAXIFARE USING parquet \n  OPTIONS(path 's3a://\nAWS_SECRET_ACCESS_KEY\n:\nAWS_ACCESS_KEY_ID\n@zeppelindemo/nyctaxifaredata_cleaned');\n//Next, create the sample sourced from this table ..\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  options  (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE);\n\n\n\n\nWhen creating a base table, if you have applied the \npartition by\n clause, the clause is also applied to the sample table. The sample table also inherits the \nnumber of buckets\n, \nredundancy\n and \npersistence\n properties from the base table.\n\n\nFor sample tables, the \noverflow\n property is set to \nFalse\n by default. (For row and column tables the default value is  \nTrue\n). \n\n\nFor example:\n\n\nCREATE TABLE BASETABLENAME \ncolumn details\n \nUSING COLUMN OPTIONS (partition_by '\ncolumn_name_a\n', Buckets '8', Redundancy '1')\n\nCREATE TABLE SAMPLETABLENAME \ncolumn details\n \nUSING COLUMN_SAMPLE OPTIONS (qcs '\ncolumn_name_b\n',fraction '0.05', \nstrataReservoirSize '50', baseTable 'baseTableName')\n// In this case, sample table 'sampleTableName' is partitioned by column 'column_name_a', has 7 buckets and 1 redundancy.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nAfter a sample table is created from a base table, any changes to the base table (for example update and delete operations) is not automatically applied to the sample table.\n\n\n\n\n\n\nFor successful creation of sample tables, the number of buckets in the sample table should be more than the number of nodes in the cluster. \n\n\n\n\n\n\n\n\nQCS (Query Column Set) and Sample Selection\n\n\nFor stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction). \n\n\nQCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like \nhour (pickup_datetime)\n.\n\n\nThe parameter \nfraction\n represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes, you can get pretty accurate answers with a very small fraction. With most data sets that follow a normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. SDE always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. \nFor instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers (a lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement. \n\n\nOne can create multiple sample tables using different sample QCS and sample fraction for a given base table. \n\n\nHere are some general guidelines to use when creating samples:\n\n\n\n\n\n\nNote that samples are only applicable when running aggregation queries. For point lookups or selective queries, the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store.\n\n\n\n\n\n\nStart by identifying the most common columns used in GroupBy/Where and Having clauses. \n\n\n\n\n\n\nThen, identify a subset of these columns where the cardinality is not too large. For instance, in the example above 'hack_license' is picked (one license per driver) as the strata and 1% of the records associated with each driver is sampled. \n\n\n\n\n\n\nAvoid using unique columns or timestamps for your QCS. For instance, in the example above, 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is a possibility that each record in the dataset has a different timesstamp. Instead, when dealing with time series the 'hour' function is used to capture data for each hour. \n\n\n\n\n\n\nWhen the accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample. \n\n\n\n\n\n\n\n\nNote\n\n\nThe value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.", 
            "title": "Working with Stratified Samples"
        }, 
        {
            "location": "/sde/working_with_stratified_samples/#working-with-stratified-samples", 
            "text": "This feature is available only in the Enterprise version of SnappyData.", 
            "title": "Working with Stratified Samples"
        }, 
        {
            "location": "/sde/working_with_stratified_samples/#create-sample-tables", 
            "text": "You can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark DataFrames, or sourced from an external data source such as S3 or HDFS.   Here is an SQL based example to create a sample on tables locally available in the SnappyData cluster.   CREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI \n  OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01') \n  AS (SELECT * FROM NYCTAXI);\n\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  OPTIONS (qcs 'hack_license', fraction '0.01') \n  AS (SELECT * FROM TAXIFARE);  Often your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. \nIn this example below, a sample table is created for an S3 (external) dataset:  CREATE EXTERNAL TABLE TAXIFARE USING parquet \n  OPTIONS(path 's3a:// AWS_SECRET_ACCESS_KEY : AWS_ACCESS_KEY_ID @zeppelindemo/nyctaxifaredata_cleaned');\n//Next, create the sample sourced from this table ..\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  options  (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE);  When creating a base table, if you have applied the  partition by  clause, the clause is also applied to the sample table. The sample table also inherits the  number of buckets ,  redundancy  and  persistence  properties from the base table.  For sample tables, the  overflow  property is set to  False  by default. (For row and column tables the default value is   True ).   For example:  CREATE TABLE BASETABLENAME  column details  \nUSING COLUMN OPTIONS (partition_by ' column_name_a ', Buckets '8', Redundancy '1')\n\nCREATE TABLE SAMPLETABLENAME  column details  \nUSING COLUMN_SAMPLE OPTIONS (qcs ' column_name_b ',fraction '0.05', \nstrataReservoirSize '50', baseTable 'baseTableName')\n// In this case, sample table 'sampleTableName' is partitioned by column 'column_name_a', has 7 buckets and 1 redundancy.   Note    After a sample table is created from a base table, any changes to the base table (for example update and delete operations) is not automatically applied to the sample table.    For successful creation of sample tables, the number of buckets in the sample table should be more than the number of nodes in the cluster.", 
            "title": "Create Sample Tables"
        }, 
        {
            "location": "/sde/working_with_stratified_samples/#qcs-query-column-set-and-sample-selection", 
            "text": "For stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction).   QCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like  hour (pickup_datetime) .  The parameter  fraction  represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes, you can get pretty accurate answers with a very small fraction. With most data sets that follow a normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. SDE always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. \nFor instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers (a lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement.   One can create multiple sample tables using different sample QCS and sample fraction for a given base table.   Here are some general guidelines to use when creating samples:    Note that samples are only applicable when running aggregation queries. For point lookups or selective queries, the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store.    Start by identifying the most common columns used in GroupBy/Where and Having clauses.     Then, identify a subset of these columns where the cardinality is not too large. For instance, in the example above 'hack_license' is picked (one license per driver) as the strata and 1% of the records associated with each driver is sampled.     Avoid using unique columns or timestamps for your QCS. For instance, in the example above, 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is a possibility that each record in the dataset has a different timesstamp. Instead, when dealing with time series the 'hour' function is used to capture data for each hour.     When the accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample.      Note  The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.", 
            "title": "QCS (Query Column Set) and Sample Selection"
        }, 
        {
            "location": "/sde/running_queries/", 
            "text": "Running Queries\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nQueries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause. \n\n\nHere is the syntax:\n\n\n \n SELECT ... FROM .. WHERE .. GROUP BY ...\nbr\n\n\n \n WITH ERROR `\nfraction\n `[CONFIDENCE` \nfraction\n`] [BEHAVIOR `\nstring\n]`\n\n\n\n\n\n\nWITH ERROR\n - this is a mandatory clause. The values are  0 \n value(double) \n 1 . \n\n\nCONFIDENCE\n - this is optional clause. The values are confidence 0 \n value(double) \n 1 . The default value is 0.95\n\n\nBEHAVIOR\n - this is an optional clause. The values are \ndo_nothing\n, \nlocal_omit\n, \nstrict\n,  \nrun_on_full_table\n, \npartial_run_on_base_table\n. The default value is \nrun_on_full_table\n   \n\n\n\n\nThese 'behavior' options are fully described in the section below. \n\n\nHere are some examples:\n\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error 0.10 \n// tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error \n// tolerate any error in the answer. Just give me a quick response.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019\n// tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95.\n// If the error for any row is greater than 10% omit the answer. i.e. the row is omitted. \n\n\n\n\nUsing the Spark DataFrame API\n\n\nThe Spark DataFrame API is extended with support for approximate queries. Here is 'withError' API on DataFrames.\n\n\ndef withError(error: Double,\nconfidence: Double = Constant.DEFAULT_CONFIDENCE,\nbehavior: String = \nDO_NOTHING\n): DataFrame\n\n\n\n\nQuery examples using the DataFrame API\n\n\nsnc.table(baseTable).agg(Map(\nArrDelay\n -\n \nsum\n)).orderBy( desc(\nMonth_\n)).withError(0.10) \nsnc.table(baseTable).agg(Map(\nArrDelay\n -\n \nsum\n)).orderBy( desc(\nMonth_\n)).withError(0.10, 0.95, 'local_omit\u2019) \n\n\n\n\nSupporting BI Tools or Existing Apps\n\n\nTo allow BI tools and existing Apps that say might be generating SQL, SDE also supports specifying these options through your SQL connection or using the Snappy SQLContext. \n\n\nsnContext.sql(s\nspark.sql.aqp.error=$error\n)\nsnContext.sql(s\nspark.sql.aqp.confidence=$confidence\n)\nsnContext.sql(s\nset spark.sql.aqp.behavior=$behavior\n)\n\n\n\n\nThese settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above.\n\n\nApplications or tools using JDBC/ODBC can set the following properties. \nFor example, when using Apache Zeppelin JDBC interpreter or the snappy-sql you can set the values as below:\n\n\nset spark.sql.aqp.error=$error;\nset spark.sql.aqp.confidence=$confidence;\nset spark.sql.aqp.behavior=$behavior;", 
            "title": "Running Queries"
        }, 
        {
            "location": "/sde/running_queries/#running-queries", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   Queries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause.   Here is the syntax:     SELECT ... FROM .. WHERE .. GROUP BY ... br     WITH ERROR ` fraction  `[CONFIDENCE`  fraction `] [BEHAVIOR ` string ]`   WITH ERROR  - this is a mandatory clause. The values are  0   value(double)   1 .   CONFIDENCE  - this is optional clause. The values are confidence 0   value(double)   1 . The default value is 0.95  BEHAVIOR  - this is an optional clause. The values are  do_nothing ,  local_omit ,  strict ,   run_on_full_table ,  partial_run_on_base_table . The default value is  run_on_full_table       These 'behavior' options are fully described in the section below.   Here are some examples:  SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error 0.10 \n// tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error \n// tolerate any error in the answer. Just give me a quick response.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019\n// tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95.\n// If the error for any row is greater than 10% omit the answer. i.e. the row is omitted.", 
            "title": "Running Queries"
        }, 
        {
            "location": "/sde/running_queries/#using-the-spark-dataframe-api", 
            "text": "The Spark DataFrame API is extended with support for approximate queries. Here is 'withError' API on DataFrames.  def withError(error: Double,\nconfidence: Double = Constant.DEFAULT_CONFIDENCE,\nbehavior: String =  DO_NOTHING ): DataFrame  Query examples using the DataFrame API  snc.table(baseTable).agg(Map( ArrDelay  -   sum )).orderBy( desc( Month_ )).withError(0.10) \nsnc.table(baseTable).agg(Map( ArrDelay  -   sum )).orderBy( desc( Month_ )).withError(0.10, 0.95, 'local_omit\u2019)", 
            "title": "Using the Spark DataFrame API"
        }, 
        {
            "location": "/sde/running_queries/#supporting-bi-tools-or-existing-apps", 
            "text": "To allow BI tools and existing Apps that say might be generating SQL, SDE also supports specifying these options through your SQL connection or using the Snappy SQLContext.   snContext.sql(s spark.sql.aqp.error=$error )\nsnContext.sql(s spark.sql.aqp.confidence=$confidence )\nsnContext.sql(s set spark.sql.aqp.behavior=$behavior )  These settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above.  Applications or tools using JDBC/ODBC can set the following properties. \nFor example, when using Apache Zeppelin JDBC interpreter or the snappy-sql you can set the values as below:  set spark.sql.aqp.error=$error;\nset spark.sql.aqp.confidence=$confidence;\nset spark.sql.aqp.behavior=$behavior;", 
            "title": "Supporting BI Tools or Existing Apps"
        }, 
        {
            "location": "/sde/more_examples/", 
            "text": "More Examples\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nExample 1\n\n\nCreate a sample table with qcs 'medallion' \n\n\nCREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI \n  OPTIONS (buckets '8', qcs 'medallion', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);\n\n\n\n\nSQL Query:\n\n\nselect medallion,avg(trip_distance) as avgTripDist,\n  absolute_error(avgTripDist),relative_error(avgTripDist),\n  lower_bound(avgTripDist),upper_bound(avgTripDist) \n  from nyctaxi group by medallion order by medallion desc limit 100\n  with error;\n  //These built-in error functions is explained in a section below.\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nmedallion\n).agg( avg(\ntrip_distance\n).alias(\navgTripDist\n),\n  absolute_error(\navgTripDist\n),  relative_error(\navgTripDist\n), lower_bound(\navgTripDist\n),\n  upper_bound(\navgTripDist\n)).withError(.6, .90, \ndo_nothing\n).sort(col(\nmedallion\n).desc).limit(100)\n\n\n\n\nExample 2\n\n\nCreate additional sample table with qcs 'hack_license' \n\n\nCREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS\n(buckets '8', qcs 'hack_license', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);\n\n\n\n\nSQL Query:\n\n\nselect  hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error\n// the engine will automitically use the HackLicense sample for a more accurate answer to this query.\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nhack_license\n).count().withError(.6,.90,\ndo_nothing\n).sort(col(\ncount\n).desc).limit(10)\n\n\n\n\nExample 3\n\n\nCreate a sample table using function \"hour(pickup_datetime) as QCS\n\n\nSample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);\n\n\n\n\nSQL Query:\n\n\nselect sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime)\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(hour(col(\npickup_datetime\n))).agg(Map(\ntrip_time_in_secs\n -\n \nsum\n)).withError(0.6,0.90,\ndo_nothing\n).limit(10)\n\n\n\n\nExample 4\n\n\nIf you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns.\n\n\nSample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);\n\n\n\n\nSQL Query:\n\n\nSelect hack_license, sum(trip_distance) as daily_trips from nyctaxi  where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license  order by daily_trips desc\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nhack_license\n,\npickup_datetime\n).agg(Map(\ntrip_distance\n -\n \nsum\n)).alias(\ndaily_trips\n).       filter(year(col(\npickup_datetime\n)).equalTo(2013) and month(col(\npickup_datetime\n)).equalTo(9)).withError(0.6,0.90,\ndo_nothing\n).sort(col(\nsum(trip_distance)\n).desc).limit(10)", 
            "title": "More Examples"
        }, 
        {
            "location": "/sde/more_examples/#more-examples", 
            "text": "This feature is available only in the Enterprise version of SnappyData.", 
            "title": "More Examples"
        }, 
        {
            "location": "/sde/more_examples/#example-1", 
            "text": "Create a sample table with qcs 'medallion'   CREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI \n  OPTIONS (buckets '8', qcs 'medallion', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);  SQL Query:  select medallion,avg(trip_distance) as avgTripDist,\n  absolute_error(avgTripDist),relative_error(avgTripDist),\n  lower_bound(avgTripDist),upper_bound(avgTripDist) \n  from nyctaxi group by medallion order by medallion desc limit 100\n  with error;\n  //These built-in error functions is explained in a section below.  DataFrame API Query:  snc.table(basetable).groupBy( medallion ).agg( avg( trip_distance ).alias( avgTripDist ),\n  absolute_error( avgTripDist ),  relative_error( avgTripDist ), lower_bound( avgTripDist ),\n  upper_bound( avgTripDist )).withError(.6, .90,  do_nothing ).sort(col( medallion ).desc).limit(100)", 
            "title": "Example 1"
        }, 
        {
            "location": "/sde/more_examples/#example-2", 
            "text": "Create additional sample table with qcs 'hack_license'   CREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS\n(buckets '8', qcs 'hack_license', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);  SQL Query:  select  hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error\n// the engine will automitically use the HackLicense sample for a more accurate answer to this query.  DataFrame API Query:  snc.table(basetable).groupBy( hack_license ).count().withError(.6,.90, do_nothing ).sort(col( count ).desc).limit(10)", 
            "title": "Example 2"
        }, 
        {
            "location": "/sde/more_examples/#example-3", 
            "text": "Create a sample table using function \"hour(pickup_datetime) as QCS  Sample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);  SQL Query:  select sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime)  DataFrame API Query:  snc.table(basetable).groupBy(hour(col( pickup_datetime ))).agg(Map( trip_time_in_secs  -   sum )).withError(0.6,0.90, do_nothing ).limit(10)", 
            "title": "Example 3"
        }, 
        {
            "location": "/sde/more_examples/#example-4", 
            "text": "If you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns.  Sample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '8', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);  SQL Query:  Select hack_license, sum(trip_distance) as daily_trips from nyctaxi  where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license  order by daily_trips desc  DataFrame API Query:  snc.table(basetable).groupBy( hack_license , pickup_datetime ).agg(Map( trip_distance  -   sum )).alias( daily_trips ).       filter(year(col( pickup_datetime )).equalTo(2013) and month(col( pickup_datetime )).equalTo(9)).withError(0.6,0.90, do_nothing ).sort(col( sum(trip_distance) ).desc).limit(10)", 
            "title": "Example 4"
        }, 
        {
            "location": "/sde/sample_selection/", 
            "text": "Sample Selection\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nSample selection logic selects most appropriate sample, based on this relatively simple logic in the current version:\n\n\n\n\n\n\nIf the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else,\n\n\n\n\n\n\nIf query QCS (columns involved in Where/GroupBy/Having matched the sample QCS, then, select that sample\n\n\n\n\n\n\nIf exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used\n\n\n\n\n\n\nIf superset of sample QCS is not available, a sample where the sample QCS is a subset of query QCS is used\n\n\n\n\n\n\nWhen multiple stratified samples with a subset of QCSs match, a sample with most matching columns is used. The largest size of the sample gets selected if multiple such samples are available.", 
            "title": "Sample Selection"
        }, 
        {
            "location": "/sde/sample_selection/#sample-selection", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   Sample selection logic selects most appropriate sample, based on this relatively simple logic in the current version:    If the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else,    If query QCS (columns involved in Where/GroupBy/Having matched the sample QCS, then, select that sample    If exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used    If superset of sample QCS is not available, a sample where the sample QCS is a subset of query QCS is used    When multiple stratified samples with a subset of QCSs match, a sample with most matching columns is used. The largest size of the sample gets selected if multiple such samples are available.", 
            "title": "Sample Selection"
        }, 
        {
            "location": "/sde/hac_contracts/", 
            "text": "High-level Accuracy Contracts (HAC)\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nSnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts.\n\n\nWhen an error constraint is not met, the action to be taken is defined in the behavior clause. \n\n\nBehavior Clause\n\n\nSynopsis Data Engine has HAC support using the following behavior clause. \n\n\ndo_nothing\n\n\nThe SDE engine returns the estimate as is. \n\n\n\n\nlocal_omit\n\n\nFor aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\". \n\n\n\nstrict\n\n\nIf any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception. \n\n\n\nrun_on_full_table\n\n\nIf any of the single output row exceeds the specified error, then the full query is re-executed on the base table.\n\n\n\n\npartial_run_on_base_table\n\n\nIf the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups.  This result is then merged (without any duplicates) with the result derived from the sample table. \n\n\n\nIn the following example, any one of the above behavior clause can be applied. \n\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_  with error \nfraction\n [CONFIDENCE \nfraction\n] [BEHAVIOR \nbehavior\n]\n\n\n\n\nError Functions\n\n\nIn addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection. \n\n\nThe following four methods are available to be used in query projection when running approximate queries:\n\n\n\n\n\n\nabsolute_error(column alias)\n: Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap) \n\n\n\n\n\n\nrelative_error(column alias)\n: Indicates ratio of absolute error to estimate.\n\n\n\n\n\n\nlower_bound(column alias)\n: Lower value of an estimate interval for a given confidence.\n\n\n\n\n\n\nupper_bound(column alias)\n: Upper value of an estimate interval for a given confidence.\n\n\n\n\n\n\nConfidence is the probability that the value of a parameter falls within a specified range of values.\n\n\nFor example:\n\n\nSELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr),\nUniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9\n\n\n\n\n\n\nThe \nabsolute_error\n and \nrelative_error\n function values returns 0 if query is executed on the base table. \n\n\nlower_bound\n and \nupper_bound\n values returns null if query is executed on the base table. \n\n\nThe values are seen in case behavior is set to \nrun_on_full_table\n or\npartial_run_on_base_table\n\n\n\n\nIn addition to using SQL syntax in the queries, you can use data frame API as well. \nFor example, if you have a data frame for the airline table, then the below query can equivalently also be written as :\n\n\nselect AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95\n\n\n\n\nsnc.table(basetable).groupBy(\nYear_\n).agg( avg(\nArrDelay\n).alias(\narrivalDelay), relative_error(\narrivalDelay\n), absolute_error(\narrivalDelay\n), col(\nYear_\n)).withError(0.10, .95).sort(col(\nYear_\n).asc) \n\n\n\n\nReserved Keywords\n\n\nKeywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword \nsample_\n is reserved for SnappyData.\n\n\nIf the aggregate function is aliased in the query as \nsample_\nany string\n, then what you get is true answers on the sample table, and not the estimates of the base table.\n\n\nselect count() rowCount, count() as sample_count from airline with error 0.1\n\n\nrowCount returns estimate of a number of rows in airline table.\nsample_count returns a number of rows (true answer) in sample table of airline table.", 
            "title": "High-level Accuracy Contracts (HAC)"
        }, 
        {
            "location": "/sde/hac_contracts/#high-level-accuracy-contracts-hac", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   SnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts.  When an error constraint is not met, the action to be taken is defined in the behavior clause.", 
            "title": "High-level Accuracy Contracts (HAC)"
        }, 
        {
            "location": "/sde/hac_contracts/#behavior-clause", 
            "text": "Synopsis Data Engine has HAC support using the following behavior clause.", 
            "title": "Behavior Clause"
        }, 
        {
            "location": "/sde/hac_contracts/#do_nothing", 
            "text": "The SDE engine returns the estimate as is.", 
            "title": "&lt;do_nothing&gt;"
        }, 
        {
            "location": "/sde/hac_contracts/#local_omit", 
            "text": "For aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\".", 
            "title": "&lt;local_omit&gt;"
        }, 
        {
            "location": "/sde/hac_contracts/#strict", 
            "text": "If any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception.", 
            "title": "&lt;strict&gt;"
        }, 
        {
            "location": "/sde/hac_contracts/#run_on_full_table", 
            "text": "If any of the single output row exceeds the specified error, then the full query is re-executed on the base table.", 
            "title": "&lt;run_on_full_table&gt;"
        }, 
        {
            "location": "/sde/hac_contracts/#partial_run_on_base_table", 
            "text": "If the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups.  This result is then merged (without any duplicates) with the result derived from the sample table.   In the following example, any one of the above behavior clause can be applied.   SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_  with error  fraction  [CONFIDENCE  fraction ] [BEHAVIOR  behavior ]", 
            "title": "&lt;partial_run_on_base_table&gt;"
        }, 
        {
            "location": "/sde/hac_contracts/#error-functions", 
            "text": "In addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection.   The following four methods are available to be used in query projection when running approximate queries:    absolute_error(column alias) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap)     relative_error(column alias) : Indicates ratio of absolute error to estimate.    lower_bound(column alias) : Lower value of an estimate interval for a given confidence.    upper_bound(column alias) : Upper value of an estimate interval for a given confidence.    Confidence is the probability that the value of a parameter falls within a specified range of values.  For example:  SELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr),\nUniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9   The  absolute_error  and  relative_error  function values returns 0 if query is executed on the base table.   lower_bound  and  upper_bound  values returns null if query is executed on the base table.   The values are seen in case behavior is set to  run_on_full_table  or partial_run_on_base_table   In addition to using SQL syntax in the queries, you can use data frame API as well. \nFor example, if you have a data frame for the airline table, then the below query can equivalently also be written as :  select AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95  snc.table(basetable).groupBy( Year_ ).agg( avg( ArrDelay ).alias( arrivalDelay), relative_error( arrivalDelay ), absolute_error( arrivalDelay ), col( Year_ )).withError(0.10, .95).sort(col( Year_ ).asc)", 
            "title": "Error Functions"
        }, 
        {
            "location": "/sde/hac_contracts/#reserved-keywords", 
            "text": "Keywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword  sample_  is reserved for SnappyData.  If the aggregate function is aliased in the query as  sample_ any string , then what you get is true answers on the sample table, and not the estimates of the base table.  select count() rowCount, count() as sample_count from airline with error 0.1  rowCount returns estimate of a number of rows in airline table.\nsample_count returns a number of rows (true answer) in sample table of airline table.", 
            "title": "Reserved Keywords"
        }, 
        {
            "location": "/sde/sketching/", 
            "text": "Sketching\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nSynopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A \nBloomFilter\n is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a \nCount-Min-Sketch\n which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API.\n\n\nCreating TopK tables\n\n\nTopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query. \nTopK\n queries aim to retrieve, from a potentially very large result set, only the \nk (k \n= 1)\n best answers.\n\n\nSQL API for creating a TopK table in SnappyData\n\n\nsnsc.sql(\ncreate topK table MostPopularTweets on tweetStreamTable \n +\n        \noptions(key 'hashtag', frequencyCol 'retweets')\n)\n\n\n\n\nThe example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables.\n\n\nScala API for creating a TopK table\n\n\nval topKOptionMap = Map(\n    \"epoch\" -\n System.currentTimeMillis().toString,\n    \"timeInterval\" -\n \"1000ms\",\n    \"size\" -\n \"40\",\n    \"frequencyCol\" -\n \"retweets\"\n  )\n  val schema = StructType(List(StructField(\"HashTag\", StringType)))\n  snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"),\n    \"HashTag\", schema, topKOptionMap)\n\n\n\nThe code above shows how to do the same thing using the SnappyData Scala API.\n\n\nQuerying the TopK table\n\n\nselect * from topkTweets order by EstimatedValue desc\n\n\n\nThe example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most re-tweets.\n\n\nApproximate TopK analytics for time series data\n\n\nTime is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the time stamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most re-tweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system.\n\n\nHere is an example of a time-based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData SDE module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals.\n\n\nselect hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where \n    startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' \n    order by EstimatedValue desc\n\n\n\nIf time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp.\n\n\nSQL API for creating a TopK table in SnappyData specifying timestampColumn\n\n\nIn the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet.\n\n\nsnsc.sql(\ncreate topK table MostPopularTweets on tweetStreamTable \n +\n        \noptions(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' )\n)\n\n\n\n\nThe example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest re-tweets value in the base table. This works for both static tables and streaming tables\n\n\nScala API for creating a TopK table\n\n\n    val topKOptionMap = Map(\n        \nepoch\n -\n System.currentTimeMillis().toString,\n        \ntimeInterval\n -\n \n1000ms\n,\n        \nsize\n -\n \n40\n,\n        \nfrequencyCol\n -\n \nretweets\n,\n        \ntimeSeriesColumn\n -\n \ntweetTime\n\n      )\n      val schema = StructType(List(StructField(\nHashTag\n, StringType)))\n      snc.createApproxTSTopK(\nMostPopularTweets\n, Some(\ntweetStreamTable\n),\n        \nHashTag\n, schema, topKOptionMap)\n\n\n\n\nThe code above shows how to do the same thing using the SnappyData Scala API.\n\n\nIt is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the \ntimeInterval\n attribute when creating the TopK table.", 
            "title": "Sketching"
        }, 
        {
            "location": "/sde/sketching/#sketching", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   Synopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A  BloomFilter  is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a  Count-Min-Sketch  which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API.", 
            "title": "Sketching"
        }, 
        {
            "location": "/sde/sketching/#creating-topk-tables", 
            "text": "TopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query.  TopK  queries aim to retrieve, from a potentially very large result set, only the  k (k  = 1)  best answers.", 
            "title": "Creating TopK tables"
        }, 
        {
            "location": "/sde/sketching/#sql-api-for-creating-a-topk-table-in-snappydata", 
            "text": "snsc.sql( create topK table MostPopularTweets on tweetStreamTable   +\n         options(key 'hashtag', frequencyCol 'retweets') )  The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables.", 
            "title": "SQL API for creating a TopK table in SnappyData"
        }, 
        {
            "location": "/sde/sketching/#scala-api-for-creating-a-topk-table", 
            "text": "val topKOptionMap = Map(\n    \"epoch\" -  System.currentTimeMillis().toString,\n    \"timeInterval\" -  \"1000ms\",\n    \"size\" -  \"40\",\n    \"frequencyCol\" -  \"retweets\"\n  )\n  val schema = StructType(List(StructField(\"HashTag\", StringType)))\n  snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"),\n    \"HashTag\", schema, topKOptionMap)  The code above shows how to do the same thing using the SnappyData Scala API.", 
            "title": "Scala API for creating a TopK table"
        }, 
        {
            "location": "/sde/sketching/#querying-the-topk-table", 
            "text": "select * from topkTweets order by EstimatedValue desc  The example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most re-tweets.", 
            "title": "Querying the TopK table"
        }, 
        {
            "location": "/sde/sketching/#approximate-topk-analytics-for-time-series-data", 
            "text": "Time is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the time stamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most re-tweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system.  Here is an example of a time-based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData SDE module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals.  select hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where \n    startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' \n    order by EstimatedValue desc  If time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp.", 
            "title": "Approximate TopK analytics for time series data"
        }, 
        {
            "location": "/sde/sketching/#sql-api-for-creating-a-topk-table-in-snappydata-specifying-timestampcolumn", 
            "text": "In the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet.  snsc.sql( create topK table MostPopularTweets on tweetStreamTable   +\n         options(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' ) )  The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest re-tweets value in the base table. This works for both static tables and streaming tables", 
            "title": "SQL API for creating a TopK table in SnappyData specifying timestampColumn"
        }, 
        {
            "location": "/sde/sketching/#scala-api-for-creating-a-topk-table_1", 
            "text": "val topKOptionMap = Map(\n         epoch  -  System.currentTimeMillis().toString,\n         timeInterval  -   1000ms ,\n         size  -   40 ,\n         frequencyCol  -   retweets ,\n         timeSeriesColumn  -   tweetTime \n      )\n      val schema = StructType(List(StructField( HashTag , StringType)))\n      snc.createApproxTSTopK( MostPopularTweets , Some( tweetStreamTable ),\n         HashTag , schema, topKOptionMap)  The code above shows how to do the same thing using the SnappyData Scala API.  It is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the  timeInterval  attribute when creating the TopK table.", 
            "title": "Scala API for creating a TopK table"
        }, 
        {
            "location": "/connectors/connector/", 
            "text": "Working with External Data Sources\n\n\nSnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. Any data source or database that supports Spark to load or save state can be accessed from within SnappyData. \n\n\nThere is built-in support for many data sources as well as data formats. You can access data from sources such as S3, file system, HDFS, Hive, and RDB. The loaders have built-in support to handle data formats such as CSV, Parquet, ORC, Avro, JSON, and Java/Scala Objects.\n\n\n\n\nAttention\n\n\nThis section currently only details the advanced connectors that SnappyData introduced. Refer to the \nhowto\n section for a brief description about working with \nexternal data sources\n and \nsome examples\n. \n\n\n\n\nSnappyData provides a utility to deploy third-party connectors using the SQL \nDeploy\n command. Refer \nDeployment of Third Party Connectors\n\n\nFor more information see:\n\n\n\n\nSTART HERE - How to load data into SnappyData Tables\n\n\nData Loading examples using Spark SQL/Data Sources API\n\n\nUsing the SnappyData Change Data Capture (CDC) Connector\n\n\nUsing the SnappyData GemFire Connector", 
            "title": "Working with External Data Sources"
        }, 
        {
            "location": "/connectors/connector/#working-with-external-data-sources", 
            "text": "SnappyData relies on the Spark SQL Data Sources API to parallelly load data from a wide variety of sources. Any data source or database that supports Spark to load or save state can be accessed from within SnappyData.   There is built-in support for many data sources as well as data formats. You can access data from sources such as S3, file system, HDFS, Hive, and RDB. The loaders have built-in support to handle data formats such as CSV, Parquet, ORC, Avro, JSON, and Java/Scala Objects.   Attention  This section currently only details the advanced connectors that SnappyData introduced. Refer to the  howto  section for a brief description about working with  external data sources  and  some examples .    SnappyData provides a utility to deploy third-party connectors using the SQL  Deploy  command. Refer  Deployment of Third Party Connectors  For more information see:   START HERE - How to load data into SnappyData Tables  Data Loading examples using Spark SQL/Data Sources API  Using the SnappyData Change Data Capture (CDC) Connector  Using the SnappyData GemFire Connector", 
            "title": "Working with External Data Sources"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/", 
            "text": "Deploying Third Party Connectors\n\n\nA job submitted to SnappyData, the creation of external tables as SQL or through API, the creation of a user-defined function etc. may have an external jar and package dependencies. For example, you may want to create an external table in SnappyData which points to a Cassandra table. For that, you would need the Spark Cassandra connector classes which are available in maven central repository.\n\n\nToday, Spark connectors are available to virtually all modern data stores - RDBs, NoSQL, cloud databases like Redshift, Snowflake, S3, etc. Most of these connectors are available as mvn or spark packages or as published jars by the respective vendors. \nSnappyData\u2019s compatibility with Spark allows SnappyData to work with the same connectors of all the popular data sources.\n\n\nSnappyData\u2019s \ndeploy\n command allows you to deploy these packages by using its maven coordinates. When it is not available, you can simply upload jars into any provisioned SnappyData cluster.\n\n\nSnappyData offers the following SQL commands:\n\n\n\n\ndeploy package\n - to deploy maven packages\n\n\ndeploy jar\n - to deploy your application or library Jars\n\n\n\n\nBesides these SQL extensions, support is provided in SnappyData 1.0.2.1 version to deploy packages as part of SnappyData Job submission. This is similar to \nSpark\u2019s support\n for \n--packages\n when submitting Spark jobs.\n\n\nThe following sections are included in this topic:\n\n\n\n\nDeploying Packages in SnappyData\n\n\nDeploying Jars in SnappyData\n\n\nViewing the Deployed Jars and Packages\n\n\nRemoving Deployed Jars\n\n\nSubmitting SnappyData Job with Packages\n\n\n\n\n \n\n\nDeploying Packages in SnappyData\n\n\nPackages can be deployed in SnappyData using the \nDEPLOY PACKAGE\n SQL. Execute the \ndeploy package\n command to deploy packages. You can pass the following through this SQL:\n\n\n\n\nName of the package.\n\n\nRepository where the package is located.\n\n\nPath to a local cache of jars.\n\n\n\n\n\n\nNote\n\n\nSnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise, the resolution of the package fails.\n\n\n\n\nFor resolving the package, Maven Central and Spark packages, located at \nhttp://dl.bintray.com/spark-packages\n, are searched by default. Hence, you must specify the repository only if the package is not there at \nMaven Central\n or in the \nspark-package\n repository.\n\n\n\n\nTip\n\n\nUse \nspark-packages.org\n to search for Spark packages. Most of the popular Spark packages are listed here.\n\n\n\n\nSQL Syntax to Deploy Package Dependencies in SnappyData\n\n\ndeploy package \nunique-alias-name\n \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ]\n\n\n\n\n\n\n\n\nunique-alias-name\n - A name to identify a package. This name can be used to remove the package from the cluster.  You can use alphabets, numbers, and underscores to create the name.\n\n\n\n\n\n\npackages\n - Comma-delimited string of maven packages. \n\n\n\n\n\n\nrepos\n - Comma-delimited string of remote repositories other than the \nMaven Central\n and \nspark-package\n repositories. These two repositories are searched by default.  The format specified by Maven is: \ngroupId:artifactId:version\n.\n\n\n\n\n\n\npath\n - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the \nivy.home\n property. if even that is not specified, the \n.ivy2\n in the user\u2019s home directory is used.\n\n\n\n\n\n\nExample\n\n\nDeploy packages from a default repository:\n\n\ndeploy package deeplearning 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work';\n\n\n\n\ndeploy package Sparkredshift 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work';\n\n\n\n\n \n\n\nDeploying Jars in SnappyData\n\n\nSnappyData provides a method to deploy a jar in a running system through \nDEPLOY JAR\n SQL. You can execute the \ndeploy jar\n command to deploy dependency jars. \n\n\nSyntax for Deploying Jars in SnappyData\n\n\ndeploy jar \nunique-alias-name\n \u2018jars\u2019\n\n\n\n\n\n\n\n\nunique-alias-name\n - A name to identify the jar. This name can be used to remove the jar from the cluster.  You can use alphabets, numbers and underscores to create the name.\n\n\n\n\n\n\njars\n - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData.\n\n\n\n\n\n\nExample\n\n\nDeploying jars:\n\n\ndeploy jar SparkDaria spark-daria_2.11.8-2.2.0_0.10.0.jar  \u2018jars\u2019\n\n\n\n\nAll the deployed commands are stored in the SnappyData cluster. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system.\n\n\n \n\n\nViewing the Deployed Jars and Packages\n\n\nYou can view all the packages and jars deployed in the system by using the \nlist packages\n command. \n\n\nSyntax for Listing Deployed Jars and Packages\n\n\nsnappy\n list packages;\nOr\nsnappy\n list jars;\n\n\n\n\nBoth of the above commands will list all the packages as well the jars that are installed in the system. Hence, you can use either one of those commands.\n\n\nSample Output of Listing Jars/Packages\n\n\nsnappy\n list jars;\nalias          |coordinate                                     |isPackage\n-------------------------------------------------------------------------\nCASSCONN       |datastax:spark-cassandra-connector:2.0.1-s_2.11|true     \nMYJAR          |b.jar                                          |false \n\n\n\n\n\n \n\n\nRemoving Deployed Jars\n\n\nYou can remove the deployed jars with the \nundeploy\n command. This command removes the jars that are directly installed and the jars that are associated with a package, from the system.\n\n\nSyntax for Removing Deployed Jars\n\n\nundeploy \nunique-alias-name\n;\n\n\n\n\n\n\nNote\n\n\nThe removal is only captured when you use the \nundeploy\n command, the jars are removed only when you restart the cluster.\n\n\n\n\nExample\n\n\nundeploy spark_deep_learning_0_3_0;\n\n\n\n\n\n\nAttention\n\n\n\n\nIf during restart, for any reason the deployed jars and packages are not reinstalled automatically by the system, a warning is shown in the log file. If you want to fail the restart, then you need to set a system property in the \nconf\n file to stop restarts completely. The name of the system property is \nFAIL_ON_JAR_UNAVAILABILITY\n.\n\n\nIf you want to use external repositories, ensure to maintain internet connectivity at least on the lead nodes.\n\n\nIt is highly recommended to use a local cache path to store the downloaded jars of a package, because the next time the same deploy is executed, it can be picked from the local path.\n\n\nEnsure that this path is available on the lead nodes.\n\n\nSimilarly keep the standalone jars also in a path where it is available to all the lead nodes.\n\n\n\n\n\n\n \n\n\nSubmitting SnappyData Job with Packages\n\n\nYou can specify the name of the packages which can be used by a job that is submitted to SnappyData. This package is visible only to the job submitted with this argument. If another job tries to access a class belonging to the jar of this package then it will get ClassNotFoundException.\n\n\nA \n--packages\n option is added to the \nsnappy-job.sh\n script where you can specify the packages. Use the following syntax:\n\n\n$./snappy-job.sh submit --app-name \napp-name\n --class \njob-class\n [--lead \nhostname:port\n]  [--app-jar \njar-path\n] [other existing options]       [--packages \ncomma separated package-coordinates\n ] [--repos \ncomma separated mvn repositories] [--jarcache \npath where resolved jars will be kept\n]\n\n\n\n\nExample of SnappyData Job Submission with Packages\n\n\n./bin/snappy-job.sh submit --app-name cassconn --class \nSnappyJobClassName\n --app-jar \napp.jar\n --lead localhost:8090 --packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.7\n\n\n\n\nIf you want global deployment, you can instead use the \ndeploy\n command SQL and then the packages become visible everywhere.\n\n\nsnappy\n deploy package cassconn 'datastax:spark-cassandra-connector:2.0.1-s_2.11' path '/home/alex/mycache';", 
            "title": "Deploying Third Party Connectors"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#deploying-third-party-connectors", 
            "text": "A job submitted to SnappyData, the creation of external tables as SQL or through API, the creation of a user-defined function etc. may have an external jar and package dependencies. For example, you may want to create an external table in SnappyData which points to a Cassandra table. For that, you would need the Spark Cassandra connector classes which are available in maven central repository.  Today, Spark connectors are available to virtually all modern data stores - RDBs, NoSQL, cloud databases like Redshift, Snowflake, S3, etc. Most of these connectors are available as mvn or spark packages or as published jars by the respective vendors. \nSnappyData\u2019s compatibility with Spark allows SnappyData to work with the same connectors of all the popular data sources.  SnappyData\u2019s  deploy  command allows you to deploy these packages by using its maven coordinates. When it is not available, you can simply upload jars into any provisioned SnappyData cluster.  SnappyData offers the following SQL commands:   deploy package  - to deploy maven packages  deploy jar  - to deploy your application or library Jars   Besides these SQL extensions, support is provided in SnappyData 1.0.2.1 version to deploy packages as part of SnappyData Job submission. This is similar to  Spark\u2019s support  for  --packages  when submitting Spark jobs.  The following sections are included in this topic:   Deploying Packages in SnappyData  Deploying Jars in SnappyData  Viewing the Deployed Jars and Packages  Removing Deployed Jars  Submitting SnappyData Job with Packages", 
            "title": "Deploying Third Party Connectors"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#deploying-packages-in-snappydata", 
            "text": "Packages can be deployed in SnappyData using the  DEPLOY PACKAGE  SQL. Execute the  deploy package  command to deploy packages. You can pass the following through this SQL:   Name of the package.  Repository where the package is located.  Path to a local cache of jars.    Note  SnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise, the resolution of the package fails.   For resolving the package, Maven Central and Spark packages, located at  http://dl.bintray.com/spark-packages , are searched by default. Hence, you must specify the repository only if the package is not there at  Maven Central  or in the  spark-package  repository.   Tip  Use  spark-packages.org  to search for Spark packages. Most of the popular Spark packages are listed here.", 
            "title": "Deploying Packages in SnappyData"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#sql-syntax-to-deploy-package-dependencies-in-snappydata", 
            "text": "deploy package  unique-alias-name  \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ]    unique-alias-name  - A name to identify a package. This name can be used to remove the package from the cluster.  You can use alphabets, numbers, and underscores to create the name.    packages  - Comma-delimited string of maven packages.     repos  - Comma-delimited string of remote repositories other than the  Maven Central  and  spark-package  repositories. These two repositories are searched by default.  The format specified by Maven is:  groupId:artifactId:version .    path  - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the  ivy.home  property. if even that is not specified, the  .ivy2  in the user\u2019s home directory is used.", 
            "title": "SQL Syntax to Deploy Package Dependencies in SnappyData"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#example", 
            "text": "Deploy packages from a default repository:  deploy package deeplearning 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work';  deploy package Sparkredshift 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work';", 
            "title": "Example"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#deploying-jars-in-snappydata", 
            "text": "SnappyData provides a method to deploy a jar in a running system through  DEPLOY JAR  SQL. You can execute the  deploy jar  command to deploy dependency jars.", 
            "title": "Deploying Jars in SnappyData"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#syntax-for-deploying-jars-in-snappydata", 
            "text": "deploy jar  unique-alias-name  \u2018jars\u2019    unique-alias-name  - A name to identify the jar. This name can be used to remove the jar from the cluster.  You can use alphabets, numbers and underscores to create the name.    jars  - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData.", 
            "title": "Syntax for Deploying Jars in SnappyData"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#example_1", 
            "text": "Deploying jars:  deploy jar SparkDaria spark-daria_2.11.8-2.2.0_0.10.0.jar  \u2018jars\u2019  All the deployed commands are stored in the SnappyData cluster. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system.", 
            "title": "Example"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#viewing-the-deployed-jars-and-packages", 
            "text": "You can view all the packages and jars deployed in the system by using the  list packages  command.", 
            "title": "Viewing the Deployed Jars and Packages"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#syntax-for-listing-deployed-jars-and-packages", 
            "text": "snappy  list packages;\nOr\nsnappy  list jars;  Both of the above commands will list all the packages as well the jars that are installed in the system. Hence, you can use either one of those commands.", 
            "title": "Syntax for Listing Deployed Jars and Packages"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#sample-output-of-listing-jarspackages", 
            "text": "snappy  list jars;\nalias          |coordinate                                     |isPackage\n-------------------------------------------------------------------------\nCASSCONN       |datastax:spark-cassandra-connector:2.0.1-s_2.11|true     \nMYJAR          |b.jar                                          |false", 
            "title": "Sample Output of Listing Jars/Packages"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#removing-deployed-jars", 
            "text": "You can remove the deployed jars with the  undeploy  command. This command removes the jars that are directly installed and the jars that are associated with a package, from the system.", 
            "title": "Removing Deployed Jars"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#syntax-for-removing-deployed-jars", 
            "text": "undeploy  unique-alias-name ;   Note  The removal is only captured when you use the  undeploy  command, the jars are removed only when you restart the cluster.", 
            "title": "Syntax for Removing Deployed Jars"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#example_2", 
            "text": "undeploy spark_deep_learning_0_3_0;   Attention   If during restart, for any reason the deployed jars and packages are not reinstalled automatically by the system, a warning is shown in the log file. If you want to fail the restart, then you need to set a system property in the  conf  file to stop restarts completely. The name of the system property is  FAIL_ON_JAR_UNAVAILABILITY .  If you want to use external repositories, ensure to maintain internet connectivity at least on the lead nodes.  It is highly recommended to use a local cache path to store the downloaded jars of a package, because the next time the same deploy is executed, it can be picked from the local path.  Ensure that this path is available on the lead nodes.  Similarly keep the standalone jars also in a path where it is available to all the lead nodes.", 
            "title": "Example"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#submitting-snappydata-job-with-packages", 
            "text": "You can specify the name of the packages which can be used by a job that is submitted to SnappyData. This package is visible only to the job submitted with this argument. If another job tries to access a class belonging to the jar of this package then it will get ClassNotFoundException.  A  --packages  option is added to the  snappy-job.sh  script where you can specify the packages. Use the following syntax:  $./snappy-job.sh submit --app-name  app-name  --class  job-class  [--lead  hostname:port ]  [--app-jar  jar-path ] [other existing options]       [--packages  comma separated package-coordinates  ] [--repos  comma separated mvn repositories] [--jarcache  path where resolved jars will be kept ]", 
            "title": "Submitting SnappyData Job with Packages"
        }, 
        {
            "location": "/connectors/deployment_dependency_jar/#example-of-snappydata-job-submission-with-packages", 
            "text": "./bin/snappy-job.sh submit --app-name cassconn --class  SnappyJobClassName  --app-jar  app.jar  --lead localhost:8090 --packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.7  If you want global deployment, you can instead use the  deploy  command SQL and then the packages become visible everywhere.  snappy  deploy package cassconn 'datastax:spark-cassandra-connector:2.0.1-s_2.11' path '/home/alex/mycache';", 
            "title": "Example of SnappyData Job Submission with Packages"
        }, 
        {
            "location": "/connectors/gemfire_connector/", 
            "text": "SnappyData GemFire Connector\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nOverview\n\n\nThe SnappyData GemFire Connector allows SnappyData/Spark programs to read from data regions as well as write into data regions within GemFire clusters. You can connect the applications to one or more GemFire clusters, expose GemFire regions as SnappyData tables/Spark DataFrames, run complex SQL queries on data stored in GemFire and save SnappyData tables onto GemFire regions. The connector is designed to execute in a highly parallelized manner targeting GemFire partitioned datasets (buckets) for the highest possible performance.\n\n\nBy exposing GemFire regions as Spark DataFrames, applications can benefit from the analytic features in SnappyData such as, flexible data transformations, analytics and moving data from/into almost all modern data stores.\n\n\nFeatures\n\n\n\n\n\n\nExpose GemFire regions as SnappyData external tables\n\n\n\n\n\n\nRun SQL queries on GemFire regions from SnappyData\n\n\n\n\n\n\nSupport joins on GemFire regions from SnappyData\n\n\n\n\n\n\nSave SnappyData tables/DataFrames to GemFire\n\n\n\n\n\n\nSupport for POJOs as well as native support for GemFire PDX\n\n\n\n\n\n\nPush query predicates and projection down to GemFire and use OQL for query execution\n\n\n\n\n\n\nQuery federation across multiple GemFire clusters\n\n\n\n\n\n\nVersion and Compatibility\n\n\nSnappyData GemFire Connector supports Spark 2.1 and has been tested with GemFire 8.2 or later.\n\n\nQuick Start Guide\n\n\nThis Quick Start guide explains, how to start a GemFire cluster, load data onto a partitioned region, access this region as an SQL table, replicate to a SnappyData column table, and then run queries on both GemFire and SnappyData tables.\n\n\n\n\nSet SnappyData GemFire Connector\n\n\nConfigure the SnappyData Cluster for GemFire Connector\n\n\nAccess GemFire as an SQL Table to Run Queries\n\n\nReplicate to SnappyData Table and Running Join Queries\n\n\n\n\n \n\n\nSetting SnappyData GemFire Connector\n\n\nThe following prerequisites are required for setting up GemFire connector:\n\n\nPrerequisites\n \n\n\n\n\nBasic knowledge of GemFire\n\n\nGemFire version 8.2 or later\n installed and running.\n\n\n\n\nThe following section provides instructions to get a two-node GemFire cluster running and to deploy the functions required by SnappyData to access GemFire.\n\n\nTo start GemFire cluster and deploy SnappyData Connector functions\n\n\n\n\n\n\nStart the GemFire shell.\n\n\n$ \nGemFire_home\n/bin/gfsh\ngfsh\n start locator --name=locator1 --port=55221\n\n\n\nYou need to use a non-default port, as SnappyData uses the same defaults as GemFire.\n\n\n\n\n\n\nStart two data servers.\n\n\ngfsh\nstart server --name=server1\n--locators=localhost[55221]\ngfsh\nstart server --name=server2\n--locators=localhost[55221] --server-port=40405\n\n\n\n\n\n\n\nCreate Region.\n\n\ngfsh\ncreate region --name=GemRegion --type=PARTITION --key-constraint=java.lang.String --value-constraint=java.lang.String\n\n\n\n\n\n\n\nAdd at least 10 entries in this region using the PUT command.\n\n\ngfsh\n put --key=1 --value='James'      --region=/GemRegion\ngfsh\n put --key=2 --value='Jim'        --region=/GemRegion\ngfsh\n put --key=3 --value='James Bond' --region=/GemRegion\ngfsh\n put --key=4 --value='007'        --region=/GemRegion\n\n\n\n\n\n\n\nDeploy these functions to the GemFire cluster.\n\n\ngfsh\ndeploy --jar=\nSnappyData-home\n/connectors/gfeFunctions-0.9.jar\n\n\n\nA two node GemFire cluster is up and running with a region \nGemRegion\n and the added entries.\n\n\n\n\n\n\n \n\n\nConfiguring the SnappyData Cluster for GemFire Connector\n\n\nThe SnappyData cluster must be configured with details of the GemFire cluster with which the connector interfaces.\nThe configuration details should be provided to both the SnappyData lead and server nodes.\n\n\n\n\n\n\nModify the server and lead configuration files that are located at:\n\n\n\n\nSnappyData-home\n/conf/leads\n\n\nSnappyData-home\n/conf/servers\n\n\n\n\n\n\n\n\nAdd the connector jar (connector-0.9.jar) to the classpath and configure the remote GemFire cluster (locators, the servers and lead files) as follows:\n\n\nlocalhost -locators=localhost:10334 -client-bind-address=localhost \n-classpath= \nSnappyData-home\n/connectors/connector-0.9.jar\n-spark.gemfire-grid.\\\nUniqueID\\\n=localhost[55221]\n\n\n\nHere, the UniqueID is a name assigned for the Grid. \n\n\nFor example, SnappyData GemFire connector can connect to multiple Grids for federated data access.\n\n\n-spark.gemfire-grid.gridOne=localhost[55221] -spark.gemfire-grid.gridTwo=localhost[65221]\n\n\n\n\n\n\n\nStart the SnappyData cluster\n using the following command:\n\n\n$ \nSnappyData-home\n/sbin/snappy-start-all.sh\n\n\n\n\n\n\n\n \n\n\nAccessing GemFire as an SQL Table to Run Queries\n\n\nThe following section provides instructions to access GemFire as an SQL table to run queries.\n\n\nTo access GemFire as an SQL table.\n\n\n\n\n\n\nStart the Snappy Shell. \n\n\n$\nSnappyData-home\n/bin/snappy\nsnappy\n connect client 'localhost:1527';\n\n\n\n\n\n\n\nRegister an external table in SnappyData pointing to the GemFire region.\n\n\nsnappy\n create external table GemTable using gemfire options(regionPath 'GemRegion', keyClass 'java.lang.String', valueClass 'java.lang.String') ;\n\n\n\nThe schema is automatically inferred from the object data in GemFire:\n\n\nsnappy\n describe gemTable;\nsnappy\n select * from gemTable;\n\n\n\n\n\n\n\n \n\n\nReplicating to SnappyData Table and Running Join Queries\n\n\nYou can replicate the data in GemFire SQL table into a SnappyData table and then run join queries.\n\n\n\n\n\n\nCreate a SnappyData table based on the external table that was created using GemFire. \n\n\nsnappy\n create table SnappyDataTable using column as (select * from gemTable);\nsnappy\n select * from SnappyDataTable;\n\n\n\n\n\n\n\nRun join queries.\n\n\nsnappy\n select t1.key_Column, t1.value_Column, t2.value_Column from GemTable t1, SnappyDataTable t2 where t1.key_Column = t2.key_Column;\n\n\n\n\n\n\n\n \n\n\nInitializing the GemFire Connector\n\n\nSnappyData uses a set of functions that are deployed in the GemFire cluster, to interact with the cluster, for accessing metadata, runnning queries, and accessing data in GemFire. You must deploy the SnappyData GemFire Connector's jar that is \ngemfire-function jar\n into the GemFire cluster to enable the connector functionality.\n\n\nEnabling Connector Functionality\n\n\nTo enable the connector functionality,  you must deploy the SnappyData GemFire Connector's \ngemfire-function\n jar.\n\n\nExecute the following to deploy the \ngemfire-function\n jar:\n\n\nDeploy SnappyData GemFire Connector's gemfire-function jar (`gfeFunct\nions-0.9.2.1.jar`):\ngfsh\ndeploy --jar=\nSnappyData Product Home\n//connectors/gfeFunctions-0.9.2.1.jar\n\n\n\n\n\n\nExecuting Queries with GemFire Connector\n\n\nDuring the query execution, snappydata passes the names of attributes and filter conditions to the GemFire cluster, to prune the data that is fetched from GemFire.\n\n\nFor example, if you query for only attribute A from a GemFire Region value and that too of only those Region values which meet the filter condition, then instead of fetching the complete value only the pruned data needs to be sent to SnappyData.\n\n\nFor this purpose by default SnappyData relies on OQL of GemFire to prune the data. However, you can write custom \nQueryExecutor\n to retrieve the pruned data from GemFire. This is done by implementing \nQueryExecutor\n interface.\nThe \nQueryExecutor\n implementation should be packaged in a jar which is loaded by SnappyData using \nServiceLoader\n API of java. As a part of the contract, the jar should include the following file with the path described:\nMETA-INF/services/io.snappydata.spark.gemfire.connector.query.QueryExecutor\nThis file should contain the fully qualified class name of the custom \nQueryExecutor\n.\n\n\n\n\nNote\n\n\nThe name of the file should be \nio.snappydata.spark.gemfire.connector.query.QueryExecutor\n.\n\n\n\n\nThis jar needs to be deployed on the GemFire cluster using \ngfsh\n.\n\n\n\n\nAttention\n\n\nIt is important that as part of deployment, \ngfeFunctions.jar\n must be deployed first and then the jars containing custom \nQueryExecutors\n.\n\n\n\n\nFollowing is an example of the implementation of \nQueryExecutor\n interface:\n\n\npackage io.snappydata.spark.gemfire.connector.query;\n\nimport java.util.Iterator;\nimport com.gemstone.gemfire.cache.Region;\nimport com.gemstone.gemfire.cache.execute.RegionFunctionContext;\nimport com.gemstone.gemfire.pdx.PdxInstance;\nimport io.snappydata.spark.gemfire.connector.internal.gemfirefunctions.shared.filter.Filter;\n\n  public interface QueryExecutor\nT, S\n {\n    /**\n     *\n     * @param region\n     * @param requiredAttributes\n     * @param filter\n     * @param isPdxData\n     * @return A stateful object which will be passed to the querying apis on every invocation.\n     * The idea is that if the user can prepare an object which could be used for every bucket, it will be\n     * efficient. So if a compiled Query object is prepared at the start of evaluation, then every bucket can\n     * use this Query object instead of creating a compiled query object from string every time.\n     */\n    public S initStatefulObject(Region region, String[] requiredAttributes,\n        T filter, boolean isPdxData);\n\n    /**\n     *\n     * @return any String which uniquely identifies the Executor\n     */\n    public String getUniqueID();\n\n    /**\n     * This method will be invoked if the query needs a single field projection\n     * User can return null in which case default implementation using OQL will be used\n     * @param region Region on which query is to be performed\n     * @param requiredAttribute projection which need to be returned\n     * @param filter The desired format of the Filter condition to filter out relevant rows\n     * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region\n     * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion\n     * @param  statefulObject which can be used across all buckets eg a compiled Query object.\n     * @return An Iterator over the projected columns\n     */\n    public Iterator\nObject\n executeSingleProjectionQuery(Region region, String requiredAttribute, T filter,\n        int forBucket, RegionFunctionContext context, S statefulObject);\n\n  /**\n   * This method will be invoked if the query needs multiple fields projection\n   * User can return null in which case default implementation using OQL will be used\n   * @param region Region on which query is to be performed\n   * @param requiredAttributes A String array containing field names which need to be projected\n   * @param filter The desired format of the Filter condition to filter out relevant rows\n   * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region\n   * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion\n   * @param holder An instance of MultiProjectionHolder which should be used to store the projections during\n   *               iteration. It is a wrapper over object[] intended to avoid creation of Object[] during ietaration\n   * @param  statefulObject which can be used across all buckets eg a compiled Query object.\n   * @return An Iterator of MultiProjectionHolder which contains the projected columns\n   */\n  public Iterator\nMultiProjectionHolder\n executeMultiProjectionQuery(Region region, String[] requiredAttributes,\n      T filter, int forBucket, RegionFunctionContext context, MultiProjectionHolder holder, S statefulObject);\n\n  /**\n   * This method is invoked if the region contains PdxInstances. User needs to return the filtered PdxInstances\n   * without applying projection, which will be applied internally . This will ensure that projected columns\n   * do not get unnecessarily deserialized\n   * User can return null in which case default implementation using OQL will be used\n   * @param region Region on which query is to be performed\n   * @param filter The desired format of the Filter condition to filter out relevant rows\n   * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region\n   * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion\n   * @param  statefulObject which can be used across all buckets eg a compiled Query object.\n   * @return Iterator over PdxInstance\n   */\n  public Iterator\nPdxInstance\n filteredPdxData(Region region,  T filter,\n      int forBucket, RegionFunctionContext context, S statefulObject);\n\n  /**\n   *\n   * @param filters Filter objects which need to be transformed by the user in the way it can be used in querying\n   * @return transformed filter\n   */\n  public T transformFilter(Filter[] filters);\n}\n\n\n\n\n\nThe working of \nQueryExecutor\n can be broken as follows:\n\n\n\n\nConverting the filter condition\n into a desired form.\n\n\nInitializing a stateful context object\n which avoids recreating a reusable unchanging object during data fetch on the individual buckets.\n\n\nRetrieval of data for Region\n as a whole, in case of replicated region, or for individual buckets, in case of partitioned region.\n\n\n\n\n \n\n\nConvert Filter Condition\n\n\nDuring execution, SnappyData checks with the available \nQueryExecutor\n by invoking the \ntransformFilter\n method, if the filter is present.\nIn case the \nQueryExecutor\n is capable of handling it, a transformed filter is returned which can be used for data retrieval or else an \nUnsupportedOperationException\n is shown.\n\n\n \n\n\nInitialize Stateful Context Object\n\n\nIf a transformed value is returned, then \ninitStatefulObject\n method is invoked. You can utilize this to instantiate a reusable object such as, a parsed QueryPlan or a compiled structure or can simply return null. At this stage too, if the query cannot be handled, \nUnsupportedOperationException\n is shown.\n\n\n \n\n\nRetrieve Data for Region\n\n\nNext step is invocation of the appropriate method to get Iterator on the pruned data. This method can get invoked multiple times based on the number of partitioned region buckets that is operated upon.\n\n\n\n\n\n\nIf the region contains \npdx\n instances, then the method \nfilteredPdxData\n is invoked.\n\n\n\n\nNote\n\n\nThis method requires the user to return the iterator on valid Pdx instances. The relevant attributes are extracted by the framework.\n\n\n\n\n\n\n\n\nIf the region contains POJOs, then depending upon single or multiple attributes, one of the following method is invoked:\n\n\n\n\nexecuteSingleProjectionQuery\n \n\n\nexecuteMultiProjectionQuery\n*\n\n\n\n\n\n\n\n\n \n\n\nConfiguring the SnappyData Cluster for GemFire Connector\n\n\nThe following configurations can be set in SnappyData Cluster for GemFire Connector:\n\n\n\n\nCustomize DistributedSystem in the cluster with additional attributes\n\n\nSpecify Static Locators\n\n\nDynamic Discovery of Grid for a Region\n\n\n\n\n \n\n\nCustomizing the \nDistributedSystem\n of SnappyData Cluster at Startup\n\n\nYou can configure the \nDistributedSystem\n with additional attributes by implementing the following interface:\n\n\npackage io.snappydata.spark.gemfire.connector.dsinit;\n/**\n * @param @link{DSConfig} instance which can be used to configure the DistributedSystem properties at the\n * time of snappydata startup\n */\npublic interface DistributedSystemInitializer {\n\n  public void configure(DSConfig config);\n}\n\n\n\n\nAny required properties of the \nDistributedSystem\n can be configured via the \nDSConfig\n instance passed.\nThe \nDistributedSystemInitializer\n implementation needs to be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. The jar should include the following file along with the path as described:\n\n\nMETA-INF/services/io.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer\n\n\nThis file should contain the fully qualified class name of the custom DistributedSystemInitializer\n\n\n\n\nNote\n\n\nThe name of the file should be \nio.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer\n\n\n\n\nThe connector interacts with the GemFire cluster. Therefore, you should configure the SnappyData cluster with the details of GemFire cluster. The configuration details must be provided in both the SnappyData lead and server nodes at the following locations:\n-   \nSnappyData-Home directory/conf/leads\n\n-   \nSnappyData-Home directory/conf/servers \n\n\nModify the servers and leads configuration file to add the connector jar (\nconnector-1.0.0.jar\n) and the \nperson.jar\n in the classpath as well as to configure the remote GemFire cluster locator.\n\n\n \n\n\nSpecifying Static Locators\n\n\nTo statically specify the running locators of the GemFire cluster, set the property as follows, where \nuniqueIDForGrid\n is any unique identifier key:\n\n\n    -snappydata.connector.gemfire-grid.\\\nuniqueIDForGrid\\\n=localhost[55221]\n\n\n\nFollowing is a sample from the servers and leads file:\n\n\nlocalhost -locators=localhost:10334 -client-bind-address=localhost -client-port=1528 -heap-size=20g\n-classpath=\nSnappyData-Product-Home\n//connectors/connector_2.11-0.9.2.1.jar:\npath-to-jar\n/persons.jar\n-snappydata.connector.gemfire-grid.\nuniqueIDForGrid\n=localhost[55221]\n\n\n\n\n \n\n\nDynamically Discovering Grid for a Region\n\n\nInstead of specifying the locators via the property, it is possible to provide custom logic for discovery of the grid for the region by implementing \nGridResolver\n trait as follows:\n\n\ntrait GridResolver {\n\n  /**\n    * Optional method to identify the locators and delegate to SnappyData for creating connection \n    * pool using [[GridHelper]]. Invoked once in the lifecycle.\n    * @param gfeGridProps\n    * @param gridHelper\n    */\n  def initConnections(gfeGridProps: java.util.Map[String, String], gridHelper: GridHelper): Unit\n\n  /**\n    * Resolve the grid for the region or return null if unable to do so\n    * @param regionName\n    * @param gfeGridProps\n    * @param gridHelper\n    * @param userOptions\n    * @return the grid name ( the name of the connection pool) to use\n    */\n  def getGridForRegion(regionName: String, gfeGridProps: java.util.Map[String, String],\n      gridHelper: GridHelper, userOptions: java.util.Map[String, String]): String\n\n  /**\n    *\n    * @return a String identifying the resolver uniquely\n    */\n  def getUniqueID: String\n}\n\n\n\n\nSnappyData attempts to resolve the grid for the region using existing \nConnectionPools\n. If it is not successful, it checks with the available resolvers by invoking \ngetGridForRegion\n. The resolver returns null, if it cannot resolve the grid.\n\n\nThe \nGridResolver\n implementation should be packaged in a jar which is loaded by SnappyData using \nServiceLoader\n API of java. The jar should also include the following file along with the described path:\nMETA-INF/services/io.snappydata.spark.gemfire.connector.grids.GridResolver\n\nThis file should contain the fully qualified class name of the custom \nGridResolver\n.\n\n\n\n\nNote\n\n\nThe name of the file should be \nio.snappydata.spark.gemfire.connector.grids.GridResolver\n.\n\n\n\n\nTo initialize the GemFire connector and enable its functions in SnappyData, you must include the following import statement before creating the external table:\n\n\nimport io.snappydata.spark.gemfire.connector\n\n\n\n\n \n\n\nAccessing Data from GemFire\n\n\nIn SnappyData applications, you can create external tables that represent GemFire regions and run SQL queries against GemFire. For accesing data from GemFire, you must first expose the GemFire regions: \n\n\nYou can use any the following options to expose GemFire regions:\n\n\n\n\nExpose GemFire PDX Regions as External Tables\n\n\nExpose Regions Containing POJOs as External Tables\n\n\nExpose GemFire Region Using Dataframe based External Tables\n\n\nExpose GemFire Regions as RDDs\n\n\n\n\n \n\n\nExposing GemFire PDX Regions as External Tables\n\n\nYou can create an external table that represents a GemFire region which stores PDX instances. The SnappyData schema for this external table is derived from the PDXType. Here, the GemFire region is already populated with data and SnappyData infers the schema based on the inspection of the PDX types.\nThe following syntax creates an external table that represents a GemFire region which stores PDX instances.\n\n\nval externalBsegTable = snc.createExternalTable(\nbsegInGem\n, \n     \ngemfire\n,\n     Map[String, String](\nregion.path\n -\n \nbseg1\n, \ndata.as.pdx\n -\n \ntrue\n))\n\n\n\n\nThe SnappyData external table schema for the GemFire region can optionally include the GemFire region key as a column in the table. To enable this, the \nkey.class \n attribute should be set when you create the table as shown in the following example:\n\n\nval externalBsegTable = snc.createExternalTable(\nbsegInGem\n,\n   \ngemfire\n,\n   Map[String, String](\nregion.path\n -\n \nbseg1\n, \ndata.as.pdx\n -\n \ntrue\n,\n   \nkey.class\n -\n \njava.lang.Long\n     \n     ))\n\n\n\n\n \n\n\nExposing Regions Containing POJOs as External Tables\n\n\nIn the following example, an external table is created using the getter methods on POJOs as SnappyData column names:\n\n\nsnc.createExternalTable(externalPersonsTable1, \ngemfire\n, \nMap[String, String](\nregion.path\n -\n personsRegionName, \nvalue.class\n -\n \nload.Person\n))\n\n\n\n\nAs in the previous case, if the GemFire key field has to be included as a column, then the \nKey.class\n attribute has to be passed in as an option.\n\n\nsnc.createExternalTable(externalPersonsTable1, \ngemfire\n, Map[String, String](\nregion.path\n -\n personsRegionName, \n\nvalue.class\n -\n \nload.Person\n), \nkey.class\n -\n \njava.lang.Long\n))\n\n\n\n\n \n\n\nExpose GemFire Region Using Dataframe Based External Tables\n\n\nIn the following example, a DataFrame is used to create an external table \nbsegTable\n, with the schema which is same as that of DataFrame \nbsegDF\n. The primary key column name should be specified for this to work correctly. In the following example, it is assumed that the dataframe contains a column named \"id1\".\n\n\nbsegDF.write.format(\ngemfire\n).  \n      option(\nregion.path\n, \nbseg1\n).\n      option(\nprimary.key.column.name\n, \nid1\n).\n      option(\npdx.identity.fields\n, \nid1\n).\n      saveAsTable(\nbsegTable\n)\n\n\n\n\n\n\nNote\n\n\nThe \npdx.identity.fields\n specification has a huge impact on performance within GemFire since this specification informs GemFire to use only the specified field for computing hashCode and equality for PdxInstance.\n\n\n\n\n \n\n\nExpose GemFire Regions as RDDs\n\n\nInvoking the \ngemfireRegion\n method on the SparkContext in SnappyData exposes the full data set of a GemFire region as a Spark RDD.\nThe same API exposes both replicated and partitioned region as RDDs.\n\n\nscala\n val rdd = sc.gemfireRegion[String, String](\ngemTable1\n)\n\nscala\n rdd.foreach(println)\n(1,one)\n(3,three)\n(2,two)\n\n\n\n\n\n\nNote\n\n\nwhen the RDD is used, it is important to specify the correct type for both the region key and value, otherwise a \nClassCastException\n is encountered.\n\n\n\n\n \n\n\nControlling the Schema of the External Table\n\n\nThe pdx fields (of pdx instances) or the getter methods (of the POJOs) define the schema of the external table by default. However, you can also control the schema by excluding the columns as per requirement. This is done by implementing the following trait:\n\n\npackage io.snappydata.spark.gemfire.connector.lifecycle\nimport org.apache.spark.sql.types.StructType\n\ntrait ColumnSelector {\n  def selectColumns(gridName: Option[String], regionName: String,\n      allColumns: StructType): StructType\n}\n\n\n\n\nBased on the requirement, a new StructType which contains fewer columns can be returned. \n\n\n\n\nNote\n\n\nThe new StructType returned can only have the subset of the columns that are passed. No new columns should be added.\n\n\n\n\nThe \nColumnSelector\n implemetation needs to be added to the startup classpath of the SnappyDataCluster.\nAt the time of table creation, an option with key = \ncolumn.selector\n and value as the fully qualified class name of the ColumnSelector implementation class should be passed.\n\n\nOther Optional Configuration Parameters\n\n\n\n\n\n\n\n\ncolumn\n\n\ncolumn\n\n\n\n\n\n\n\n\n\n\nmax.rows.restrict\n\n\nThis parameter is used to restrict the number of rows that are fetched from an external table, when a query of type \nselect * from external_table\n is executed. This restriction, if specified,  is applied to queries without any filter, limit, aggregate function, and projection such that it tends to bring all the rows from the external region. Note that this restriction is not applied in case of DDL statement such as \ncreate table X using row column as select * from external_table\n. The default value for is 10,000.\n\n\n\n\n\n\ncolumn.selector\n\n\nThis parameter is used to control the columns that must be included in the schema as described \nhere\n.\n\n\n\n\n\n\nmax.buckets.per.partition\n\n\nThis parameter is used to control the concurrency and number of tasks created to fetch data from an external region. This property is useful only for Partitioned Region. For more details, refer to \nControlling Task Concurrency in SnappyData When Accessing GemFire\n.  The default value is 3.\n\n\n\n\n\n\nsecurity.username\n  and \nsecurity.password\n\n\nBy default the external table is created and queried using the credential of the current user. For example user X creates external table, and if user Y is querying, then credentials of user Y is fetched from data. But if the parameters \nsecurity.username\n  and \nsecurity.password\n are passed then those credentials are used to create external table and for querying the data irrespective of the current user.\n\n\n\n\n\n\npdxtype.maxscan\n\n\nThis parameter determines the maximum number of entries from the region which is scanned to completely determine the schema of the external table from the Pdx instances that is stored in the region. The default value is 100.\n\n\n\n\n\n\n\n\n \n\n\nControlling Task Concurrency in SnappyData When Accessing GemFire\n\n\nThere are two types of regions in GemFire; \nreplicated\n and \npartitioned\n. All the data for a replicated region is present on every server where the region is defined. A GemFire partitioned region splits the data across the servers that define the partitioned region. \n\n\nWhen operating with replicated regions, there is only a single RDD partition representing the replicated region in SnappyData. Since there is only one data bucket for the replicated region. As compared to this, a GemFire partitioned region can be represented by a configurable number of RDD partitions in SnappyData. \n\n\nChoosing the number of RDD partitions directly controls the task concurrency in SnappyData when you run queries on GemFire regions. By default, the GemFire connector works out the number of buckets per GemFire server, assigns partitions to each server, and uses a default value of maximum three buckets per partition. You can configure the \nmax.buckets.per.partition\n attribute to change this value. \n\n\nWhen queries are executed on an external table, the degree of parallelism in query execution is directly proportional to the number of RDD partitions that represents the table.\n\n\nThe following example shows how to configure the RDD partitions count for an external table representing a GemFire region:\n\n\nimport io.snappydata.spark.gemfire.connector._\n\nval externalBsegTable = snc.createExternalTable(\nbsegInGem\n,\n   \ngemfire\n,\n   Map[String, String](\nregion.path\n -\n \nbseg1\n, \ndata.as.pdx\n -\n \ntrue\n,\n   \nkey.class\n -\n \njava.lang.Long\n , \nmax.buckets.per.partition\n -\n \n5\n    \n     ))\n\n\n\n\nSaving Data to GemFire Region\n\n\nYou can save data to GemFire Regions using any of the following :\n\n\n\n\nSaving Pair RDD to GemFire Region\n\n\nSaving Non-Pair RDD to GemFire\n\n\nSaving DataFrame to GemFire\n\n\n\n\n \n\n\nSaving Pair RDD to GemFire Region\n\n\nA pair RDD can be saved from SnappyData into a GemFire region as follows: \n\n\n\n\n\n\nImport the implicits as shown:\n\n\nimport io.snappydata.spark.gemfire.connector\n\n\n\n\n\n\n\nIn the Spark shell, create a simple pair RDD and save it to GemFire Region:\n\n\nscala\n import io.snappydata.spark.gemfire.connector._\nscala\n val data = Array((\"1\", \"one\"), (\"2\", \"two\"), (\"3\", \"three\"))\ndata: Array[(String, String)] = Array((1,one), (2,two), (3,three))\n\nscala\n val distData = sc.parallelize(data)\ndistData: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at \nconsole\n:14\n\nscala\n distData.saveToGemFire(\"gemTable1\")\n15/02/17 07:11:54 INFO DAGScheduler: Job 0 finished: runJob at GemFireRDDFunctions.scala:29, took 0.341288 s\n\n\n\n\n\n\n\nVerify the data is saved in GemFire using \ngfsh\n:\n\n\ngfsh\nquery --query=\"select key,value from /gemTable1.entries\"\n\nResult     : true\nstartCount : 0\nendCount   : 20\nRows       : 3\n\nkey | value\n--- | -----\n1   | one\n3   | three\n2   | two\n\n\n\n\n\n\n\n \n\n\nSaving Non-Pair RDD to GemFire\n\n\nSaving a non-pair RDD to GemFire requires an extra function that converts each element of RDD to a key-value pair. \nHere's a sample session in Spark shell:\n\n\nscala\n import io.snappydata.spark.gemfire.connector._\nscala\n val data2 = Array(\na\n,\nab\n,\nabc\n)\ndata2: Array[String] = Array(a, ab, abc)\n\nscala\n val distData2 = sc.parallelize(data2)\ndistData2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at \nconsole\n:17\n\nscala\n distData2.saveToGemFire(\ngemTable1\n, e =\n (e.length, e))\n[info 2015/02/17 12:43:21.174 PST \nmain\n tid=0x1]\n...\n15/02/17 12:43:21 INFO DAGScheduler: Job 0 finished: runJob at GemFireRDDFunctions.scala:52, took 0.251194 s\n\n\n\n\nVerify the result with \ngfsh\n:\n\n\ngfsh\nquery --query=\nselect key,value from /gemTable1.entrySet\n\n\nResult     : true\nstartCount : 0\nendCount   : 20\nRows       : 3\n\nkey | value\n--- | -----\n2   | ab\n3   | abc\n1   | a\n\n\n\n\n \n\n\nSaving a DataFrame to GemFire\n\n\nTo save a DataFrame, that is dataSet of row objects, into GemFire, use the following API which is available as an implicit definition. The rows of the dataframes are converted into PDX instances for storage in the GemFire's region.\n\n\nIn the following example, it is assumed that there is a column \"id1\" present in the dataframe's schema. To specify the PDX Identity Fields for the PDX Type, use the option as (\"pdx.identity.fields\", \"Col1, Col2, Col3\") to specify one or more columns to be used as PDX Identity fields. \nSnappyData recommends to define the identity fields for performance during comparison of PDX Instances.\n\n\nimport io.snappydata.spark.gemfire.connector._\n\ndf.write.format(\ngemfire\n).\n          option(\nregion.path\n,\n/region1\n).\n          option(\nprimary.key.column.name\n, \nid1\n).\n          option(\npdx.identity.fields\n, \nid1\n)\n          .save()\n\n\n\n\nTo dynamically generate the GemFire Region's key, import the implicits and use the following API:\n\n\nsaveToGemFire[K](regionPath: String, keyExtractor: Row =\n K, opConf: Map[String, String] = Map.empty )\n\n\n\n\nimport io.snappydata.spark.gemfire.connector._\n\ndf.saveToGemFire[String](\n/region1\n, \n(row: Row) =\n ( row.getString(1) + \n_\n + row.getString(10)),\nMap[String, String](\npdx.identity.fields\n -\n \nid1, id10\n)\n)\n\n\n\n\n \n\n\nRunning OQL queries Directly on GemFire from SnappyData\n\n\nMost applications using SnappyData will choose to run regular SQL queries on GemFire regions. Refer to \nAccessing Data From GemFire\n \nAdditionally, you can directly execute OQL queries on GemFire regions using the GemFire connector. In scenarios where the data stored in GemFire regions is neither PDX nor Java bean compliant POJO, you can execute OQL queries and retrieve the data from the server and make it available as a data frame.\n\n\nAn instance of \nSQLContext\n is required to run OQL query.\n\n\nval snc = new org.apache.spark.sql.SnappyContext(sc)\n\n\n\n\nCreate a \nDataFrame\n using OQL:\n\n\nval dataFrame = snc.gemfireOQL(\nSELECT iter.name,itername.address.city, iter.id FROM /personRegion iter\n)\n\n\n\n\nYou can repartition the \nDataFrame\n using \nDataFrame.repartition()\n if required. \nAfter you have the \nDataFrame\n, you can register it as a table and use Spark \nSQL to query:\n\n\ndataFrame.registerTempTable(\nperson\n)\nval SQLResult = sqlContext.sql(\nSELECT * FROM person WHERE id \n 100\n)", 
            "title": "Using the SnappyData GemFire Connector"
        }, 
        {
            "location": "/connectors/gemfire_connector/#snappydata-gemfire-connector", 
            "text": "This feature is available only in the Enterprise version of SnappyData.", 
            "title": "SnappyData GemFire Connector"
        }, 
        {
            "location": "/connectors/gemfire_connector/#overview", 
            "text": "The SnappyData GemFire Connector allows SnappyData/Spark programs to read from data regions as well as write into data regions within GemFire clusters. You can connect the applications to one or more GemFire clusters, expose GemFire regions as SnappyData tables/Spark DataFrames, run complex SQL queries on data stored in GemFire and save SnappyData tables onto GemFire regions. The connector is designed to execute in a highly parallelized manner targeting GemFire partitioned datasets (buckets) for the highest possible performance.  By exposing GemFire regions as Spark DataFrames, applications can benefit from the analytic features in SnappyData such as, flexible data transformations, analytics and moving data from/into almost all modern data stores.", 
            "title": "Overview"
        }, 
        {
            "location": "/connectors/gemfire_connector/#features", 
            "text": "Expose GemFire regions as SnappyData external tables    Run SQL queries on GemFire regions from SnappyData    Support joins on GemFire regions from SnappyData    Save SnappyData tables/DataFrames to GemFire    Support for POJOs as well as native support for GemFire PDX    Push query predicates and projection down to GemFire and use OQL for query execution    Query federation across multiple GemFire clusters", 
            "title": "Features"
        }, 
        {
            "location": "/connectors/gemfire_connector/#version-and-compatibility", 
            "text": "SnappyData GemFire Connector supports Spark 2.1 and has been tested with GemFire 8.2 or later.", 
            "title": "Version and Compatibility"
        }, 
        {
            "location": "/connectors/gemfire_connector/#quick-start-guide", 
            "text": "This Quick Start guide explains, how to start a GemFire cluster, load data onto a partitioned region, access this region as an SQL table, replicate to a SnappyData column table, and then run queries on both GemFire and SnappyData tables.   Set SnappyData GemFire Connector  Configure the SnappyData Cluster for GemFire Connector  Access GemFire as an SQL Table to Run Queries  Replicate to SnappyData Table and Running Join Queries", 
            "title": "Quick Start Guide"
        }, 
        {
            "location": "/connectors/gemfire_connector/#setting-snappydata-gemfire-connector", 
            "text": "The following prerequisites are required for setting up GemFire connector:  Prerequisites     Basic knowledge of GemFire  GemFire version 8.2 or later  installed and running.   The following section provides instructions to get a two-node GemFire cluster running and to deploy the functions required by SnappyData to access GemFire.  To start GemFire cluster and deploy SnappyData Connector functions    Start the GemFire shell.  $  GemFire_home /bin/gfsh\ngfsh  start locator --name=locator1 --port=55221  You need to use a non-default port, as SnappyData uses the same defaults as GemFire.    Start two data servers.  gfsh start server --name=server1\n--locators=localhost[55221]\ngfsh start server --name=server2\n--locators=localhost[55221] --server-port=40405    Create Region.  gfsh create region --name=GemRegion --type=PARTITION --key-constraint=java.lang.String --value-constraint=java.lang.String    Add at least 10 entries in this region using the PUT command.  gfsh  put --key=1 --value='James'      --region=/GemRegion\ngfsh  put --key=2 --value='Jim'        --region=/GemRegion\ngfsh  put --key=3 --value='James Bond' --region=/GemRegion\ngfsh  put --key=4 --value='007'        --region=/GemRegion    Deploy these functions to the GemFire cluster.  gfsh deploy --jar= SnappyData-home /connectors/gfeFunctions-0.9.jar  A two node GemFire cluster is up and running with a region  GemRegion  and the added entries.", 
            "title": "Setting SnappyData GemFire Connector"
        }, 
        {
            "location": "/connectors/gemfire_connector/#configuring-the-snappydata-cluster-for-gemfire-connector", 
            "text": "The SnappyData cluster must be configured with details of the GemFire cluster with which the connector interfaces. The configuration details should be provided to both the SnappyData lead and server nodes.    Modify the server and lead configuration files that are located at:   SnappyData-home /conf/leads  SnappyData-home /conf/servers     Add the connector jar (connector-0.9.jar) to the classpath and configure the remote GemFire cluster (locators, the servers and lead files) as follows:  localhost -locators=localhost:10334 -client-bind-address=localhost \n-classpath=  SnappyData-home /connectors/connector-0.9.jar\n-spark.gemfire-grid.\\ UniqueID\\ =localhost[55221]  Here, the UniqueID is a name assigned for the Grid.   For example, SnappyData GemFire connector can connect to multiple Grids for federated data access.  -spark.gemfire-grid.gridOne=localhost[55221] -spark.gemfire-grid.gridTwo=localhost[65221]    Start the SnappyData cluster  using the following command:  $  SnappyData-home /sbin/snappy-start-all.sh", 
            "title": "Configuring the SnappyData Cluster for GemFire Connector"
        }, 
        {
            "location": "/connectors/gemfire_connector/#accessing-gemfire-as-an-sql-table-to-run-queries", 
            "text": "The following section provides instructions to access GemFire as an SQL table to run queries.  To access GemFire as an SQL table.    Start the Snappy Shell.   $ SnappyData-home /bin/snappy\nsnappy  connect client 'localhost:1527';    Register an external table in SnappyData pointing to the GemFire region.  snappy  create external table GemTable using gemfire options(regionPath 'GemRegion', keyClass 'java.lang.String', valueClass 'java.lang.String') ;  The schema is automatically inferred from the object data in GemFire:  snappy  describe gemTable;\nsnappy  select * from gemTable;", 
            "title": "Accessing GemFire as an SQL Table to Run Queries"
        }, 
        {
            "location": "/connectors/gemfire_connector/#replicating-to-snappydata-table-and-running-join-queries", 
            "text": "You can replicate the data in GemFire SQL table into a SnappyData table and then run join queries.    Create a SnappyData table based on the external table that was created using GemFire.   snappy  create table SnappyDataTable using column as (select * from gemTable);\nsnappy  select * from SnappyDataTable;    Run join queries.  snappy  select t1.key_Column, t1.value_Column, t2.value_Column from GemTable t1, SnappyDataTable t2 where t1.key_Column = t2.key_Column;", 
            "title": "Replicating to SnappyData Table and Running Join Queries"
        }, 
        {
            "location": "/connectors/gemfire_connector/#initializing-the-gemfire-connector", 
            "text": "SnappyData uses a set of functions that are deployed in the GemFire cluster, to interact with the cluster, for accessing metadata, runnning queries, and accessing data in GemFire. You must deploy the SnappyData GemFire Connector's jar that is  gemfire-function jar  into the GemFire cluster to enable the connector functionality.", 
            "title": "Initializing the GemFire Connector"
        }, 
        {
            "location": "/connectors/gemfire_connector/#enabling-connector-functionality", 
            "text": "To enable the connector functionality,  you must deploy the SnappyData GemFire Connector's  gemfire-function  jar.  Execute the following to deploy the  gemfire-function  jar:  Deploy SnappyData GemFire Connector's gemfire-function jar (`gfeFunct\nions-0.9.2.1.jar`):\ngfsh deploy --jar= SnappyData Product Home //connectors/gfeFunctions-0.9.2.1.jar", 
            "title": "Enabling Connector Functionality"
        }, 
        {
            "location": "/connectors/gemfire_connector/#executing-queries-with-gemfire-connector", 
            "text": "During the query execution, snappydata passes the names of attributes and filter conditions to the GemFire cluster, to prune the data that is fetched from GemFire.  For example, if you query for only attribute A from a GemFire Region value and that too of only those Region values which meet the filter condition, then instead of fetching the complete value only the pruned data needs to be sent to SnappyData.  For this purpose by default SnappyData relies on OQL of GemFire to prune the data. However, you can write custom  QueryExecutor  to retrieve the pruned data from GemFire. This is done by implementing  QueryExecutor  interface.\nThe  QueryExecutor  implementation should be packaged in a jar which is loaded by SnappyData using  ServiceLoader  API of java. As a part of the contract, the jar should include the following file with the path described: META-INF/services/io.snappydata.spark.gemfire.connector.query.QueryExecutor This file should contain the fully qualified class name of the custom  QueryExecutor .   Note  The name of the file should be  io.snappydata.spark.gemfire.connector.query.QueryExecutor .   This jar needs to be deployed on the GemFire cluster using  gfsh .   Attention  It is important that as part of deployment,  gfeFunctions.jar  must be deployed first and then the jars containing custom  QueryExecutors .   Following is an example of the implementation of  QueryExecutor  interface:  package io.snappydata.spark.gemfire.connector.query;\n\nimport java.util.Iterator;\nimport com.gemstone.gemfire.cache.Region;\nimport com.gemstone.gemfire.cache.execute.RegionFunctionContext;\nimport com.gemstone.gemfire.pdx.PdxInstance;\nimport io.snappydata.spark.gemfire.connector.internal.gemfirefunctions.shared.filter.Filter;\n\n  public interface QueryExecutor T, S  {\n    /**\n     *\n     * @param region\n     * @param requiredAttributes\n     * @param filter\n     * @param isPdxData\n     * @return A stateful object which will be passed to the querying apis on every invocation.\n     * The idea is that if the user can prepare an object which could be used for every bucket, it will be\n     * efficient. So if a compiled Query object is prepared at the start of evaluation, then every bucket can\n     * use this Query object instead of creating a compiled query object from string every time.\n     */\n    public S initStatefulObject(Region region, String[] requiredAttributes,\n        T filter, boolean isPdxData);\n\n    /**\n     *\n     * @return any String which uniquely identifies the Executor\n     */\n    public String getUniqueID();\n\n    /**\n     * This method will be invoked if the query needs a single field projection\n     * User can return null in which case default implementation using OQL will be used\n     * @param region Region on which query is to be performed\n     * @param requiredAttribute projection which need to be returned\n     * @param filter The desired format of the Filter condition to filter out relevant rows\n     * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region\n     * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion\n     * @param  statefulObject which can be used across all buckets eg a compiled Query object.\n     * @return An Iterator over the projected columns\n     */\n    public Iterator Object  executeSingleProjectionQuery(Region region, String requiredAttribute, T filter,\n        int forBucket, RegionFunctionContext context, S statefulObject);\n\n  /**\n   * This method will be invoked if the query needs multiple fields projection\n   * User can return null in which case default implementation using OQL will be used\n   * @param region Region on which query is to be performed\n   * @param requiredAttributes A String array containing field names which need to be projected\n   * @param filter The desired format of the Filter condition to filter out relevant rows\n   * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region\n   * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion\n   * @param holder An instance of MultiProjectionHolder which should be used to store the projections during\n   *               iteration. It is a wrapper over object[] intended to avoid creation of Object[] during ietaration\n   * @param  statefulObject which can be used across all buckets eg a compiled Query object.\n   * @return An Iterator of MultiProjectionHolder which contains the projected columns\n   */\n  public Iterator MultiProjectionHolder  executeMultiProjectionQuery(Region region, String[] requiredAttributes,\n      T filter, int forBucket, RegionFunctionContext context, MultiProjectionHolder holder, S statefulObject);\n\n  /**\n   * This method is invoked if the region contains PdxInstances. User needs to return the filtered PdxInstances\n   * without applying projection, which will be applied internally . This will ensure that projected columns\n   * do not get unnecessarily deserialized\n   * User can return null in which case default implementation using OQL will be used\n   * @param region Region on which query is to be performed\n   * @param filter The desired format of the Filter condition to filter out relevant rows\n   * @param forBucket Bucket ID on which the query is to be performed. will be -1 for replicated region\n   * @param context RegionFunctionContext which can provide metadata to get handle of BucketRegion\n   * @param  statefulObject which can be used across all buckets eg a compiled Query object.\n   * @return Iterator over PdxInstance\n   */\n  public Iterator PdxInstance  filteredPdxData(Region region,  T filter,\n      int forBucket, RegionFunctionContext context, S statefulObject);\n\n  /**\n   *\n   * @param filters Filter objects which need to be transformed by the user in the way it can be used in querying\n   * @return transformed filter\n   */\n  public T transformFilter(Filter[] filters);\n}  The working of  QueryExecutor  can be broken as follows:   Converting the filter condition  into a desired form.  Initializing a stateful context object  which avoids recreating a reusable unchanging object during data fetch on the individual buckets.  Retrieval of data for Region  as a whole, in case of replicated region, or for individual buckets, in case of partitioned region.", 
            "title": "Executing Queries with GemFire Connector"
        }, 
        {
            "location": "/connectors/gemfire_connector/#convert-filter-condition", 
            "text": "During execution, SnappyData checks with the available  QueryExecutor  by invoking the  transformFilter  method, if the filter is present.\nIn case the  QueryExecutor  is capable of handling it, a transformed filter is returned which can be used for data retrieval or else an  UnsupportedOperationException  is shown.", 
            "title": "Convert Filter Condition"
        }, 
        {
            "location": "/connectors/gemfire_connector/#initialize-stateful-context-object", 
            "text": "If a transformed value is returned, then  initStatefulObject  method is invoked. You can utilize this to instantiate a reusable object such as, a parsed QueryPlan or a compiled structure or can simply return null. At this stage too, if the query cannot be handled,  UnsupportedOperationException  is shown.", 
            "title": "Initialize Stateful Context Object"
        }, 
        {
            "location": "/connectors/gemfire_connector/#retrieve-data-for-region", 
            "text": "Next step is invocation of the appropriate method to get Iterator on the pruned data. This method can get invoked multiple times based on the number of partitioned region buckets that is operated upon.    If the region contains  pdx  instances, then the method  filteredPdxData  is invoked.   Note  This method requires the user to return the iterator on valid Pdx instances. The relevant attributes are extracted by the framework.     If the region contains POJOs, then depending upon single or multiple attributes, one of the following method is invoked:   executeSingleProjectionQuery    executeMultiProjectionQuery *", 
            "title": "Retrieve Data for Region"
        }, 
        {
            "location": "/connectors/gemfire_connector/#configuring-the-snappydata-cluster-for-gemfire-connector_1", 
            "text": "The following configurations can be set in SnappyData Cluster for GemFire Connector:   Customize DistributedSystem in the cluster with additional attributes  Specify Static Locators  Dynamic Discovery of Grid for a Region", 
            "title": "Configuring the SnappyData Cluster for GemFire Connector"
        }, 
        {
            "location": "/connectors/gemfire_connector/#customizing-the-distributedsystem-of-snappydata-cluster-at-startup", 
            "text": "You can configure the  DistributedSystem  with additional attributes by implementing the following interface:  package io.snappydata.spark.gemfire.connector.dsinit;\n/**\n * @param @link{DSConfig} instance which can be used to configure the DistributedSystem properties at the\n * time of snappydata startup\n */\npublic interface DistributedSystemInitializer {\n\n  public void configure(DSConfig config);\n}  Any required properties of the  DistributedSystem  can be configured via the  DSConfig  instance passed.\nThe  DistributedSystemInitializer  implementation needs to be packaged in a jar which is loaded by SnappyData using ServiceLoader API of java. The jar should include the following file along with the path as described:  META-INF/services/io.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer  This file should contain the fully qualified class name of the custom DistributedSystemInitializer   Note  The name of the file should be  io.snappydata.spark.gemfire.connector.dsinit.DistributedSystemInitializer   The connector interacts with the GemFire cluster. Therefore, you should configure the SnappyData cluster with the details of GemFire cluster. The configuration details must be provided in both the SnappyData lead and server nodes at the following locations:\n-    SnappyData-Home directory/conf/leads \n-    SnappyData-Home directory/conf/servers   Modify the servers and leads configuration file to add the connector jar ( connector-1.0.0.jar ) and the  person.jar  in the classpath as well as to configure the remote GemFire cluster locator.", 
            "title": "Customizing the DistributedSystem of SnappyData Cluster at Startup"
        }, 
        {
            "location": "/connectors/gemfire_connector/#specifying-static-locators", 
            "text": "To statically specify the running locators of the GemFire cluster, set the property as follows, where  uniqueIDForGrid  is any unique identifier key:      -snappydata.connector.gemfire-grid.\\ uniqueIDForGrid\\ =localhost[55221]  Following is a sample from the servers and leads file:  localhost -locators=localhost:10334 -client-bind-address=localhost -client-port=1528 -heap-size=20g\n-classpath= SnappyData-Product-Home //connectors/connector_2.11-0.9.2.1.jar: path-to-jar /persons.jar\n-snappydata.connector.gemfire-grid. uniqueIDForGrid =localhost[55221]", 
            "title": "Specifying Static Locators"
        }, 
        {
            "location": "/connectors/gemfire_connector/#dynamically-discovering-grid-for-a-region", 
            "text": "Instead of specifying the locators via the property, it is possible to provide custom logic for discovery of the grid for the region by implementing  GridResolver  trait as follows:  trait GridResolver {\n\n  /**\n    * Optional method to identify the locators and delegate to SnappyData for creating connection \n    * pool using [[GridHelper]]. Invoked once in the lifecycle.\n    * @param gfeGridProps\n    * @param gridHelper\n    */\n  def initConnections(gfeGridProps: java.util.Map[String, String], gridHelper: GridHelper): Unit\n\n  /**\n    * Resolve the grid for the region or return null if unable to do so\n    * @param regionName\n    * @param gfeGridProps\n    * @param gridHelper\n    * @param userOptions\n    * @return the grid name ( the name of the connection pool) to use\n    */\n  def getGridForRegion(regionName: String, gfeGridProps: java.util.Map[String, String],\n      gridHelper: GridHelper, userOptions: java.util.Map[String, String]): String\n\n  /**\n    *\n    * @return a String identifying the resolver uniquely\n    */\n  def getUniqueID: String\n}  SnappyData attempts to resolve the grid for the region using existing  ConnectionPools . If it is not successful, it checks with the available resolvers by invoking  getGridForRegion . The resolver returns null, if it cannot resolve the grid.  The  GridResolver  implementation should be packaged in a jar which is loaded by SnappyData using  ServiceLoader  API of java. The jar should also include the following file along with the described path: META-INF/services/io.snappydata.spark.gemfire.connector.grids.GridResolver \nThis file should contain the fully qualified class name of the custom  GridResolver .   Note  The name of the file should be  io.snappydata.spark.gemfire.connector.grids.GridResolver .   To initialize the GemFire connector and enable its functions in SnappyData, you must include the following import statement before creating the external table:  import io.snappydata.spark.gemfire.connector", 
            "title": "Dynamically Discovering Grid for a Region"
        }, 
        {
            "location": "/connectors/gemfire_connector/#accessing-data-from-gemfire", 
            "text": "In SnappyData applications, you can create external tables that represent GemFire regions and run SQL queries against GemFire. For accesing data from GemFire, you must first expose the GemFire regions:   You can use any the following options to expose GemFire regions:   Expose GemFire PDX Regions as External Tables  Expose Regions Containing POJOs as External Tables  Expose GemFire Region Using Dataframe based External Tables  Expose GemFire Regions as RDDs", 
            "title": "Accessing Data from GemFire"
        }, 
        {
            "location": "/connectors/gemfire_connector/#exposing-gemfire-pdx-regions-as-external-tables", 
            "text": "You can create an external table that represents a GemFire region which stores PDX instances. The SnappyData schema for this external table is derived from the PDXType. Here, the GemFire region is already populated with data and SnappyData infers the schema based on the inspection of the PDX types.\nThe following syntax creates an external table that represents a GemFire region which stores PDX instances.  val externalBsegTable = snc.createExternalTable( bsegInGem , \n      gemfire ,\n     Map[String, String]( region.path  -   bseg1 ,  data.as.pdx  -   true ))  The SnappyData external table schema for the GemFire region can optionally include the GemFire region key as a column in the table. To enable this, the  key.class   attribute should be set when you create the table as shown in the following example:  val externalBsegTable = snc.createExternalTable( bsegInGem ,\n    gemfire ,\n   Map[String, String]( region.path  -   bseg1 ,  data.as.pdx  -   true ,\n    key.class  -   java.lang.Long      \n     ))", 
            "title": "Exposing GemFire PDX Regions as External Tables"
        }, 
        {
            "location": "/connectors/gemfire_connector/#exposing-regions-containing-pojos-as-external-tables", 
            "text": "In the following example, an external table is created using the getter methods on POJOs as SnappyData column names:  snc.createExternalTable(externalPersonsTable1,  gemfire , \nMap[String, String]( region.path  -  personsRegionName,  value.class  -   load.Person ))  As in the previous case, if the GemFire key field has to be included as a column, then the  Key.class  attribute has to be passed in as an option.  snc.createExternalTable(externalPersonsTable1,  gemfire , Map[String, String]( region.path  -  personsRegionName,  value.class  -   load.Person ),  key.class  -   java.lang.Long ))", 
            "title": "Exposing Regions Containing POJOs as External Tables"
        }, 
        {
            "location": "/connectors/gemfire_connector/#expose-gemfire-region-using-dataframe-based-external-tables", 
            "text": "In the following example, a DataFrame is used to create an external table  bsegTable , with the schema which is same as that of DataFrame  bsegDF . The primary key column name should be specified for this to work correctly. In the following example, it is assumed that the dataframe contains a column named \"id1\".  bsegDF.write.format( gemfire ).  \n      option( region.path ,  bseg1 ).\n      option( primary.key.column.name ,  id1 ).\n      option( pdx.identity.fields ,  id1 ).\n      saveAsTable( bsegTable )   Note  The  pdx.identity.fields  specification has a huge impact on performance within GemFire since this specification informs GemFire to use only the specified field for computing hashCode and equality for PdxInstance.", 
            "title": "Expose GemFire Region Using Dataframe Based External Tables"
        }, 
        {
            "location": "/connectors/gemfire_connector/#expose-gemfire-regions-as-rdds", 
            "text": "Invoking the  gemfireRegion  method on the SparkContext in SnappyData exposes the full data set of a GemFire region as a Spark RDD.\nThe same API exposes both replicated and partitioned region as RDDs.  scala  val rdd = sc.gemfireRegion[String, String]( gemTable1 )\n\nscala  rdd.foreach(println)\n(1,one)\n(3,three)\n(2,two)   Note  when the RDD is used, it is important to specify the correct type for both the region key and value, otherwise a  ClassCastException  is encountered.", 
            "title": "Expose GemFire Regions as RDDs"
        }, 
        {
            "location": "/connectors/gemfire_connector/#controlling-the-schema-of-the-external-table", 
            "text": "The pdx fields (of pdx instances) or the getter methods (of the POJOs) define the schema of the external table by default. However, you can also control the schema by excluding the columns as per requirement. This is done by implementing the following trait:  package io.snappydata.spark.gemfire.connector.lifecycle\nimport org.apache.spark.sql.types.StructType\n\ntrait ColumnSelector {\n  def selectColumns(gridName: Option[String], regionName: String,\n      allColumns: StructType): StructType\n}  Based on the requirement, a new StructType which contains fewer columns can be returned.    Note  The new StructType returned can only have the subset of the columns that are passed. No new columns should be added.   The  ColumnSelector  implemetation needs to be added to the startup classpath of the SnappyDataCluster.\nAt the time of table creation, an option with key =  column.selector  and value as the fully qualified class name of the ColumnSelector implementation class should be passed.", 
            "title": "Controlling the Schema of the External Table"
        }, 
        {
            "location": "/connectors/gemfire_connector/#other-optional-configuration-parameters", 
            "text": "column  column      max.rows.restrict  This parameter is used to restrict the number of rows that are fetched from an external table, when a query of type  select * from external_table  is executed. This restriction, if specified,  is applied to queries without any filter, limit, aggregate function, and projection such that it tends to bring all the rows from the external region. Note that this restriction is not applied in case of DDL statement such as  create table X using row column as select * from external_table . The default value for is 10,000.    column.selector  This parameter is used to control the columns that must be included in the schema as described  here .    max.buckets.per.partition  This parameter is used to control the concurrency and number of tasks created to fetch data from an external region. This property is useful only for Partitioned Region. For more details, refer to  Controlling Task Concurrency in SnappyData When Accessing GemFire .  The default value is 3.    security.username   and  security.password  By default the external table is created and queried using the credential of the current user. For example user X creates external table, and if user Y is querying, then credentials of user Y is fetched from data. But if the parameters  security.username   and  security.password  are passed then those credentials are used to create external table and for querying the data irrespective of the current user.    pdxtype.maxscan  This parameter determines the maximum number of entries from the region which is scanned to completely determine the schema of the external table from the Pdx instances that is stored in the region. The default value is 100.", 
            "title": "Other Optional Configuration Parameters"
        }, 
        {
            "location": "/connectors/gemfire_connector/#controlling-task-concurrency-in-snappydata-when-accessing-gemfire", 
            "text": "There are two types of regions in GemFire;  replicated  and  partitioned . All the data for a replicated region is present on every server where the region is defined. A GemFire partitioned region splits the data across the servers that define the partitioned region.   When operating with replicated regions, there is only a single RDD partition representing the replicated region in SnappyData. Since there is only one data bucket for the replicated region. As compared to this, a GemFire partitioned region can be represented by a configurable number of RDD partitions in SnappyData.   Choosing the number of RDD partitions directly controls the task concurrency in SnappyData when you run queries on GemFire regions. By default, the GemFire connector works out the number of buckets per GemFire server, assigns partitions to each server, and uses a default value of maximum three buckets per partition. You can configure the  max.buckets.per.partition  attribute to change this value.   When queries are executed on an external table, the degree of parallelism in query execution is directly proportional to the number of RDD partitions that represents the table.  The following example shows how to configure the RDD partitions count for an external table representing a GemFire region:  import io.snappydata.spark.gemfire.connector._\n\nval externalBsegTable = snc.createExternalTable( bsegInGem ,\n    gemfire ,\n   Map[String, String]( region.path  -   bseg1 ,  data.as.pdx  -   true ,\n    key.class  -   java.lang.Long  ,  max.buckets.per.partition  -   5     \n     ))", 
            "title": "Controlling Task Concurrency in SnappyData When Accessing GemFire"
        }, 
        {
            "location": "/connectors/gemfire_connector/#saving-data-to-gemfire-region", 
            "text": "You can save data to GemFire Regions using any of the following :   Saving Pair RDD to GemFire Region  Saving Non-Pair RDD to GemFire  Saving DataFrame to GemFire", 
            "title": "Saving Data to GemFire Region"
        }, 
        {
            "location": "/connectors/gemfire_connector/#saving-pair-rdd-to-gemfire-region", 
            "text": "A pair RDD can be saved from SnappyData into a GemFire region as follows:     Import the implicits as shown:  import io.snappydata.spark.gemfire.connector    In the Spark shell, create a simple pair RDD and save it to GemFire Region:  scala  import io.snappydata.spark.gemfire.connector._\nscala  val data = Array((\"1\", \"one\"), (\"2\", \"two\"), (\"3\", \"three\"))\ndata: Array[(String, String)] = Array((1,one), (2,two), (3,three))\n\nscala  val distData = sc.parallelize(data)\ndistData: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at  console :14\n\nscala  distData.saveToGemFire(\"gemTable1\")\n15/02/17 07:11:54 INFO DAGScheduler: Job 0 finished: runJob at GemFireRDDFunctions.scala:29, took 0.341288 s    Verify the data is saved in GemFire using  gfsh :  gfsh query --query=\"select key,value from /gemTable1.entries\"\n\nResult     : true\nstartCount : 0\nendCount   : 20\nRows       : 3\n\nkey | value\n--- | -----\n1   | one\n3   | three\n2   | two", 
            "title": "Saving Pair RDD to GemFire Region"
        }, 
        {
            "location": "/connectors/gemfire_connector/#saving-non-pair-rdd-to-gemfire", 
            "text": "Saving a non-pair RDD to GemFire requires an extra function that converts each element of RDD to a key-value pair. \nHere's a sample session in Spark shell:  scala  import io.snappydata.spark.gemfire.connector._\nscala  val data2 = Array( a , ab , abc )\ndata2: Array[String] = Array(a, ab, abc)\n\nscala  val distData2 = sc.parallelize(data2)\ndistData2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at  console :17\n\nscala  distData2.saveToGemFire( gemTable1 , e =  (e.length, e))\n[info 2015/02/17 12:43:21.174 PST  main  tid=0x1]\n...\n15/02/17 12:43:21 INFO DAGScheduler: Job 0 finished: runJob at GemFireRDDFunctions.scala:52, took 0.251194 s  Verify the result with  gfsh :  gfsh query --query= select key,value from /gemTable1.entrySet \n\nResult     : true\nstartCount : 0\nendCount   : 20\nRows       : 3\n\nkey | value\n--- | -----\n2   | ab\n3   | abc\n1   | a", 
            "title": "Saving Non-Pair RDD to GemFire"
        }, 
        {
            "location": "/connectors/gemfire_connector/#saving-a-dataframe-to-gemfire", 
            "text": "To save a DataFrame, that is dataSet of row objects, into GemFire, use the following API which is available as an implicit definition. The rows of the dataframes are converted into PDX instances for storage in the GemFire's region.  In the following example, it is assumed that there is a column \"id1\" present in the dataframe's schema. To specify the PDX Identity Fields for the PDX Type, use the option as (\"pdx.identity.fields\", \"Col1, Col2, Col3\") to specify one or more columns to be used as PDX Identity fields. \nSnappyData recommends to define the identity fields for performance during comparison of PDX Instances.  import io.snappydata.spark.gemfire.connector._\n\ndf.write.format( gemfire ).\n          option( region.path , /region1 ).\n          option( primary.key.column.name ,  id1 ).\n          option( pdx.identity.fields ,  id1 )\n          .save()  To dynamically generate the GemFire Region's key, import the implicits and use the following API:  saveToGemFire[K](regionPath: String, keyExtractor: Row =  K, opConf: Map[String, String] = Map.empty )  import io.snappydata.spark.gemfire.connector._\n\ndf.saveToGemFire[String]( /region1 , \n(row: Row) =  ( row.getString(1) +  _  + row.getString(10)),\nMap[String, String]( pdx.identity.fields  -   id1, id10 )\n)", 
            "title": "Saving a DataFrame to GemFire"
        }, 
        {
            "location": "/connectors/gemfire_connector/#running-oql-queries-directly-on-gemfire-from-snappydata", 
            "text": "Most applications using SnappyData will choose to run regular SQL queries on GemFire regions. Refer to  Accessing Data From GemFire  \nAdditionally, you can directly execute OQL queries on GemFire regions using the GemFire connector. In scenarios where the data stored in GemFire regions is neither PDX nor Java bean compliant POJO, you can execute OQL queries and retrieve the data from the server and make it available as a data frame.  An instance of  SQLContext  is required to run OQL query.  val snc = new org.apache.spark.sql.SnappyContext(sc)  Create a  DataFrame  using OQL:  val dataFrame = snc.gemfireOQL( SELECT iter.name,itername.address.city, iter.id FROM /personRegion iter )  You can repartition the  DataFrame  using  DataFrame.repartition()  if required. \nAfter you have the  DataFrame , you can register it as a table and use Spark \nSQL to query:  dataFrame.registerTempTable( person )\nval SQLResult = sqlContext.sql( SELECT * FROM person WHERE id   100 )", 
            "title": "Running OQL queries Directly on GemFire from SnappyData"
        }, 
        {
            "location": "/connectors/cdc_connector/", 
            "text": "Using the SnappyData Change Data Capture (CDC) Connector\n\n\nThis feature is available only in the Enterprise version of SnappyData.\n\n\nAs data keeps growing rapidly techniques like Change Data Capture (CDC) is crucial for handling and processing the data inflow.\nThe CDC technology is used in a database to track changed data so that the identified changes can be used to keep target systems in sync with the changes made to the source systems.\n\n\nA CDC enabled system (SQL database) automatically captures changes from the source table, these changes are then updated on the target system (SnappyData tables).\n\nIt provides an efficient framework which allows users to capture \nindividual data changes\n like insert, update, and delete in the source tables (instead of dealing with the entire data), and apply them to the SnappyData tables to keep both the source and target tables in sync.\n\n\n\n\nInfo\n\n\nSpark structured streaming and SnappyData mutable APIs are used to keep the source and target tables in sync. For writing a Spark structured streaming application, refer to the Spark documentation.\n\n\n\n\n\n\nCDC is supported on both the Smart Connector Mode and the Embedded mode. For more  information on the modes, refer to the documentation on \nSmart Connector Mode\n and \nEmbedded SnappyData Store Mode\n.\n\n\nIn this topic, we explain how SnappyData uses the JDBC streaming connector to pull changed data from the SQL database and ingest it into SnappyData tables.\n\n\nThe following image illustrates the data flow for change data capture:\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nEnsure that change data capture is enabled for the database and table. Refer to your database documentation for more information on enabling CDC.\n\n\n\n\n\n\nA user account with the required roles and privileges to the database.\n\n\n\n\n\n\nEnsure that a JDBC source to which SnappyData CDC Connector can connect is running and available from the node where CDC connector is running.\n\n\n\n\n\n\nThe \nsnappydata-jdbc-stream-connector_\n.jar\n, which is available in the \n$SNAPPY_HOME/jars\n directory. \nIf you are using Maven or Gradle project to develop the streaming application, you need to publish the above jar into a local maven repository.\n\n\n\n\n\n\nUnderstanding the Program Structure\n\n\nThe RDB CDC connector ingests data into SnappyData from any CDC enabled JDBC source. We have a custom source with alias \u201cjdbcStream\u201d and a custom Sink with alias \u201csnappystore\u201d. \n\n\nSource\n has the capability to read from a JDBC source and \nSink\n can perform inserts, updates or deletes based on CDC operations.\n\n\nConfiguring the Stream Reader\n\n\nStructured streaming and Spark\u2019s JDBC source is used to read from the source database system.\n\n\nTo enable this, set a stream referring to the source table.\n\n\nFor example:\n\n\n   DataStreamReader reader = snappySession.readStream()\n      .format(\"jdbcStream\")\n      .option(\"spec\", \"org.apache.spark.sql.streaming.jdbc.SqlServerSpec\")\n      .option(\"sourceTableName\", sourceTable)\n      .option(\"maxEvents\", \"50000\")\n      .options(connectionOptions);\n\n\n\nThe JDBC Stream Reader options are:\n\n\n\n\n\n\njdbcStream\n: The format in which the source data is received from the JDBC source. This parameter is mandatory. Currently, only JDBC stream is supported.\n\n\n\n\n\n\nspec\n: A CDC spec class name which is used to query offsets from different databases. We have a default specification for CDC enabled SQL server. You can extend the \norg.apache.spark.sql.streaming.jdbc.SourceSpec\n trait to provide any other implementation.\n\n\ntrait SourceSpec {\n\n  /** A fixed offset column name in the source table. */\n  def offsetColumn: String\n\n  /** An optional function to convert offset\n    * to string format, if offset is not in string format.\n    */\n  def offsetToStrFunc: Option[String]\n\n  /** An optional function to convert string offset to native offset format. */\n  def strToOffsetFunc: Option[String]\n\n  /** Query to find the next offset */\n  def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String\n}\n\n\n\nThe default implementation of SQLServerSpec can be:\n\n\nclass SqlServerSpec extends SourceSpec {\n\n  override def offsetColumn: String = \"__$start_lsn\"\n\n  override def offsetToStrFunc: Option[String] = Some(\"master.dbo.fn_varbintohexstr\")\n\n  override def strToOffsetFunc: Option[String] = Some(\"master.dbo.fn_cdc_hexstrtobin\")\n\n  /** make sure to convert the LSN to string and give a column alias for the\n    * user defined query one is supplying. Alternatively, a procedure can be invoked\n    * with $table, $currentOffset Snappy provided variables returning one string column\n    * representing the next LSN string.\n    */\n  override def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String =\n    s\"\"\"select master.dbo.fn_varbintohexstr(max(${offsetColumn})) nextLSN\n         from (select ${offsetColumn}, sum(count(1)) over\n         (order by ${offsetColumn}) runningCount\n        from $tableName where ${offsetColumn} \n master.dbo.fn_cdc_hexstrtobin('$currentOffset')\n          group by ${offsetColumn}) x where runningCount \n= $maxEvents\"\"\"\n}\n\n\n\n\n\n\n\nsourceTableName\n: The source table from which the data is read. For example, \ntestdatabase.cdc.dbo_customer_CT\n.\n\n\n\n\n\n\nmaxEvents\n: Number of rows to be read in one batch.\n\n\n\n\n\n\nconnectionOptions\n: A map of key values containing different parameters to access the source database.\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ndriver\n\n\nThe JDBC driver to be used to access source database. E.g. \ncom.microsoft.sqlserver.jdbc.SQLServerDriver\n\n\n\n\n\n\nurl\n\n\nThe JDBC connection url\n\n\n\n\n\n\nuser\n\n\nUser name to access the database\n\n\n\n\n\n\npassword\n\n\nPassword to access the database\n\n\n\n\n\n\ndatabaseName\n\n\nThe name of the database\n\n\n\n\n\n\npoolImpl\n\n\nConnection pool implementation(default Tomcat). Currently, only Tomcat and Hikari are supported\n\n\n\n\n\n\npoolProperties\n\n\nComma separated pool specific properties. For more details see Tomcat or Hikari JDBC connection pool documents. Example: connectionTestQuery=select 1,minimumIdle=5\n\n\n\n\n\n\n\n\n\n\n\n\nOptionally, you can use any Spark API to transform your data obtained from the stream reader.\n\n\nThe sample usage can be as follows:\n\n\nDataset\nRow\n ds = reader.load();\nds.filter(\nfilter_condition\n)\n\n\n\n\nWriting into SnappyData tables\n\n\nTo write into SnappyData tables you need to have a StreamWriter as follows:\n\n\nds.writeStream()\n        .trigger(ProcessingTime.create(10, TimeUnit.SECONDS))\n        .format(\nsnappystore\n)\n        .option(\nsink\n, ProcessEvents.class.getName())\n        .option(\ntableName\n, tableName)\n        .start();\n\n\n\n\nHere, the value of the \n.format\n parameter is always \nsnappystore\n and is a mandatory.\n\n\nSnappyData Stream Writer options\n\n\nThe \nsink\n option is mandatory for SnappyStore sink. This option is required to give the user control of the obtained data frame. When writing streaming data to the tables, you can also provide any custom option.\n\n\n\n\n\n\n\n\nKey\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nsink\n\n\nA user-defined callback class which gets a data frame in each batch. The class must implement \norg.apache.spark.sql.streaming.jdbc.SnappyStreamSink\n interface.\n\n\n\n\n\n\n\n\norg.apache.spark.sql.streaming.jdbc.SnappyStreamSink\n\n\nThe above trait contains a single method, which user needs to implement. A user can use SnappyData mutable APIs (INSERT, UPDATE, DELETE, PUT INTO) to maintain tables.\n\n\n    def process(snappySession: SnappySession, sinkProps: Properties,\n        batchId: Long, df: Dataset[Row]): Unit\n\n\n\n\nThe following examples illustrates how you can write into a \nSnappyData table\n:\n\n\npackage io.snappydata.app;\n\nimport java.util.List;\nimport java.util.Properties;\n\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SnappySession;\nimport org.apache.spark.sql.streaming.jdbc.SnappyStreamSink;\n\nimport static java.util.Arrays.asList;\nimport static org.apache.spark.SnappyJavaUtils.snappyJavaUtil;\n\npublic class ProcessEvents implements SnappyStreamSink {\n\n  private static Logger log = Logger.getLogger(ProcessEvents.class.getName());\n\n  private static List\nString\n metaColumns = asList(\n__$start_lsn\n,\n      \n__$end_lsn\n, \n__$seqval\n, \n__$operation\n, \n__$update_mask\n, \n__$command_id\n);\n\n  @Override\n  public void process(SnappySession snappySession, Properties sinkProps,\n      long batchId, Dataset\nRow\n df) {\n\n    String snappyTable = sinkProps.getProperty(\ntablename\n).toUpperCase();\n\n    log.info(\nSB: Processing for \n + snappyTable + \n batchId \n + batchId);\n\n    df.cache();\n\n    Dataset\nRow\n snappyCustomerDelete = df\n        // pick only delete ops\n        .filter(\n\\\n__$operation\\\n = 1\n)\n        // exclude the first 5 columns and pick the columns that needs to control\n        // the WHERE clause of the delete operation.\n        .drop(metaColumns.toArray(new String[metaColumns.size()]));\n\n    if(snappyCustomerDelete.count() \n 0) {\n      snappyJavaUtil(snappyCustomerDelete.write()).deleteFrom(\nAPP.\n + snappyTable);\n    }\n\n    Dataset\nRow\n snappyCustomerUpsert = df\n        // pick only insert/update ops\n        .filter(\n\\\n__$operation\\\n = 4 OR \\\n__$operation\\\n = 2\n)\n        .drop(metaColumns.toArray(new String[metaColumns.size()]));\n    snappyJavaUtil(snappyCustomerUpsert.write()).putInto(\nAPP.\n + snappyTable);\n\n  }\n}\n\n\n\n\nAdditional Information\n\n\n\n\n\n\nOffset Management\n: \n\n    SnappyData keeps a persistent table for offset management (Offset table name for application = SNAPPY_JDBC_STREAM_OFFSET_TABLE). \n\n    The schema of the table is:\n\n\n\n\n\n\n\n\nAPP_NAME\n\n\nTABLE_NAME\n\n\nLAST_OFFSET\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitially, the table contains no rows. The connector inserts a row for the table with minimum offset queried from the database. On subsequent intervals, it consults this table to get the offset number. \n\nIf connector application crashes, it refers to this table on a restart to query further.\n\n\n\n\n\n\nIdempotency for Streaming application\n: \n\n    If \nDELETE\n and \nPUT INTO\n APIs are used, SnappyData ensures idempotent behavior. This is useful when the application restarts after a crash and some of the CDC events are replayed. The PUT INTO API either inserts a record (if not present) or updates the record (if already exists).\n The existence of the record is checked based on the key columns defined when a table is created. As primary keys are not supported for column tables, you can use \nkey_columns\n instead, to uniquely identify each row/record in a database table. \n For more information, see \nCREATE TABLE\n.\n\n\nFor example:\n\n\nCREATE TABLE \ntable_name\n USING column OPTIONS(partition_by '\ncolumn_name\n', buckets '\nnum_partitions\n',key_columns '\nprimary_key\n') AS (SELECT * FROM from external table);\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nWriting data from different tables to a single table is currently not supported as the schema for incoming data frame cannot be changed. \n\n\n\n\n\n\nFor every source table that needs to be tracked for changes, ensure that there is a corresponding destination table in SnappyData.", 
            "title": "Using the SnappyData CDC (Change Data Capture) Connector"
        }, 
        {
            "location": "/connectors/cdc_connector/#using-the-snappydata-change-data-capture-cdc-connector", 
            "text": "This feature is available only in the Enterprise version of SnappyData.  As data keeps growing rapidly techniques like Change Data Capture (CDC) is crucial for handling and processing the data inflow.\nThe CDC technology is used in a database to track changed data so that the identified changes can be used to keep target systems in sync with the changes made to the source systems.  A CDC enabled system (SQL database) automatically captures changes from the source table, these changes are then updated on the target system (SnappyData tables). \nIt provides an efficient framework which allows users to capture  individual data changes  like insert, update, and delete in the source tables (instead of dealing with the entire data), and apply them to the SnappyData tables to keep both the source and target tables in sync.   Info  Spark structured streaming and SnappyData mutable APIs are used to keep the source and target tables in sync. For writing a Spark structured streaming application, refer to the Spark documentation.    CDC is supported on both the Smart Connector Mode and the Embedded mode. For more  information on the modes, refer to the documentation on  Smart Connector Mode  and  Embedded SnappyData Store Mode .  In this topic, we explain how SnappyData uses the JDBC streaming connector to pull changed data from the SQL database and ingest it into SnappyData tables.  The following image illustrates the data flow for change data capture:", 
            "title": "Using the SnappyData Change Data Capture (CDC) Connector"
        }, 
        {
            "location": "/connectors/cdc_connector/#prerequisites", 
            "text": "Ensure that change data capture is enabled for the database and table. Refer to your database documentation for more information on enabling CDC.    A user account with the required roles and privileges to the database.    Ensure that a JDBC source to which SnappyData CDC Connector can connect is running and available from the node where CDC connector is running.    The  snappydata-jdbc-stream-connector_ .jar , which is available in the  $SNAPPY_HOME/jars  directory.  If you are using Maven or Gradle project to develop the streaming application, you need to publish the above jar into a local maven repository.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/connectors/cdc_connector/#understanding-the-program-structure", 
            "text": "The RDB CDC connector ingests data into SnappyData from any CDC enabled JDBC source. We have a custom source with alias \u201cjdbcStream\u201d and a custom Sink with alias \u201csnappystore\u201d.   Source  has the capability to read from a JDBC source and  Sink  can perform inserts, updates or deletes based on CDC operations.", 
            "title": "Understanding the Program Structure"
        }, 
        {
            "location": "/connectors/cdc_connector/#configuring-the-stream-reader", 
            "text": "Structured streaming and Spark\u2019s JDBC source is used to read from the source database system.  To enable this, set a stream referring to the source table.  For example:     DataStreamReader reader = snappySession.readStream()\n      .format(\"jdbcStream\")\n      .option(\"spec\", \"org.apache.spark.sql.streaming.jdbc.SqlServerSpec\")\n      .option(\"sourceTableName\", sourceTable)\n      .option(\"maxEvents\", \"50000\")\n      .options(connectionOptions);  The JDBC Stream Reader options are:    jdbcStream : The format in which the source data is received from the JDBC source. This parameter is mandatory. Currently, only JDBC stream is supported.    spec : A CDC spec class name which is used to query offsets from different databases. We have a default specification for CDC enabled SQL server. You can extend the  org.apache.spark.sql.streaming.jdbc.SourceSpec  trait to provide any other implementation.  trait SourceSpec {\n\n  /** A fixed offset column name in the source table. */\n  def offsetColumn: String\n\n  /** An optional function to convert offset\n    * to string format, if offset is not in string format.\n    */\n  def offsetToStrFunc: Option[String]\n\n  /** An optional function to convert string offset to native offset format. */\n  def strToOffsetFunc: Option[String]\n\n  /** Query to find the next offset */\n  def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String\n}  The default implementation of SQLServerSpec can be:  class SqlServerSpec extends SourceSpec {\n\n  override def offsetColumn: String = \"__$start_lsn\"\n\n  override def offsetToStrFunc: Option[String] = Some(\"master.dbo.fn_varbintohexstr\")\n\n  override def strToOffsetFunc: Option[String] = Some(\"master.dbo.fn_cdc_hexstrtobin\")\n\n  /** make sure to convert the LSN to string and give a column alias for the\n    * user defined query one is supplying. Alternatively, a procedure can be invoked\n    * with $table, $currentOffset Snappy provided variables returning one string column\n    * representing the next LSN string.\n    */\n  override def getNextOffset(tableName : String, currentOffset : String, maxEvents : Int): String =\n    s\"\"\"select master.dbo.fn_varbintohexstr(max(${offsetColumn})) nextLSN\n         from (select ${offsetColumn}, sum(count(1)) over\n         (order by ${offsetColumn}) runningCount\n        from $tableName where ${offsetColumn}   master.dbo.fn_cdc_hexstrtobin('$currentOffset')\n          group by ${offsetColumn}) x where runningCount  = $maxEvents\"\"\"\n}    sourceTableName : The source table from which the data is read. For example,  testdatabase.cdc.dbo_customer_CT .    maxEvents : Number of rows to be read in one batch.    connectionOptions : A map of key values containing different parameters to access the source database.     Key  Value      driver  The JDBC driver to be used to access source database. E.g.  com.microsoft.sqlserver.jdbc.SQLServerDriver    url  The JDBC connection url    user  User name to access the database    password  Password to access the database    databaseName  The name of the database    poolImpl  Connection pool implementation(default Tomcat). Currently, only Tomcat and Hikari are supported    poolProperties  Comma separated pool specific properties. For more details see Tomcat or Hikari JDBC connection pool documents. Example: connectionTestQuery=select 1,minimumIdle=5       Optionally, you can use any Spark API to transform your data obtained from the stream reader.  The sample usage can be as follows:  Dataset Row  ds = reader.load();\nds.filter( filter_condition )", 
            "title": "Configuring the Stream Reader"
        }, 
        {
            "location": "/connectors/cdc_connector/#writing-into-snappydata-tables", 
            "text": "To write into SnappyData tables you need to have a StreamWriter as follows:  ds.writeStream()\n        .trigger(ProcessingTime.create(10, TimeUnit.SECONDS))\n        .format( snappystore )\n        .option( sink , ProcessEvents.class.getName())\n        .option( tableName , tableName)\n        .start();  Here, the value of the  .format  parameter is always  snappystore  and is a mandatory.", 
            "title": "Writing into SnappyData tables"
        }, 
        {
            "location": "/connectors/cdc_connector/#snappydata-stream-writer-options", 
            "text": "The  sink  option is mandatory for SnappyStore sink. This option is required to give the user control of the obtained data frame. When writing streaming data to the tables, you can also provide any custom option.     Key  Value      sink  A user-defined callback class which gets a data frame in each batch. The class must implement  org.apache.spark.sql.streaming.jdbc.SnappyStreamSink  interface.     org.apache.spark.sql.streaming.jdbc.SnappyStreamSink  The above trait contains a single method, which user needs to implement. A user can use SnappyData mutable APIs (INSERT, UPDATE, DELETE, PUT INTO) to maintain tables.      def process(snappySession: SnappySession, sinkProps: Properties,\n        batchId: Long, df: Dataset[Row]): Unit  The following examples illustrates how you can write into a  SnappyData table :  package io.snappydata.app;\n\nimport java.util.List;\nimport java.util.Properties;\n\nimport org.apache.log4j.Logger;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SnappySession;\nimport org.apache.spark.sql.streaming.jdbc.SnappyStreamSink;\n\nimport static java.util.Arrays.asList;\nimport static org.apache.spark.SnappyJavaUtils.snappyJavaUtil;\n\npublic class ProcessEvents implements SnappyStreamSink {\n\n  private static Logger log = Logger.getLogger(ProcessEvents.class.getName());\n\n  private static List String  metaColumns = asList( __$start_lsn ,\n       __$end_lsn ,  __$seqval ,  __$operation ,  __$update_mask ,  __$command_id );\n\n  @Override\n  public void process(SnappySession snappySession, Properties sinkProps,\n      long batchId, Dataset Row  df) {\n\n    String snappyTable = sinkProps.getProperty( tablename ).toUpperCase();\n\n    log.info( SB: Processing for   + snappyTable +   batchId   + batchId);\n\n    df.cache();\n\n    Dataset Row  snappyCustomerDelete = df\n        // pick only delete ops\n        .filter( \\ __$operation\\  = 1 )\n        // exclude the first 5 columns and pick the columns that needs to control\n        // the WHERE clause of the delete operation.\n        .drop(metaColumns.toArray(new String[metaColumns.size()]));\n\n    if(snappyCustomerDelete.count()   0) {\n      snappyJavaUtil(snappyCustomerDelete.write()).deleteFrom( APP.  + snappyTable);\n    }\n\n    Dataset Row  snappyCustomerUpsert = df\n        // pick only insert/update ops\n        .filter( \\ __$operation\\  = 4 OR \\ __$operation\\  = 2 )\n        .drop(metaColumns.toArray(new String[metaColumns.size()]));\n    snappyJavaUtil(snappyCustomerUpsert.write()).putInto( APP.  + snappyTable);\n\n  }\n}", 
            "title": "SnappyData Stream Writer options"
        }, 
        {
            "location": "/connectors/cdc_connector/#additional-information", 
            "text": "Offset Management :  \n    SnappyData keeps a persistent table for offset management (Offset table name for application = SNAPPY_JDBC_STREAM_OFFSET_TABLE).  \n    The schema of the table is:     APP_NAME  TABLE_NAME  LAST_OFFSET            Initially, the table contains no rows. The connector inserts a row for the table with minimum offset queried from the database. On subsequent intervals, it consults this table to get the offset number.  \nIf connector application crashes, it refers to this table on a restart to query further.    Idempotency for Streaming application :  \n    If  DELETE  and  PUT INTO  APIs are used, SnappyData ensures idempotent behavior. This is useful when the application restarts after a crash and some of the CDC events are replayed. The PUT INTO API either inserts a record (if not present) or updates the record (if already exists).  The existence of the record is checked based on the key columns defined when a table is created. As primary keys are not supported for column tables, you can use  key_columns  instead, to uniquely identify each row/record in a database table.   For more information, see  CREATE TABLE .  For example:  CREATE TABLE  table_name  USING column OPTIONS(partition_by ' column_name ', buckets ' num_partitions ',key_columns ' primary_key ') AS (SELECT * FROM from external table);     Note    Writing data from different tables to a single table is currently not supported as the schema for incoming data frame cannot be changed.     For every source table that needs to be tracked for changes, ensure that there is a corresponding destination table in SnappyData.", 
            "title": "Additional Information"
        }, 
        {
            "location": "/monitoring/managing_and_monitoring/", 
            "text": "Managing and Monitoring\n\n\nManaging and Monitoring SnappyData describes how to use log files, system tables, and statistical data to understand the behavior of SnappyData deployment.\n\n\nThis guide also provides general guidelines for tuning the performance of SnappyData members, applications, and individual queries.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nSnappyData Pulse\n\n\n\n\n\n\nConfiguring High Availability for a Partitioned Table\n\n\n\n\n\n\nConfiguring Logging\n\n\n\n\n\n\nSecuring SnappyData Pulse UI Connections\n\n\n\n\n\n\nGetting Information from SnappyData System Tables", 
            "title": "Managing and Monitoring"
        }, 
        {
            "location": "/monitoring/managing_and_monitoring/#managing-and-monitoring", 
            "text": "Managing and Monitoring SnappyData describes how to use log files, system tables, and statistical data to understand the behavior of SnappyData deployment.  This guide also provides general guidelines for tuning the performance of SnappyData members, applications, and individual queries.  The following topics are covered in this section:    SnappyData Pulse    Configuring High Availability for a Partitioned Table    Configuring Logging    Securing SnappyData Pulse UI Connections    Getting Information from SnappyData System Tables", 
            "title": "Managing and Monitoring"
        }, 
        {
            "location": "/monitoring/monitoring/", 
            "text": "SnappyData Pulse\n\n\nSnappyData Pulse is a dashboard that provides a real-time view into cluster members, member logs, resource usage, running Jobs, SQL queries along with performance data.  This simple widget based view allows you to navigate easily, visualize, and monitor your cluster. You can monitor the overall status of the cluster as well as the status of each member in the cluster.\nAll the usage details are automatically refreshed after every five seconds.\n\n\nTo access SnappyData Pulse, start your cluster and open \nhttp:\nleadhost\n:5050/dashboard/\n in the web browser.\n\n\n\n\nNote\n\n\nleadhost\n is the hostname or IP of the lead node in your cluster which is provided in the \nconf/leads\n file.\n\n\n\n\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nDashboard\n\n\n\n\n\n\nMember Details\n\n\n\n\n\n\nJobs\n\n\n\n\n\n\nStages\n\n\n\n\n\n\nSpark Cache\n\n\n\n\n\n\nEnvironment\n\n\n\n\n\n\nExecutors\n\n\n\n\n\n\nSQL\n\n\n\n\n\n\n\n\nNote\n\n\nWhen connecting to a SnappyData cluster using Smart Connector, the information related to \nSQL\n, \nJobs\n, and \nStages\n are NOT displayed, as the Jobs and queries are primarily executed in your client Spark cluster. You can find this information on the Spark UI console of your client cluster. Read more about SnappyData Smart Connector Mode \nhere\n.\n\n\n\n\nOn the top-right side of the SnappyData Pulse page, you can view the version details of SnappyData Snapshot. When you click this, the name and version of the product, the build details, the source revision details and the version number of the underlying spark are displayed.\n\n\n\n\n\n\nDashboard\n\n\nThe Dashboard page graphically presents various cluster-level statistics that can be used to monitor the current health status of a cluster. The statistics on the dashboard page can be set to update automatically after every five seconds.  If you want to turn off the auto-refresh, use the \nAuto Refresh\n switch that is provided on the upper-right corner.\n\n\n\nYou can view the total number of physical CPU cores present in your cluster on the top-right side of the page.\n\n\n\nThe \nDashboard\n page displays the following sections:\n\n\n\n\n\n\nCluster\n\n\n\n\n\n\nMembers\n\n\n\n\n\n\nTables\n\n\n\n\n\n\nExternal Tables\n\n\n\n\n\n\nYou can use the search and sort functionalities in any of the sections, except for the \nCluster\n section.  Sorting is enabled to sort items in an ascending and descending order. Further, you can also set the number of items that must be listed in each of these sections.\n\n\n\n\nCluster\n\n\nIn the \nCluster\n section, you can view the following graphs which are automatically refreshed:\n\n\n\n\n\n\n\n\n\n\nGraphs\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCPU Usage\n\n\nGraphically presents the trend of CPU utilization by all the nodes in the cluster for the last 15     minutes. The utilization is represented in percentage value.\n\n\n\n\n\n\nHeap Usage\n\n\nGraphically presents the collective utilization of Heap Memory by all the nodes in the cluster. This  graph displays three trend lines which corresponds to the utilization of Heap Memory for the following:\n \nStorage\nExecution\nJVM\n\n\n\n\n\n\nOff-Heap Usage\n\n\nGraphically presents the collective utilization of Off-Heap Memory by all the nodes in the cluster. This  graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following: \n \nStorage\nExecution\n\n\n\n\n\n\nDisk Space\n\n\nGraphically presents the collective utilization of disk space memory by all the nodes in the cluster.\n\n\n\n\n\n\n\n\n\n\nMembers\n\n\nIn the \nMembers\n section,  you can view, in a tabular format, the details of each locator, data server, and lead member within a cluster. The details are automatically refreshed after every five seconds.\n\n\n\nThis table provides member details in the following columns:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStatus\n\n\nDisplays the status of the members, which can be either \nRunning or Stopped\n.\n\n\n\n\n\n\nMember\n\n\nDisplays a brief description of the member. Click the link in the column to view the \nMember Details\n where the usage trends and statistics of the members are shown along with the \nMember Logs\n. Click the drop-down arrow to find information such as the IP address of the host, the current working directory, and the Process ID number.\n\n\n\n\n\n\nType\n\n\nDisplays the type of the member. The type can be LEAD, LOCATOR, or DATA SERVER. The name of the active lead member is displayed in bold letters.\n\n\n\n\n\n\nCPU Usage\n\n\nDisplays the CPU utilized by the member's host.\n\n\n\n\n\n\nMemory Usage\n\n\nDisplays the collective Heap and Off-Heap memory utilization of a cluster member.\n\n\n\n\n\n\nHeap Memory\n\n\nDisplays the member's utilized Heap memory versus total Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Heap Memory for storage, execution, and JVM.\n\n\n\n\n\n\nOff-Heap Memory\n\n\nDisplays the member's used Off-Heap memory and total Off-Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Off-Heap memory for storage and execution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatus\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\nMember is running.\n\n\n\n\n\n\n\n\nMember has stopped or is unavailable.\n\n\n\n\n\n\n\n\n\n\nTables\n\n\nThe \nTables\n section lists all the tables in the cluster along with their corresponding statistical details. All these details are automatically refreshed after every five seconds.\n\n\n\n\nThe following columns are displayed in this section:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nDisplays the name of the data table.\n\n\n\n\n\n\nStorage Model\n\n\nDisplays the data storage model of the data table. Possible models are \nROW\n and \nCOLUMN\n.\n\n\n\n\n\n\nDistribution Type\n\n\nDisplays the data distribution type for the table. Possible values are: \nPARTITION\nREPLICATE\n \n\n\n\n\n\n\nRow Count\n\n\nDisplays the row count, which is the number of records present in the data table.\n\n\n\n\n\n\nMemory Size\n\n\nDisplays the heap memory used by data table to store its data. If less than \nTotal Size\n then the data is overflowing to disk.\n\n\n\n\n\n\nTotal Size\n\n\nDisplays the collective physical memory and disk overflow space used by the data table to store its data.\n\n\n\n\n\n\nBuckets\n\n\nDisplays the total number of buckets in the data table.\n\n\n\n\n\n\n\n\nExternal Tables\n\n\nThe \nExternal Tables\n section lists all the external tables present in the cluster along with their various statistical details. The displayed details are automatically refreshed after every five seconds.\n\n\n\n\nThe following columns are displayed in this section:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nDisplays the name of the external table.\n\n\n\n\n\n\nProvider\n\n\nDisplays the data store provider that is used when the external table was created. For example, Parquet, CSV, JDBC, etc.\n\n\n\n\n\n\nSource\n\n\nFor Parquet and CSV format, the path of the data file used to create the external table is displayed. For JDBC, the name of the client driver is displayed.\n\n\n\n\n\n\n\n\n\n\nMember Details\n\n\nThe \nMember Details\n view shows the usage trend and \nstatistics\n of a specific cluster member. To check the \nMember\n \nDetails\n view,  go to the \nMembers\n section and click the link in the \nMember\n column. Here you can also view the \nMember Logs\n generated for a cluster member.\nThe usage trends and the statistics of a specific member are auto updated periodically after every five seconds. If you want to turn off the auto-refresh, use the \nAuto Refresh\n switch that is provided on the upper-right corner. You can view, on demand, the latest logs by clicking on the \nLoad New\n button provided at the bottom of the logs. You can also click the \nLoad More\n button to view the older logs.\n\n\n\n\n\n\nMember Statistics\n\n\nThe following member specific statistics are displayed:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMember Name/ID\n\n\nDisplays the name or ID of the member.\n\n\n\n\n\n\nType\n\n\nDisplays the type of member, which can be LEAD, LOCATOR or DATA SERVER.\n\n\n\n\n\n\nProcess ID\n\n\nDisplays the process ID of the member.\n\n\n\n\n\n\nStatus\n\n\nDisplays the status of the member. This can be either \nRunning\n or \nUnavailable\n\n\n\n\n\n\nHeap Memory\n\n\nDisplays the total available heap memory, used heap memory, their distribution into heap storage, heap execution memory and their utilization.\n\n\n\n\n\n\nOff-Heap Memory Usage\n\n\nDisplays the members total off-heap memory, used off-heap memory, their distribution into off-heap storage and off-heap execution memory, and their utilization.\n\n\n\n\n\n\n\n\nThe usage trends of the member are represented in the following graphs:\n\n\n\n\n\n\n\n\nGraphs\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCPU Usage\n\n\nGraphically presents the trend of CPU utilization by the member host for the last 15 minutes. The utilization is represented in percentage value.\n\n\n\n\n\n\nHeap Usage\n\n\nGraphically presents the utilization of Heap Memory by the member host. This  graph displays three trend lines which corresponds to the utilization of Heap Memory for the following:\n \nStorage\nExecution\nJVM\n\n\n\n\n\n\nOff-Heap Usage\n\n\nGraphically presents the utilization of Off-Heap Memory by the member host. This  graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following: \n \nStorage\nExecution\n\n\n\n\n\n\nDisk Space\n\n\nGraphically presents the utilization of disk space memory by the member host.\n\n\n\n\n\n\n\n\n\n\nMember Logs\n\n\nIn the Member Details page, you can view the logs generated for a single member in the cluster. \n\n\n\n\nThe following details are included:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLog File Location\n\n\nDisplays the absolute path of the member's primary log file, which is on the host where the current member's processes are running.\n\n\n\n\n\n\nLog Details \n\n\nDisplays details of the loaded logs such as  Loaded Bytes, Start and End Indexes of Loaded Bytes, and Total Bytes of logs content.\n\n\n\n\n\n\nLogs\n\n\nDisplays the actual log entries from the log files. \n It also displays the following buttons: \n \nLoad New\n - Loads the latest log entries from the log file, if generated, after logs were last loaded or updated.\nLoad More\n - Loads older log entries from log files, if available.\n\n\n\n\n\n\n\n\n\n\nSQL\n\n\nThe SQL section shows all the queries and their corresponding details along with their execution plans and stagewise breakups.\n\n\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nColocated\n\n\nWhen colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. This improves the performance of the query significantly instead of broadcasting the data across all the data partitions.\n\n\n\n\n\n\nWhole-Stage Code Generation\n\n\nA whole stage code generation node compiles a sub-tree of plans that support code generation together into a single Java function, which helps improve execution performance.\n\n\n\n\n\n\nPer node execution timing\n\n\nDisplays the time required for the execution of each node. If there are too many rows that are not getting filtered or exchanged.\n\n\n\n\n\n\nPool Name\n\n\nDefault/Low Latency. Applications can explicitly configure the use of this pool using a SQL command \nset snappydata.scheduler.pool=lowlatency\n.\n\n\n\n\n\n\nQuery Node Details\n\n\nHover over a component to view its details.\n\n\n\n\n\n\nFilter\n\n\nDisplays the number of rows that are filtered for each node.\n\n\n\n\n\n\nJoins\n\n\nIf HashJoin puts pressure on memory, you can change the HashJoin size to use SortMergeJoin to avoid on-heap memory pressure.\n\n\n\n\n\n\n\n\n\n\nSpark Cache\n\n\nSpark Cache is the inbuilt storage mechanism of Spark. When you do a \ndataSet.cache()\n, it uses this storage to store the dataset's data in a columnar format. This storage can be configured to be one of the following:\n\n\n\n\nMEMORY_ONLY,\n\n\nMEMORY_AND_DISK,\n\n\nMEMORY_ONLY_SER,\n\n\nMEMORY_AND_DISK_SER,\n\n\nDISK_ONLY,\n\n\nMEMORY_ONLY_2,\n\n\nMEMORY_AND_DISK_2\n\n\n\n\nFor more details, see \nRDD Persistence section\n.\n\n\n\n\nEnvironment\n\n\nThe Environment page provides detailed configurations for Spark environment including JVM, SparkContext, and SparkSession.\n\n\n\n\nExecutors\n\n\nExecutors are the entities that perform the tasks within a Spark job. Each Spark job is divided into multiple stages which can have one or more tasks depending on the number of partitions to be processed. All these tasks are scheduled on executor nodes which actually run them.\n\n\n\n\nJobs\n\n\nThe \nJobs\n page lists all the Spark jobs. Each Spark action is translated as a Spark job. A job encapsulates the whole execution of an API or SQL. For example, \ndataSet.count()\n triggers a job.\n\n\n\n\n\n\n\n\nStatus\n: Displays the status of the job.\n\n\n\n\n\n\nStages\n: Click on the stage to view its details. The table displays the time taken for the completion of each stage. \n\n\n\n\n\n\n\n\nTip\n\n\nYou can cancel a long running job, using the \nKill\n option. \n\n\n\n\n\n\nStages\n\n\nThe \nStages\n page displays the stage details of a Spark Job. Each Spark job is segregated into one or more stages. Each stage is an execution boundary where data exchange between nodes is required.\n\n\nOn this page, you can view the total time required for all the tasks in a job to complete. You can also view if any of the tasks got delayed for completion. This may occur in case of uneven data distribution.\n\n\n\n\n\n\n\n\nScheduler Delay\n indicates the waiting period for the task. Delays can be caused if there are too many concurrent jobs.\n\n\n\n\n\n\nShuffle reads and writes\n: Shuffles are written to disk and can take a lot of time to write and read. This can be avoided by using colocated and replicated tables. You can use high-performance SSD drives for temporary storage (spark.local.dir) to improve shuffle time.\n\n\n\n\n\n\nNumber of parallel tasks\n: Due to concurrency, multiple queries may take cores and a specific query can take longer. To fix this, you can create a new scheduler and \nassign appropriate cores to it\n.\n\n\n\n\n\n\nGC time\n: Occasionally, on-heap object creation can slow down a query because of garbage collection. In these cases, it is recommended that you increase the on-heap memory, especially when you have row tables.", 
            "title": "SnappyData Pulse"
        }, 
        {
            "location": "/monitoring/monitoring/#snappydata-pulse", 
            "text": "SnappyData Pulse is a dashboard that provides a real-time view into cluster members, member logs, resource usage, running Jobs, SQL queries along with performance data.  This simple widget based view allows you to navigate easily, visualize, and monitor your cluster. You can monitor the overall status of the cluster as well as the status of each member in the cluster.\nAll the usage details are automatically refreshed after every five seconds.  To access SnappyData Pulse, start your cluster and open  http: leadhost :5050/dashboard/  in the web browser.   Note  leadhost  is the hostname or IP of the lead node in your cluster which is provided in the  conf/leads  file.    The following topics are covered in this section:    Dashboard    Member Details    Jobs    Stages    Spark Cache    Environment    Executors    SQL     Note  When connecting to a SnappyData cluster using Smart Connector, the information related to  SQL ,  Jobs , and  Stages  are NOT displayed, as the Jobs and queries are primarily executed in your client Spark cluster. You can find this information on the Spark UI console of your client cluster. Read more about SnappyData Smart Connector Mode  here .   On the top-right side of the SnappyData Pulse page, you can view the version details of SnappyData Snapshot. When you click this, the name and version of the product, the build details, the source revision details and the version number of the underlying spark are displayed.", 
            "title": "SnappyData Pulse"
        }, 
        {
            "location": "/monitoring/monitoring/#dashboard", 
            "text": "The Dashboard page graphically presents various cluster-level statistics that can be used to monitor the current health status of a cluster. The statistics on the dashboard page can be set to update automatically after every five seconds.  If you want to turn off the auto-refresh, use the  Auto Refresh  switch that is provided on the upper-right corner.  You can view the total number of physical CPU cores present in your cluster on the top-right side of the page.  The  Dashboard  page displays the following sections:    Cluster    Members    Tables    External Tables    You can use the search and sort functionalities in any of the sections, except for the  Cluster  section.  Sorting is enabled to sort items in an ascending and descending order. Further, you can also set the number of items that must be listed in each of these sections.", 
            "title": "Dashboard"
        }, 
        {
            "location": "/monitoring/monitoring/#cluster", 
            "text": "In the  Cluster  section, you can view the following graphs which are automatically refreshed:      Graphs  Description      CPU Usage  Graphically presents the trend of CPU utilization by all the nodes in the cluster for the last 15     minutes. The utilization is represented in percentage value.    Heap Usage  Graphically presents the collective utilization of Heap Memory by all the nodes in the cluster. This  graph displays three trend lines which corresponds to the utilization of Heap Memory for the following:   Storage Execution JVM    Off-Heap Usage  Graphically presents the collective utilization of Off-Heap Memory by all the nodes in the cluster. This  graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following:    Storage Execution    Disk Space  Graphically presents the collective utilization of disk space memory by all the nodes in the cluster.", 
            "title": "Cluster"
        }, 
        {
            "location": "/monitoring/monitoring/#members", 
            "text": "In the  Members  section,  you can view, in a tabular format, the details of each locator, data server, and lead member within a cluster. The details are automatically refreshed after every five seconds.  This table provides member details in the following columns:     Column  Description      Status  Displays the status of the members, which can be either  Running or Stopped .    Member  Displays a brief description of the member. Click the link in the column to view the  Member Details  where the usage trends and statistics of the members are shown along with the  Member Logs . Click the drop-down arrow to find information such as the IP address of the host, the current working directory, and the Process ID number.    Type  Displays the type of the member. The type can be LEAD, LOCATOR, or DATA SERVER. The name of the active lead member is displayed in bold letters.    CPU Usage  Displays the CPU utilized by the member's host.    Memory Usage  Displays the collective Heap and Off-Heap memory utilization of a cluster member.    Heap Memory  Displays the member's utilized Heap memory versus total Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Heap Memory for storage, execution, and JVM.    Off-Heap Memory  Displays the member's used Off-Heap memory and total Off-Heap memory. Click the down arrow in this column to view the detailed distribution of the member's Off-Heap memory for storage and execution.         Status  Description       Member is running.     Member has stopped or is unavailable.", 
            "title": "Members"
        }, 
        {
            "location": "/monitoring/monitoring/#tables", 
            "text": "The  Tables  section lists all the tables in the cluster along with their corresponding statistical details. All these details are automatically refreshed after every five seconds.   The following columns are displayed in this section:     Column  Description      Name  Displays the name of the data table.    Storage Model  Displays the data storage model of the data table. Possible models are  ROW  and  COLUMN .    Distribution Type  Displays the data distribution type for the table. Possible values are:  PARTITION REPLICATE      Row Count  Displays the row count, which is the number of records present in the data table.    Memory Size  Displays the heap memory used by data table to store its data. If less than  Total Size  then the data is overflowing to disk.    Total Size  Displays the collective physical memory and disk overflow space used by the data table to store its data.    Buckets  Displays the total number of buckets in the data table.", 
            "title": "Tables"
        }, 
        {
            "location": "/monitoring/monitoring/#external-tables", 
            "text": "The  External Tables  section lists all the external tables present in the cluster along with their various statistical details. The displayed details are automatically refreshed after every five seconds.   The following columns are displayed in this section:     Column  Description      Name  Displays the name of the external table.    Provider  Displays the data store provider that is used when the external table was created. For example, Parquet, CSV, JDBC, etc.    Source  For Parquet and CSV format, the path of the data file used to create the external table is displayed. For JDBC, the name of the client driver is displayed.", 
            "title": "External Tables"
        }, 
        {
            "location": "/monitoring/monitoring/#member-details", 
            "text": "The  Member Details  view shows the usage trend and  statistics  of a specific cluster member. To check the  Member   Details  view,  go to the  Members  section and click the link in the  Member  column. Here you can also view the  Member Logs  generated for a cluster member.\nThe usage trends and the statistics of a specific member are auto updated periodically after every five seconds. If you want to turn off the auto-refresh, use the  Auto Refresh  switch that is provided on the upper-right corner. You can view, on demand, the latest logs by clicking on the  Load New  button provided at the bottom of the logs. You can also click the  Load More  button to view the older logs.", 
            "title": "Member Details"
        }, 
        {
            "location": "/monitoring/monitoring/#member-statistics", 
            "text": "The following member specific statistics are displayed:     Item  Description      Member Name/ID  Displays the name or ID of the member.    Type  Displays the type of member, which can be LEAD, LOCATOR or DATA SERVER.    Process ID  Displays the process ID of the member.    Status  Displays the status of the member. This can be either  Running  or  Unavailable    Heap Memory  Displays the total available heap memory, used heap memory, their distribution into heap storage, heap execution memory and their utilization.    Off-Heap Memory Usage  Displays the members total off-heap memory, used off-heap memory, their distribution into off-heap storage and off-heap execution memory, and their utilization.     The usage trends of the member are represented in the following graphs:     Graphs  Description      CPU Usage  Graphically presents the trend of CPU utilization by the member host for the last 15 minutes. The utilization is represented in percentage value.    Heap Usage  Graphically presents the utilization of Heap Memory by the member host. This  graph displays three trend lines which corresponds to the utilization of Heap Memory for the following:   Storage Execution JVM    Off-Heap Usage  Graphically presents the utilization of Off-Heap Memory by the member host. This  graph displays two trend lines which corresponds to the utilization of Off-Heap Memory for the following:    Storage Execution    Disk Space  Graphically presents the utilization of disk space memory by the member host.", 
            "title": "Member Statistics"
        }, 
        {
            "location": "/monitoring/monitoring/#member-logs", 
            "text": "In the Member Details page, you can view the logs generated for a single member in the cluster.    The following details are included:     Item  Description      Log File Location  Displays the absolute path of the member's primary log file, which is on the host where the current member's processes are running.    Log Details   Displays details of the loaded logs such as  Loaded Bytes, Start and End Indexes of Loaded Bytes, and Total Bytes of logs content.    Logs  Displays the actual log entries from the log files.   It also displays the following buttons:    Load New  - Loads the latest log entries from the log file, if generated, after logs were last loaded or updated. Load More  - Loads older log entries from log files, if available.", 
            "title": "Member Logs"
        }, 
        {
            "location": "/monitoring/monitoring/#sql", 
            "text": "The SQL section shows all the queries and their corresponding details along with their execution plans and stagewise breakups.      Item  Description      Colocated  When colocated tables are joined on the partitioning columns, the join happens locally on the node where data is present, without the need of shuffling the data. This improves the performance of the query significantly instead of broadcasting the data across all the data partitions.    Whole-Stage Code Generation  A whole stage code generation node compiles a sub-tree of plans that support code generation together into a single Java function, which helps improve execution performance.    Per node execution timing  Displays the time required for the execution of each node. If there are too many rows that are not getting filtered or exchanged.    Pool Name  Default/Low Latency. Applications can explicitly configure the use of this pool using a SQL command  set snappydata.scheduler.pool=lowlatency .    Query Node Details  Hover over a component to view its details.    Filter  Displays the number of rows that are filtered for each node.    Joins  If HashJoin puts pressure on memory, you can change the HashJoin size to use SortMergeJoin to avoid on-heap memory pressure.", 
            "title": "SQL"
        }, 
        {
            "location": "/monitoring/monitoring/#spark-cache", 
            "text": "Spark Cache is the inbuilt storage mechanism of Spark. When you do a  dataSet.cache() , it uses this storage to store the dataset's data in a columnar format. This storage can be configured to be one of the following:   MEMORY_ONLY,  MEMORY_AND_DISK,  MEMORY_ONLY_SER,  MEMORY_AND_DISK_SER,  DISK_ONLY,  MEMORY_ONLY_2,  MEMORY_AND_DISK_2   For more details, see  RDD Persistence section .", 
            "title": "Spark Cache"
        }, 
        {
            "location": "/monitoring/monitoring/#environment", 
            "text": "The Environment page provides detailed configurations for Spark environment including JVM, SparkContext, and SparkSession.", 
            "title": "Environment"
        }, 
        {
            "location": "/monitoring/monitoring/#executors", 
            "text": "Executors are the entities that perform the tasks within a Spark job. Each Spark job is divided into multiple stages which can have one or more tasks depending on the number of partitions to be processed. All these tasks are scheduled on executor nodes which actually run them.", 
            "title": "Executors"
        }, 
        {
            "location": "/monitoring/monitoring/#jobs", 
            "text": "The  Jobs  page lists all the Spark jobs. Each Spark action is translated as a Spark job. A job encapsulates the whole execution of an API or SQL. For example,  dataSet.count()  triggers a job.     Status : Displays the status of the job.    Stages : Click on the stage to view its details. The table displays the time taken for the completion of each stage.      Tip  You can cancel a long running job, using the  Kill  option.", 
            "title": "Jobs"
        }, 
        {
            "location": "/monitoring/monitoring/#stages", 
            "text": "The  Stages  page displays the stage details of a Spark Job. Each Spark job is segregated into one or more stages. Each stage is an execution boundary where data exchange between nodes is required.  On this page, you can view the total time required for all the tasks in a job to complete. You can also view if any of the tasks got delayed for completion. This may occur in case of uneven data distribution.     Scheduler Delay  indicates the waiting period for the task. Delays can be caused if there are too many concurrent jobs.    Shuffle reads and writes : Shuffles are written to disk and can take a lot of time to write and read. This can be avoided by using colocated and replicated tables. You can use high-performance SSD drives for temporary storage (spark.local.dir) to improve shuffle time.    Number of parallel tasks : Due to concurrency, multiple queries may take cores and a specific query can take longer. To fix this, you can create a new scheduler and  assign appropriate cores to it .    GC time : Occasionally, on-heap object creation can slow down a query because of garbage collection. In these cases, it is recommended that you increase the on-heap memory, especially when you have row tables.", 
            "title": "Stages"
        }, 
        {
            "location": "/monitoring/configure_high_availability/", 
            "text": "Configuring High Availability for a Partitioned Table\n\n\nConfigure in-memory high availability for your partitioned table. Set other high-availability options, like redundancy zones and redundancy recovery strategies.\n\n\nSet the Number of Redundant Copies\n\n\nConfigure in-memory high availability for your partitioned table by specifying the number of secondary copies you want to maintain REDUNDANCY clause of \nCREATE TABLE\n.\n\n\nConfigure Redundancy Zones for Members\n\n\nGroup members into redundancy zones so that SnappyData places redundant data copies in different zones.\n\n\nUnderstand how to set a member's \ngemfirexd.properties\n settings. \nSee Configuration Properties\n.\nGroup the datastore members that host partitioned tables into redundancy zones by using the setting \nredundancy-zone\n.\n\n\nFor example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. To do this, you set this zone in the \ngemfirexd.properties\n file for all members that run on one rack:\n\n\nredundancy-zone=rack1\n\n\n\n\nYou would set this zone in \ngemfirexd.properties\n for all members on the other rack:\n\n\nredundancy-zone=rack2\n\n\n\n\nEach secondary copy would be hosted on the rack opposite the rack where its primary copy is hosted.\n\n\nSet Enforce Unique Host\n\n\nConfigure SnappyData to use only unique physical machines for redundant copies of partitioned table data.\n\n\nUnderstand how to set a member's \ngemfirexd.properties\n settings. See \nConfiguration Properties\n.\n\n\nConfigure your members so SnappyData always uses different physical machines for redundant copies of partitioned table data using the setting \nenforce-unique-host\n. The default for this setting is \nfalse\n.\n\n\nExample:\n\n\nenforce-unique-host=true", 
            "title": "Configuring High Availability for a Partitioned Table"
        }, 
        {
            "location": "/monitoring/configure_high_availability/#configuring-high-availability-for-a-partitioned-table", 
            "text": "Configure in-memory high availability for your partitioned table. Set other high-availability options, like redundancy zones and redundancy recovery strategies.", 
            "title": "Configuring High Availability for a Partitioned Table"
        }, 
        {
            "location": "/monitoring/configure_high_availability/#set-the-number-of-redundant-copies", 
            "text": "Configure in-memory high availability for your partitioned table by specifying the number of secondary copies you want to maintain REDUNDANCY clause of  CREATE TABLE .", 
            "title": "Set the Number of Redundant Copies"
        }, 
        {
            "location": "/monitoring/configure_high_availability/#configure-redundancy-zones-for-members", 
            "text": "Group members into redundancy zones so that SnappyData places redundant data copies in different zones.  Understand how to set a member's  gemfirexd.properties  settings.  See Configuration Properties .\nGroup the datastore members that host partitioned tables into redundancy zones by using the setting  redundancy-zone .  For example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. To do this, you set this zone in the  gemfirexd.properties  file for all members that run on one rack:  redundancy-zone=rack1  You would set this zone in  gemfirexd.properties  for all members on the other rack:  redundancy-zone=rack2  Each secondary copy would be hosted on the rack opposite the rack where its primary copy is hosted.", 
            "title": "Configure Redundancy Zones for Members"
        }, 
        {
            "location": "/monitoring/configure_high_availability/#set-enforce-unique-host", 
            "text": "Configure SnappyData to use only unique physical machines for redundant copies of partitioned table data.  Understand how to set a member's  gemfirexd.properties  settings. See  Configuration Properties .  Configure your members so SnappyData always uses different physical machines for redundant copies of partitioned table data using the setting  enforce-unique-host . The default for this setting is  false .  Example:  enforce-unique-host=true", 
            "title": "Set Enforce Unique Host"
        }, 
        {
            "location": "/monitoring/configure_logging/", 
            "text": "Configuring Logging\n\n\nBy default, log files for SnappyData members are created inside the working directory of the member. To change the log file directory, you can specify a property \n-log-file\n as the path of the directory, while starting a member. \n\n\nSnappyData uses \nlog4j\n for logging.\nYou can configure logging by copying the existing template file \nlog4j.properties.template\n to the \nconf\n directory and renaming it to \nlog4j.properties\n.\n\n\nFor example, the following can be added to the \nlog4j.properties\n file to change the logging level of the classes of Spark scheduler.\n\n\n$ cat conf/log4j.properties \nlog4j.logger.org.apache.spark.scheduler=DEBUG\n\n\n\n\nThe default template uses a custom layout (io.snappydata.log4j.PatternLayout) that adds a thread ID at the end of thread name in the logs for the \n%t\n pattern.\n\n\nFor example, the default pattern:\n\n\nlog4j.appender.file.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss.SSS zzz} %t %p %c{1}: %m%n\n\n\n\n\nproduces\n\n\n17/11/07 16:42:05.115 IST serverConnector\ntid=0xe\n INFO snappystore: GemFire P2P Listener started on  tcp:///192.168.1.6:53116\n\n\n\n\nThis is the recommended PatternLayout to use for SnappyData logging. \n\n\nWhen using a custom \nlog4j.properties\n, and the mentioned layout cannot be used or when using AsyncAppender, then a custom appender \nThreadIdAppender\n has been provided that can be inserted as the first appender to get the same output.\n\n\nExamples:\n\n\nWhen using the default PatternLayout in the log4j.properties file:\n\n\nlog4j.rootCategory=INFO, file\n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.layout=io.snappydata.log4j.PatternLayout\n\n\n\n\nWhen adding the custom appender\n\n\nlog4j.rootCategory=INFO, threadId, file\n\nlog4j.appender.threadId=io.snappydata.log4j.ThreadIdAppender\n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\n\n\n\n\nSetting Log Level at Runtime\n\n\nThe inbuilt procedure \nset_log_level\n can be used to set the log level of SnappyData classes at runtime. You must execute the procedure as a system user. \n\n\nFollowing is the usage of the procedure: \n\n\ncall sys.set_log_level (loggerName, logLevel);\n\n\n\n\nThe \nlogLevel\n can be a log4j level, that is, ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF, TRACE. \nloggerName\n can be a class name or a package name. If it is left empty, the root logger's level is set.\n\n\nFor example:\n\n\n// sets the root logger's level as WARN\nsnappy\n call sys.set_log_level ('', 'WARN' );\n\n// sets the WholeStageCodegenExec class level as DEBUG\nsnappy\n call sys.set_log_level ('org.apache.spark.sql.execution.WholeStageCodegenExec', 'DEBUG');\n\n// sets the apache spark package's log level as INFO\nsnappy\n call sys.set_log_level ('org.apache.spark', 'INFO');\n\n\n\n\nSnappyData Store logging\n\n\nThe fine-grained log settings are applicable for classes other than the SnappyData store classes. SnappyData store does not honor fine-grained log settings. That is, you can only set the log level for the root category. However, log level of specific features of SnappyData store can be controlled both during the start and during runtime.\n\n\nUsing Trace Flags for Advanced Logging For SnappyData Store\n\n\n\nSnappyData Store provides the following trace flags that you can use with the \nsnappydata.debug.true\n system property to log additional details about specific features:\n\n\n\n\n\n\n\n\nTrace flag\n\n\nEnables\n\n\n\n\n\n\n\n\n\n\nQueryDistribution\n\n\nDetailed logging for distributed queries and DML statements, including information about message distribution to SnappyData members and scan types that were opened.\n\n\n\n\n\n\nStatementMatching\n\n\nLogging for optimizations that are related to unprepared statements.\n\n\n\n\n\n\nTraceAuthentication\n\n\nAdditional logging for authentication.\n\n\n\n\n\n\nTraceDBSynchronizer\n\n\nDBSynchronizer and WAN distribution logging.\n\n\n\n\n\n\nTraceClientConn\n\n\nClient-side connection open and close stack traces.\n\n\n\n\n\n\nTraceClientStatement\n\n\nClient-side, basic timing logging.\n\n\n\n\n\n\nTraceClientStatementMillis\n\n\nClient-side wall clock timing.\n\n\n\n\n\n\nTraceIndex\n\n\nDetailed index logging.\n\n\n\n\n\n\nTraceJars\n\n\nLogging for JAR installation, replace, and remove events.\n\n\n\n\n\n\nTraceTran\n\n\nDetailed logging for transaction events and operations, including commit and rollback.\n\n\n\n\n\n\nTraceLock_*\n\n\nLocking and unlocking information for all internal locks.\n\n\n\n\n\n\nTraceLock_DD\n\n\nLogging for all DataDictionary and table locks that are acquired or released.\n\n\n\n\n\n\n\n\nTo enable logging of specific features of SnappyData, set the required trace flag in the \nsnappydata.debug.true\n system property. For example, you can add the following setting inside the configuration file of the SnappyData member to enable logging for query distribution and indexing:\n\n\nlocalhost -J-Dsnappydata.debug.true=QueryDistribution,TraceIndex\n\n\n\n\nIf you need to set a trace flag in a running system, use the \nSYS.SET_TRACE_FLAG\n system procedure. The procedure sets the trace flag in all members of the distributed system, including locators. You must execute the procedure as a system user. For example:\n\n\nsnappy\n call sys.set_trace_flag('traceindex', 'true');\nStatement executed.\n\n\n\n\n\n\nNote\n\n\nTrace flags work only for \nsnappy\n and \njdbc\n and not for \nsnappy-sql\n.", 
            "title": "Configuring Logging"
        }, 
        {
            "location": "/monitoring/configure_logging/#configuring-logging", 
            "text": "By default, log files for SnappyData members are created inside the working directory of the member. To change the log file directory, you can specify a property  -log-file  as the path of the directory, while starting a member.   SnappyData uses  log4j  for logging.\nYou can configure logging by copying the existing template file  log4j.properties.template  to the  conf  directory and renaming it to  log4j.properties .  For example, the following can be added to the  log4j.properties  file to change the logging level of the classes of Spark scheduler.  $ cat conf/log4j.properties \nlog4j.logger.org.apache.spark.scheduler=DEBUG  The default template uses a custom layout (io.snappydata.log4j.PatternLayout) that adds a thread ID at the end of thread name in the logs for the  %t  pattern.  For example, the default pattern:  log4j.appender.file.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss.SSS zzz} %t %p %c{1}: %m%n  produces  17/11/07 16:42:05.115 IST serverConnector tid=0xe  INFO snappystore: GemFire P2P Listener started on  tcp:///192.168.1.6:53116  This is the recommended PatternLayout to use for SnappyData logging.   When using a custom  log4j.properties , and the mentioned layout cannot be used or when using AsyncAppender, then a custom appender  ThreadIdAppender  has been provided that can be inserted as the first appender to get the same output.  Examples:  When using the default PatternLayout in the log4j.properties file:  log4j.rootCategory=INFO, file\n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.layout=io.snappydata.log4j.PatternLayout  When adding the custom appender  log4j.rootCategory=INFO, threadId, file\n\nlog4j.appender.threadId=io.snappydata.log4j.ThreadIdAppender\n\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout", 
            "title": "Configuring Logging"
        }, 
        {
            "location": "/monitoring/configure_logging/#setting-log-level-at-runtime", 
            "text": "The inbuilt procedure  set_log_level  can be used to set the log level of SnappyData classes at runtime. You must execute the procedure as a system user.   Following is the usage of the procedure:   call sys.set_log_level (loggerName, logLevel);  The  logLevel  can be a log4j level, that is, ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF, TRACE.  loggerName  can be a class name or a package name. If it is left empty, the root logger's level is set.  For example:  // sets the root logger's level as WARN\nsnappy  call sys.set_log_level ('', 'WARN' );\n\n// sets the WholeStageCodegenExec class level as DEBUG\nsnappy  call sys.set_log_level ('org.apache.spark.sql.execution.WholeStageCodegenExec', 'DEBUG');\n\n// sets the apache spark package's log level as INFO\nsnappy  call sys.set_log_level ('org.apache.spark', 'INFO');", 
            "title": "Setting Log Level at Runtime"
        }, 
        {
            "location": "/monitoring/configure_logging/#snappydata-store-logging", 
            "text": "The fine-grained log settings are applicable for classes other than the SnappyData store classes. SnappyData store does not honor fine-grained log settings. That is, you can only set the log level for the root category. However, log level of specific features of SnappyData store can be controlled both during the start and during runtime.", 
            "title": "SnappyData Store logging"
        }, 
        {
            "location": "/monitoring/configure_logging/#using-trace-flags-for-advanced-logging-for-snappydata-store", 
            "text": "SnappyData Store provides the following trace flags that you can use with the  snappydata.debug.true  system property to log additional details about specific features:     Trace flag  Enables      QueryDistribution  Detailed logging for distributed queries and DML statements, including information about message distribution to SnappyData members and scan types that were opened.    StatementMatching  Logging for optimizations that are related to unprepared statements.    TraceAuthentication  Additional logging for authentication.    TraceDBSynchronizer  DBSynchronizer and WAN distribution logging.    TraceClientConn  Client-side connection open and close stack traces.    TraceClientStatement  Client-side, basic timing logging.    TraceClientStatementMillis  Client-side wall clock timing.    TraceIndex  Detailed index logging.    TraceJars  Logging for JAR installation, replace, and remove events.    TraceTran  Detailed logging for transaction events and operations, including commit and rollback.    TraceLock_*  Locking and unlocking information for all internal locks.    TraceLock_DD  Logging for all DataDictionary and table locks that are acquired or released.     To enable logging of specific features of SnappyData, set the required trace flag in the  snappydata.debug.true  system property. For example, you can add the following setting inside the configuration file of the SnappyData member to enable logging for query distribution and indexing:  localhost -J-Dsnappydata.debug.true=QueryDistribution,TraceIndex  If you need to set a trace flag in a running system, use the  SYS.SET_TRACE_FLAG  system procedure. The procedure sets the trace flag in all members of the distributed system, including locators. You must execute the procedure as a system user. For example:  snappy  call sys.set_trace_flag('traceindex', 'true');\nStatement executed.   Note  Trace flags work only for  snappy  and  jdbc  and not for  snappy-sql .", 
            "title": "Using Trace Flags for Advanced Logging For SnappyData Store"
        }, 
        {
            "location": "/monitoring/monitor-manage/", 
            "text": "Getting Information from SnappyData System Tables\n\n\nYou can monitor many common aspects of SnappyData by using SQL commands (system procedures and simple queries) to collect and analyze data in SnappyData system tables.\n\n\nDistributed System Membership Information\n\n\nThe SYS.MEMBERS table provides information about all peers and servers that make up the SnappyData system. You can use different queries to obtain details about individual members and their role in the cluster.\n\n\n\n\nDetermining Cluster Membership\n\n\nTo display a list of all members that participate in a given cluster, simply query all ID entries in sys.members. For example:\n\n\nsnappy\n select ID from SYS.MEMBERS;\nID\n-----------------------------------\n127.0.0.1(10889)\nv1\n:43634\n127.0.0.1(11045)\nv2\n:19283\n127.0.0.1(10749)\nec\nv0\n:51430\n\n3 rows selected\n\n\n\n\nThe number of rows returned corresponds to the total number of peers, servers, and locators in the cluster.\n\n\nTo determine each member's role in the system, include the KIND column in the query:\n\n\nsnappy\n select ID, KIND from SYS.MEMBERS;\nID                                       |KIND\n-----------------------------------------------------------------------\n127.0.0.1(10889)\nv1\n:43634               |datastore(normal)\n127.0.0.1(11045)\nv2\n:19283               |accessor(normal)\n127.0.0.1(10749)\nec\nv0\n:51430           |locator(normal)\n\n3 rows selected\n\n\n\n\nTo view the members of cluster, query:\n\n\nsnappy\n show members;\nID                            |HOST     |KIND             |STATUS |THRIFTSERVERS            |SERVERGROUPS\n--------------------------------------------------------------------------------------------------------- \n127.0.0.1(10749)\nec\nv0\n:51430|localhost|locator(normal)  |RUNNING|localhost/127.0.0.1[1527]|\n127.0.0.1(10889)\nv1\n:43634    |localhost|datastore(normal)|RUNNING|localhost/127.0.0.1[1528]|\n127.0.0.1(11045)\nv2\n:19283    |localhost|accessor(normal) |RUNNING|                         |IMPLICIT_LEADER_SERVERGROUP \n\n3 rows selected\n\n\n\n\nData store members host data in the cluster, while accessor members do not host data. This role is determined by the \nhost-data\n boot property. If a cluster contains only a single data store, its KIND is listed as \"loner\".\n\n\nTable and Data Storage Information\n\n\nThe SYS.SYSTABLES table provides information about all tables that are created in the SnappyData system. You can use different queries to obtain details about tables and the server groups that host data for those tables.\n\n\n\n\nDisplaying a List of Tables\n\n\nDetermining Whether a Table Is Replicated or Partitioned\n\n\nDetermining How Persistent Data Is Stored\n\n\nDisplaying Eviction Settings\n\n\nDisplaying Indexes\n\n\n\n\n\n\nDisplaying a List of Tables\n\n\nTo display a list of all tables in the cluster:\n\n\nsnappy\n select TABLESCHEMANAME, TABLENAME from SYS.SYSTABLES order by TABLESCHEMANAME;\nTABLESCHEMANAME                                    |TABLENAME \n------------------------------------------------------------------------------------------------------------------------\nAPP                                                |SUPPLIER_1\nAPP                                                |SUPPLIER\nAPP                                                |AIRLINEREF\nAPP                                                |AIRLINE\nAPP                                                |SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_\nAPP                                                |EMPLOYEE\nSNAPPY_HIVE_METASTORE                              |TBLS\nSNAPPY_HIVE_METASTORE                              |PARTITION_PARAMS\nSNAPPY_HIVE_METASTORE                              |SKEWED_COL_VALUE_LOC_MAP\nSNAPPY_HIVE_METASTORE                              |FUNCS\nSNAPPY_HIVE_METASTORE                              |SDS\nSNAPPY_HIVE_METASTORE                              |SERDE_PARAMS\nSNAPPY_HIVE_METASTORE                              |PART_COL_STATS\nSNAPPY_HIVE_METASTORE                              |SKEWED_STRING_LIST\nSNAPPY_HIVE_METASTORE                              |DBS\nSNAPPY_HIVE_METASTORE                              |PARTITIONS\nSNAPPY_HIVE_METASTORE                              |BUCKETING_COLS\nSNAPPY_HIVE_METASTORE                              |FUNC_RU\nSNAPPY_HIVE_METASTORE                              |SKEWED_VALUES\nSNAPPY_HIVE_METASTORE                              |ROLES\nSNAPPY_HIVE_METASTORE                              |SORT_COLS\nSNAPPY_HIVE_METASTORE                              |SD_PARAMS\nSNAPPY_HIVE_METASTORE                              |TAB_COL_STATS\nSNAPPY_HIVE_METASTORE                              |GLOBAL_PRIVS\nSNAPPY_HIVE_METASTORE                              |SKEWED_COL_NAMES\nSNAPPY_HIVE_METASTORE                              |SKEWED_STRING_LIST_VALUES\nSNAPPY_HIVE_METASTORE                              |VERSION\nSNAPPY_HIVE_METASTORE                              |CDS\nSNAPPY_HIVE_METASTORE                              |SEQUENCE_TABLE\nSNAPPY_HIVE_METASTORE                              |PARTITION_KEYS\nSNAPPY_HIVE_METASTORE                              |TABLE_PARAMS\nSNAPPY_HIVE_METASTORE                              |DATABASE_PARAMS\nSNAPPY_HIVE_METASTORE                              |COLUMNS_V2\nSNAPPY_HIVE_METASTORE                              |SERDES\nSNAPPY_HIVE_METASTORE                              |PARTITION_KEY_VALS\nSYS                                                |GATEWAYSENDERS\nSYS                                                |SYSSTATEMENTS\nSYS                                                |SYSKEYS\nSYS                                                |SYSROLES\nSYS                                                |SYSFILES\nSYS                                                |SYSROUTINEPERMS\nSYS                                                |SYSCONSTRAINTS\nSYS                                                |SYSCOLPERMS\nSYS                                                |SYSHDFSSTORES\nSYS                                                |SYSDEPENDS\nSYS                                                |SYSALIASES\nSYS                                                |SYSTABLEPERMS\nSYS                                                |SYSTABLES\nSYS                                                |SYSVIEWS\nSYS                                                |ASYNCEVENTLISTENERS\nSYS                                                |SYSCHECKS\nSYS                                                |SYSSTATISTICS\nSYS                                                |SYSCONGLOMERATES\nSYS                                                |GATEWAYRECEIVERS\nSYS                                                |SYSTRIGGERS\nSYS                                                |SYSDISKSTORES\nSYS                                                |SYSSCHEMAS\nSYS                                                |SYSFOREIGNKEYS\nSYS                                                |SYSCOLUMNS\nSYSIBM                                             |SYSDUMMY1\nSYSSTAT                                            |SYSXPLAIN_RESULTSETS\nSYSSTAT                                            |SYSXPLAIN_STATEMENTS\n\n60 rows selected\n\n\n\n\n\n\nDetermining Whether a Table Is Replicated or Partitioned\n\n\nThe DATAPOLICY column specifies whether a table is replicated or partitioned, and whether a table is persisted to a disk store. For example:\n\n\nsnappy\n select TABLENAME, DATAPOLICY from SYS.SYSTABLES where TABLESCHEMANAME = 'APP';\nTABLENAME                                          |DATAPOLICY\n--------------------------------------------------------------------------\nSUPPLIER_1                                         |PERSISTENT_PARTITION\nSUPPLIER                                           |PERSISTENT_PARTITION\nAIRLINEREF                                         |PERSISTENT_REPLICATE\nAIRLINE                                            |PERSISTENT_REPLICATE\nSNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_       |PERSISTENT_PARTITION\nEMPLOYEE                                           |PERSISTENT_PARTITION\n\n6 rows selected\n\n\n\n\n\n\nDetermining How Persistent Data Is Stored\n\n\nFor persistent tables, you can also display the disk store that persists the table's data, and whether the table uses synchronous or asynchronous persistence:\n\n\nsnappy\n select TABLENAME, DISKATTRS from SYS.SYSTABLES where TABLESCHEMANAME = 'APP';\nTABLENAME                                          |DISKATTRS\n------------------------------------------------------------------------------------------------------------------------\nSUPPLIER_1                                         |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk\nSUPPLIER                                           |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk\nAIRLINEREF                                         |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk\nAIRLINE                                            |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk\nSNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_       |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk\nEMPLOYEE                                           |DiskStore is SNAPPY-INTERNAL-DELTA; Synchronous writes to disk\n\n6 rows selected\n\n\n\n\n\n\nDisplaying Eviction Settings\n\n\nUse the EVICTIONATTRS column to determine if a table uses eviction settings and whether a table is configured to overflow to disk. For example:\n\n\nsnappy\n select TABLENAME, EVICTIONATTRS  from SYS.SYSTABLES where TABLESCHEMANAME = 'APP';\nTABLENAME                                   |EVICTIONATTRS\n---------------------------------------------------------------------------------------------------------------------------\nSUPPLIER_1                                  | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nSUPPLIER                                    | algorithm=lru-entry-count; action=overflow-to-disk; maximum=3\nAIRLINEREF                                  | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nAIRLINE                                     | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nSNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_| algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nEMPLOYEE                                    | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\n\n6 rows selected", 
            "title": "Getting Information from SnappyData System Tables"
        }, 
        {
            "location": "/monitoring/monitor-manage/#getting-information-from-snappydata-system-tables", 
            "text": "You can monitor many common aspects of SnappyData by using SQL commands (system procedures and simple queries) to collect and analyze data in SnappyData system tables.", 
            "title": "Getting Information from SnappyData System Tables"
        }, 
        {
            "location": "/monitoring/monitor-manage/#distributed-system-membership-information", 
            "text": "The SYS.MEMBERS table provides information about all peers and servers that make up the SnappyData system. You can use different queries to obtain details about individual members and their role in the cluster.", 
            "title": "Distributed System Membership Information"
        }, 
        {
            "location": "/monitoring/monitor-manage/#determining-cluster-membership", 
            "text": "To display a list of all members that participate in a given cluster, simply query all ID entries in sys.members. For example:  snappy  select ID from SYS.MEMBERS;\nID\n-----------------------------------\n127.0.0.1(10889) v1 :43634\n127.0.0.1(11045) v2 :19283\n127.0.0.1(10749) ec v0 :51430\n\n3 rows selected  The number of rows returned corresponds to the total number of peers, servers, and locators in the cluster.  To determine each member's role in the system, include the KIND column in the query:  snappy  select ID, KIND from SYS.MEMBERS;\nID                                       |KIND\n-----------------------------------------------------------------------\n127.0.0.1(10889) v1 :43634               |datastore(normal)\n127.0.0.1(11045) v2 :19283               |accessor(normal)\n127.0.0.1(10749) ec v0 :51430           |locator(normal)\n\n3 rows selected  To view the members of cluster, query:  snappy  show members;\nID                            |HOST     |KIND             |STATUS |THRIFTSERVERS            |SERVERGROUPS\n--------------------------------------------------------------------------------------------------------- \n127.0.0.1(10749) ec v0 :51430|localhost|locator(normal)  |RUNNING|localhost/127.0.0.1[1527]|\n127.0.0.1(10889) v1 :43634    |localhost|datastore(normal)|RUNNING|localhost/127.0.0.1[1528]|\n127.0.0.1(11045) v2 :19283    |localhost|accessor(normal) |RUNNING|                         |IMPLICIT_LEADER_SERVERGROUP \n\n3 rows selected  Data store members host data in the cluster, while accessor members do not host data. This role is determined by the  host-data  boot property. If a cluster contains only a single data store, its KIND is listed as \"loner\".", 
            "title": "Determining Cluster Membership"
        }, 
        {
            "location": "/monitoring/monitor-manage/#table-and-data-storage-information", 
            "text": "The SYS.SYSTABLES table provides information about all tables that are created in the SnappyData system. You can use different queries to obtain details about tables and the server groups that host data for those tables.   Displaying a List of Tables  Determining Whether a Table Is Replicated or Partitioned  Determining How Persistent Data Is Stored  Displaying Eviction Settings  Displaying Indexes", 
            "title": "Table and Data Storage Information"
        }, 
        {
            "location": "/monitoring/monitor-manage/#displaying-a-list-of-tables", 
            "text": "To display a list of all tables in the cluster:  snappy  select TABLESCHEMANAME, TABLENAME from SYS.SYSTABLES order by TABLESCHEMANAME;\nTABLESCHEMANAME                                    |TABLENAME \n------------------------------------------------------------------------------------------------------------------------\nAPP                                                |SUPPLIER_1\nAPP                                                |SUPPLIER\nAPP                                                |AIRLINEREF\nAPP                                                |AIRLINE\nAPP                                                |SNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_\nAPP                                                |EMPLOYEE\nSNAPPY_HIVE_METASTORE                              |TBLS\nSNAPPY_HIVE_METASTORE                              |PARTITION_PARAMS\nSNAPPY_HIVE_METASTORE                              |SKEWED_COL_VALUE_LOC_MAP\nSNAPPY_HIVE_METASTORE                              |FUNCS\nSNAPPY_HIVE_METASTORE                              |SDS\nSNAPPY_HIVE_METASTORE                              |SERDE_PARAMS\nSNAPPY_HIVE_METASTORE                              |PART_COL_STATS\nSNAPPY_HIVE_METASTORE                              |SKEWED_STRING_LIST\nSNAPPY_HIVE_METASTORE                              |DBS\nSNAPPY_HIVE_METASTORE                              |PARTITIONS\nSNAPPY_HIVE_METASTORE                              |BUCKETING_COLS\nSNAPPY_HIVE_METASTORE                              |FUNC_RU\nSNAPPY_HIVE_METASTORE                              |SKEWED_VALUES\nSNAPPY_HIVE_METASTORE                              |ROLES\nSNAPPY_HIVE_METASTORE                              |SORT_COLS\nSNAPPY_HIVE_METASTORE                              |SD_PARAMS\nSNAPPY_HIVE_METASTORE                              |TAB_COL_STATS\nSNAPPY_HIVE_METASTORE                              |GLOBAL_PRIVS\nSNAPPY_HIVE_METASTORE                              |SKEWED_COL_NAMES\nSNAPPY_HIVE_METASTORE                              |SKEWED_STRING_LIST_VALUES\nSNAPPY_HIVE_METASTORE                              |VERSION\nSNAPPY_HIVE_METASTORE                              |CDS\nSNAPPY_HIVE_METASTORE                              |SEQUENCE_TABLE\nSNAPPY_HIVE_METASTORE                              |PARTITION_KEYS\nSNAPPY_HIVE_METASTORE                              |TABLE_PARAMS\nSNAPPY_HIVE_METASTORE                              |DATABASE_PARAMS\nSNAPPY_HIVE_METASTORE                              |COLUMNS_V2\nSNAPPY_HIVE_METASTORE                              |SERDES\nSNAPPY_HIVE_METASTORE                              |PARTITION_KEY_VALS\nSYS                                                |GATEWAYSENDERS\nSYS                                                |SYSSTATEMENTS\nSYS                                                |SYSKEYS\nSYS                                                |SYSROLES\nSYS                                                |SYSFILES\nSYS                                                |SYSROUTINEPERMS\nSYS                                                |SYSCONSTRAINTS\nSYS                                                |SYSCOLPERMS\nSYS                                                |SYSHDFSSTORES\nSYS                                                |SYSDEPENDS\nSYS                                                |SYSALIASES\nSYS                                                |SYSTABLEPERMS\nSYS                                                |SYSTABLES\nSYS                                                |SYSVIEWS\nSYS                                                |ASYNCEVENTLISTENERS\nSYS                                                |SYSCHECKS\nSYS                                                |SYSSTATISTICS\nSYS                                                |SYSCONGLOMERATES\nSYS                                                |GATEWAYRECEIVERS\nSYS                                                |SYSTRIGGERS\nSYS                                                |SYSDISKSTORES\nSYS                                                |SYSSCHEMAS\nSYS                                                |SYSFOREIGNKEYS\nSYS                                                |SYSCOLUMNS\nSYSIBM                                             |SYSDUMMY1\nSYSSTAT                                            |SYSXPLAIN_RESULTSETS\nSYSSTAT                                            |SYSXPLAIN_STATEMENTS\n\n60 rows selected", 
            "title": "Displaying a List of Tables"
        }, 
        {
            "location": "/monitoring/monitor-manage/#determining-whether-a-table-is-replicated-or-partitioned", 
            "text": "The DATAPOLICY column specifies whether a table is replicated or partitioned, and whether a table is persisted to a disk store. For example:  snappy  select TABLENAME, DATAPOLICY from SYS.SYSTABLES where TABLESCHEMANAME = 'APP';\nTABLENAME                                          |DATAPOLICY\n--------------------------------------------------------------------------\nSUPPLIER_1                                         |PERSISTENT_PARTITION\nSUPPLIER                                           |PERSISTENT_PARTITION\nAIRLINEREF                                         |PERSISTENT_REPLICATE\nAIRLINE                                            |PERSISTENT_REPLICATE\nSNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_       |PERSISTENT_PARTITION\nEMPLOYEE                                           |PERSISTENT_PARTITION\n\n6 rows selected", 
            "title": "Determining Whether a Table Is Replicated or Partitioned"
        }, 
        {
            "location": "/monitoring/monitor-manage/#determining-how-persistent-data-is-stored", 
            "text": "For persistent tables, you can also display the disk store that persists the table's data, and whether the table uses synchronous or asynchronous persistence:  snappy  select TABLENAME, DISKATTRS from SYS.SYSTABLES where TABLESCHEMANAME = 'APP';\nTABLENAME                                          |DISKATTRS\n------------------------------------------------------------------------------------------------------------------------\nSUPPLIER_1                                         |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk\nSUPPLIER                                           |DiskStore is GFXD-DEFAULT-DISKSTORE;Asynchronous writes to disk\nAIRLINEREF                                         |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk\nAIRLINE                                            |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk\nSNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_       |DiskStore is GFXD-DEFAULT-DISKSTORE; Synchronous writes to disk\nEMPLOYEE                                           |DiskStore is SNAPPY-INTERNAL-DELTA; Synchronous writes to disk\n\n6 rows selected", 
            "title": "Determining How Persistent Data Is Stored"
        }, 
        {
            "location": "/monitoring/monitor-manage/#displaying-eviction-settings", 
            "text": "Use the EVICTIONATTRS column to determine if a table uses eviction settings and whether a table is configured to overflow to disk. For example:  snappy  select TABLENAME, EVICTIONATTRS  from SYS.SYSTABLES where TABLESCHEMANAME = 'APP';\nTABLENAME                                   |EVICTIONATTRS\n---------------------------------------------------------------------------------------------------------------------------\nSUPPLIER_1                                  | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nSUPPLIER                                    | algorithm=lru-entry-count; action=overflow-to-disk; maximum=3\nAIRLINEREF                                  | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nAIRLINE                                     | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nSNAPPYSYS_INTERNAL____EMPLOYEE_COLUMN_STORE_| algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\nEMPLOYEE                                    | algorithm=lru-heap-percentage; action=overflow-to-disk; sizer=GfxdObjectSizer\n\n6 rows selected", 
            "title": "Displaying Eviction Settings"
        }, 
        {
            "location": "/security/security/", 
            "text": "Managing Security\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nIn the current release, SnappyData only supports LDAP authentication which allows users to authenticate against an existing LDAP directory service in your organization. LDAP (lightweight directory access protocol) provides an open directory access protocol that runs over TCP/IP. \nThis feature provides a secure way for users to use their existing login credentials (usernames and passwords) to access the cluster and data.\n\n\nSnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system. \n\n\n\n\nNote\n\n\n\n\n\n\nCurrently, only LDAP based authentication and authorization is supported\n\n\n\n\n\n\nThe user launching the cluster becomes the admin user of the cluster.\n\n\n\n\n\n\nAll members of the cluster (leads, locators, and servers) must be started by the same user\n\n\n\n\n\n\n\n\n\n\n\nRefer to \nUser Names for Authentication, Authorization, and Membership\n for more information on how user names are treated by each system.\n\n\n\n\n\n\nLaunching the Cluster in Secure Mode\n\n\n\n\n\n\nSpecifying Encrypted Passwords in Conf Files or in Client Connections\n\n\n\n\n\n\nAuthentication - Connecting to a Secure Cluster\n\n\n\n\n\n\nAuthorization\n\n\n\n\n\n\nImplementing Row Level Security\n\n\n\n\n\n\nConfiguring Network Encryption and Authentication using SSL\n\n\n\n\n\n\nSecuring SnappyData Pulse UI Connection\n\n\n\n\n\n\nUser Names for Authentication, Authorization, and Membership", 
            "title": "Managing Security"
        }, 
        {
            "location": "/security/security/#managing-security", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   In the current release, SnappyData only supports LDAP authentication which allows users to authenticate against an existing LDAP directory service in your organization. LDAP (lightweight directory access protocol) provides an open directory access protocol that runs over TCP/IP.  This feature provides a secure way for users to use their existing login credentials (usernames and passwords) to access the cluster and data.  SnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system.    Note    Currently, only LDAP based authentication and authorization is supported    The user launching the cluster becomes the admin user of the cluster.    All members of the cluster (leads, locators, and servers) must be started by the same user      Refer to  User Names for Authentication, Authorization, and Membership  for more information on how user names are treated by each system.    Launching the Cluster in Secure Mode    Specifying Encrypted Passwords in Conf Files or in Client Connections    Authentication - Connecting to a Secure Cluster    Authorization    Implementing Row Level Security    Configuring Network Encryption and Authentication using SSL    Securing SnappyData Pulse UI Connection    User Names for Authentication, Authorization, and Membership", 
            "title": "Managing Security"
        }, 
        {
            "location": "/security/launching_the_cluster_in_secure_mode/", 
            "text": "Launching the Cluster in Secure Mode\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nSnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system.\n\n\n \n\n\nAuthentication Properties\n\n\nTo enable LDAP authentication, set the following authentication properties in the \nconfiguration files\n \nconf/locators\n, \nconf/servers\n, and \nconf/leads\n files.\n\n\n\n\n\n\nauth-provider\n: The authentication provider. Set the \nauth-provider\n property to \nLDAP\n, to enable LDAP for authenticating all distributed system members as well as clients to the distributed system.\n\n\n\n\n\n\nserver-auth-provider\n: Peer-to-peer authentication of cluster members is configured in the SnappyData cluster. You can set \nserver-auth-provider\n property to \nNONE\n if you want to disable the peer-to-peer authentication.\n\n\n\n\n\n\nuser\n: The user name of the administrator starting the cluster\n\n\n\n\n\n\npassword\n: The password of the administrator starting the cluster\n\n\n\n\n\n\nJ-Dgemfirexd.auth-ldap-server\n: Set this property to the URL to the LDAP server.\n\n\n\n\n\n\nJ-Dgemfirexd.auth-ldap-search-base\n: Use this property to limit the search space used when SnappyData verifies a user login ID. Specify the name of the context or object to search, that is a parameter to \njavax.naming.directory.DirContext.search()\n. \n\n\n\n\n\n\nJ-Dgemfirexd.auth-ldap-search-dn\n: If the LDAP server does not allow anonymous binding (or if this functionality is disabled), specify the user distinguished name (DN) to use for binding to the LDAP server for searching.\n\n\n\n\n\n\nJ-Dgemfirexd.auth-ldap-search-pw\n: The password for the LDAP search user which is used for looking up the DN indicated by configuration parameter \nDgemfirexd.auth-ldap-search-dn\n. \n\n\n\n\n\n\nExample - Launching Locator in Secure Mode\n\n\nIn the below example, we are launching the locator in secure mode, which communicates with the LDAP server at localhost listening on port 389.\n\n\nlocalhost -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/  \\\n          -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-pw=user123\n\n\n\n\n\n\nNote\n\n\nYou must specify \ngemfirexd.auth-ldap-*\n properties as Java system properties by prefixing '-J-D'.\n\n\nIf you use SSL-encrypted LDAP and your LDAP server certificate is not recognized by a valid Certificate Authority (CA), you must create a local trust store for each SnappyData member and import the LDAP server certificate to the trust store. See the document on \nCreating a Keystore\n for more information.\n\n\nSpecify the javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword system properties when you start individual SnappyData members. For example:\n\n\nlocalhost -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/  \\\n    -J-Dgemfirexd.auth-ldap-server=ldaps://ldapserver:636/ -user=user_name -password=user_pwd \\\n        -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\\n        -J-Dgemfirexd.auth-ldap-search-pw=user123\n        -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\\n    -J-Djavax.net.ssl.trustStore=/Users/user1/snappydata/keystore_name \\\n    -J-Djavax.net.ssl.trustStorePassword=keystore_password\n\n\n\njavax.net.ssl.trustStore and javax.net.ssl.trustStorePassword must be specified as Java system properties (using the -J option on the Snappy shell).", 
            "title": "Launching the Cluster in Secure Mode"
        }, 
        {
            "location": "/security/launching_the_cluster_in_secure_mode/#launching-the-cluster-in-secure-mode", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   SnappyData uses mutual authentication between the SnappyData locator and subsequent SnappyData members that boot and join the distributed system.", 
            "title": "Launching the Cluster in Secure Mode"
        }, 
        {
            "location": "/security/launching_the_cluster_in_secure_mode/#authentication-properties", 
            "text": "To enable LDAP authentication, set the following authentication properties in the  configuration files   conf/locators ,  conf/servers , and  conf/leads  files.    auth-provider : The authentication provider. Set the  auth-provider  property to  LDAP , to enable LDAP for authenticating all distributed system members as well as clients to the distributed system.    server-auth-provider : Peer-to-peer authentication of cluster members is configured in the SnappyData cluster. You can set  server-auth-provider  property to  NONE  if you want to disable the peer-to-peer authentication.    user : The user name of the administrator starting the cluster    password : The password of the administrator starting the cluster    J-Dgemfirexd.auth-ldap-server : Set this property to the URL to the LDAP server.    J-Dgemfirexd.auth-ldap-search-base : Use this property to limit the search space used when SnappyData verifies a user login ID. Specify the name of the context or object to search, that is a parameter to  javax.naming.directory.DirContext.search() .     J-Dgemfirexd.auth-ldap-search-dn : If the LDAP server does not allow anonymous binding (or if this functionality is disabled), specify the user distinguished name (DN) to use for binding to the LDAP server for searching.    J-Dgemfirexd.auth-ldap-search-pw : The password for the LDAP search user which is used for looking up the DN indicated by configuration parameter  Dgemfirexd.auth-ldap-search-dn .", 
            "title": "Authentication Properties"
        }, 
        {
            "location": "/security/launching_the_cluster_in_secure_mode/#example-launching-locator-in-secure-mode", 
            "text": "In the below example, we are launching the locator in secure mode, which communicates with the LDAP server at localhost listening on port 389.  localhost -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/  \\\n          -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-pw=user123   Note  You must specify  gemfirexd.auth-ldap-*  properties as Java system properties by prefixing '-J-D'.  If you use SSL-encrypted LDAP and your LDAP server certificate is not recognized by a valid Certificate Authority (CA), you must create a local trust store for each SnappyData member and import the LDAP server certificate to the trust store. See the document on  Creating a Keystore  for more information.  Specify the javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword system properties when you start individual SnappyData members. For example:  localhost -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/  \\\n    -J-Dgemfirexd.auth-ldap-server=ldaps://ldapserver:636/ -user=user_name -password=user_pwd \\\n        -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\\n        -J-Dgemfirexd.auth-ldap-search-pw=user123\n        -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\\n    -J-Djavax.net.ssl.trustStore=/Users/user1/snappydata/keystore_name \\\n    -J-Djavax.net.ssl.trustStorePassword=keystore_password  javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword must be specified as Java system properties (using the -J option on the Snappy shell).", 
            "title": "Example - Launching Locator in Secure Mode"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/", 
            "text": "Specifying Encrypted Passwords in Conf Files or in Client Connections\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nSnappyData allows you to specify encrypted passwords, if you do not want to specify plain text passwords, in conf files or in JDBC/ODBC client connections URLs, while launching the SnappyData cluster in a secure mode.SnappyData provides a utility script called \nsnappy-encrypt-password.sh\n to generate encrypted passwords of system users who launch the cluster.  This script is located under \nsbin\n directory of SnappyData product installation. \n\n\nYou can generate encrypted passwords before starting the SnappyData cluster and use it in \nconf\n files of the SnappyData servers, locators and leads. \n\n\nYou can also generate encrypted passwords for JDBC/ODBC client connections when SnappyData cluster is already started. For that case, using the \nSYS.ENCRYPT_PASSWORD\n system procedure is a preferable approach as mentioned in the following note.\n\n\nThis script accepts list of users as input, prompts for each user\u2019s password and outputs the encrypted password on the console. A special user AUTH_LDAP_SEARCH_PW can be used to generate encrypted password for the LDAP search user, used for looking up the DN indicated by configuration parameter \n-J-Dgemfirexd.auth-ldap-search-pw \nin SnappyData \nconf\n files.\n\n\n\n\nNote\n\n\n\n\nMake sure that SnappyData system is not running when this script is run, because this script launches a locator using the parameters specified in user\u2019s conf/locators file. The script connects to the existing cluster to generate the password if locators could not be started. However, this may not work if the user is a LDAP search user who does not have permissions to access Snappydata cluster. It is recommended to directly connect to cluster and then run the SQL \nCALL SYS.ENCRYPT_PASSWORD('\nuser\n', '\npassword\n', 'AES', 0)\n,  if encrypted password is to be generated after you start the cluster.\n\n\nThe encrypted secret that is returned is specific to this particular SnappyData distributed system, because the system uses a unique private key to generate the secret. An obfuscated version of the private key is stored in the persistent data dictionary (SnappyData catalog). If  the existing data dictionary is ever deleted and recreated, then you must generate and use a new encrypted secret for use with the new distributed system. Also the encrypted password cannot be used in any other SnappyData installation, even if the user and password is the same. You need to generate the encrypted password separately for every other SnappyData installation.\n\n\n\n\n\n\nScript Usage\n\n\nsnappy-encrypt-password.sh \nuser1\n \nuser2\n ...\n\n\n\nExample Output\n\n\nIn the example output shown, \nsnappy-encrypt-password.sh\n script is invoked for users \nuser1\n and \nAUTH_LDAP_SEARCH_PW\n (special user used to indicate LDAP search user, used for looking up the DN). The script outputs encrypted password for \nuser1\n and \nAUTH_LDAP_SEARCH_PW\n.\n\n\n$ ./sbin/snappy-encrypt-password.sh user1 AUTH_LDAP_SEARCH_PW\nEnter password for user1: user123 (atual password is not shown on the console)\nRe-enter password for user1: user123 (atual password is not shown on the console)\nEnter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console)\nRe-enter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console)\nLogs generated in /home/xyz/\nsnappydata_install_dir\n/work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 2379 status: running\n  Distributed system now has 1 members.\n  Started DRDA server on: localhost/127.0.0.1[1527]\nSnappyData version 1.0.2.1 \nsnappy\n Using CONNECTION0\nsnappy\n ENCRYPTED_PASSWORD                                                                                                              \n--------------------------------------------------------------------------------------------------------------------------------\nuser1 = v13b607k2j611b8584423b2ea584c970fefd041f77f                                                                             \n\n1 row selected\nsnappy\n ENCRYPTED_PASSWORD                                                                                                              \n--------------------------------------------------------------------------------------------------------------------------------\nAUTH_LDAP_SEARCH_PW = v13b607k2j65384028e5090a8990e2ce17d43da3de9                                                               \n\n1 row selected\nsnappy\n \nThe SnappyData Locator on 172.16.62.1(localhost-locator-1) has stopped.\n\n\n\n\n\nStartup Configuration Examples\n\n\nYou can then use the above encrypted password in the configuration of servers, locators and leads.  You can either edit the \nconf\n files or use the environment variables for startup options of locators, servers, and leads.\n\n\nConfiguring by Editing conf Files\n\n\nFollowing is an example of the \nconf/locators\n file where instead of plain text, encrypted passwords are used.  Similar change must be done to \nconf/servers\n and \nconf/leads\n files.\n\n\n$ cat conf/locators\nlocalhost -auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j611b8584423b2ea584c970fefd041f77f -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j65384028e5090a8990e2ce17d43da3de9\n\n\n\n\n\nYou can then start the SnappyData system.\n\n\nConfiguring with Environment Variables for Startup Options\n\n\nAn alternate way to specify the security options is by using the environment variable for  \nLOCATOR_STARTUP_OPTIONS\n, \nSERVER_STARTUP_OPTIONS\n and \nLEAD_STARTUP_OPTIONS\n for locators, servers, and leads respectively. These startup variables can be specified in \nconf/spark-env.sh\n file. This file is sourced when starting the SnappyData system. A template file (\nconf/spark-env.sh.template\n) is provided in the \nconf\n directory for reference. You can copy this file and use it to configure security options. \n\n\nFor example:\n\n\n# create a spark-env.sh from the template file\n$cp conf/spark-env.sh.template conf/spark-env.sh \n\n# edit the conf/spark-env.sh file to add security configuration as shown below\n\nSECURITY_ARGS=\n-auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j637b2ae24e60be46613391117b7f234d0 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j6174404428eee3374411d97d1d497d3b8\n\n\nLOCATOR_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\nSERVER_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\nLEAD_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\n\n\n\n\n\nUsing Encrypted Password in Client Connections\n\n\nYou can generate encrypted password for use in JDBC/ODBC client connections by executing a system procedure with a statement such as \nCALL SYS.ENCRYPT_PASSWORD('\nuser\n', '\npassword\n', 'AES', 0)\n. \n\n\nFor example: \n\n\nsnappy\n call sys.encrypt_password('user2', 'user2123', 'AES', 0);\nENCRYPTED_PASSWORD                                                                                                              \n--------------------------------------------------------------------------------------------------------------------------------\nuser2 = v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610                                                                             \n\n1 row selected\n\n\n\n\n\nThis procedure accepts user id and plain text password as input arguments and outputs the encrypted password. \n\n\n\n\nNote\n\n\nIn the current release of SnappyData, the last two parameters should be \u201cAES\u201d and \u201c0\u201d. \n\n\n\n\nThe encrypted password can be used in JDBC/ODBC client connections.  \n\n\nFor example using snappy shell:\n\n\nsnappy\n connect client 'localhost:1527;user=user2;password=v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610';\nUsing CONNECTION1", 
            "title": "Specifying Encrypted Passwords in Conf Files or in Client Connections"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/#specifying-encrypted-passwords-in-conf-files-or-in-client-connections", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   SnappyData allows you to specify encrypted passwords, if you do not want to specify plain text passwords, in conf files or in JDBC/ODBC client connections URLs, while launching the SnappyData cluster in a secure mode.SnappyData provides a utility script called  snappy-encrypt-password.sh  to generate encrypted passwords of system users who launch the cluster.  This script is located under  sbin  directory of SnappyData product installation.   You can generate encrypted passwords before starting the SnappyData cluster and use it in  conf  files of the SnappyData servers, locators and leads.   You can also generate encrypted passwords for JDBC/ODBC client connections when SnappyData cluster is already started. For that case, using the  SYS.ENCRYPT_PASSWORD  system procedure is a preferable approach as mentioned in the following note.  This script accepts list of users as input, prompts for each user\u2019s password and outputs the encrypted password on the console. A special user AUTH_LDAP_SEARCH_PW can be used to generate encrypted password for the LDAP search user, used for looking up the DN indicated by configuration parameter  -J-Dgemfirexd.auth-ldap-search-pw  in SnappyData  conf  files.   Note   Make sure that SnappyData system is not running when this script is run, because this script launches a locator using the parameters specified in user\u2019s conf/locators file. The script connects to the existing cluster to generate the password if locators could not be started. However, this may not work if the user is a LDAP search user who does not have permissions to access Snappydata cluster. It is recommended to directly connect to cluster and then run the SQL  CALL SYS.ENCRYPT_PASSWORD(' user ', ' password ', 'AES', 0) ,  if encrypted password is to be generated after you start the cluster.  The encrypted secret that is returned is specific to this particular SnappyData distributed system, because the system uses a unique private key to generate the secret. An obfuscated version of the private key is stored in the persistent data dictionary (SnappyData catalog). If  the existing data dictionary is ever deleted and recreated, then you must generate and use a new encrypted secret for use with the new distributed system. Also the encrypted password cannot be used in any other SnappyData installation, even if the user and password is the same. You need to generate the encrypted password separately for every other SnappyData installation.", 
            "title": "Specifying Encrypted Passwords in Conf Files or in Client Connections"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/#script-usage", 
            "text": "snappy-encrypt-password.sh  user1   user2  ...", 
            "title": "Script Usage"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/#example-output", 
            "text": "In the example output shown,  snappy-encrypt-password.sh  script is invoked for users  user1  and  AUTH_LDAP_SEARCH_PW  (special user used to indicate LDAP search user, used for looking up the DN). The script outputs encrypted password for  user1  and  AUTH_LDAP_SEARCH_PW .  $ ./sbin/snappy-encrypt-password.sh user1 AUTH_LDAP_SEARCH_PW\nEnter password for user1: user123 (atual password is not shown on the console)\nRe-enter password for user1: user123 (atual password is not shown on the console)\nEnter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console)\nRe-enter password for AUTH_LDAP_SEARCH_PW: admin123 (atual password is not shown on the console)\nLogs generated in /home/xyz/ snappydata_install_dir /work/localhost-locator-1/snappylocator.log\nSnappyData Locator pid: 2379 status: running\n  Distributed system now has 1 members.\n  Started DRDA server on: localhost/127.0.0.1[1527]\nSnappyData version 1.0.2.1 \nsnappy  Using CONNECTION0\nsnappy  ENCRYPTED_PASSWORD                                                                                                              \n--------------------------------------------------------------------------------------------------------------------------------\nuser1 = v13b607k2j611b8584423b2ea584c970fefd041f77f                                                                             \n\n1 row selected\nsnappy  ENCRYPTED_PASSWORD                                                                                                              \n--------------------------------------------------------------------------------------------------------------------------------\nAUTH_LDAP_SEARCH_PW = v13b607k2j65384028e5090a8990e2ce17d43da3de9                                                               \n\n1 row selected\nsnappy  \nThe SnappyData Locator on 172.16.62.1(localhost-locator-1) has stopped.", 
            "title": "Example Output"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/#startup-configuration-examples", 
            "text": "You can then use the above encrypted password in the configuration of servers, locators and leads.  You can either edit the  conf  files or use the environment variables for startup options of locators, servers, and leads.", 
            "title": "Startup Configuration Examples"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/#configuring-by-editing-conf-files", 
            "text": "Following is an example of the  conf/locators  file where instead of plain text, encrypted passwords are used.  Similar change must be done to  conf/servers  and  conf/leads  files.  $ cat conf/locators\nlocalhost -auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j611b8584423b2ea584c970fefd041f77f -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j65384028e5090a8990e2ce17d43da3de9  You can then start the SnappyData system.", 
            "title": "Configuring by Editing conf Files"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/#configuring-with-environment-variables-for-startup-options", 
            "text": "An alternate way to specify the security options is by using the environment variable for   LOCATOR_STARTUP_OPTIONS ,  SERVER_STARTUP_OPTIONS  and  LEAD_STARTUP_OPTIONS  for locators, servers, and leads respectively. These startup variables can be specified in  conf/spark-env.sh  file. This file is sourced when starting the SnappyData system. A template file ( conf/spark-env.sh.template ) is provided in the  conf  directory for reference. You can copy this file and use it to configure security options.   For example:  # create a spark-env.sh from the template file\n$cp conf/spark-env.sh.template conf/spark-env.sh \n\n# edit the conf/spark-env.sh file to add security configuration as shown below\n\nSECURITY_ARGS= -auth-provider=LDAP -J-Dgemfirexd.auth-ldap-server=ldap://192.168.1.162:389/ -user=user1 -password=v13b607k2j637b2ae24e60be46613391117b7f234d0 -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=v13b607k2j6174404428eee3374411d97d1d497d3b8 \n\nLOCATOR_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\nSERVER_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d\nLEAD_STARTUP_OPTIONS=\u201d$SECURITY_ARGS\u201d", 
            "title": "Configuring with Environment Variables for Startup Options"
        }, 
        {
            "location": "/security/specify_encrypt_passwords_conf_client/#using-encrypted-password-in-client-connections", 
            "text": "You can generate encrypted password for use in JDBC/ODBC client connections by executing a system procedure with a statement such as  CALL SYS.ENCRYPT_PASSWORD(' user ', ' password ', 'AES', 0) .   For example:   snappy  call sys.encrypt_password('user2', 'user2123', 'AES', 0);\nENCRYPTED_PASSWORD                                                                                                              \n--------------------------------------------------------------------------------------------------------------------------------\nuser2 = v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610                                                                             \n\n1 row selected  This procedure accepts user id and plain text password as input arguments and outputs the encrypted password.    Note  In the current release of SnappyData, the last two parameters should be \u201cAES\u201d and \u201c0\u201d.    The encrypted password can be used in JDBC/ODBC client connections.    For example using snappy shell:  snappy  connect client 'localhost:1527;user=user2;password=v13b607k2j6c519cc88605e5e8fa778f46fb8b2b610';\nUsing CONNECTION1", 
            "title": "Using Encrypted Password in Client Connections"
        }, 
        {
            "location": "/security/authentication_connecting_to_a_secure_cluster/", 
            "text": "Authentication - Connecting to a Secure Cluster\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nAuthentication is the process of verifying someone's identity. When a user tries to log in, that request is forwarded to the specified LDAP directory to verify if the credentials are correct.\nThere are a few different ways to connect to a secure cluster using either JDBC (Thin) Client, Smart Connector Mode and Snappy Jobs. Accessing a secure cluster requires users to provide their user credentials.\n\n\nUsing JDBC (Thin) Client\n\n\nWhen using the JDBC client, provide the user credentials using connection properties 'user' and 'password'.\n\n\nExample: JDBC Client\n\n\nval props = new Properties()\nprops.setProperty(\nuser\n, username);\nprops.setProperty(\npassword\n, password);\n\nval url: String = s\njdbc:snappydata://localhost:1527/\n\nval conn = DriverManager.getConnection(url, props)\n\n\n\n\nExample: Snappy shell\n\n\nconnect client 'localhost:1527;user=user1;password=user123';\n\n\n\n\nFor more information, refer \nHow to connect using JDBC driver\n.\n\n\nUsing ODBC Driver\n\n\nYou can also connect to the SnappyData Cluster using SnappyData ODBC Driver using the following command:\n\n\nDriver=SnappyData ODBC Driver;server=\nServerHost\n;port=\nServerPort\n;user=\nuserName\n;password=\npassword\n\n\n\n\n\nFor more information refer to, \nHow to Connect using ODBC Driver\n.\n\n\nUsing Smart Connector Mode\n\n\nIn Smart Connector mode, provide the user credentials as Spark configuration properties named \nspark.snappydata.store.user\n and \nspark.snappydata.store.password\n.\n\n\nExample\n \nIn the below example, these properties are set in the \nSparkConf\n which is used to create \nSnappyContext\n in your job.\n\n\nval conf = new SparkConf()\n    .setAppName(\nMy Spark Application with SnappyData\n)\n    .setMaster(s\nspark://$hostName:7077\n)\n    .set(\nspark.executor.cores\n, TestUtils.defaultCores.toString)\n    .set(\nspark.executor.extraClassPath\n,\n      getEnvironmentVariable(\nSNAPPY_HOME\n) + \n/jars/*\n )\n    .set(\nsnappydata.connection\n, snappydataLocatorURL)\n    .set(\nspark.snappydata.store.user\n, username)\n    .set(\nspark.snappydata.store.password\n, password)\nval sc = SparkContext.getOrCreate(conf)\nval snc = SnappyContext(sc)\n\n\n\n\nExample\n \nThe below example demonstrates how to connect to the cluster via Spark shell using the \n--conf\n option to specify the properties.\n\n\n$./bin/spark-shell  \n    --master local[*] \n    --conf spark.snappydata.connection=localhost:1527 \n    --conf spark.snappydata.store.user=user1\n    --conf spark.snappydata.store.password=user123\n\n\n\n\nExample\n:\n\nAlternatively, you can specify the user credentials in the Spark conf file. \n Spark reads these properties when you lauch the spark-shell or invoke spark-submit. \nTo do so, specify the user credentials in the \nspark-defaults.conf\n file, located in the \nconf\n directory.\n\n\nIn this file, you can specify:\n\n\nspark.snappydata.store.user     \nusername\n\nspark.snappydata.store.password \npassword\n\n\n\n\n\nUsing Snappy Jobs\n\n\nWhen submitting Snappy jobs using \nsnappy-job.sh\n, provide user credentials through a configuration file using the option \n--passfile\n. \n\n\nFor example, a sample configuration file is provided below: \n\n\n$ cat /home/user1/snappy/job.config \n-u user1:password\n\n\n\n\nIn the below example, the above configuration file is passed when submitting a job.\n\n\n$./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\\n    --passfile /home/user1/snappy/job.config\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nWhen checking the status of a job using \nsnappyjob.sh status --jobid\n, provide user credentials through a configuration file using the option \n--passfile\n\n\n\n\n\n\nOnly trusted users should be allowed to submit jobs, as an untrusted user may be able to do harm through jobs by invoking internal APIs which can bypass the authorization checks. \n\n\n\n\n\n\nCurrently, SparkJobServer UI may not be accessible when security is enabled, but you can use the \nsnappy-job.sh\n script to access any information required using commands like \nstatus\n, \nlistcontexts\n, etc. \n Execute \n./bin/snappy-job.sh\n for more details.\n\n\n\n\n\n\nThe configuration file should be in a secure location with read access only to an authorized user.\n\n\n\n\n\n\nThese user credentials are passed to the Snappy instance available in the job, and it will be used to authorize the operations in the job.", 
            "title": "Authentication - Connecting to a Secure Cluster"
        }, 
        {
            "location": "/security/authentication_connecting_to_a_secure_cluster/#authentication-connecting-to-a-secure-cluster", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   Authentication is the process of verifying someone's identity. When a user tries to log in, that request is forwarded to the specified LDAP directory to verify if the credentials are correct.\nThere are a few different ways to connect to a secure cluster using either JDBC (Thin) Client, Smart Connector Mode and Snappy Jobs. Accessing a secure cluster requires users to provide their user credentials.", 
            "title": "Authentication - Connecting to a Secure Cluster"
        }, 
        {
            "location": "/security/authentication_connecting_to_a_secure_cluster/#using-jdbc-thin-client", 
            "text": "When using the JDBC client, provide the user credentials using connection properties 'user' and 'password'.  Example: JDBC Client  val props = new Properties()\nprops.setProperty( user , username);\nprops.setProperty( password , password);\n\nval url: String = s jdbc:snappydata://localhost:1527/ \nval conn = DriverManager.getConnection(url, props)  Example: Snappy shell  connect client 'localhost:1527;user=user1;password=user123';  For more information, refer  How to connect using JDBC driver .", 
            "title": "Using JDBC (Thin) Client"
        }, 
        {
            "location": "/security/authentication_connecting_to_a_secure_cluster/#using-odbc-driver", 
            "text": "You can also connect to the SnappyData Cluster using SnappyData ODBC Driver using the following command:  Driver=SnappyData ODBC Driver;server= ServerHost ;port= ServerPort ;user= userName ;password= password   For more information refer to,  How to Connect using ODBC Driver .", 
            "title": "Using ODBC Driver"
        }, 
        {
            "location": "/security/authentication_connecting_to_a_secure_cluster/#using-smart-connector-mode", 
            "text": "In Smart Connector mode, provide the user credentials as Spark configuration properties named  spark.snappydata.store.user  and  spark.snappydata.store.password .  Example  \nIn the below example, these properties are set in the  SparkConf  which is used to create  SnappyContext  in your job.  val conf = new SparkConf()\n    .setAppName( My Spark Application with SnappyData )\n    .setMaster(s spark://$hostName:7077 )\n    .set( spark.executor.cores , TestUtils.defaultCores.toString)\n    .set( spark.executor.extraClassPath ,\n      getEnvironmentVariable( SNAPPY_HOME ) +  /jars/*  )\n    .set( snappydata.connection , snappydataLocatorURL)\n    .set( spark.snappydata.store.user , username)\n    .set( spark.snappydata.store.password , password)\nval sc = SparkContext.getOrCreate(conf)\nval snc = SnappyContext(sc)  Example  \nThe below example demonstrates how to connect to the cluster via Spark shell using the  --conf  option to specify the properties.  $./bin/spark-shell  \n    --master local[*] \n    --conf spark.snappydata.connection=localhost:1527 \n    --conf spark.snappydata.store.user=user1\n    --conf spark.snappydata.store.password=user123  Example : \nAlternatively, you can specify the user credentials in the Spark conf file.   Spark reads these properties when you lauch the spark-shell or invoke spark-submit. \nTo do so, specify the user credentials in the  spark-defaults.conf  file, located in the  conf  directory.  In this file, you can specify:  spark.snappydata.store.user      username \nspark.snappydata.store.password  password", 
            "title": "Using Smart Connector Mode"
        }, 
        {
            "location": "/security/authentication_connecting_to_a_secure_cluster/#using-snappy-jobs", 
            "text": "When submitting Snappy jobs using  snappy-job.sh , provide user credentials through a configuration file using the option  --passfile .   For example, a sample configuration file is provided below:   $ cat /home/user1/snappy/job.config \n-u user1:password  In the below example, the above configuration file is passed when submitting a job.  $./bin/snappy-job.sh submit  \\\n    --lead localhost:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart.jar \\\n    --passfile /home/user1/snappy/job.config   Note    When checking the status of a job using  snappyjob.sh status --jobid , provide user credentials through a configuration file using the option  --passfile    Only trusted users should be allowed to submit jobs, as an untrusted user may be able to do harm through jobs by invoking internal APIs which can bypass the authorization checks.     Currently, SparkJobServer UI may not be accessible when security is enabled, but you can use the  snappy-job.sh  script to access any information required using commands like  status ,  listcontexts , etc.   Execute  ./bin/snappy-job.sh  for more details.    The configuration file should be in a secure location with read access only to an authorized user.    These user credentials are passed to the Snappy instance available in the job, and it will be used to authorize the operations in the job.", 
            "title": "Using Snappy Jobs"
        }, 
        {
            "location": "/security/authorization/", 
            "text": "Authorization\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nAuthorization is the process of determining what access permissions the authenticated user has. Users are authorized to perform tasks based on their role assignments. SnappyData also supports LDAP group authorization.\n\n\nThe administrator can manage user permissions in a secure cluster using the \nGRANT\n and \nREVOKE\n SQL statements which allow you to set permission for the user for specific database objects or for specific SQL actions. \n\n\nThe \nGRANT\n statement is used to grant specific permissions to users. The \nREVOKE\n statement is used to revoke permissions.\n\n\n\n\nNote\n\n\n\n\n\n\nA user requiring \nINSERT\n, \nUPDATE\n or \nDELETE\n permissions may also require explicit \nSELECT\n permission on a table\n\n\n\n\n\n\nOnly administrators can execute built-in procedures (like INSTALL-JAR)\n\n\n\n\n\n\n\n\nAdding Restrictions in Default Schema\n\n\nUsers in SnappyData cluster have their own schema by default when they log into the cluster. They have full access within this schema.\nBut in some cases, cluster administrators may need to ensure controlled use of the cluster resources by its users and may need to enforce restrictions on them.\n\n\nThis can be achieved by setting the system property \nsnappydata.RESTRICT_TABLE_CREATION\n to true in \nconf\n files at the time of starting the cluster.\nThis forbids the users to create tables in their default schema. Users also cannot execute queries on tables in the schema.\n\n\nAdministrators, however, can explicitly grant permissions to these users on their respective default schemas using GRANT command. The default value of the property is false.\n\n\nYou need to prefix \n-J-D\n to the property name while specifying it in the conf files (locators, leads, and servers).\n\n\n$ cat conf/servers\nlocalhost -auth-provider=LDAP -J-Dsnappydata.RESTRICT_TABLE_CREATION=true -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/  \\\n          -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-pw=user123\n\n\n\n\nLDAP Groups in SnappyData Authorization\n\n\nSnappyData extends the SQL GRANT statement to support LDAP Group names as Grantees.\n\n\nHere is an example SQL to grant privileges to individual users:\n\n\nGRANT SELECT ON TABLE t TO sam,bob;\n\n\n\n\nYou can also grant privileges to LDAP groups using the following syntax:\n\n\nGRANT SELECT ON Table t TO ldapGroup:\ngroupName\n, bob;\nGRANT INSERT ON Table t TO ldapGroup:\ngroupName\n, bob;\n\n\n\n\nSnappyData fetches the current list of members for the LDAP Group and grants each member privileges individually (stored in SnappyData). \n\nSimilarly, when a REVOKE SQL statement is executed SnappyData removes the privileges individually for all members that make up a group. To support changes to Group membership within the LDAP Server, there is an additional System procedure to refresh the privileges recorded in SnappyData.\n\n\nCALL SYS.REFRESH_LDAP_GROUP('\nGROUP NAME\n');\n\n\n\n\nThis step has to be performed manually by admin when relevant LDAP groups change on the server.\n\n\nTo optimize searching for groups in the LDAP server the following optional properties can be specified. These are similar to the current ones used for authentication: \ngemfirexd.auth-ldap-search-base\n and \ngemfirexd.auth-ldap-search-filter\n. The support for LDAP groups requires using LDAP as also the authentication mechanism.\n\n\ngemfirexd.group-ldap-search-base\n// base to identify objects of type group\ngemfirexd.group-ldap-search-filter\n// any additional search filter for groups\ngemfirexd.group-ldap-member-attributes\n//attributes specifying the list of members\n\n\n\n\nIf no \ngemfirexd.group-ldap-search-base\n property has been provided then the one used for authentication \ngemfirexd.auth-ldap-search-base\n is used. \n\nIf no search filter is specified then SnappyData uses the standard objectClass groupOfMembers (rfc2307) or groupOfNames with attribute as member, or objectClass groupOfUniqueMembers with attribute as uniqueMember.\nTo be precise, the default search filter is:\n\n\n(\n(|(objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfMembers)\n  (objectClass=groupOfUniqueNames))(|(cn=%GROUP%)(name=%GROUP%)))\n\n\n\n\nThe token \"%GROUP%\" is replaced by the actual group name in the search pattern. A custom search filter should use the same as a placeholder, for the group name. The default member attribute list is member, uniqueMember. The LDAP group resolution is recursive, meaning a group can refer to another group (see example below). There is no detection for broken LDAP group definitions having a cycle of group references and such a situation leads to a failure in GRANT or REFRESH_LDAP_GROUP with StackOverflowError.\n\n\nAn LDAP group entry can look like below:\n\n\ndn: cn=group1,ou=group,dc=example,dc=com\nobjectClass: groupOfNames\ncn: group1\ngidNumber: 1001\nmember: uid=user1,ou=group,dc=example,dc=com\nmember: uid=user2,ou=group,dc=example,dc=com\nmember: cn=group11,ou=group,dc=example,dc=com\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nThere is NO multi-group support for users yet, so if a user has been granted access by two LDAP groups only the first one will take effect.\n\n\n\n\n\n\nIf a user belongs to LDAP group as well as granted permissions separately as a user, then the latter is given precedence. So even if LDAP group permission is later revoked (or user is removed from LDAP group), the user will continue to have permissions unless explicitly revoked as a user.\n\n\n\n\n\n\nLDAPGROUP is now a reserved word, so cannot be used for a user name.", 
            "title": "Authorization"
        }, 
        {
            "location": "/security/authorization/#authorization", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   Authorization is the process of determining what access permissions the authenticated user has. Users are authorized to perform tasks based on their role assignments. SnappyData also supports LDAP group authorization.  The administrator can manage user permissions in a secure cluster using the  GRANT  and  REVOKE  SQL statements which allow you to set permission for the user for specific database objects or for specific SQL actions.   The  GRANT  statement is used to grant specific permissions to users. The  REVOKE  statement is used to revoke permissions.   Note    A user requiring  INSERT ,  UPDATE  or  DELETE  permissions may also require explicit  SELECT  permission on a table    Only administrators can execute built-in procedures (like INSTALL-JAR)", 
            "title": "Authorization"
        }, 
        {
            "location": "/security/authorization/#adding-restrictions-in-default-schema", 
            "text": "Users in SnappyData cluster have their own schema by default when they log into the cluster. They have full access within this schema.\nBut in some cases, cluster administrators may need to ensure controlled use of the cluster resources by its users and may need to enforce restrictions on them.  This can be achieved by setting the system property  snappydata.RESTRICT_TABLE_CREATION  to true in  conf  files at the time of starting the cluster.\nThis forbids the users to create tables in their default schema. Users also cannot execute queries on tables in the schema.  Administrators, however, can explicitly grant permissions to these users on their respective default schemas using GRANT command. The default value of the property is false.  You need to prefix  -J-D  to the property name while specifying it in the conf files (locators, leads, and servers).  $ cat conf/servers\nlocalhost -auth-provider=LDAP -J-Dsnappydata.RESTRICT_TABLE_CREATION=true -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/  \\\n          -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com \\\n          -J-Dgemfirexd.auth-ldap-search-pw=user123", 
            "title": "Adding Restrictions in Default Schema"
        }, 
        {
            "location": "/security/authorization/#ldap-groups-in-snappydata-authorization", 
            "text": "SnappyData extends the SQL GRANT statement to support LDAP Group names as Grantees.  Here is an example SQL to grant privileges to individual users:  GRANT SELECT ON TABLE t TO sam,bob;  You can also grant privileges to LDAP groups using the following syntax:  GRANT SELECT ON Table t TO ldapGroup: groupName , bob;\nGRANT INSERT ON Table t TO ldapGroup: groupName , bob;  SnappyData fetches the current list of members for the LDAP Group and grants each member privileges individually (stored in SnappyData).  \nSimilarly, when a REVOKE SQL statement is executed SnappyData removes the privileges individually for all members that make up a group. To support changes to Group membership within the LDAP Server, there is an additional System procedure to refresh the privileges recorded in SnappyData.  CALL SYS.REFRESH_LDAP_GROUP(' GROUP NAME ');  This step has to be performed manually by admin when relevant LDAP groups change on the server.  To optimize searching for groups in the LDAP server the following optional properties can be specified. These are similar to the current ones used for authentication:  gemfirexd.auth-ldap-search-base  and  gemfirexd.auth-ldap-search-filter . The support for LDAP groups requires using LDAP as also the authentication mechanism.  gemfirexd.group-ldap-search-base\n// base to identify objects of type group\ngemfirexd.group-ldap-search-filter\n// any additional search filter for groups\ngemfirexd.group-ldap-member-attributes\n//attributes specifying the list of members  If no  gemfirexd.group-ldap-search-base  property has been provided then the one used for authentication  gemfirexd.auth-ldap-search-base  is used.  \nIf no search filter is specified then SnappyData uses the standard objectClass groupOfMembers (rfc2307) or groupOfNames with attribute as member, or objectClass groupOfUniqueMembers with attribute as uniqueMember.\nTo be precise, the default search filter is:  ( (|(objectClass=group)(objectClass=groupOfNames)(objectClass=groupOfMembers)\n  (objectClass=groupOfUniqueNames))(|(cn=%GROUP%)(name=%GROUP%)))  The token \"%GROUP%\" is replaced by the actual group name in the search pattern. A custom search filter should use the same as a placeholder, for the group name. The default member attribute list is member, uniqueMember. The LDAP group resolution is recursive, meaning a group can refer to another group (see example below). There is no detection for broken LDAP group definitions having a cycle of group references and such a situation leads to a failure in GRANT or REFRESH_LDAP_GROUP with StackOverflowError.  An LDAP group entry can look like below:  dn: cn=group1,ou=group,dc=example,dc=com\nobjectClass: groupOfNames\ncn: group1\ngidNumber: 1001\nmember: uid=user1,ou=group,dc=example,dc=com\nmember: uid=user2,ou=group,dc=example,dc=com\nmember: cn=group11,ou=group,dc=example,dc=com   Note    There is NO multi-group support for users yet, so if a user has been granted access by two LDAP groups only the first one will take effect.    If a user belongs to LDAP group as well as granted permissions separately as a user, then the latter is given precedence. So even if LDAP group permission is later revoked (or user is removed from LDAP group), the user will continue to have permissions unless explicitly revoked as a user.    LDAPGROUP is now a reserved word, so cannot be used for a user name.", 
            "title": "LDAP Groups in SnappyData Authorization"
        }, 
        {
            "location": "/security/row_level_security/", 
            "text": "Implementing  Row Level Security\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nThe following topics are covered in this section:\n\n\n\n\nOverview of Row Level Security\n\n\nActivating Row Level Security\n\n\nCreating a Policy\n\n\nEnabling Row Level Security\n\n\nViewing Policy Details\n\n\nCombining Multiple Policies\n\n\nPolicy Application on Join Queries\n\n\nDropping a Policy\n\n\n\n\n \n\n\nOverview of Row Level Security\n\n\nPolicy is a rule that is implemented by using a filter expression.  In SnappyData, you can apply security policies to a table at row level that can restrict, on a per-user basis, the rows that must be returned for normal queries by data modification commands. \nFor \nactivating this row level security\n, a system property must be added to the configuration files of servers, leads, and locators. \nTo restrict the permissions of a user at row level, \ncreate a simple policy\n for a table that can be applied on a per user basis and then \nenable the row level security\n for that table.\n\n\n \n\n\nActivating Row Level Security\n\n\nFor activating Row Level Security, a system property \n-J-Dsnappydata.enable-rls=true\n must be added to the configuration files of servers, leads, and locators when you \nconfigure the cluster\n. By default this is off.\nIf this property is not added, you cannot enable the Row Level Security and an exception is thrown when you attempt to create the policy.\n\n\n\n\nWarning\n\n\nWhen this property is set to \ntrue\n, the Smart Connector access to SnappyData will fail with \njava.lang.IllegalStateException: Row level security (snappydata.enable-rls) does not allow smart connector mode\n exception.\n\n\n\n\n \n\n\nCreating a Policy\n\n\nA policy, which is a rule, can be created for row tables, column tables, and external GemFire tables.  Permissions can be granted to users on DML ops to access the tables for which policies are created.\n\n\n\n\nNote\n\n\nPolicies are restrictive by default.\n\n\n\n\nHere in the following example, a table named \nclients\n is \ncreated\n and access permissions are \ngranted\n to three users:\n\n\n# Create table named clients.\nCREATE TABLE clients (\n    ID INT PRIMARY KEY,\n    ACCOUNT_NAME STRING NOT NULL,\n    ACCOUNT_MANAGER STRING NOT NULL\n) USING ROW OPTIONS();\n\n# Grant access to three users.\nGRANT SELECT ON clients TO tom, harris, greg;\n\n\n\n\nInitially all the users can view all the records in the table. You can restrict the permissions for viewing the records,  only after you enable the \nrow level security\n and apply a policy. For example, you are  connected to database \ntestrls\n as user \ntom\n. Initially user \ntom\n can view all the records from the table as shown:\n\n\n$ SELECT * FROM clients;\n id | account_name | account_manager\n----+--------------+-----------------\n  1 | ABC          | tom\n  2 | PQR          | harris\n  3 | XYZ          | greg\n\n(3 rows)\n\n\n\n\nYou can create a policy for a table which can be applied to a user or a LDAP group using the following syntax. \n\n\nCREATE POLICY name ON table_name\n     FOR SELECT\n     [ TO { LDAP GROUP | CURRENT_USER | \nUSER_NAME\n } [, ...] ]\n    [ USING ( using_expression ) ]\n\n\n\n\n\n\nNote\n\n\nCURRENT_USER implies to any user who is excecuting the query. \n\n\n\n\nIn the following example, we create a policy named \njust_own_clients\n where a user can view only the row where that user is the account manager.\nFor example, if we want this rule to apply only to user \nTom\n, then we can create the policy as shown:\n\n\nCREATE POLICY just_own_clients ON clients\n    FOR SELECT\n    TO TOM\n    USING ACCOUNT_MANAGER = CURRENT_USER();\n\n\n\n\nAs per the the above policy, user \nTom\n can see only one row, where as other users can view all the rows.\n\n\nThe same can be also applied to a LDAP group as shown:\n\n\nCREATE POLICY just_own_clients ON clients\n    FOR SELECT\n    TO ldapgroup:group1\n    USING ACCOUNT_MANAGER = CURRENT_USER();\n\n\n\n\nAfter the row level security policy is enabled, the policy gets applied to the corresponding users.\n\n\n \n\n\nEnabling Row Level Security\n\n\nThe policy which is created to restrict row level permissions is effective only after you enable the row level security. To enable row level security, execute the following \nALTER DDL\n command:\n\n\nALTER TABLE \ntable_name\n ENABLE ROW LEVEL SECURITY;\n\n\n\n\nFor example, \n\n\nALTER TABLE clients ENABLE ROW LEVEL SECURITY;\n\n\n\n\nNow the users are permitted to view the records of only those rows that are permitted on the basis of the policy. For example, user \ntom\n is allowed to view the records of a specific row in table \nclients\n.\n\n\n// tom is the user\n$ SELECT * FROM clients;\n id | account_name | account_manager \n----+--------------+-----------------\n  2 | PQR          | tom\n\n(1 row)\n\n\n\n\n\n \n\n\nViewing Policy Details\n\n\nThe policy details can be viewed from a virtual table named \nSYS.SYSPOLICIES\n. The details of the policy are shown in the following columns:\n\n\n\n\n\n\n\n\nColumn\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAME\n\n\nName of the policy.\n\n\n\n\n\n\nSCHEMANAME\n\n\nSchema Name of the table on which policy is applied.\n\n\n\n\n\n\nTABLENAME\n\n\nTable on which policy is applied.\n\n\n\n\n\n\nPOLICYFOR\n\n\nThe operation for which the policy is intended. For example, SELECT, UPDATE, INSERT etc. For now it will be only \u201cSELECT\u201d\n\n\n\n\n\n\nAPPLYTO\n\n\nThe comma separated string of User Names ( or CURRENT_USER) or LDAP group on which the policy applies.\n\n\n\n\n\n\nFILTER\n\n\nThe filter associated with the policy.\n\n\n\n\n\n\nOWNER\n\n\nOwner of the policy which is the same as the owner of the target table.\n\n\n\n\n\n\n\n\n \n\n\nCombining Multiple Policies\n\n\nMultiple policies can be defined for a table and combined as a statement. As policies are table-specific, each policy, that is applied to a table, must have a unique name.  Tables in different schemas can have policies with the same name which can then have different conditions. If multiple policies are applied on a table, the filters will be evaluated as \nAND\n.\n\n\nHere in the following example, multiple policies are created for the table named \nmytable\n and row level security is enabled. The current user here is \ntom\n.\n\n\nCREATE POLICY mypolicy1 on mytable using user_col = current_user();\nCREATE POLICY mypolicy2 on mytable using id \n 4;\nCREATE POLICY mypolicy3 on mytable using account_name = \u2018XYZ\u2019;\n\nALTER TABLE mytable ENABLE ROW LEVEL SECURITY;\n\n\n\n\n\nThese policies are combined as shown in this example:\n\n\nSELECT * FROM mytable\nWHERE user_col = current_user() # current_user is  \ntable owner\n\nAND id\n4\nAND account_name = \u2018XYZ\u2019;\n\n\n$ select * from mytable;\n id | account_name | account_manager \n----+--------------+-----------------\n  3 | XYZ        | tom\n\n(1 row)\n\n\n\n\n\n \n\n\nPolicy Application on Join Queries\n\n\nRefer the following example for policy application in case of join queries: \n\n\nCREATE TABLE customers (\n    customer_id int,\n    name string,\n    hidden boolean\n) using column options();\n\nINSERT INTO customers (customer_id, name)...\n\nCREATE TABLE orders (\n    order_id int,\n    customer_id int\n) using column options();\n\nINSERT INTO orders (order_id, customer_id) ...\n\nAn untrusted user that will be doing SELECTs only:\n\nLet the untrusted user group be named by LDAP (AD group) group as untrusted_group\n\nGRANT SELECT ON customers TO ldapgroup:untrusted_group;\nGRANT SELECT ON orders TO ldapgroup:untrusted_group;\n\nA policy that makes hidden customers invisible to the untrusted user:\n\nCREATE POLICY no_hidden_customers ON customers FOR SELECT TO ldapgroup:untrusted_group USING (hidden is false);\nALTER TABLE customers ENABLE ROW LEVEL SECURITY;\nSELECT name FROM orders JOIN customers ON  customer_id WHERE order_id = 4711;\n\n\n\n\n \n\n\nDropping a Policy\n\n\nSince the policy is dependent on the table, only the owner of the table can drop a policy.\n\n\nTo drop a policy, use the following syntax:\n\n\nDROP POLICY policy_name\n\n\n\n\nFor example,\n\n\nDROP POLICY just_own_clients\n\n\n\n\n\n\nCaution\n\n\nIf you drop a table, all the policies associated with the table will also get dropped.", 
            "title": "Implementing Row Level Security"
        }, 
        {
            "location": "/security/row_level_security/#implementing-row-level-security", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   The following topics are covered in this section:   Overview of Row Level Security  Activating Row Level Security  Creating a Policy  Enabling Row Level Security  Viewing Policy Details  Combining Multiple Policies  Policy Application on Join Queries  Dropping a Policy", 
            "title": "Implementing  Row Level Security"
        }, 
        {
            "location": "/security/row_level_security/#overview-of-row-level-security", 
            "text": "Policy is a rule that is implemented by using a filter expression.  In SnappyData, you can apply security policies to a table at row level that can restrict, on a per-user basis, the rows that must be returned for normal queries by data modification commands. \nFor  activating this row level security , a system property must be added to the configuration files of servers, leads, and locators. \nTo restrict the permissions of a user at row level,  create a simple policy  for a table that can be applied on a per user basis and then  enable the row level security  for that table.", 
            "title": "Overview of Row Level Security"
        }, 
        {
            "location": "/security/row_level_security/#activating-row-level-security", 
            "text": "For activating Row Level Security, a system property  -J-Dsnappydata.enable-rls=true  must be added to the configuration files of servers, leads, and locators when you  configure the cluster . By default this is off.\nIf this property is not added, you cannot enable the Row Level Security and an exception is thrown when you attempt to create the policy.   Warning  When this property is set to  true , the Smart Connector access to SnappyData will fail with  java.lang.IllegalStateException: Row level security (snappydata.enable-rls) does not allow smart connector mode  exception.", 
            "title": "Activating Row Level Security"
        }, 
        {
            "location": "/security/row_level_security/#creating-a-policy", 
            "text": "A policy, which is a rule, can be created for row tables, column tables, and external GemFire tables.  Permissions can be granted to users on DML ops to access the tables for which policies are created.   Note  Policies are restrictive by default.   Here in the following example, a table named  clients  is  created  and access permissions are  granted  to three users:  # Create table named clients.\nCREATE TABLE clients (\n    ID INT PRIMARY KEY,\n    ACCOUNT_NAME STRING NOT NULL,\n    ACCOUNT_MANAGER STRING NOT NULL\n) USING ROW OPTIONS();\n\n# Grant access to three users.\nGRANT SELECT ON clients TO tom, harris, greg;  Initially all the users can view all the records in the table. You can restrict the permissions for viewing the records,  only after you enable the  row level security  and apply a policy. For example, you are  connected to database  testrls  as user  tom . Initially user  tom  can view all the records from the table as shown:  $ SELECT * FROM clients;\n id | account_name | account_manager\n----+--------------+-----------------\n  1 | ABC          | tom\n  2 | PQR          | harris\n  3 | XYZ          | greg\n\n(3 rows)  You can create a policy for a table which can be applied to a user or a LDAP group using the following syntax.   CREATE POLICY name ON table_name\n     FOR SELECT\n     [ TO { LDAP GROUP | CURRENT_USER |  USER_NAME  } [, ...] ]\n    [ USING ( using_expression ) ]   Note  CURRENT_USER implies to any user who is excecuting the query.    In the following example, we create a policy named  just_own_clients  where a user can view only the row where that user is the account manager.\nFor example, if we want this rule to apply only to user  Tom , then we can create the policy as shown:  CREATE POLICY just_own_clients ON clients\n    FOR SELECT\n    TO TOM\n    USING ACCOUNT_MANAGER = CURRENT_USER();  As per the the above policy, user  Tom  can see only one row, where as other users can view all the rows.  The same can be also applied to a LDAP group as shown:  CREATE POLICY just_own_clients ON clients\n    FOR SELECT\n    TO ldapgroup:group1\n    USING ACCOUNT_MANAGER = CURRENT_USER();  After the row level security policy is enabled, the policy gets applied to the corresponding users.", 
            "title": "Creating a Policy"
        }, 
        {
            "location": "/security/row_level_security/#enabling-row-level-security", 
            "text": "The policy which is created to restrict row level permissions is effective only after you enable the row level security. To enable row level security, execute the following  ALTER DDL  command:  ALTER TABLE  table_name  ENABLE ROW LEVEL SECURITY;  For example,   ALTER TABLE clients ENABLE ROW LEVEL SECURITY;  Now the users are permitted to view the records of only those rows that are permitted on the basis of the policy. For example, user  tom  is allowed to view the records of a specific row in table  clients .  // tom is the user\n$ SELECT * FROM clients;\n id | account_name | account_manager \n----+--------------+-----------------\n  2 | PQR          | tom\n\n(1 row)", 
            "title": "Enabling Row Level Security"
        }, 
        {
            "location": "/security/row_level_security/#viewing-policy-details", 
            "text": "The policy details can be viewed from a virtual table named  SYS.SYSPOLICIES . The details of the policy are shown in the following columns:     Column  Description      NAME  Name of the policy.    SCHEMANAME  Schema Name of the table on which policy is applied.    TABLENAME  Table on which policy is applied.    POLICYFOR  The operation for which the policy is intended. For example, SELECT, UPDATE, INSERT etc. For now it will be only \u201cSELECT\u201d    APPLYTO  The comma separated string of User Names ( or CURRENT_USER) or LDAP group on which the policy applies.    FILTER  The filter associated with the policy.    OWNER  Owner of the policy which is the same as the owner of the target table.", 
            "title": "Viewing Policy Details"
        }, 
        {
            "location": "/security/row_level_security/#combining-multiple-policies", 
            "text": "Multiple policies can be defined for a table and combined as a statement. As policies are table-specific, each policy, that is applied to a table, must have a unique name.  Tables in different schemas can have policies with the same name which can then have different conditions. If multiple policies are applied on a table, the filters will be evaluated as  AND .  Here in the following example, multiple policies are created for the table named  mytable  and row level security is enabled. The current user here is  tom .  CREATE POLICY mypolicy1 on mytable using user_col = current_user();\nCREATE POLICY mypolicy2 on mytable using id   4;\nCREATE POLICY mypolicy3 on mytable using account_name = \u2018XYZ\u2019;\n\nALTER TABLE mytable ENABLE ROW LEVEL SECURITY;  These policies are combined as shown in this example:  SELECT * FROM mytable\nWHERE user_col = current_user() # current_user is   table owner \nAND id 4\nAND account_name = \u2018XYZ\u2019;\n\n\n$ select * from mytable;\n id | account_name | account_manager \n----+--------------+-----------------\n  3 | XYZ        | tom\n\n(1 row)", 
            "title": "Combining Multiple Policies"
        }, 
        {
            "location": "/security/row_level_security/#policy-application-on-join-queries", 
            "text": "Refer the following example for policy application in case of join queries:   CREATE TABLE customers (\n    customer_id int,\n    name string,\n    hidden boolean\n) using column options();\n\nINSERT INTO customers (customer_id, name)...\n\nCREATE TABLE orders (\n    order_id int,\n    customer_id int\n) using column options();\n\nINSERT INTO orders (order_id, customer_id) ...\n\nAn untrusted user that will be doing SELECTs only:\n\nLet the untrusted user group be named by LDAP (AD group) group as untrusted_group\n\nGRANT SELECT ON customers TO ldapgroup:untrusted_group;\nGRANT SELECT ON orders TO ldapgroup:untrusted_group;\n\nA policy that makes hidden customers invisible to the untrusted user:\n\nCREATE POLICY no_hidden_customers ON customers FOR SELECT TO ldapgroup:untrusted_group USING (hidden is false);\nALTER TABLE customers ENABLE ROW LEVEL SECURITY;\nSELECT name FROM orders JOIN customers ON  customer_id WHERE order_id = 4711;", 
            "title": "Policy Application on Join Queries"
        }, 
        {
            "location": "/security/row_level_security/#dropping-a-policy", 
            "text": "Since the policy is dependent on the table, only the owner of the table can drop a policy.  To drop a policy, use the following syntax:  DROP POLICY policy_name  For example,  DROP POLICY just_own_clients   Caution  If you drop a table, all the policies associated with the table will also get dropped.", 
            "title": "Dropping a Policy"
        }, 
        {
            "location": "/security/configuring_network_encryption_and_authentication_using_ssl/", 
            "text": "Configuring Network Encryption and Authentication using SSL\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nThe network communication between the server and client can be encrypted using SSL. For more information refer to \nEnabling SSL Encryption in Different Socket Endpoints of SnappyData\n.", 
            "title": "Configuring Network Encryption and Authentication using SSL"
        }, 
        {
            "location": "/security/configuring_network_encryption_and_authentication_using_ssl/#configuring-network-encryption-and-authentication-using-ssl", 
            "text": "This feature is available only in the Enterprise version of SnappyData.   The network communication between the server and client can be encrypted using SSL. For more information refer to  Enabling SSL Encryption in Different Socket Endpoints of SnappyData .", 
            "title": "Configuring Network Encryption and Authentication using SSL"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/", 
            "text": "Enabling SSL Encryption in Different Socket Endpoints of SnappyData\n\n\nSnappyData supports SSL encryption for the following:\n\n\n\n\nClient-server\n\n\nPeer-to-peer (P2P)\n\n\nSpark layer\n\n\n\n\nSSL encryption for the three socket endpoints (P2P, client-server, and Spark) must be \nconfigured individually\n, and the corresponding configuration must be added in configuration files of the respective SnappyData cluster members.\n\n\n\n\nNote\n\n\nProperties that begin with \nthrift\n is for client-server, \njavax.net.ssl\n is for P2P, and \nspark\n is for Spark layer SSL settings.\n\n\n\n\n \n\n\nEnabling SSL Encryption Simultaneously for all the Socket Endpoints in SnappyData Cluster\n\n\nUsing the following configuration files, you can simultaneously enable SSL encryption for all the three socket endpoints (P2P, client-server, and Spark layer SSL encryption) in a SnappyData cluster.\n\n\n\n\nNote\n\n\nCurrently, you must configure the SSL encryption for all the socket endpoints separately. SnappyData intends to integrate Spark and P2P endpoints in a future release.\n\n\n\n\nIn the following configuration files, a two-node SnappyData cluster setup is done using the physical hosts.\n\n\nAll examples given here includes one locator, one server, and one lead member in a SnappyData cluster configuration.\n\n\nIf you want to configure multiple locators, servers and lead members in a cluster, then ensure to copy all the SSL properties for each member configuration into the respective configuration files.\n\n\nFor more information about each of these properties mentioned in the respective conf files, as well as for details on configuring multiple locators, servers, and lead members in a cluster,  refer to \nSnappyData Configuration\n.\n\n\nLocator Configuration File (conf/locators)\n\n\n# hostname locators \nall ssl properties\n\ndev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-locatorKeyStore.keyfile\n -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-locatorKeyStore.keyfile\n  -J-Djavax.net.ssl.trustStorePassword=\npassword\n -thrift-ssl=true -thrift-ssl-properties=keystore=\npath-to-serverKeyStoreRSA.jks file\n,keystore-password=\npassword\n,,truststore=\npath-to-trustStore.key file\n,truststore-password=\npassword\n,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256\n\n\n\n\nServer Configuration File (conf/servers)\n\n\n# hostname locators \nall ssl properties\n\ndev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-serverKeyStore.key file\n -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-serverKeyStore.key file\n -J-Djavax.net.ssl.trustStorePassword=\npassword\n -spark.ssl.enabled=true -spark.ssl.keyPassword=\npassword\n -spark.ssl.keyStore=\npath-to-serverKeyStore.key file\n -spark.ssl.keyStorePassword=\npassword\n -spark.ssl.trustStore=\npath-to-serverKeyStore.key file\n -spark.ssl.trustStorePassword=\npassword\n -spark.ssl.protocol=TLS -client-port=1528 -thrift-ssl=true -thrift-ssl-properties=keystore=\npath-to-serverKeyStoreRSA.jks file\n,keystore-password=\npassword\n,,truststore=\npath-to-trustStore.key file\n,truststore-password=\npassword\n,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256\n\n\n\n\n\nLead Configuration File (conf/leads)\n\n\n# hostname locators \nall ssl properties\n\ndev14 -locators=dev15:10334 -ssl-enabled=true  -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-leadKeyStore.key file\n  -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-leadKeyStore.key file\n -J-Djavax.net.ssl.trustStorePassword=\npassword\n -spark.ssl.enabled=true -spark.ssl.keyPassword=\npassword\n -spark.ssl.keyStore=\npath-to-leadKeyStore.key file\n  -spark.ssl.keyStorePassword=\npassword\n -spark.ssl.trustStore=\npath-to-leadKeyStore.key file\n -spark.ssl.trustStorePassword=\npassword\n -spark.ssl.protocol=TLS -client-port=1529\n\n\n\n\n \n\n\nConfiguring SSL Encryption for Different Socket Endpoints in SnappyData Cluster\n\n\nYou can individually configure the SnappyData socket endpoints as per your requirements. This section provides the instructions to configure each of the socket endpoints.\n\n\n\n\nConfiguring SSL encryption for client-server\n\n\nConfiguring SSL encryption for p2p\n\n\nConfiguring SSL encryption for Spark\n\n\n\n\n \n\n\nConfiguring SSL Encryption for Client-server\n\n\nSnappyData store supports the Thrift protocol that provides the functionality that is equivalent to JDBC/ODBC protocols and can be used to access the store from other languages that are not yet supported directly by SnappyData. In the command-line, SnappyData locators and servers accept the \n-thrift-server-address\n and \n-thrift-server-port\n arguments to start a Thrift server.\nThe Thrift servers use the \nThrift Compact Protocol\n by default which is not SSL enabled. When using the \nsnappy-start-all.sh\n script, these properties can be specified in the \nconf/locators\n and \nconf/servers\n files in the product directory like any other locator/server properties. For more information, refer to \nSnappyData Configuration\n.\n\n\nIn the \nconf/locators\n and \nconf/servers\n files, you need to add \n-thrift-ssl\n and the required SSL setup in \n-thrift-ssl-properties\n. Refer to the \nSnappyData thrift properties\n section for more information.\n\n\nIn the following example, the SSL configuration for client-server is demonstrated along with the startup of SnappyData members with SSL encryption. \n\n\nRequirements\n\n\n\n\nConfigure SSL keypairs and certificates as needed for client and server.\n\n\nEnsure that all the locator and server members, in the cluster, use the same SSL boot parameters at startup.\n\n\n\n\nProvider-Specific Configuration Files\n\n\nThis example uses keystores created by the Java keytool application to provide the proper credentials to the provider. To create the keystore and certificate for the client and server, run the following:\n\n\nkeytool -genkey -alias myserverkey -keystore serverKeyStoreRSA.jks -keyalg RSA\nkeytool -export -alias myserverkey -keystore serverKeyStoreRSA.jks  -rfc -file myServerRSA.cert\nkeytool -import -alias myserverkey -file myServerRSA.cert -keystore trustStore.key\n\n\n\n\nThe same keystore is used for SnappyData locator and server members as well as for client connection. You can enable SSL encryption for client-server connections by specifying the properties as the startup options for locator and server members. In the following example, the SSL encryption is enabled for communication between client-server.\n\n\nLocator Configuration File (conf/locators)\n\n\n# hostname locators \nssl properties for configuring SSL for SnappyData client-server connections\n\n\nlocalhost -thrift-ssl=true -thrift-ssl-properties=keystore=\npath-to-serverKeyStoreRSA.jks file\n,keystore-password=\npassword\n,,truststore=\npath-to-trustStore.key file\n,truststore-password=\npassword\n,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256\n\n\n\n\n\nServer Configuration File (conf/servers)\n\n\n# hostname locators \nssl properties for configuring SSL for SnappyData client-server connections\n\n\nlocalhost -thrift-ssl=true -thrift-ssl-properties=keystore=\npath-to-serverKeyStoreRSA.jks file\n,keystore-password=\npassword\n,truststore=\npath-to-trustStore.key file\n,truststore-password=\npassword\n,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256\n\n\n\n\n\nStart the SnappyData cluster using \nsnappy-start-all.sh\n script and perform operations either by using SnappyData shell or through JDBC connection. You can run the SnappyData quickstart example scripts for this. Refer to \nSnappyData Cluster SQL Tutorial\n for the same. \n\n\nUse the protocol/ciphers as per requirement. The corresponding setup on client-side can appear as follows:\n\n\nsnappy\n connect client 'localhost:1527;ssl=true;ssl-properties=truststore=\npath-to-trustStore.key file\n,truststore-password=\npassword\n,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256';\n\n\n\n\n\n \n\n\nConfiguring SSL Encryption for P2P\n\n\nIn addition to using SSL for client-server connections, you can optionally configure SnappyData members to use SSL encryption and authorization for peer-to-peer(P2P) connections in the distributed system.\n\n\nPeer SSL configuration is managed using \njavax.net.ssl\n system properties and the following SnappyData boot properties:\n\n\n\n\nssl-enabled\n\n\nssl-protocols\n\n\nssl-ciphers\n\n\nssl-require-authentication\n \n\n\n\n\nThe following sections provide an example of P2P connections encryption that demonstrates the configuration and startup of SnappyData members with SSL encryption.\n\n\nRequirements\n\n\nTo configure SSL for SnappyData peer connections:\n\n\n\n\nConfigure SSL keypairs and certificates as needed for each SnappyData member. Refer to the following example.  \n\n\nEnsure that all SnappyData members use the same SSL boot parameters at startup.\n\n\n\n\nProvider-Specific Configuration Files\n\n\nThis example uses keystores created by the Java keytool application to provide the proper credentials to the provider.\n\n\nTo create the keystore and certificate for the locator, run the following:\n\n\nkeytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key\nkeytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert\n\n\n\n\nYou can use similar commands for a server member and a lead member respectively:\n\n\nkeytool -genkey -alias mySnappyServer -keystore serverKeyStore.key\nkeytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert\n\n\n\n\nkeytool -genkey -alias mySnappyLead -keystore leadKeyStore.key\nkeytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert\n\n\n\n\nEach of the member's certificate is then imported into the other member's trust store.\n\n\nFor the server member:\n\n\nkeytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key\n\n\n\n\nFor the locator member:\n\n\nkeytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key\nkeytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key\n\n\n\n\nFor the lead member:\n\n\nkeytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key\n\n\n\n\nYou can enable SSL encryption for peer connections by specifying the properties as the startup options for each member. \n\n\nIn the following example, SSL encryption is enabled for communication between the members.\n\n\nLocator Configuration File (conf/locators)\n\n\n# hostname locators \nssl properties for configuring SSL for SnappyData P2P connections\n\nlocalhost -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-locatorKeyStore.key file\n -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-locatorKeyStore.key file\n -J-Djavax.net.ssl.trustStorePassword=\npassword\n\n\n\n\n\nServer Configuration File (conf/servers)\n\n\n# hostname locators \nssl properties for configuring SSL for #SnappyData P2P connections\n\n\nlocalhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-serverKeyStore.keyfile\n  -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-serverKeyStore.key file\n -J-Djavax.net.ssl.trustStorePassword=\npassword\n -client-port=1528\n\n\n\n\n\nLead Configuration File (conf/leads)\n\n\n# hostname locators \nssl properties for configuring SSL for #SnappyData P2P connections\n\n\nlocalhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-leadKeyStore.key file\n -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-leadKeyStore.key file\n  -J-Djavax.net.ssl.trustStorePassword=\npassword\n -client-port=1529\n\n\n\n\n\nStart the SnappyData cluster using \nsnappy-start-all.sh\n script and perform operations using the SnappyData shell, SnappyData job, and Smart Connector mode. You can run the SnappyData quickstart example scripts for this. Refer to \nSnappyData Cluster SQL Tutorial\n for more information.\n\n\n \n\n\nConfiguring SSL Encryption for Spark layer\n\n\nSpark layer  SSL configuration is managed using the following SnappyData boot properties:\n\n\n\n\nspark.ssl.enabled\n\n\nspark.ssl.keyPassword\n\n\nspark.ssl.keyStore\n\n\nspark.ssl.keyStorePassword\n\n\nspark.ssl.trustStore\n\n\nspark.ssl.trustStorePassword\n\n\nspark.ssl.protocol\n\n\n\n\nThe following sections provide a simple example that demonstrates the configuration and startup of SnappyData members for enabling Spark layer for Wire Encryption.\n\n\nRequirements\n\n\nTo configure SSL for Spark layer:\n\n\n\n\nConfigure SSL keypairs and certificates as needed for each SnappyData member.\n\n\nEnsure that SnappyData locator, server, and lead members use the same SSL boot parameters at startup.\n\n\n\n\n\n\nNote\n\n\nFor enabling Spark layer SSL encryption, you must first enable the P2P encryption for SSL.\n\n\n\n\nProvider-Specific Configuration Files\n\n\nThis example uses keystores created by the Java keytool application to provide the proper credentials to the provider.  On each node, create keystore files, certificates, and truststore files. Here, in this example, to create the keystore and certificate for the locator, the following was run:\n\n\nkeytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key\nkeytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert\n\n\n\n\n\nSimilar commands were used for a server member and lead member respectively:\n\n\nkeytool -genkey -alias mySnappyServer -keystore serverKeyStore.key\nkeytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert\n\n\n\n\nkeytool -genkey -alias mySnappyLead -keystore leadKeyStore.key\nkeytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert\n\n\n\n\nAfter this, each of the member's certificate is imported into the other member's trust store.\n\n\nFor the locator member:\n\n\nkeytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key\nkeytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key\n\n\n\n\nFor the server member:\n\n\nkeytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key\n\n\n\n\nFor the lead member:\n\n\nkeytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key\n\n\n\n\nYou can enable SSL encryption for Spark layer by specifying the properties as the startup options for server and lead members. This example demonstrates how SSL encryption is enabled for a Spark layer connection.\n\n\nLocator Configuration File (conf/locators)\n\n\n# hostname locators \nssl properties for configuring SSL for SnappyData Spark layer\n\nlocalhost -ssl-enabled=true -spark.ssl.enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-locatorKeyStore.key file\n -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-locatorKeyStore.key file\n -J-Djavax.net.ssl.trustStorePassword=\npassword\n \n-spark.ssl.keyPassword=\npassword\n -spark.ssl.keyStore=\npath-to-locatorKeyStore.key file\n -spark.ssl.keyStorePassword=\npassword\n -spark.ssl.trustStore=\npath-to-locatorKeyStore.key file\n -spark.ssl.trustStorePassword=\npassword\n -spark.ssl.protocol=TLS \n\n\n\n\nServer Configuration File (conf/servers)\n\n\n# hostname locators \nssl properties for configuring SSL for #SnappyData spark layer\n\n\nlocalhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-serverKeyStore.key file\n -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-serverKeyStore.key file\n -J-Djavax.net.ssl.trustStorePassword=\npassword\n\n-spark.ssl.enabled=true -spark.ssl.keyPassword=\npassword\n -spark.ssl.keyStore=\npath-to-serverKeyStore.key file\n -spark.ssl.keyStorePassword=\npassword\n -spark.ssl.trustStore=\npath-to-serverKeyStore.key file\n -spark.ssl.trustStorePassword=\npassword\n -spark.ssl.protocol=TLS \n\n\n\n\nLead Configuration File (conf/leads)\n\n\n# hostname locators \nssl properties for configuring SSL for #SnappyData spark layer\n\nlocalhost -locators=localhost:10334  -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore=\npath-to-leadKeyStore.key file\n -J-Djavax.net.ssl.keyStorePassword=\npassword\n -J-Djavax.net.ssl.trustStore=\npath-to-leadKeyStore.key file\n -J-Djavax.net.ssl.trustStorePassword=\npassword\n\n-spark.ssl.enabled=true -spark.ssl.keyPassword=\npassword\n -spark.ssl.keyStore=\npath-to-leadKeyStore.key file\n -spark.ssl.keyStorePassword=\npassword\n -spark.ssl.trustStore=/\npath-to-leadKeyStore.key file\n -spark.ssl.trustStorePassword=\npassword\n -spark.ssl.protocol=TLS \nStart the SnappyData cluster using snappy-start-all.sh script and perform operations using SnappyData shell, SnappyData  job, and Smart Connector mode.\n\n\n\n\nYou can run the SnappyData quickstart example scripts for this. Refer to \nSnappyData Cluster SQL Tutorial\n for more information.", 
            "title": "Enabling SSL Encryption in Different Socket Endpoints of SnappyData"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#enabling-ssl-encryption-in-different-socket-endpoints-of-snappydata", 
            "text": "SnappyData supports SSL encryption for the following:   Client-server  Peer-to-peer (P2P)  Spark layer   SSL encryption for the three socket endpoints (P2P, client-server, and Spark) must be  configured individually , and the corresponding configuration must be added in configuration files of the respective SnappyData cluster members.   Note  Properties that begin with  thrift  is for client-server,  javax.net.ssl  is for P2P, and  spark  is for Spark layer SSL settings.", 
            "title": "Enabling SSL Encryption in Different Socket Endpoints of SnappyData"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#enabling-ssl-encryption-simultaneously-for-all-the-socket-endpoints-in-snappydata-cluster", 
            "text": "Using the following configuration files, you can simultaneously enable SSL encryption for all the three socket endpoints (P2P, client-server, and Spark layer SSL encryption) in a SnappyData cluster.   Note  Currently, you must configure the SSL encryption for all the socket endpoints separately. SnappyData intends to integrate Spark and P2P endpoints in a future release.   In the following configuration files, a two-node SnappyData cluster setup is done using the physical hosts.  All examples given here includes one locator, one server, and one lead member in a SnappyData cluster configuration.  If you want to configure multiple locators, servers and lead members in a cluster, then ensure to copy all the SSL properties for each member configuration into the respective configuration files.  For more information about each of these properties mentioned in the respective conf files, as well as for details on configuring multiple locators, servers, and lead members in a cluster,  refer to  SnappyData Configuration .", 
            "title": "Enabling SSL Encryption Simultaneously for all the Socket Endpoints in SnappyData Cluster"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#locator-configuration-file-conflocators", 
            "text": "# hostname locators  all ssl properties \ndev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-locatorKeyStore.keyfile  -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-locatorKeyStore.keyfile   -J-Djavax.net.ssl.trustStorePassword= password  -thrift-ssl=true -thrift-ssl-properties=keystore= path-to-serverKeyStoreRSA.jks file ,keystore-password= password ,,truststore= path-to-trustStore.key file ,truststore-password= password ,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256", 
            "title": "Locator Configuration File (conf/locators)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#server-configuration-file-confservers", 
            "text": "# hostname locators  all ssl properties \ndev15 -locators=dev15:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-serverKeyStore.key file  -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-serverKeyStore.key file  -J-Djavax.net.ssl.trustStorePassword= password  -spark.ssl.enabled=true -spark.ssl.keyPassword= password  -spark.ssl.keyStore= path-to-serverKeyStore.key file  -spark.ssl.keyStorePassword= password  -spark.ssl.trustStore= path-to-serverKeyStore.key file  -spark.ssl.trustStorePassword= password  -spark.ssl.protocol=TLS -client-port=1528 -thrift-ssl=true -thrift-ssl-properties=keystore= path-to-serverKeyStoreRSA.jks file ,keystore-password= password ,,truststore= path-to-trustStore.key file ,truststore-password= password ,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256", 
            "title": "Server Configuration File (conf/servers)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#lead-configuration-file-confleads", 
            "text": "# hostname locators  all ssl properties \ndev14 -locators=dev15:10334 -ssl-enabled=true  -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-leadKeyStore.key file   -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-leadKeyStore.key file  -J-Djavax.net.ssl.trustStorePassword= password  -spark.ssl.enabled=true -spark.ssl.keyPassword= password  -spark.ssl.keyStore= path-to-leadKeyStore.key file   -spark.ssl.keyStorePassword= password  -spark.ssl.trustStore= path-to-leadKeyStore.key file  -spark.ssl.trustStorePassword= password  -spark.ssl.protocol=TLS -client-port=1529", 
            "title": "Lead Configuration File (conf/leads)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-different-socket-endpoints-in-snappydata-cluster", 
            "text": "You can individually configure the SnappyData socket endpoints as per your requirements. This section provides the instructions to configure each of the socket endpoints.   Configuring SSL encryption for client-server  Configuring SSL encryption for p2p  Configuring SSL encryption for Spark", 
            "title": "Configuring SSL Encryption for Different Socket Endpoints in SnappyData Cluster"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-client-server", 
            "text": "SnappyData store supports the Thrift protocol that provides the functionality that is equivalent to JDBC/ODBC protocols and can be used to access the store from other languages that are not yet supported directly by SnappyData. In the command-line, SnappyData locators and servers accept the  -thrift-server-address  and  -thrift-server-port  arguments to start a Thrift server.\nThe Thrift servers use the  Thrift Compact Protocol  by default which is not SSL enabled. When using the  snappy-start-all.sh  script, these properties can be specified in the  conf/locators  and  conf/servers  files in the product directory like any other locator/server properties. For more information, refer to  SnappyData Configuration .  In the  conf/locators  and  conf/servers  files, you need to add  -thrift-ssl  and the required SSL setup in  -thrift-ssl-properties . Refer to the  SnappyData thrift properties  section for more information.  In the following example, the SSL configuration for client-server is demonstrated along with the startup of SnappyData members with SSL encryption.", 
            "title": "Configuring SSL Encryption for Client-server"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#requirements", 
            "text": "Configure SSL keypairs and certificates as needed for client and server.  Ensure that all the locator and server members, in the cluster, use the same SSL boot parameters at startup.", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#provider-specific-configuration-files", 
            "text": "This example uses keystores created by the Java keytool application to provide the proper credentials to the provider. To create the keystore and certificate for the client and server, run the following:  keytool -genkey -alias myserverkey -keystore serverKeyStoreRSA.jks -keyalg RSA\nkeytool -export -alias myserverkey -keystore serverKeyStoreRSA.jks  -rfc -file myServerRSA.cert\nkeytool -import -alias myserverkey -file myServerRSA.cert -keystore trustStore.key  The same keystore is used for SnappyData locator and server members as well as for client connection. You can enable SSL encryption for client-server connections by specifying the properties as the startup options for locator and server members. In the following example, the SSL encryption is enabled for communication between client-server.", 
            "title": "Provider-Specific Configuration Files"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#locator-configuration-file-conflocators_1", 
            "text": "# hostname locators  ssl properties for configuring SSL for SnappyData client-server connections \n\nlocalhost -thrift-ssl=true -thrift-ssl-properties=keystore= path-to-serverKeyStoreRSA.jks file ,keystore-password= password ,,truststore= path-to-trustStore.key file ,truststore-password= password ,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256", 
            "title": "Locator Configuration File (conf/locators)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#server-configuration-file-confservers_1", 
            "text": "# hostname locators  ssl properties for configuring SSL for SnappyData client-server connections \n\nlocalhost -thrift-ssl=true -thrift-ssl-properties=keystore= path-to-serverKeyStoreRSA.jks file ,keystore-password= password ,truststore= path-to-trustStore.key file ,truststore-password= password ,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256  Start the SnappyData cluster using  snappy-start-all.sh  script and perform operations either by using SnappyData shell or through JDBC connection. You can run the SnappyData quickstart example scripts for this. Refer to  SnappyData Cluster SQL Tutorial  for the same.   Use the protocol/ciphers as per requirement. The corresponding setup on client-side can appear as follows:  snappy  connect client 'localhost:1527;ssl=true;ssl-properties=truststore= path-to-trustStore.key file ,truststore-password= password ,protocol=TLS,enabled-protocols=TLSv1:TLSv1.1:TLSv1.2,cipher-suites=TLS_RSA_WITH_AES_128_CBC_SHA:TLS_RSA_WITH_AES_256_CBC_SHA:TLS_RSA_WITH_AES_128_CBC_SHA256:TLS_RSA_WITH_AES_256_CBC_SHA256';", 
            "title": "Server Configuration File (conf/servers)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-p2p", 
            "text": "In addition to using SSL for client-server connections, you can optionally configure SnappyData members to use SSL encryption and authorization for peer-to-peer(P2P) connections in the distributed system.  Peer SSL configuration is managed using  javax.net.ssl  system properties and the following SnappyData boot properties:   ssl-enabled  ssl-protocols  ssl-ciphers  ssl-require-authentication     The following sections provide an example of P2P connections encryption that demonstrates the configuration and startup of SnappyData members with SSL encryption.", 
            "title": "Configuring SSL Encryption for P2P"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#requirements_1", 
            "text": "To configure SSL for SnappyData peer connections:   Configure SSL keypairs and certificates as needed for each SnappyData member. Refer to the following example.    Ensure that all SnappyData members use the same SSL boot parameters at startup.", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#provider-specific-configuration-files_1", 
            "text": "This example uses keystores created by the Java keytool application to provide the proper credentials to the provider.  To create the keystore and certificate for the locator, run the following:  keytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key\nkeytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert  You can use similar commands for a server member and a lead member respectively:  keytool -genkey -alias mySnappyServer -keystore serverKeyStore.key\nkeytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert  keytool -genkey -alias mySnappyLead -keystore leadKeyStore.key\nkeytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert  Each of the member's certificate is then imported into the other member's trust store.  For the server member:  keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key  For the locator member:  keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key\nkeytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key  For the lead member:  keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key  You can enable SSL encryption for peer connections by specifying the properties as the startup options for each member.   In the following example, SSL encryption is enabled for communication between the members.", 
            "title": "Provider-Specific Configuration Files"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#locator-configuration-file-conflocators_2", 
            "text": "# hostname locators  ssl properties for configuring SSL for SnappyData P2P connections \nlocalhost -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-locatorKeyStore.key file  -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-locatorKeyStore.key file  -J-Djavax.net.ssl.trustStorePassword= password", 
            "title": "Locator Configuration File (conf/locators)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#server-configuration-file-confservers_2", 
            "text": "# hostname locators  ssl properties for configuring SSL for #SnappyData P2P connections \n\nlocalhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-serverKeyStore.keyfile   -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-serverKeyStore.key file  -J-Djavax.net.ssl.trustStorePassword= password  -client-port=1528", 
            "title": "Server Configuration File (conf/servers)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#lead-configuration-file-confleads_1", 
            "text": "# hostname locators  ssl properties for configuring SSL for #SnappyData P2P connections \n\nlocalhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-leadKeyStore.key file  -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-leadKeyStore.key file   -J-Djavax.net.ssl.trustStorePassword= password  -client-port=1529  Start the SnappyData cluster using  snappy-start-all.sh  script and perform operations using the SnappyData shell, SnappyData job, and Smart Connector mode. You can run the SnappyData quickstart example scripts for this. Refer to  SnappyData Cluster SQL Tutorial  for more information.", 
            "title": "Lead Configuration File (conf/leads)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#configuring-ssl-encryption-for-spark-layer", 
            "text": "Spark layer  SSL configuration is managed using the following SnappyData boot properties:   spark.ssl.enabled  spark.ssl.keyPassword  spark.ssl.keyStore  spark.ssl.keyStorePassword  spark.ssl.trustStore  spark.ssl.trustStorePassword  spark.ssl.protocol   The following sections provide a simple example that demonstrates the configuration and startup of SnappyData members for enabling Spark layer for Wire Encryption.", 
            "title": "Configuring SSL Encryption for Spark layer"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#requirements_2", 
            "text": "To configure SSL for Spark layer:   Configure SSL keypairs and certificates as needed for each SnappyData member.  Ensure that SnappyData locator, server, and lead members use the same SSL boot parameters at startup.    Note  For enabling Spark layer SSL encryption, you must first enable the P2P encryption for SSL.", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#provider-specific-configuration-files_2", 
            "text": "This example uses keystores created by the Java keytool application to provide the proper credentials to the provider.  On each node, create keystore files, certificates, and truststore files. Here, in this example, to create the keystore and certificate for the locator, the following was run:  keytool -genkey -alias mySnappyLocator -keystore locatorKeyStore.key\nkeytool -export -alias mySnappyLocator -keystore locatorKeyStore.key -rfc -file myLocator.cert  Similar commands were used for a server member and lead member respectively:  keytool -genkey -alias mySnappyServer -keystore serverKeyStore.key\nkeytool -export -alias mySnappyServer -keystore serverKeyStore.key -rfc -file myServer.cert  keytool -genkey -alias mySnappyLead -keystore leadKeyStore.key\nkeytool -export -alias mySnappyLead -keystore leadKeyStore.key -rfc -file myLead.cert  After this, each of the member's certificate is imported into the other member's trust store.  For the locator member:  keytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./serverKeyStore.key\nkeytool -import -alias mySnappyLocator -file ./myLocator.cert -keystore ./leadKeyStore.key  For the server member:  keytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyServer -file ./myServer.cert -keystore ./leadKeyStore.key  For the lead member:  keytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./locatorKeyStore.key\nkeytool -import -alias mySnappyLead -file ./myLead.cert -keystore ./serverKeyStore.key  You can enable SSL encryption for Spark layer by specifying the properties as the startup options for server and lead members. This example demonstrates how SSL encryption is enabled for a Spark layer connection.", 
            "title": "Provider-Specific Configuration Files"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#locator-configuration-file-conflocators_3", 
            "text": "# hostname locators  ssl properties for configuring SSL for SnappyData Spark layer \nlocalhost -ssl-enabled=true -spark.ssl.enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-locatorKeyStore.key file  -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-locatorKeyStore.key file  -J-Djavax.net.ssl.trustStorePassword= password  \n-spark.ssl.keyPassword= password  -spark.ssl.keyStore= path-to-locatorKeyStore.key file  -spark.ssl.keyStorePassword= password  -spark.ssl.trustStore= path-to-locatorKeyStore.key file  -spark.ssl.trustStorePassword= password  -spark.ssl.protocol=TLS", 
            "title": "Locator Configuration File (conf/locators)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#server-configuration-file-confservers_3", 
            "text": "# hostname locators  ssl properties for configuring SSL for #SnappyData spark layer \n\nlocalhost -locators=localhost:10334 -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-serverKeyStore.key file  -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-serverKeyStore.key file  -J-Djavax.net.ssl.trustStorePassword= password \n-spark.ssl.enabled=true -spark.ssl.keyPassword= password  -spark.ssl.keyStore= path-to-serverKeyStore.key file  -spark.ssl.keyStorePassword= password  -spark.ssl.trustStore= path-to-serverKeyStore.key file  -spark.ssl.trustStorePassword= password  -spark.ssl.protocol=TLS", 
            "title": "Server Configuration File (conf/servers)"
        }, 
        {
            "location": "/configuring_cluster/ssl_setup/#lead-configuration-file-confleads_2", 
            "text": "# hostname locators  ssl properties for configuring SSL for #SnappyData spark layer \nlocalhost -locators=localhost:10334  -ssl-enabled=true -J-Djavax.net.ssl.keyStoreType=jks -J-Djavax.net.ssl.keyStore= path-to-leadKeyStore.key file  -J-Djavax.net.ssl.keyStorePassword= password  -J-Djavax.net.ssl.trustStore= path-to-leadKeyStore.key file  -J-Djavax.net.ssl.trustStorePassword= password \n-spark.ssl.enabled=true -spark.ssl.keyPassword= password  -spark.ssl.keyStore= path-to-leadKeyStore.key file  -spark.ssl.keyStorePassword= password  -spark.ssl.trustStore=/ path-to-leadKeyStore.key file  -spark.ssl.trustStorePassword= password  -spark.ssl.protocol=TLS \nStart the SnappyData cluster using snappy-start-all.sh script and perform operations using SnappyData shell, SnappyData  job, and Smart Connector mode.  You can run the SnappyData quickstart example scripts for this. Refer to  SnappyData Cluster SQL Tutorial  for more information.", 
            "title": "Lead Configuration File (conf/leads)"
        }, 
        {
            "location": "/configuring_cluster/securinguiconnection/", 
            "text": "Securing SnappyData Pulse UI Connection\n\n\nYou can secure the SnappyData Pulse UI with SSL authentication, so that the UI can be accessed only over HTTPS. The following configurations are needed to set up SSL enabled connections for SnappyData Pulse UI:\n\n\nTo set up SSL enabled connections for SnappyData Pulse UI:\n\n\n\n\nMake sure that you have valid SSL certificate imported into truststore.\n\n\n\n\nProvide the following spark configuration in the conf/lead files:\n\n\nlocalhost -spark.ssl.enabled=true -spark.ssl.protocol=\nssl-protocol\n -spark.ssl.enabledAlgorithms=\ncomma-separated-list-of-ciphers\n -spark.ssl.keyPassword=\nkey-password\n -spark.ssl.keyStore=\npath-to-key-store\n -spark.ssl.keyStorePassword=\nkey-store-password\n -spark.ssl.keyStoreType=\nkey-store-type\n -spark.ssl.trustStore=\npath-to-trust-store\n -spark.ssl.trustStorePassword=\ntrust-store-password\n -spark.ssl.trustStoreType=\ntrust-store-type\n\n\n\n\n\n\nNote\n\n\n\n\nIf using TLS SSL protocol, the enabledAlgorithms can be TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA\n\n\nStore types could be JKS or PKCS12.\n\n\n\n\n\n\n\n\n\n\nLaunch the Snappy cluster.\n\n    \n./sbin/snappy-start-all.sh\n \n\n\n\n\nLaunch the Snappy Pulse UI in your web browser. You are directed to the HTTPS site.\n\n\n\n\n\n\nNote\n\n\nYou are automatically redirected to HTTPS (on port 5450) even if the SnappyData Pulse UI is accessed with HTTP protocol.", 
            "title": "Securing SnappyData Pulse UI Connection"
        }, 
        {
            "location": "/configuring_cluster/securinguiconnection/#securing-snappydata-pulse-ui-connection", 
            "text": "You can secure the SnappyData Pulse UI with SSL authentication, so that the UI can be accessed only over HTTPS. The following configurations are needed to set up SSL enabled connections for SnappyData Pulse UI:  To set up SSL enabled connections for SnappyData Pulse UI:   Make sure that you have valid SSL certificate imported into truststore.   Provide the following spark configuration in the conf/lead files:  localhost -spark.ssl.enabled=true -spark.ssl.protocol= ssl-protocol  -spark.ssl.enabledAlgorithms= comma-separated-list-of-ciphers  -spark.ssl.keyPassword= key-password  -spark.ssl.keyStore= path-to-key-store  -spark.ssl.keyStorePassword= key-store-password  -spark.ssl.keyStoreType= key-store-type  -spark.ssl.trustStore= path-to-trust-store  -spark.ssl.trustStorePassword= trust-store-password  -spark.ssl.trustStoreType= trust-store-type    Note   If using TLS SSL protocol, the enabledAlgorithms can be TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA  Store types could be JKS or PKCS12.      Launch the Snappy cluster. \n     ./sbin/snappy-start-all.sh     Launch the Snappy Pulse UI in your web browser. You are directed to the HTTPS site.    Note  You are automatically redirected to HTTPS (on port 5450) even if the SnappyData Pulse UI is accessed with HTTP protocol.", 
            "title": "Securing SnappyData Pulse UI Connection"
        }, 
        {
            "location": "/security/user_names_for_authentication_authorization_and_membership/", 
            "text": "User Names for Authentication, Authorization, and Membership \n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\nUser Names are Converted to Authorization Identifiers\n\n\nUser names in the SnappyData system are known as authorization identifiers. The authorization identifier is a string that represents the name of the user if one was provided in the connection request. \n\n\nAfter the authorization identifier is passed to the SnappyData system, it becomes an SQL92Identifier. SQL92Identifier is a kind of identifier that represents a database object such as a table or column. A SQL92Identifier is case-insensitive (it is converted to all caps) unless it is delimited with double quotes. A SQL92Identifier is limited to 128 characters and has other limitations.\n\n\nAll user names must be valid authorization identifiers even if user authentication is turned off, and even if all users are allowed access to all databases.\n\n\nHandling Case Sensitivity and Special Characters in User Names\n\n\nIf an external authentication system is used, SnappyData does not convert a user's name to an authorization identifier until after authentication has occurred (but before the user is authorized). For example, with an example user named Fred:\n\n\nWithin the user authentication system, Fred might be known as FRed. If the external user authentication service is case-sensitive, Fred must always be typed as:\n\n\nconnect client 'localhost:1527;user=FRed;password=flintstone';\n\n\n\n\nWithin the SnappyData user authorization system, Fred becomes a case-insensitive authorization identifier. Here, FRed is known as FRED.\n\n\nAlso consider a second example, where Fred has a slightly different name within the user authentication system:\n\n\nWithin the user authentication system, Fred is known as Fred. You must now put double quotes around the username, because it is not a valid SQL92Identifier. SnappyData removes the double quotes when passing the name to the external authentication system.\n\n\nconnect client 'localhost:1527;user=\nFred!\n;password=flintstone';\n\n\n\n\nWithin the SnappyData user authorization system, Fred now becomes a case-sensitive authorization identifier. In this case, Fred is known as Fred.\n\n\nAs shown in the first example, the external authentication system may be case-sensitive, whereas the authorization identifier within SnappyData may not be. If your authentication system allows two distinct users whose names differ by case, delimit all user names within the connection request to make all user names case-sensitive within the SnappyData system. In addition, you must also delimit user names that do not conform to SQL92Identifier rules with double quotes.", 
            "title": "User Names for Authentication, Authorization, and Membership"
        }, 
        {
            "location": "/security/user_names_for_authentication_authorization_and_membership/#user-names-for-authentication-authorization-and-membership", 
            "text": "This feature is available only in the Enterprise version of SnappyData.", 
            "title": "User Names for Authentication, Authorization, and Membership "
        }, 
        {
            "location": "/security/user_names_for_authentication_authorization_and_membership/#user-names-are-converted-to-authorization-identifiers", 
            "text": "User names in the SnappyData system are known as authorization identifiers. The authorization identifier is a string that represents the name of the user if one was provided in the connection request.   After the authorization identifier is passed to the SnappyData system, it becomes an SQL92Identifier. SQL92Identifier is a kind of identifier that represents a database object such as a table or column. A SQL92Identifier is case-insensitive (it is converted to all caps) unless it is delimited with double quotes. A SQL92Identifier is limited to 128 characters and has other limitations.  All user names must be valid authorization identifiers even if user authentication is turned off, and even if all users are allowed access to all databases.", 
            "title": "User Names are Converted to Authorization Identifiers"
        }, 
        {
            "location": "/security/user_names_for_authentication_authorization_and_membership/#handling-case-sensitivity-and-special-characters-in-user-names", 
            "text": "If an external authentication system is used, SnappyData does not convert a user's name to an authorization identifier until after authentication has occurred (but before the user is authorized). For example, with an example user named Fred:  Within the user authentication system, Fred might be known as FRed. If the external user authentication service is case-sensitive, Fred must always be typed as:  connect client 'localhost:1527;user=FRed;password=flintstone';  Within the SnappyData user authorization system, Fred becomes a case-insensitive authorization identifier. Here, FRed is known as FRED.  Also consider a second example, where Fred has a slightly different name within the user authentication system:  Within the user authentication system, Fred is known as Fred. You must now put double quotes around the username, because it is not a valid SQL92Identifier. SnappyData removes the double quotes when passing the name to the external authentication system.  connect client 'localhost:1527;user= Fred! ;password=flintstone';  Within the SnappyData user authorization system, Fred now becomes a case-sensitive authorization identifier. In this case, Fred is known as Fred.  As shown in the first example, the external authentication system may be case-sensitive, whereas the authorization identifier within SnappyData may not be. If your authentication system allows two distinct users whose names differ by case, delimit all user names within the connection request to make all user names case-sensitive within the SnappyData system. In addition, you must also delimit user names that do not conform to SQL92Identifier rules with double quotes.", 
            "title": "Handling Case Sensitivity and Special Characters in User Names"
        }, 
        {
            "location": "/vsd/system_performance/", 
            "text": "Evaluating Statistics for the System\n\n\nSnappyData provides statistics for analyzing system performance. Any member of a distributed system, including SnappyData servers, locators, and peer clients, can collect and archive this statistical data.\n\n\nSnappyData samples statistics at a configurable interval and writes them to an archive. The archives can be read at any time, including at runtime.\n\n\nYou can view and analyze runtime or archived historical data using these tools:\n\n\n\n\n\n\nsnappy-shell stats\n is a command-line tool provided with the SnappyData product.\n\n\n\n\n\n\nSnappyData \nVisual Statistics Display (VSD)\n is a graphical tool that is installed in the \nvsd\n subdirectory of the SnappyData installation.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSnappyData statistics use the Java System.nanoTimer for nanosecond timing. This method provides nanosecond precision, but not necessarily nanosecond accuracy. For more information, see the online Java documentation for System.nanoTimer for the JRE you are using with SnappyData. \n\n\nRuntime viewing of statistics archives files is not necessarily real-time, because of file system buffering. \n\n\n\n\n\n\nMore Information\n\n\n\n\n\n\nCollecting System Statistics\n\n    Enable SnappyData system statistics using a system procedure, member boot properties, or connection properties.\n\n\n\n\n\n\nUsing VSD to Analyze Statistics\n\n    The Visual Statistics Display (VSD) reads the sampled statistics from one or more archives and produces graphical displays for analysis. VSD is installed with SnappyData in the tools subdirectory.", 
            "title": "Evaluating Statistics for the System and Applications"
        }, 
        {
            "location": "/vsd/system_performance/#evaluating-statistics-for-the-system", 
            "text": "SnappyData provides statistics for analyzing system performance. Any member of a distributed system, including SnappyData servers, locators, and peer clients, can collect and archive this statistical data.  SnappyData samples statistics at a configurable interval and writes them to an archive. The archives can be read at any time, including at runtime.  You can view and analyze runtime or archived historical data using these tools:    snappy-shell stats  is a command-line tool provided with the SnappyData product.    SnappyData  Visual Statistics Display (VSD)  is a graphical tool that is installed in the  vsd  subdirectory of the SnappyData installation.     Note   SnappyData statistics use the Java System.nanoTimer for nanosecond timing. This method provides nanosecond precision, but not necessarily nanosecond accuracy. For more information, see the online Java documentation for System.nanoTimer for the JRE you are using with SnappyData.   Runtime viewing of statistics archives files is not necessarily real-time, because of file system buffering.     More Information    Collecting System Statistics \n    Enable SnappyData system statistics using a system procedure, member boot properties, or connection properties.    Using VSD to Analyze Statistics \n    The Visual Statistics Display (VSD) reads the sampled statistics from one or more archives and produces graphical displays for analysis. VSD is installed with SnappyData in the tools subdirectory.", 
            "title": "Evaluating Statistics for the System"
        }, 
        {
            "location": "/vsd/collecting_system_stats/", 
            "text": "Collecting System Statistics\n\n\nEnable SnappyData system statistics using a system procedure, member boot properties, or connection properties.\n\n\nYou can enable statistics collection per-member using the boot properties:\n\n\n\n\n\n\nstatistic-sampling-enabled\n\n\n\n\n\n\nenable-time-statistics\n\n\n\n\n\n\nstatistic-archive-file\n\n\n\n\n\n\n\n\nNote\n\n\nYou must include the \nstatistic-archive-file\n and specify a valid file name in order to enable statistics collection.\n\n\n\n\nThese boot properties help you configure a member's statistic archive file location and size:\n\n\n\n\n\n\narchive-disk-space-limit\n\n\n\n\n\n\narchive-file-size-limit\n\n\n\n\n\n\nstatistic-sample-rate\n\n\n\n\n\n\nTo collect statement-level statistics and time-based, statement-level statistics for a specific connection (rather than globally or per-member), use these connection properties with a peer client connection:\n\n\n\n\n\n\nenable-stats\n\n\n\n\n\n\nenable-timestats\n\n\n\n\n\n\nstatistic-archive-file\n\n\n\n\n\n\nThese properties can only be used with a peer client connect.\n\n\n\n\nNote\n\n\n\n\n\n\nBecause of the overhead required for taking many timestamps, it is recommended that you enable time-based statistics only during testing and debugging. \n\n\n\n\n\n\nUse statement-level statistics only when the number of individual statements is small, such as when using prepared statements. SnappyData creates a separate statistics instance for each individual statement. With a large number of these statements, it can be difficult to load archives into VSD and navigate to those statistics of interest.", 
            "title": "Collecting System Statistics"
        }, 
        {
            "location": "/vsd/collecting_system_stats/#collecting-system-statistics", 
            "text": "Enable SnappyData system statistics using a system procedure, member boot properties, or connection properties.  You can enable statistics collection per-member using the boot properties:    statistic-sampling-enabled    enable-time-statistics    statistic-archive-file     Note  You must include the  statistic-archive-file  and specify a valid file name in order to enable statistics collection.   These boot properties help you configure a member's statistic archive file location and size:    archive-disk-space-limit    archive-file-size-limit    statistic-sample-rate    To collect statement-level statistics and time-based, statement-level statistics for a specific connection (rather than globally or per-member), use these connection properties with a peer client connection:    enable-stats    enable-timestats    statistic-archive-file    These properties can only be used with a peer client connect.   Note    Because of the overhead required for taking many timestamps, it is recommended that you enable time-based statistics only during testing and debugging.     Use statement-level statistics only when the number of individual statements is small, such as when using prepared statements. SnappyData creates a separate statistics instance for each individual statement. With a large number of these statements, it can be difficult to load archives into VSD and navigate to those statistics of interest.", 
            "title": "Collecting System Statistics"
        }, 
        {
            "location": "/vsd/vsd_overview/", 
            "text": "Using VSD to Analyze Statistics\n\n\nThe Visual Statistics Display (VSD) reads the sampled statistics from one or more archives and produces graphical displays for analysis. VSD is installed with SnappyData in the \ntools\n subdirectory.\n\n\nVSD\u2019s extensive online help offers complete reference information about the tool. \n\n\n\n\nNote\n\n\nEnsure the following for running VSD:\n\n\n\n\n\n\nInstall 32-bit libraries on 64-bit Linux:\n\n    \"yum install glibc.i686 libX11.i686\" on RHEL/CentOS\n\n    \"apt-get install libc6:i386 libx11-6:i386\" on Ubuntu/Debian like systems\n\n\n\n\n\n\nLocally running X server. For example, an X server implementation like, XQuartz for Mac OS, Xming for Windows OS, and Xorg which is installed by default for Linux systems.\n\n\n\n\n\n\n\n\nMore information\n\n\n\n\n\n\nInstalling and Running VSD\n\n\n\n\n\n\nTransaction Performance\n\n\n\n\n\n\nTable Performance\n\n\n\n\n\n\nSQL Statement Performance\n\n\n\n\n\n\nMemory Usage\n\n\n\n\n\n\nClient Connections\n\n\n\n\n\n\nCPU Usage", 
            "title": "Using VSD to Analyze Statistics"
        }, 
        {
            "location": "/vsd/vsd_overview/#using-vsd-to-analyze-statistics", 
            "text": "The Visual Statistics Display (VSD) reads the sampled statistics from one or more archives and produces graphical displays for analysis. VSD is installed with SnappyData in the  tools  subdirectory.  VSD\u2019s extensive online help offers complete reference information about the tool.    Note  Ensure the following for running VSD:    Install 32-bit libraries on 64-bit Linux: \n    \"yum install glibc.i686 libX11.i686\" on RHEL/CentOS \n    \"apt-get install libc6:i386 libx11-6:i386\" on Ubuntu/Debian like systems    Locally running X server. For example, an X server implementation like, XQuartz for Mac OS, Xming for Windows OS, and Xorg which is installed by default for Linux systems.     More information    Installing and Running VSD    Transaction Performance    Table Performance    SQL Statement Performance    Memory Usage    Client Connections    CPU Usage", 
            "title": "Using VSD to Analyze Statistics"
        }, 
        {
            "location": "/vsd/running_vsd/", 
            "text": "Installing and Running VSD\n\n\nStart the VSD tool, load statistics files, and maintain the view you want on your statistics.\n\n\n\n\nInstall VSD\n\n\nVSD is a free analysis tool and is provided as-is. VSD is distributed with SnappyData. To install VSD, install SnappyData. See \nInstalling SnappyData\n for instructions.\n\n\nAfter you install SnappyData, you can find VSD in the following location of your installation:\n\n\nsnappydata-installdir\n/vsd\n\n\n\n\nWhere \nsnappydata-installdir\n corresponds to the location where SnappyData is installed. \n\n\nThe VSD tool installation has the following subdirectories:\n\n\n\n\n\n\nbin\n. The scripts and binaries that can be used to run VSD on a variety of operating systems. The following scripts are included with this release:\n\n\n\n\n\n\nvsd\n\n\n\n\n\n\nvsd.bat\n\n\n\n\n\n\nThe following binaries are included with this release:\n-   vsdwishLinux - for Linux\n\n\n\n\n\n\nlib\n. The jars and binary libraries needed to run VSD\n\n\n\n\n\n\n\n\nStart VSD\n\n\n\n\n\n\nLinux/Unix, MacOS or Other OS:\n\n\npre \n$ ./vsd\n\n\n\n\n\n\n\n\nLoad a Statistics File into VSD\n\n\nYou have several options for loading a statistics file into VSD:\n\n\n\n\n\n\nInclude the name of one or more statistics files on the VSD command line. Example:\n\n\npre\nvsd \nfilename.gfs\n ...\n\n\n\n\n\n\nBrowse for an existing statistics file through \nMain\n \n \nLoad Data\n File.\n\n\n\n\nType the full path in the \nDirectories\n field, in the \nFiles\n field select the file name, and then press \nOK\n to load the data file.\n\n\nSwitch to a statistics file that you\u2019ve already loaded by clicking the down-arrow next to the Files field.\n\n\n\n\nAfter you load the data file, the VSD main window displays a list of entities for which statistics are available. VSD uses color to distinguish between entities that are still running (shown in green) and those that have stopped (shown in black).\n\n\n\n\nMaintain a Current View of the Data File\n\n\nIf you select the menu item \nFile\n \n \nAuto Update\n, VSD automatically updates your display, and any associated charts, whenever the data file changes. Alternatively, you can choose \nFile\n \n \nUpdate\n periodically to update the display manually.\n\n\n\n\nAbout Statistics\n\n\nThe statistics values (Y-axis) are plotted over time (X-axis). This makes it easy to see how statistics are trending, and to correlate different statistics.\n\n\nSome statistics are cumulative from when the SnappyData system was started. Other statistics are instantaneous values that may change in any way between sample collection.\n\n\nCumulative statistics are best charted per second or per sample, so that the VSD chart is readable. Absolute values are best charted as No Filter.", 
            "title": "Installing and Running VSD"
        }, 
        {
            "location": "/vsd/running_vsd/#installing-and-running-vsd", 
            "text": "Start the VSD tool, load statistics files, and maintain the view you want on your statistics.", 
            "title": "Installing and Running VSD"
        }, 
        {
            "location": "/vsd/running_vsd/#install-vsd", 
            "text": "VSD is a free analysis tool and is provided as-is. VSD is distributed with SnappyData. To install VSD, install SnappyData. See  Installing SnappyData  for instructions.  After you install SnappyData, you can find VSD in the following location of your installation:  snappydata-installdir /vsd  Where  snappydata-installdir  corresponds to the location where SnappyData is installed.   The VSD tool installation has the following subdirectories:    bin . The scripts and binaries that can be used to run VSD on a variety of operating systems. The following scripts are included with this release:    vsd    vsd.bat    The following binaries are included with this release:\n-   vsdwishLinux - for Linux    lib . The jars and binary libraries needed to run VSD", 
            "title": "Install VSD"
        }, 
        {
            "location": "/vsd/running_vsd/#start-vsd", 
            "text": "Linux/Unix, MacOS or Other OS:  pre \n$ ./vsd", 
            "title": "Start VSD"
        }, 
        {
            "location": "/vsd/running_vsd/#load-a-statistics-file-into-vsd", 
            "text": "You have several options for loading a statistics file into VSD:    Include the name of one or more statistics files on the VSD command line. Example:  pre\nvsd  filename.gfs  ...    Browse for an existing statistics file through  Main     Load Data  File.   Type the full path in the  Directories  field, in the  Files  field select the file name, and then press  OK  to load the data file.  Switch to a statistics file that you\u2019ve already loaded by clicking the down-arrow next to the Files field.   After you load the data file, the VSD main window displays a list of entities for which statistics are available. VSD uses color to distinguish between entities that are still running (shown in green) and those that have stopped (shown in black).", 
            "title": "Load a Statistics File into VSD"
        }, 
        {
            "location": "/vsd/running_vsd/#maintain-a-current-view-of-the-data-file", 
            "text": "If you select the menu item  File     Auto Update , VSD automatically updates your display, and any associated charts, whenever the data file changes. Alternatively, you can choose  File     Update  periodically to update the display manually.", 
            "title": "Maintain a Current View of the Data File"
        }, 
        {
            "location": "/vsd/running_vsd/#about-statistics", 
            "text": "The statistics values (Y-axis) are plotted over time (X-axis). This makes it easy to see how statistics are trending, and to correlate different statistics.  Some statistics are cumulative from when the SnappyData system was started. Other statistics are instantaneous values that may change in any way between sample collection.  Cumulative statistics are best charted per second or per sample, so that the VSD chart is readable. Absolute values are best charted as No Filter.", 
            "title": "About Statistics"
        }, 
        {
            "location": "/vsd/vsd_transactions/", 
            "text": "Transaction Performance\n\n\nSnappyData provides statistics for transaction commits, rollbacks, and failures You can monitor SnappyData transactions with VSD.\n\n\nYou can use these statistics to see the transaction rate. An example follows the table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nCachePerfStats\n\n\n\n\n\n\nName\n\n\ncachePerfStats\n\n\n\n\n\n\nStatistic\n\n\ntxCommits, txRollbacks\n\n\n\n\n\n\nDescription\n\n\nThe number of times a transaction has committed or rolled back.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nCachePerfStats\n\n\n\n\n\n\nName\n\n\ncachePerfStats\n\n\n\n\n\n\nStatistic\n\n\ntxSuccessLifeTime, txRollbackLifeTime\n\n\n\n\n\n\nDescription\n\n\nThe total time spent in a transaction that committed or rolled back.\n\n\n\n\n\n\n\n\n\n\nExample: Transaction Rate and Latency\n\n\nThis VSD chart shows the aggregated transaction rate across three FabricServer instances, along with the rollback rate.\n\n\n\n\nWith time statistics enabled, you can compute the average latency for each sample point as the total time spent in the transaction divided by the number of transactions in each sample. The chart also shows that the average latency across all samples is 6.76 ms.", 
            "title": "Transaction Performance"
        }, 
        {
            "location": "/vsd/vsd_transactions/#transaction-performance", 
            "text": "SnappyData provides statistics for transaction commits, rollbacks, and failures You can monitor SnappyData transactions with VSD.  You can use these statistics to see the transaction rate. An example follows the table.           Type  CachePerfStats    Name  cachePerfStats    Statistic  txCommits, txRollbacks    Description  The number of times a transaction has committed or rolled back.              Type  CachePerfStats    Name  cachePerfStats    Statistic  txSuccessLifeTime, txRollbackLifeTime    Description  The total time spent in a transaction that committed or rolled back.", 
            "title": "Transaction Performance"
        }, 
        {
            "location": "/vsd/vsd_transactions/#example-transaction-rate-and-latency", 
            "text": "This VSD chart shows the aggregated transaction rate across three FabricServer instances, along with the rollback rate.   With time statistics enabled, you can compute the average latency for each sample point as the total time spent in the transaction divided by the number of transactions in each sample. The chart also shows that the average latency across all samples is 6.76 ms.", 
            "title": "Example: Transaction Rate and Latency"
        }, 
        {
            "location": "/vsd/vsd_tables/", 
            "text": "Table Performance\n\n\nYou can get an idea of the relative performance of inserts, updates, and selects by looking at the underlying statistics.\n\n\nAn example follows the table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nCachePerfStats\n\n\n\n\n\n\nName\n\n\nRegionStats-\ntable\n\n\n\n\n\n\nStatistic\n\n\ncreates, puts, deletes\n\n\n\n\n\n\nDescription\n\n\nNumber of times that an entry is added to, replaced in, or read from the replicated table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nCachePerfStats\n\n\n\n\n\n\nName\n\n\nRegionStats-partition-\ntable\n\n\n\n\n\n\nStatistic\n\n\ncreates, puts, deletes\n\n\n\n\n\n\nDescription\n\n\nNumber of times that an entry is added to, replaced in, or read from the partitioned table.\n\n\n\n\n\n\n\n\n\n\nExample: Read and Write Performance\n\n\nThese VSD charts show the rates of reads and writes to the various tables in the application.\n\n\n\n\n\n\nYou can use these statistics to see that the NEW_ORDER table is growing over time. After the initial orders are loaded, new orders are being placed (created) faster than customers are paying for (destroying) them.", 
            "title": "Table Performance"
        }, 
        {
            "location": "/vsd/vsd_tables/#table-performance", 
            "text": "You can get an idea of the relative performance of inserts, updates, and selects by looking at the underlying statistics.  An example follows the table.           Type  CachePerfStats    Name  RegionStats- table    Statistic  creates, puts, deletes    Description  Number of times that an entry is added to, replaced in, or read from the replicated table.              Type  CachePerfStats    Name  RegionStats-partition- table    Statistic  creates, puts, deletes    Description  Number of times that an entry is added to, replaced in, or read from the partitioned table.", 
            "title": "Table Performance"
        }, 
        {
            "location": "/vsd/vsd_tables/#example-read-and-write-performance", 
            "text": "These VSD charts show the rates of reads and writes to the various tables in the application.    You can use these statistics to see that the NEW_ORDER table is growing over time. After the initial orders are loaded, new orders are being placed (created) faster than customers are paying for (destroying) them.", 
            "title": "Example: Read and Write Performance"
        }, 
        {
            "location": "/vsd/vsd_statements/", 
            "text": "SQL Statement Performance\n\n\nWith statement-level statistics enabled, statistics are available for all DML statements.\n\n\nAn example follows the table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nStatementStats\n\n\n\n\n\n\nName\n\n\nINSERT\ndml\n, SELECT\ndml\n, UPDATE\ndml\n, DELETE\ndml\n\n\n\n\n\n\nStatistic\n\n\nNumExecutesEnded\n\n\n\n\n\n\nDescription\n\n\nThe number of times that a statement was executed.\n\n\n\n\n\n\n\n\n\n\nExample: Prepared Statements\n\n\nThis VSD chart shows statistics for two fabric servers for prepared statements that insert rows in the history table. The first round of inserts occurs during initial data loading. The second round occurs as part of normal business operations. This chart also shows that the application used different prepared statements for data loading and subsequent transactions, with slightly different ordering in the fields. If the statements had been identical, there would be only one instance per server.", 
            "title": "SQL Statement Performance"
        }, 
        {
            "location": "/vsd/vsd_statements/#sql-statement-performance", 
            "text": "With statement-level statistics enabled, statistics are available for all DML statements.  An example follows the table.           Type  StatementStats    Name  INSERT dml , SELECT dml , UPDATE dml , DELETE dml    Statistic  NumExecutesEnded    Description  The number of times that a statement was executed.", 
            "title": "SQL Statement Performance"
        }, 
        {
            "location": "/vsd/vsd_statements/#example-prepared-statements", 
            "text": "This VSD chart shows statistics for two fabric servers for prepared statements that insert rows in the history table. The first round of inserts occurs during initial data loading. The second round occurs as part of normal business operations. This chart also shows that the application used different prepared statements for data loading and subsequent transactions, with slightly different ordering in the fields. If the statements had been identical, there would be only one instance per server.", 
            "title": "Example: Prepared Statements"
        }, 
        {
            "location": "/vsd/vsd_memory/", 
            "text": "Memory Usage\n\n\nSnappyData provides statistics for system memory, JVM heap, garbage collection, and table sizes.\n\n\nYou can use these statistics to analyze your application's memory usage. An example follows the table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nCachePerfStats\n\n\n\n\n\n\nName\n\n\nRegionStats-\ntable\n and RegsionStats-partition-\ntable\n\n\n\n\n\n\nStatistic\n\n\nentries\n\n\n\n\n\n\nDescription\n\n\nNumber of rows in the replicated or partitioned table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nPartitionedRegionStats\n\n\n\n\n\n\nName\n\n\nPartitionedRegion/\nschema\n/\ntable\nStatistics\n\n\n\n\n\n\nStatistic\n\n\ndataStoreEntryCount and dataStoreBytesInUse\n\n\n\n\n\n\nDescription\n\n\nNumber of rows/bytes in a partitioned table, including redundant copies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nVMMemoryUsageStats\n\n\n\n\n\n\nName\n\n\nvmHeapMemoryStats\n\n\n\n\n\n\nStatistic\n\n\nusedMemory and maxMemory\n\n\n\n\n\n\nDescription\n\n\nUsed heap and maximum heap, in bytes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nVMMemoryUsageStats\n\n\n\n\n\n\nName\n\n\nOffHeapMemoryStats\n\n\n\n\n\n\nStatistic\n\n\nusedMemory, maxMemory, freeMemory\n\n\n\n\n\n\nDescription\n\n\nUsed off-heap memory, maximum (amount allocated) off-heap memory, and free off-heap memory in bytes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nVMMemoryPoolStats\n\n\n\n\n\n\nName\n\n\nParSurvivorSpace, ParEdenSpace, CMSOldGen\n\n\n\n\n\n\nStatistic\n\n\ncurrentUsedMemory\n\n\n\n\n\n\nDescription\n\n\nEstimated used memory, in bytes, for each heap memory pool.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nVMGCStats\n\n\n\n\n\n\nName\n\n\nParNew, ConcurrentMarkSweep\n\n\n\n\n\n\nStatistic\n\n\ncollectionTime\n\n\n\n\n\n\nDescription\n\n\nApproximate elapsed time that this garbage collector spent doing collections.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nLinuxSystemStats\n\n\n\n\n\n\nName\n\n\nhostname\n\n\n\n\n\n\nStatistic\n\n\npagesSwappedIn, pagesSwappedOut\n\n\n\n\n\n\nDescription\n\n\nNumber of pages that have been brought into memory from disk or flushed out of memory to disk by the operating system. These paging operations seriously degrade performance.\n\n\n\n\n\n\n\n\n\n\nExample: Heap Usage\n\n\nThis VSD chart shows the heap usage in a fabric server plotted against the entry counts in each of the replicated and partitioned tables used in an application. This suggests that heap is growing over time due to an increase in the number of rows in the ORDER_LINE table. Several associated tables are growing in size as well.\n\n\n\nThis growth can be verified by looking at the bytes used by each of the partitioned tables. The ORDER_LINE table is responsible for most of the heap growth.", 
            "title": "Memory Usage"
        }, 
        {
            "location": "/vsd/vsd_memory/#memory-usage", 
            "text": "SnappyData provides statistics for system memory, JVM heap, garbage collection, and table sizes.  You can use these statistics to analyze your application's memory usage. An example follows the table.           Type  CachePerfStats    Name  RegionStats- table  and RegsionStats-partition- table    Statistic  entries    Description  Number of rows in the replicated or partitioned table.              Type  PartitionedRegionStats    Name  PartitionedRegion/ schema / table Statistics    Statistic  dataStoreEntryCount and dataStoreBytesInUse    Description  Number of rows/bytes in a partitioned table, including redundant copies.              Type  VMMemoryUsageStats    Name  vmHeapMemoryStats    Statistic  usedMemory and maxMemory    Description  Used heap and maximum heap, in bytes.              Type  VMMemoryUsageStats    Name  OffHeapMemoryStats    Statistic  usedMemory, maxMemory, freeMemory    Description  Used off-heap memory, maximum (amount allocated) off-heap memory, and free off-heap memory in bytes.              Type  VMMemoryPoolStats    Name  ParSurvivorSpace, ParEdenSpace, CMSOldGen    Statistic  currentUsedMemory    Description  Estimated used memory, in bytes, for each heap memory pool.              Type  VMGCStats    Name  ParNew, ConcurrentMarkSweep    Statistic  collectionTime    Description  Approximate elapsed time that this garbage collector spent doing collections.              Type  LinuxSystemStats    Name  hostname    Statistic  pagesSwappedIn, pagesSwappedOut    Description  Number of pages that have been brought into memory from disk or flushed out of memory to disk by the operating system. These paging operations seriously degrade performance.", 
            "title": "Memory Usage"
        }, 
        {
            "location": "/vsd/vsd_memory/#example-heap-usage", 
            "text": "This VSD chart shows the heap usage in a fabric server plotted against the entry counts in each of the replicated and partitioned tables used in an application. This suggests that heap is growing over time due to an increase in the number of rows in the ORDER_LINE table. Several associated tables are growing in size as well.  \nThis growth can be verified by looking at the bytes used by each of the partitioned tables. The ORDER_LINE table is responsible for most of the heap growth.", 
            "title": "Example: Heap Usage"
        }, 
        {
            "location": "/vsd/vsd-connection-stats/", 
            "text": "Client Connections\n\n\nSnappyData provides several statistics to help you determine the frequency and duration of client connections to the distributed system.\n\n\nThe following tables describe some commonly-used connection statistics. Examine the VSD output for information about additional connection statistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nConnectionStats\n\n\n\n\n\n\nName\n\n\nConnectionStats\n\n\n\n\n\n\nStatistic\n\n\nclientConnectionsIdle\n\n\n\n\n\n\nDescription\n\n\nThe number of client connections that were idle at the statistic sampling time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nConnectionStats\n\n\n\n\n\n\nName\n\n\nConnectionStats\n\n\n\n\n\n\nStatistic\n\n\nclientConnectionsOpen\n\n\n\n\n\n\nDescription\n\n\nThe total number of client connections that were open (both active and idle connections) at the statistic sampling time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nConnectionStats\n\n\n\n\n\n\nName\n\n\nConnectionStats\n\n\n\n\n\n\nStatistic\n\n\nclientConnectionsOpened, peerConnectionsOpened\n\n\n\n\n\n\nDescription\n\n\nThe total number of client connections that were successfully opened up to the statistic sampling time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nConnectionStats\n\n\n\n\n\n\nName\n\n\nConnectionStats\n\n\n\n\n\n\nStatistic\n\n\nclientConnectionsFailed, peerConnectionsFailed\n\n\n\n\n\n\nDescription\n\n\nThe total number of unsuccessful connection attempts that were made up to the statistic sampling time.\n\n\n\n\n\n\n\n\n\n\nExample: Selecting Connection Statistics\n\n\nThis figure illustrates how to select the connection statistics for graphing.", 
            "title": "Client Connections"
        }, 
        {
            "location": "/vsd/vsd-connection-stats/#client-connections", 
            "text": "SnappyData provides several statistics to help you determine the frequency and duration of client connections to the distributed system.  The following tables describe some commonly-used connection statistics. Examine the VSD output for information about additional connection statistics.           Type  ConnectionStats    Name  ConnectionStats    Statistic  clientConnectionsIdle    Description  The number of client connections that were idle at the statistic sampling time.              Type  ConnectionStats    Name  ConnectionStats    Statistic  clientConnectionsOpen    Description  The total number of client connections that were open (both active and idle connections) at the statistic sampling time.              Type  ConnectionStats    Name  ConnectionStats    Statistic  clientConnectionsOpened, peerConnectionsOpened    Description  The total number of client connections that were successfully opened up to the statistic sampling time.              Type  ConnectionStats    Name  ConnectionStats    Statistic  clientConnectionsFailed, peerConnectionsFailed    Description  The total number of unsuccessful connection attempts that were made up to the statistic sampling time.", 
            "title": "Client Connections"
        }, 
        {
            "location": "/vsd/vsd-connection-stats/#example-selecting-connection-statistics", 
            "text": "This figure illustrates how to select the connection statistics for graphing.", 
            "title": "Example: Selecting Connection Statistics"
        }, 
        {
            "location": "/vsd/vsd_cpu/", 
            "text": "CPU Usage\n\n\nSnappyData provides host and JVM statistics for examining system load.\n\n\nAn example follows the table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nLinuxSystemStats\n\n\n\n\n\n\nName\n\n\nhostname\n\n\n\n\n\n\nStatistic\n\n\ncpus\n\n\n\n\n\n\nDescription\n\n\nNumber of CPUs on this host.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nLinuxSystemStats\n\n\n\n\n\n\nName\n\n\nhostname\n\n\n\n\n\n\nStatistic\n\n\ncpuActive, cpuSystem, cpuUser\n\n\n\n\n\n\nDescription\n\n\nPercentage of the total available time that has been used in a non-idle state, in system (kernel) code and in user code. This is aggregated across all CPUs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nLinuxSystemStats\n\n\n\n\n\n\nName\n\n\nhostname\n\n\n\n\n\n\nStatistic\n\n\nloadAverage1, loadAverage5, loadAverage15\n\n\n\n\n\n\nDescription\n\n\nAverage number of threads in the run queue or waiting for disk I/O over the last 1, 5, 15 minutes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\nVMStats\n\n\n\n\n\n\nName\n\n\nvmStats\n\n\n\n\n\n\nStatistic\n\n\nprocessCpuTime\n\n\n\n\n\n\nDescription\n\n\nCPU time, measured in nanoseconds, used by the process.\n\n\n\n\n\n\n\n\n\n\nExample: CPU Usage by System and User\n\n\nThis VSD chart shows the CPU usage in a fabric server, and breaks it down into system and user. The majority of the time is spent in user code. The first few minutes correspond to loading various tables. The rest is the main workload, during which many clients are executing transactions.", 
            "title": "CPU Usage"
        }, 
        {
            "location": "/vsd/vsd_cpu/#cpu-usage", 
            "text": "SnappyData provides host and JVM statistics for examining system load.  An example follows the table.           Type  LinuxSystemStats    Name  hostname    Statistic  cpus    Description  Number of CPUs on this host.              Type  LinuxSystemStats    Name  hostname    Statistic  cpuActive, cpuSystem, cpuUser    Description  Percentage of the total available time that has been used in a non-idle state, in system (kernel) code and in user code. This is aggregated across all CPUs.              Type  LinuxSystemStats    Name  hostname    Statistic  loadAverage1, loadAverage5, loadAverage15    Description  Average number of threads in the run queue or waiting for disk I/O over the last 1, 5, 15 minutes.              Type  VMStats    Name  vmStats    Statistic  processCpuTime    Description  CPU time, measured in nanoseconds, used by the process.", 
            "title": "CPU Usage"
        }, 
        {
            "location": "/vsd/vsd_cpu/#example-cpu-usage-by-system-and-user", 
            "text": "This VSD chart shows the CPU usage in a fabric server, and breaks it down into system and user. The majority of the time is spent in user code. The first few minutes correspond to loading various tables. The rest is the main workload, during which many clients are executing transactions.", 
            "title": "Example: CPU Usage by System and User"
        }, 
        {
            "location": "/reference/", 
            "text": "Reference Guides\n\n\nThe SnappyData Reference provides a list and description of command-line utilities, APIs, and Standard Query Language (SQL) implementation.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nSQL Reference Guide\n\n\n\n\n\n\nBuilt-in System Procedures\n\n\n\n\n\n\nSystem Tables\n\n\n\n\n\n\nCommand Line Utilities\n\n\n\n\n\n\nSnappy-SQL Shell Interactive Commands\n\n\n\n\n\n\nConfiguration Parameters\n\n\n\n\n\n\n\n\nTip\n\n\nSnappyData supports the built-in functions supported by Spark. For more information, refer to the \nApache Spark Documentation\n.", 
            "title": "Reference Guides"
        }, 
        {
            "location": "/reference/#reference-guides", 
            "text": "The SnappyData Reference provides a list and description of command-line utilities, APIs, and Standard Query Language (SQL) implementation.  The following topics are covered in this section:    SQL Reference Guide    Built-in System Procedures    System Tables    Command Line Utilities    Snappy-SQL Shell Interactive Commands    Configuration Parameters     Tip  SnappyData supports the built-in functions supported by Spark. For more information, refer to the  Apache Spark Documentation .", 
            "title": "Reference Guides"
        }, 
        {
            "location": "/sql_reference/", 
            "text": "SQL Reference Guide\n\n\nThis section provides a complete description of the Structured Query Language (SQL) used to manage information in SnappyData. It includes syntax, usage, keywords, and examples of the SQL statements used on SnappyData.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nALTER TABLE\n\n\n\n\n\n\nCREATE STATEMENTS\n\n\n\n\n\n\nCREATE DISKSTORE\n\n\n\n\n\n\nCREATE EXTERNAL TABLE\n\n\n\n\n\n\nCREATE FUNCTION\n\n\n\n\n\n\nCREATE INDEX\n\n\n\n\n\n\nCREATE SCHEMA\n\n\n\n\n\n\nCREATE SAMPLE TABLE\n\n\n\n\n\n\nCREATE STREAM TABLE\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nCREATE TEMPORARY TABLE\n\n\n\n\n\n\nCREATE VIEW\n\n\n\n\n\n\n\n\n\n\nDELETE\n\n\n\n\n\n\nDEPLOY STATEMENTS\n\n\n\n\n\n\nDEPLOY PACKAGES\n\n\n\n\n\n\nDEPLOY JARS\n\n\n\n\n\n\nUNDEPLOY\n\n\n\n\n\n\n\n\n\n\nDROP STATEMENTS\n\n\n\n\n\n\nDROP DISKSTORE\n\n\n\n\n\n\nDROP FUNCTION\n\n\n\n\n\n\nDROP INDEX\n\n\n\n\n\n\nDROP TABLE/EXTERNAL TABLE/SAMPLE TABLE\n\n\n\n\n\n\nDROP SCHEMA\n\n\n\n\n\n\n\n\n\n\nGRANT\n\n\n\n\n\n\nINSERT\n\n\n\n\n\n\nLATERAL VIEW\n\n\n\n\n\n\nPUT INTO\n\n\n\n\n\n\nREVOKE\n\n\n\n\n\n\nSELECT\n\n\n\n\n\n\nSET ISOLATION\n\n\n\n\n\n\nSET SCHEMA\n\n\n\n\n\n\nTRUNCATE TABLE\n\n\n\n\n\n\nUPDATE", 
            "title": "SQL Reference Guide"
        }, 
        {
            "location": "/sql_reference/#sql-reference-guide", 
            "text": "This section provides a complete description of the Structured Query Language (SQL) used to manage information in SnappyData. It includes syntax, usage, keywords, and examples of the SQL statements used on SnappyData.  The following topics are covered in this section:    ALTER TABLE    CREATE STATEMENTS    CREATE DISKSTORE    CREATE EXTERNAL TABLE    CREATE FUNCTION    CREATE INDEX    CREATE SCHEMA    CREATE SAMPLE TABLE    CREATE STREAM TABLE    CREATE TABLE    CREATE TEMPORARY TABLE    CREATE VIEW      DELETE    DEPLOY STATEMENTS    DEPLOY PACKAGES    DEPLOY JARS    UNDEPLOY      DROP STATEMENTS    DROP DISKSTORE    DROP FUNCTION    DROP INDEX    DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE    DROP SCHEMA      GRANT    INSERT    LATERAL VIEW    PUT INTO    REVOKE    SELECT    SET ISOLATION    SET SCHEMA    TRUNCATE TABLE    UPDATE", 
            "title": "SQL Reference Guide"
        }, 
        {
            "location": "/reference/sql_reference/alter-table/", 
            "text": "ALTER TABLE\n\n\nUse the ALTER TABLE statement to add and drop columns in row tables using SnappyData API or SQL.\n\n\n\n\nNote\n\n\n\n\n\n\nALTER TABLE is not supported on column, temporary and external tables.\n\n\n\n\n\n\nFor row tables, only adding and dropping a column is supported using Snappy APIs or SQL.\n\n\n\n\n\n\n\n\nSyntax\n\n\nSQL\n\n\nALTER TABLE table-name\n{\n  ADD COLUMN column-definition |\n  DROP COLUMN column-name\n}\n\n\n\n\nAPI:\n\n\nsnc.alterTable(tableName, isAddColumn, column)\n\n\n\n\nExample\n\n\nSQL:\n\n\n-- create a table\nCREATE TABLE trade.customers (\n    cid int not null,\n    cust_name varchar(100),\n    addr varchar(100),\n    tid int);\n\n-- drop a non-primary key column if the column is not used for table partitioning, and the column has no dependents\nALTER TABLE trade.customers DROP COLUMN addr;\n\n-- add the column back with a default value\nALTER TABLE trade.customers ADD COLUMN addr varchar(100);\n\n\n\n\nAPI:\n\n\n//create a table in Snappy store\n    snc.createTable(\norders\n, \nrow\n, ordersDF.schema, Map.empty[String, String])\n\n//alter table add/drop specified column, only supported for row tables.\n\n// for adding a column isAddColumn should be true\n    snc.alterTable(\norders\n, true, StructField(\nFirstName\n, StringType, true))\n\n// for dropping a column isAddColumn should be false\n    snc.alterTable(\norders\n, false, StructField(\nFirstName\n, StringType, true))", 
            "title": "ALTER TABLE"
        }, 
        {
            "location": "/reference/sql_reference/alter-table/#alter-table", 
            "text": "Use the ALTER TABLE statement to add and drop columns in row tables using SnappyData API or SQL.   Note    ALTER TABLE is not supported on column, temporary and external tables.    For row tables, only adding and dropping a column is supported using Snappy APIs or SQL.", 
            "title": "ALTER TABLE"
        }, 
        {
            "location": "/reference/sql_reference/alter-table/#syntax", 
            "text": "SQL  ALTER TABLE table-name\n{\n  ADD COLUMN column-definition |\n  DROP COLUMN column-name\n}  API:  snc.alterTable(tableName, isAddColumn, column)", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/alter-table/#example", 
            "text": "SQL:  -- create a table\nCREATE TABLE trade.customers (\n    cid int not null,\n    cust_name varchar(100),\n    addr varchar(100),\n    tid int);\n\n-- drop a non-primary key column if the column is not used for table partitioning, and the column has no dependents\nALTER TABLE trade.customers DROP COLUMN addr;\n\n-- add the column back with a default value\nALTER TABLE trade.customers ADD COLUMN addr varchar(100);  API:  //create a table in Snappy store\n    snc.createTable( orders ,  row , ordersDF.schema, Map.empty[String, String])\n\n//alter table add/drop specified column, only supported for row tables.\n\n// for adding a column isAddColumn should be true\n    snc.alterTable( orders , true, StructField( FirstName , StringType, true))\n\n// for dropping a column isAddColumn should be false\n    snc.alterTable( orders , false, StructField( FirstName , StringType, true))", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-statements/", 
            "text": "Create Statements\n\n\nUse Create statements to create functions, indexes, procedures, schemas, tables, and views.\n\n\n\n\n\n\nCREATE DISKSTORE\n\n\n\n\n\n\nCREATE FUNCTION\n\n\n\n\n\n\nCREATE INDEX\n\n\n\n\n\n\nCREATE SCHEMA\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nCREATE EXTERNAL TABLE\n\n\n\n\n\n\nCREATE SAMPLE TABLE\n\n\n\n\n\n\nCREATE STREAM TABLE", 
            "title": "CREATE Statements"
        }, 
        {
            "location": "/reference/sql_reference/create-statements/#create-statements", 
            "text": "Use Create statements to create functions, indexes, procedures, schemas, tables, and views.    CREATE DISKSTORE    CREATE FUNCTION    CREATE INDEX    CREATE SCHEMA    CREATE TABLE    CREATE EXTERNAL TABLE    CREATE SAMPLE TABLE    CREATE STREAM TABLE", 
            "title": "Create Statements"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/", 
            "text": "CREATE DISKSTORE\n\n\nDisk stores provide disk storage for tables that need to overflow or persist.\n\n\nCREATE DISKSTORE diskstore_name\n\n    [ MAXLOGSIZE max-log-size-in-mb ]\n    [ AUTOCOMPACT boolean-constant ]\n    [ ALLOWFORCECOMPACTION boolean-constant ]\n    [ COMPACTIONTHRESHOLD garbage-threshold ]\n    [ TIMEINTERVAL time-after-which-data-is-flused-to-disk ]\n    [ WRITEBUFFERSIZE buffer-size-in-mb ]\n    [ QUEUESIZE max-row-operations-to-disk ]\n    [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]\n\n\n\n\nDescription\n\n\nSnappyData attempts to preallocate oplog files when you execute the CREATE DISKSTORE command. \n\n\n\n\n\nAll tables that target the same disk store share that disk store's persistence attributes. A table that does not target a named disk store uses the default disk store for overflow or persistence. By default, SnappyData uses the working directory of the member as the default disk store.\n\n\nMAXLOGSIZE\n\n\nSnappyData records DML statements in an operation log (oplog) files. This option sets the maximum size in megabytes that the oplog can become before SnappyData automatically rolls to a new file. This size is the combined sizes of the oplog files. When SnappyData creates an oplog file, it immediately reserves this amount of file space. SnappyData only truncates the unused space on a clean shutdown (for example, \nsnappy server stop\n or \n./sbin/snappy-stop-all\n).\n\n\nThe default value is 1 GB.\n\n\nAUTOCOMPACT\n\n\nSet this option to \"true\" (the default) to automatically compact disk files. Set the option to \"false\" if compaction is not needed or if you intend to manually compact disk files using the \nsnappy\n utility.\n\n\nSnappyData performs compaction by removing \"garbage\" data that DML statements generate in the oplog file.\n\n\nALLOWFORCECOMPACTION\n\n\nSet this option to \"true\" to enable online compaction of oplog files using the \nsnappy\n utility. By default, this option is set to \"false\" (disabled).\n\n\nCOMPACTIONTHRESHOLD\n\n\nSets the threshold for the amount of \"garbage\" data that can exist in the oplog before SnappyData initiates automatic compaction. Garbage data is created as DML operations create, update, and delete rows in a table. The threshold is defined as a percentage (an integer from 0\u2013100). The default is 50. When the amount of \"garbage\" data exceeds this percentage, the disk store becomes eligible for auto-compaction if AUTOCOMPACT is enabled.\n\n\nTIMEINTERVAL\n\n\nSets the number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. TIMEINTERVAL is only used for tables that were created using the \nasynchronous\n option in the persistence clause of the CREATE TABLE statement. See \nCREATE TABLE\n. The default value is 1000 milliseconds (1 second).\n\n\nWRITEBUFFERSIZE\n\n\nSets the buffer size in bytes to use when persisting data to disk. The default is 32768 bytes.\n\n\nQUEUESIZE\n\n\nSets the maximum number of row operations that SnappyData asynchronously queues to disk. After this number of asynchronous operations are queued, additional asynchronous operations block until existing writes are flushed to disk. A single DML operation may affect multiple rows, and each row modification, insertion, and deletion are considered a separate operation. The default QUEUESIZE value is 0, which specifies no limit.\n\n\ndir-name\n\n\nThe optional \ndir-name\n entry defines a specific host system directory to use for the disk store. You can include one or more \ndir-name\n entries using the syntax:\n\n\n[ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]\n\n\n\n\nIn each entry:\n\n\n\n\n\n\ndir-name\n specifies the name of a directory to use for the disk store. The disk store directory is created on each member if necessary. If you do not specify an absolute path, then SnappyData creates or uses the named directory in each member's working directory (or in the value specified by the \nsys-disk-dir\n boot property, if defined). If you specify an absolute path, then all parent directories in the path must exist at the time you execute the command.\n\n\n\n\nNote\n\n\nSnappyData uses a \"shared nothing\" disk store design, and you cannot use a single disk store directory to store oplog files from multiple SnappyData members. \n\n\n\n\n\n\n\n\ndisk-space-in-mb\n optionally specifies the maximum amount of space, in megabytes, to use for the disk store in that directory. The space used is calculated as the combined sizes of all oplog files in the directory.\n\n\nIf you do not specify the \ndisk-space-in-mb\n value, then SnappyData does not impose a limit on the amount of space used by disk store files in that directory. If you do specify a limit, the size must be large enough to accommodate the disk store oplog files (the \nMAXLOGSIZE\n value, or 1 GB by default) and leave enough free space in the directory to avoid low disk space warnings. If you specify a size that cannot accommodate the oplog files and maintain enough free space, SnappyData fails to create the disk store with SQLState error XOZ33: Cannot create oplogs with size {0}MB which is greater than the maximum size {1}MB for store directory ''{2}''.\n\n\n\n\n\n\nYou can specify any number of \ndir-name\n entries in a \nCREATE DISKSTORE\n statement. The default diskstore does not have an option for multiple directories. It is recommended that you use separate data area from the server working directory.\nCreate a separate diskstore and specify multiple directories. The data is spread evenly among the active disk files in the directories, keeping within any limits you set.\n\n\nExample\n\n\nThis example uses the default base directory and parameter values to create a named disk store:\n\n\nsnappy\n CREATE DISKSTORE STORE1;\n\n\n\n\nThis example configures disk store parameters and specifies a storage directory:\n\n\nsnappy\n CREATE DISKSTORE STORE1\n      MAXLOGSIZE 1024 \n      AUTOCOMPACT TRUE\n      ALLOWFORCECOMPACTION  FALSE \n      COMPACTIONTHRESHOLD  80\n      TIMEINTERVAL  223344\n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 10240);\n\n\n\n\nThis example specifies multiple storage directories and directory sizes for oplog files:\n\n\nsnappy\n CREATE DISKSTORE STORE1 \n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 456 , 'dir2', 'dir3' 532 );\n\n\n\n\nRelated Topics\n\n\n\n\n\n\nDROP DISKSTORE\n\n\n\n\n\n\nSYSDISKSTORES", 
            "title": "CREATE DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/#create-diskstore", 
            "text": "Disk stores provide disk storage for tables that need to overflow or persist.  CREATE DISKSTORE diskstore_name\n\n    [ MAXLOGSIZE max-log-size-in-mb ]\n    [ AUTOCOMPACT boolean-constant ]\n    [ ALLOWFORCECOMPACTION boolean-constant ]\n    [ COMPACTIONTHRESHOLD garbage-threshold ]\n    [ TIMEINTERVAL time-after-which-data-is-flused-to-disk ]\n    [ WRITEBUFFERSIZE buffer-size-in-mb ]\n    [ QUEUESIZE max-row-operations-to-disk ]\n    [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]", 
            "title": "CREATE DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/#description", 
            "text": "SnappyData attempts to preallocate oplog files when you execute the CREATE DISKSTORE command.    All tables that target the same disk store share that disk store's persistence attributes. A table that does not target a named disk store uses the default disk store for overflow or persistence. By default, SnappyData uses the working directory of the member as the default disk store.  MAXLOGSIZE  SnappyData records DML statements in an operation log (oplog) files. This option sets the maximum size in megabytes that the oplog can become before SnappyData automatically rolls to a new file. This size is the combined sizes of the oplog files. When SnappyData creates an oplog file, it immediately reserves this amount of file space. SnappyData only truncates the unused space on a clean shutdown (for example,  snappy server stop  or  ./sbin/snappy-stop-all ).  The default value is 1 GB.  AUTOCOMPACT  Set this option to \"true\" (the default) to automatically compact disk files. Set the option to \"false\" if compaction is not needed or if you intend to manually compact disk files using the  snappy  utility.  SnappyData performs compaction by removing \"garbage\" data that DML statements generate in the oplog file.  ALLOWFORCECOMPACTION  Set this option to \"true\" to enable online compaction of oplog files using the  snappy  utility. By default, this option is set to \"false\" (disabled).  COMPACTIONTHRESHOLD  Sets the threshold for the amount of \"garbage\" data that can exist in the oplog before SnappyData initiates automatic compaction. Garbage data is created as DML operations create, update, and delete rows in a table. The threshold is defined as a percentage (an integer from 0\u2013100). The default is 50. When the amount of \"garbage\" data exceeds this percentage, the disk store becomes eligible for auto-compaction if AUTOCOMPACT is enabled.  TIMEINTERVAL  Sets the number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk. TIMEINTERVAL is only used for tables that were created using the  asynchronous  option in the persistence clause of the CREATE TABLE statement. See  CREATE TABLE . The default value is 1000 milliseconds (1 second).  WRITEBUFFERSIZE  Sets the buffer size in bytes to use when persisting data to disk. The default is 32768 bytes.  QUEUESIZE  Sets the maximum number of row operations that SnappyData asynchronously queues to disk. After this number of asynchronous operations are queued, additional asynchronous operations block until existing writes are flushed to disk. A single DML operation may affect multiple rows, and each row modification, insertion, and deletion are considered a separate operation. The default QUEUESIZE value is 0, which specifies no limit.  dir-name  The optional  dir-name  entry defines a specific host system directory to use for the disk store. You can include one or more  dir-name  entries using the syntax:  [ ( 'dir-name' [ disk-space-in-mb ] [,'dir-name' [ disk-space-in-mb ] ]* ) ]  In each entry:    dir-name  specifies the name of a directory to use for the disk store. The disk store directory is created on each member if necessary. If you do not specify an absolute path, then SnappyData creates or uses the named directory in each member's working directory (or in the value specified by the  sys-disk-dir  boot property, if defined). If you specify an absolute path, then all parent directories in the path must exist at the time you execute the command.   Note  SnappyData uses a \"shared nothing\" disk store design, and you cannot use a single disk store directory to store oplog files from multiple SnappyData members.      disk-space-in-mb  optionally specifies the maximum amount of space, in megabytes, to use for the disk store in that directory. The space used is calculated as the combined sizes of all oplog files in the directory.  If you do not specify the  disk-space-in-mb  value, then SnappyData does not impose a limit on the amount of space used by disk store files in that directory. If you do specify a limit, the size must be large enough to accommodate the disk store oplog files (the  MAXLOGSIZE  value, or 1 GB by default) and leave enough free space in the directory to avoid low disk space warnings. If you specify a size that cannot accommodate the oplog files and maintain enough free space, SnappyData fails to create the disk store with SQLState error XOZ33: Cannot create oplogs with size {0}MB which is greater than the maximum size {1}MB for store directory ''{2}''.    You can specify any number of  dir-name  entries in a  CREATE DISKSTORE  statement. The default diskstore does not have an option for multiple directories. It is recommended that you use separate data area from the server working directory.\nCreate a separate diskstore and specify multiple directories. The data is spread evenly among the active disk files in the directories, keeping within any limits you set.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-diskstore/#example", 
            "text": "This example uses the default base directory and parameter values to create a named disk store:  snappy  CREATE DISKSTORE STORE1;  This example configures disk store parameters and specifies a storage directory:  snappy  CREATE DISKSTORE STORE1\n      MAXLOGSIZE 1024 \n      AUTOCOMPACT TRUE\n      ALLOWFORCECOMPACTION  FALSE \n      COMPACTIONTHRESHOLD  80\n      TIMEINTERVAL  223344\n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 10240);  This example specifies multiple storage directories and directory sizes for oplog files:  snappy  CREATE DISKSTORE STORE1 \n      WRITEBUFFERSIZE 19292393\n      QUEUESIZE 17374\n      ('dir1' 456 , 'dir2', 'dir3' 532 );  Related Topics    DROP DISKSTORE    SYSDISKSTORES", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-external-table/", 
            "text": "CREATE EXTERNAL TABLE\n\n\nCREATE EXTERNAL TABLE [IF NOT EXISTS] [schema_name.]table_name\n    [( column-definition    [ , column-definition  ] * )]\n    USING datasource\n     [OPTIONS (key1 val1, key2 val2, ...)]\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating Sample Table\n, \nCreating Temporary Table\n and \nCreating Stream Table\n.\n\n\nEXTERNAL\n\n\nExternal tables point to external data sources. SnappyData supports all the data sources supported by Spark. You should use external tables to load data in parallel from any of the external sources. The table definition is persisted in the catalog and visible across all sessions. \n\n\nUSING \ndata source\n\n\nSpecify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister. \nNote that most of the prominent datastores provide an implementation of 'DataSource' and accessible as a table. For instance, you can use the Cassandra spark package to create external tables pointing to Cassandra tables and directly run queries on them. You can mix any external table and SnappyData managed tables in your queries. \n\n\nExample\n\n\nCreate an external table using PARQUET data source\n\n\nsnappy\n CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData');\n\n\n\n\nCreate an external table using CSV data source\n\n\nCREATE EXTERNAL TABLE IF NOT EXISTS CUSTOMER_STAGING USING csv OPTIONS(path '../../quickstart/src/main/resources/customer.csv');\n\n\n\n\nCREATE EXTERNAL TABLE CUSTOMER_STAGING_1 (C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, \nC_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, \nC_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) \nUSING csv OPTIONS (path '../../quickstart/src/main/resources/customer.csv');\n\n\n\n\nYou can also load data from AWS S3, as given in the example below:\n\n\nCREATE EXTERNAL TABLE NYCTAXI USING parquet OPTIONS(path 's3a://\nAWS_SECRET_KEY\n:\nAWS_SECRET_ID\n@\nfolder\n/\ndata\n');\n\n\n\n\nRelated Topics\n\n\n\n\nDROP EXTERNAL TABLE", 
            "title": "CREATE EXTERNAL TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-external-table/#create-external-table", 
            "text": "CREATE EXTERNAL TABLE [IF NOT EXISTS] [schema_name.]table_name\n    [( column-definition    [ , column-definition  ] * )]\n    USING datasource\n     [OPTIONS (key1 val1, key2 val2, ...)]  For more information on column-definition, refer to  Column Definition For Column Table .  Refer to these sections for more information on  Creating Table ,  Creating Sample Table ,  Creating Temporary Table  and  Creating Stream Table .  EXTERNAL  External tables point to external data sources. SnappyData supports all the data sources supported by Spark. You should use external tables to load data in parallel from any of the external sources. The table definition is persisted in the catalog and visible across all sessions.   USING  data source  Specify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister.  Note that most of the prominent datastores provide an implementation of 'DataSource' and accessible as a table. For instance, you can use the Cassandra spark package to create external tables pointing to Cassandra tables and directly run queries on them. You can mix any external table and SnappyData managed tables in your queries.", 
            "title": "CREATE EXTERNAL TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-external-table/#example", 
            "text": "Create an external table using PARQUET data source  snappy  CREATE EXTERNAL TABLE STAGING_AIRLINE USING parquet OPTIONS(path '../../quickstart/data/airlineParquetData');  Create an external table using CSV data source  CREATE EXTERNAL TABLE IF NOT EXISTS CUSTOMER_STAGING USING csv OPTIONS(path '../../quickstart/src/main/resources/customer.csv');  CREATE EXTERNAL TABLE CUSTOMER_STAGING_1 (C_CUSTKEY INTEGER NOT NULL, C_NAME VARCHAR(25) NOT NULL, \nC_ADDRESS VARCHAR(40) NOT NULL, C_NATIONKEY INTEGER NOT NULL, C_PHONE VARCHAR(15) NOT NULL, \nC_ACCTBAL DECIMAL(15,2) NOT NULL, C_MKTSEGMENT VARCHAR(10) NOT NULL, C_COMMENT VARCHAR(117) NOT NULL) \nUSING csv OPTIONS (path '../../quickstart/src/main/resources/customer.csv');  You can also load data from AWS S3, as given in the example below:  CREATE EXTERNAL TABLE NYCTAXI USING parquet OPTIONS(path 's3a:// AWS_SECRET_KEY : AWS_SECRET_ID @ folder / data ');  Related Topics   DROP EXTERNAL TABLE", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-function/", 
            "text": "CREATE FUNCTION\n\n\nCREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'\n\n\n\n\nDescription\n\n\nCreates a function. Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.\n\n\nYou can extend any one of the interfaces in the package \norg.apache.spark.sql.api.java\n. These interfaces can be included in your client application by adding \nsnappy-spark-sql_2.11-2.0.3-2.jar\n to your classpath.\n\n\n\n\nNote\n\n\nFor input/output types: \n The framework always returns the Java types to the UDFs. So, if you are writing \nscala.math.BigDecimal\n as an input type or output type, an exception is reported. You can use \njava.math.BigDecimal\n in the SCALA code.\n\n\n\n\nReturn Types to UDF Program Type Mapping\n\n\n\n\n\n\n\n\nSnappyData Type\n\n\nUDF Type\n\n\n\n\n\n\n\n\n\n\nSTRING\n\n\njava.lang.String\n\n\n\n\n\n\nINTEGER\n\n\njava.lang.Integer\n\n\n\n\n\n\nLONG\n\n\njava.lang.Long\n\n\n\n\n\n\nDOUBLE\n\n\njava.lang.Double\n\n\n\n\n\n\nDECIMAL\n\n\njava.math.BigDecimal\n\n\n\n\n\n\nDATE\n\n\njava.sql.Date\n\n\n\n\n\n\nTIMESTAMP\n\n\njava.sql.Timestamp\n\n\n\n\n\n\nFLOAT\n\n\njava.lang.Float\n\n\n\n\n\n\nBOOLEAN\n\n\njava.lang.Boolean\n\n\n\n\n\n\nSHORT\n\n\njava.lang.Short\n\n\n\n\n\n\nBYTE\n\n\njava.lang.Byte\n\n\n\n\n\n\nCHAR\n\n\njava.lang.String\n\n\n\n\n\n\nVARCHAR\n\n\njava.lang.String\n\n\n\n\n\n\n\n\nExample\n\n\nCREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'\n\n\n\n\nYou can write a JAVA or SCALA class to write an UDF implementation. \n\n\nRelated Topics\n\n\n\n\nDROP FUNCTION", 
            "title": "CREATE FUNCTION"
        }, 
        {
            "location": "/reference/sql_reference/create-function/#create-function", 
            "text": "CREATE FUNCTION udf_name AS qualified_class_name RETURNS data_type USING JAR '/path/to/file/udf.jar'", 
            "title": "CREATE FUNCTION"
        }, 
        {
            "location": "/reference/sql_reference/create-function/#description", 
            "text": "Creates a function. Users can define a function and completely customize how SnappyData evaluates data and manipulates queries using UDF and UDAF functions across sessions. The definition of the functions is stored in a persistent catalog, which enables it to be used after node restart as well.  You can extend any one of the interfaces in the package  org.apache.spark.sql.api.java . These interfaces can be included in your client application by adding  snappy-spark-sql_2.11-2.0.3-2.jar  to your classpath.   Note  For input/output types:   The framework always returns the Java types to the UDFs. So, if you are writing  scala.math.BigDecimal  as an input type or output type, an exception is reported. You can use  java.math.BigDecimal  in the SCALA code.   Return Types to UDF Program Type Mapping     SnappyData Type  UDF Type      STRING  java.lang.String    INTEGER  java.lang.Integer    LONG  java.lang.Long    DOUBLE  java.lang.Double    DECIMAL  java.math.BigDecimal    DATE  java.sql.Date    TIMESTAMP  java.sql.Timestamp    FLOAT  java.lang.Float    BOOLEAN  java.lang.Boolean    SHORT  java.lang.Short    BYTE  java.lang.Byte    CHAR  java.lang.String    VARCHAR  java.lang.String", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-function/#example", 
            "text": "CREATE FUNCTION APP.strnglen AS some.package.StringLengthUDF RETURNS Integer USING JAR '/path/to/file/udf.jar'  You can write a JAVA or SCALA class to write an UDF implementation.   Related Topics   DROP FUNCTION", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-index/", 
            "text": "CREATE INDEX\n\n\nCREATE INDEX index_name\n    ON table-name (\n    column-name \n    [ , column-name] * ) \n\n\n\n\nDescription\n\n\nThe \nCREATE INDEX\n statement creates an index on one or more columns of a table. Indexes can speed up queries that use those columns for filtering data, or can also enforce a unique constraint on the indexed columns.\n\n\n\n\nNote\n\n\nCREATE INDEX\n is currently under development for column tables and does not work if the data is updated.\n\n\n\n\nExample\n\n\nCreate an index on two columns:\n\n\nCREATE INDEX idx ON FLIGHTS (flight_id, segment_number);", 
            "title": "CREATE INDEX"
        }, 
        {
            "location": "/reference/sql_reference/create-index/#create-index", 
            "text": "CREATE INDEX index_name\n    ON table-name (\n    column-name \n    [ , column-name] * )", 
            "title": "CREATE INDEX"
        }, 
        {
            "location": "/reference/sql_reference/create-index/#description", 
            "text": "The  CREATE INDEX  statement creates an index on one or more columns of a table. Indexes can speed up queries that use those columns for filtering data, or can also enforce a unique constraint on the indexed columns.   Note  CREATE INDEX  is currently under development for column tables and does not work if the data is updated.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-index/#example", 
            "text": "Create an index on two columns:  CREATE INDEX idx ON FLIGHTS (flight_id, segment_number);", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/", 
            "text": "CREATE SCHEMA\n\n\nCREATE SCHEMA schema-name;\n\n\n\n\nDescription\n\n\nThis creates a schema with the given name which provides a mechanism to logically group objects by providing a namespace for objects. This can then be used by other CREATE statements as the namespace prefix. For example, CREATE TABLE SCHEMA1.TABLE1 ( ... ) will create a table TABLE1 in the schema SCHEMA1. \n\n\n\n\nNote\n\n\nSchema names with trailing underscores are not supported.\n\n\n\n\nThe CREATE SCHEMA statement is subject to access control when the \ngemfirexd.sql-authorization \n property is set to true for the system. Only the system user can create a schema with a name different from the current user name, and only the system user can specify AUTHORIZATION user-name with a user-name other than the current user name.\n\n\nExample\n\n\n\n\nCreate schema\n\n\n\n\nCREATE SCHEMA myschema;\n\n\n\n\n\n\nCreate schema that uses the authorization id '\nshared\n' as schema-name\n\n\n\n\nCREATE SCHEMA AUTHORIZATION shared;\n\n\n\n\n\n\nCreate schema \nflights\n and authorize \nanita\n to all the objects that use the schema.\n\n\n\n\nCREATE SCHEMA flights AUTHORIZATION anita;\n\n\n\n\n\n\nCreate schema \nreports\n and authorize all members of LDAP group \nfinance\n to all the objects that use the schema. Any member of this LDAP group can GRANT or REVOKE permissions on objects in this schema to other users.\n\n\n\n\nCREATE SCHEMA reports AUTHORIZATION ldapgroup:finance;", 
            "title": "CREATE SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/#create-schema", 
            "text": "CREATE SCHEMA schema-name;", 
            "title": "CREATE SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/#description", 
            "text": "This creates a schema with the given name which provides a mechanism to logically group objects by providing a namespace for objects. This can then be used by other CREATE statements as the namespace prefix. For example, CREATE TABLE SCHEMA1.TABLE1 ( ... ) will create a table TABLE1 in the schema SCHEMA1.    Note  Schema names with trailing underscores are not supported.   The CREATE SCHEMA statement is subject to access control when the  gemfirexd.sql-authorization   property is set to true for the system. Only the system user can create a schema with a name different from the current user name, and only the system user can specify AUTHORIZATION user-name with a user-name other than the current user name.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-schema/#example", 
            "text": "Create schema   CREATE SCHEMA myschema;   Create schema that uses the authorization id ' shared ' as schema-name   CREATE SCHEMA AUTHORIZATION shared;   Create schema  flights  and authorize  anita  to all the objects that use the schema.   CREATE SCHEMA flights AUTHORIZATION anita;   Create schema  reports  and authorize all members of LDAP group  finance  to all the objects that use the schema. Any member of this LDAP group can GRANT or REVOKE permissions on objects in this schema to other users.   CREATE SCHEMA reports AUTHORIZATION ldapgroup:finance;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/", 
            "text": "CREATE STREAM TABLE\n\n\nTo Create Stream Table:\n\n\n// DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n    ( column-definition [ , column-definition  ] * )\n    USING kafka_stream | file_stream | twitter_stream | socket_stream\n    OPTIONS (\n    // multiple stream source specific options\n      storagelevel 'cache-data-option',\n      rowConverter 'rowconverter-class-name',\n      subscribe 'comma-seperated-topic-name',\n      kafkaParams 'kafka-related-params',\n      consumerKey 'consumer-key',\n      consumerSecret 'consumer-secret',\n      accessToken 'access-token',\n      accessTokenSecret 'access-token-secret',\n      hostname 'socket-streaming-hostname',\n      port 'socket-streaming-port-number',\n      directory 'file-streaming-directory'\n    )\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating Sample Table\n, \nCreating External Table\n and \nCreating Temporary Table\n.\n\n\nDescription\n\n\nCreate a stream table using a stream data source. If a table with the same name already exists in the database, an exception will be thrown.\n\n\nUSING \ndata source\n \n\nSpecify the streaming source to be used for this table.\n\n\nstorageLevel\n\nProvides different trade-offs between memory usage and CPU efficiency.\n\n\nrowConverter\n\nConverts the unstructured streaming data to a set of rows.\n\n\ntopics\n\nSubscribed Kafka topics.\n\n\nkafkaParams\n\nKafka configuration parameters such as \nmetadata.broker.list\n, \nbootstrap.servers\n etc.\n\n\ndirectory\n\nHDFS directory to monitor for the new file.\n\n\nhostname\n\nHostname to connect to, for receiving data.\n\n\nport\n\nPort to connect to, for receiving data.\n\n\nconsumerKey\n\nConsumer Key (API Key) for your Twitter account.\n\n\nconsumerSecret\n\nConsumer Secret key for your Twitter account.\n\n\naccessToken\n\nAccess token for your Twitter account.\n\n\naccessTokenSecret\n\nAccess token secret for your Twitter account.\n\n\n\n\nNote\n\n\nYou need to register to \nhttps://apps.twitter.com/\n to get the \nconsumerKey\n, \nconsumerSecret\n, \naccessToken\n and \naccessTokenSecret\n credentials.\n\n\n\n\nExample\n\n\n//create a connection\nsnappy\n connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy\n streaming init 2secs;\n\n// Create a stream table\nsnappy\n create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy\n streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy\n select id, text, fullName from streamTable where text like '%snappy%';\n\n// Drop the streamTable\nsnappy\n drop table streamTable;\n\n// Stop the streaming\nsnappy\n streaming stop;", 
            "title": "CREATE STREAM TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/#create-stream-table", 
            "text": "To Create Stream Table:  // DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n    ( column-definition [ , column-definition  ] * )\n    USING kafka_stream | file_stream | twitter_stream | socket_stream\n    OPTIONS (\n    // multiple stream source specific options\n      storagelevel 'cache-data-option',\n      rowConverter 'rowconverter-class-name',\n      subscribe 'comma-seperated-topic-name',\n      kafkaParams 'kafka-related-params',\n      consumerKey 'consumer-key',\n      consumerSecret 'consumer-secret',\n      accessToken 'access-token',\n      accessTokenSecret 'access-token-secret',\n      hostname 'socket-streaming-hostname',\n      port 'socket-streaming-port-number',\n      directory 'file-streaming-directory'\n    )  For more information on column-definition, refer to  Column Definition For Column Table .  Refer to these sections for more information on  Creating Table ,  Creating Sample Table ,  Creating External Table  and  Creating Temporary Table .", 
            "title": "CREATE STREAM TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/#description", 
            "text": "Create a stream table using a stream data source. If a table with the same name already exists in the database, an exception will be thrown.  USING  data source   \nSpecify the streaming source to be used for this table.  storageLevel \nProvides different trade-offs between memory usage and CPU efficiency.  rowConverter \nConverts the unstructured streaming data to a set of rows.  topics \nSubscribed Kafka topics.  kafkaParams \nKafka configuration parameters such as  metadata.broker.list ,  bootstrap.servers  etc.  directory \nHDFS directory to monitor for the new file.  hostname \nHostname to connect to, for receiving data.  port \nPort to connect to, for receiving data.  consumerKey \nConsumer Key (API Key) for your Twitter account.  consumerSecret \nConsumer Secret key for your Twitter account.  accessToken \nAccess token for your Twitter account.  accessTokenSecret \nAccess token secret for your Twitter account.   Note  You need to register to  https://apps.twitter.com/  to get the  consumerKey ,  consumerSecret ,  accessToken  and  accessTokenSecret  credentials.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-stream-table/#example", 
            "text": "//create a connection\nsnappy  connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy  streaming init 2secs;\n\n// Create a stream table\nsnappy  create stream table streamTable (id long, text string, fullName string, country string,\n        retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\n        accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming\nsnappy  streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy  select id, text, fullName from streamTable where text like '%snappy%';\n\n// Drop the streamTable\nsnappy  drop table streamTable;\n\n// Stop the streaming\nsnappy  streaming stop;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/", 
            "text": "CREATE SAMPLE TABLE\n\n\nMode 1\n\n\nCREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )\n    USING column_sample\n    OPTIONS (\n    baseTable 'baseTableName',\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    [AS select_statement];\n\n\n\n\nMode 2\n\n\nCREATE SAMPLE TABLE table_name ON base_table_name\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    REDUNDANCY        'num-redundant-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    AS select_statement\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nWhen creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. The sample table also inherits the number of buckets, redundancy and persistence properties from the base table.\nFor sample tables, the overflow property is set to \nFalse\n by default. For column tables the default value is \nTrue\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating External Table\n, \nCreating Temporary Table\n and \nCreating Stream Table\n.\n\n\nDescription\n\n\n\n\n\n\nQCS\n: Query Column Set. These columns are used for stratification in stratified sampling. \n\n\n\n\n\n\nFRACTION\n: This represents the fraction of the full population (base table) that is managed in the sample. \n\n\n\n\n\n\nSTRATARESERVOIRSIZE\n: The initial capacity of each stratum.\n\n\n\n\n\n\nbaseTable\n: Table on which sampling is done.\n\n\n\n\n\n\nExamples:\n\n\nMode 1 Example\n\n\nsnappy\nCREATE TABLE CUSTOMER_SAMPLE ( \n      C_CUSTKEY     INTEGER NOT NULL,\n      C_NAME        VARCHAR(25) NOT NULL,\n      C_ADDRESS     VARCHAR(40) NOT NULL,\n      C_NATIONKEY   INTEGER NOT NULL,\n      C_PHONE       VARCHAR(15) NOT NULL,\n      C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n      C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n      C_COMMENT     VARCHAR(117) NOT NULL)\n      USING COLUMN_SAMPLE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50', baseTable 'CUSTOMER_BASE');\n\n\n\n\nMode 2 Example\n\n\nsnappy\nCREATE SAMPLE TABLE CUSTOMER_SAMPLE on CUSTOMER_BASE\n      OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50') AS (SELECT * FROM CUSTOMER_BASE);\n\n\n\n\n\n\nNote\n\n\nRefer to \ncreate sample tables in SDE section\n for more information on creating sample tables on datasets that can be sourced from any source supported in Spark/SnappyData.", 
            "title": "CREATE SAMPLE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#create-sample-table", 
            "text": "Mode 1  CREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )\n    USING column_sample\n    OPTIONS (\n    baseTable 'baseTableName',\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    REDUNDANCY        'num-of-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    [AS select_statement];  Mode 2  CREATE SAMPLE TABLE table_name ON base_table_name\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    REDUNDANCY        'num-redundant-copies' , // Must be an integer\n    EVICTION_BY \u2018LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  \u2018ASYNCHRONOUS | SYNCHRONOUS\u2019,\n    DISKSTORE 'diskstore-name', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event\n    EXPIRE \u2018time-to-live-in-seconds',\n    QCS 'column-name', // column-name [, column-name ] *\n    FRACTION 'population-fraction',  //Must be a double\n    STRATARESERVOIRSIZE 'strata-initial-capacity',  // Default 50 Must be an integer.\n    )\n    AS select_statement  For more information on column-definition, refer to  Column Definition For Column Table .  When creating a base table, if you have applied the partition by clause, the clause is also applied to the sample table. The sample table also inherits the number of buckets, redundancy and persistence properties from the base table.\nFor sample tables, the overflow property is set to  False  by default. For column tables the default value is  True .  Refer to these sections for more information on  Creating Table ,  Creating External Table ,  Creating Temporary Table  and  Creating Stream Table .", 
            "title": "CREATE SAMPLE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#description", 
            "text": "QCS : Query Column Set. These columns are used for stratification in stratified sampling.     FRACTION : This represents the fraction of the full population (base table) that is managed in the sample.     STRATARESERVOIRSIZE : The initial capacity of each stratum.    baseTable : Table on which sampling is done.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#examples", 
            "text": "", 
            "title": "Examples:"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#mode-1-example", 
            "text": "snappy CREATE TABLE CUSTOMER_SAMPLE ( \n      C_CUSTKEY     INTEGER NOT NULL,\n      C_NAME        VARCHAR(25) NOT NULL,\n      C_ADDRESS     VARCHAR(40) NOT NULL,\n      C_NATIONKEY   INTEGER NOT NULL,\n      C_PHONE       VARCHAR(15) NOT NULL,\n      C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n      C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n      C_COMMENT     VARCHAR(117) NOT NULL)\n      USING COLUMN_SAMPLE OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50', baseTable 'CUSTOMER_BASE');", 
            "title": "Mode 1 Example"
        }, 
        {
            "location": "/reference/sql_reference/create-sample-table/#mode-2-example", 
            "text": "snappy CREATE SAMPLE TABLE CUSTOMER_SAMPLE on CUSTOMER_BASE\n      OPTIONS (qcs 'C_NATIONKEY',fraction '0.05', \n      strataReservoirSize '50') AS (SELECT * FROM CUSTOMER_BASE);   Note  Refer to  create sample tables in SDE section  for more information on creating sample tables on datasets that can be sourced from any source supported in Spark/SnappyData.", 
            "title": "Mode 2 Example"
        }, 
        {
            "location": "/reference/sql_reference/create-table/", 
            "text": "CREATE TABLE\n\n\nFollowing is the syntax used to create a Row/Column table:\n\n\nCREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )    \n    USING [row | column] // If not specified, a row table is created.\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables.\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    COMPRESSION 'NONE', //By default COMPRESSION is 'ON'. \n    REDUNDANCY       'num-of-copies' , // Must be an integer. By default, REDUNDANCY is set to 0 (zero). '1' is recommended value. Maximum limit is '3'\n    EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\n    DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set.\n    EXPIRE 'time_to_live_in_seconds',\n    COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table.\n    KEY_COLUMNS  'column_name,..', // Only for column table if putInto support is required\n    COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer \n 0 and \n 2GB. Only for column table.\n    )\n    [AS select_statement];\n\n\n\n\nRefer to the following sections for:\n\n\n\n\nCreating Sample Table\n\n\nCreating External Table\n\n\nCreating Temporary Table\n\n\nCreating Stream Table\n.\n\n\n\n\nColumn Definition\n\n\nThe column definition defines the name of a column and its data type.\n\n\n\n\ncolumn-definition\n (for Column Table)\n\n\ncolumn-definition: column-name column-data-type [NOT NULL]\n\ncolumn-name: 'unique column name'\n\n\n\n\n\n\ncolumn-definition\n (for Row Table)\n\n\ncolumn-definition: column-definition-for-row-table | table-constraint\n\ncolumn-definition-for-row-table: column-name column-data-type [ column-constraint ] *\n    [ [ WITH ] DEFAULT { constant-expression | NULL } \n      | [ GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY\n          [ ( START WITH start-value-as-integer [, INCREMENT BY step-value-as-integer ] ) ] ] ]\n    [ column-constraint ] *\n\n\n\n\n\n\nRefer to the \nidentity\n section for more information on GENERATED.\n\n\nRefer to the \nconstraint\n section for more information on table-constraint and column-constraint.\n\n\n\n\ncolumn-data-type\n\nThe following data types are supported:\n\n\ncolumn-data-type: \n    BIGINT |\n    BINARY |\n    BLOB |\n    BOOLEAN |\n    BYTE |\n    CLOB |\n    DATE |\n    DECIMAL |\n    DOUBLE |\n    FLOAT |\n    INT |\n    INTEGER |\n    LONG |\n    NUMERIC |\n    REAL |\n    SHORT |\n    SMALLINT |\n    STRING |\n    TIMESTAMP |\n    TINYINT |\n    VARBINARY |\n    VARCHAR |\n\n\n\n\nColumn tables can also use \nARRAY\n, \nMAP\n and \nSTRUCT\n types.\n\n\nDecimal and numeric has default precision of 38 and scale of 18.\n\n\nUsing\n\n\nYou can specify if you want to create a row table or a column table. If this is not specified, a row table is created by default. \n\n\nOptions\n\n\nYou can specify the following options when you create a table:\n\n\n\n\nCOLOCATE_WITH\n\n\nPARTITION_BY\n\n\nBUCKETS\n\n\nCOMPRESSION\n\n\nREDUNDANCY\n\n\nEVICTION_BY\n\n\nPERSISTENCE\n\n\nDISKSTORE\n\n\nOVERFLOW\n\n\nEXPIRE\n\n\nCOLUMN_BATCH_SIZE\n\n\nCOLUMN_MAX_DELTA_ROWS\n\n\n\n\n\n\nNote\n\n\nIf options are not specified, then the default values are used to create the table.\n\n\n\n\n\n\n\n\nCOLOCATE_WITH\n\nThe COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. \n\n\n\n\nPARTITION_BY\n\nUse the PARTITION_BY {COLUMN} clause to provide a set of column names that determine the partitioning. \n\nIf not specified, for row table (mentioned further for the case of column table) it is a 'replicated row table'.\n \nColumn and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally.\n The default number of storage partitions (BUCKETS) is 128 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API.\n\n\n\n\nBUCKETS\n \n\nThe optional BUCKETS attribute specifies the fixed number of \"buckets\" to use for the partitioned row or column tables. Each data server JVM manages one or more buckets. A bucket is a container of data and is the smallest unit of partitioning and migration in the system. For instance, in a cluster of five nodes and a bucket count of 25 would result in 5 buckets on each node. But, if you configured the reverse - 25 nodes and a bucket count of 5, only 5 data servers hosts all the data for this table. If not specified, the number of buckets defaults to 128. See \nbest practices\n for more information. \nFor row tables, \nBUCKETS\n must be created with the \nPARTITION_BY\n clause, else an error is reported.\n\n\n\n\nCOMPRESSION\n\nColumn tables use compression of data by default. This reduces the total storage footprint for large tables. SnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. By default, compression is on for column tables. To disable data compression, you can set the COMPRESSION option to \nnone\n when you create a table. For example:\n\n\nCREATE TABLE AIRLINE USING column OPTIONS(compression 'none')  AS (select * from STAGING_AIRLINE);\n\n\n\n\nSee \nbest practices\n for more information.\n\n\n\n\nREDUNDANCY\n\nUse the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. It is important to note that redundancy of '1' implies two physical copies of data. By default, REDUNDANCY is set to 0 (zero). A REDUNDANCY value of '1' is recommended. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage. A maximum limit of '3' can be set for REDUNDANCY. See \nbest practices\n for more information.\n\n\n\n\nEVICTION_BY\n\nUse the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store. It is important to note that all tables (expected to host larger data sets) overflow to disk, by default. See \nbest practices\n for more information. The value for this parameter is set in MB.\nFor column tables, the default eviction setting is \nLRUHEAPPERCENT\n and the default action is to overflow to disk. You can also specify the \nOVERFLOW\n parameter along with the \nEVICTION_BY\n clause.\n\n\n\n\nNote\n\n\n\n\n\n\nEVICTION_BY is not supported for replicated tables.\n\n\n\n\n\n\nFor column tables, you cannot use the LRUMEMSIZE or LRUCOUNT eviction settings. For row tables, no such defaults are set. Row tables allow all the eviction settings.\n\n\n\n\n\n\n\n\n\n\nPERSISTENCE\n\nWhen you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member. \n\n\n\n\nNote\n\n\n\n\n\n\nBy default, both row and column tables are persistent.\n\n\n\n\n\n\nThe option \nPERSISTENT\n has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use \nPERSISTENCE\n instead.\n\n\n\n\n\n\n\n\n\n\nDISKSTORE\n\nThe disk directories where you want to persist the table data. By default, SnappyData creates a \"default\" disk store on each member node. You can use this option to control the location where data is stored. For instance, you may decide to use a network file system or specify multiple disk mount points to uniformly scatter the data across disks. For more information, refer to \nCREATE DISKSTORE\n.\n\n\n\n\nOVERFLOW\n \nUse the OVERFLOW clause to specify the action to be taken upon the eviction event. For persistent tables, setting this to 'true' overflows the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' is not allowed except when EVICTION_BY is set. In such case, the eviction itself is disabled.\n\nWhen you configure an overflow table, only the evicted rows are written to disk. If you restart or shut down a member that hosts the overflow table, the table data that was in memory is not restored unless you explicitly configure persistence (or you configure one or more replicas with a partitioned table).\n\n\n\n\nNote\n\n\nThe tables are evicted to disk by default, which means table data overflows to a local SnappyStore disk store.\n\n\n\n\n\n\nEXPIRE\n\nUse the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured \ntime_to_live_in_seconds\n.\n\n\n\n\nCOLUMN_BATCH_SIZE\n\nThe default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M).\n\n\n\n\nCOLUMN_MAX_DELTA_ROWS\n\nThe maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by COLUMN_BATCH_SIZE (and thus limits the size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The value should be \n 0 and \n 2GB. The default value is 10000.\n\n\n\n\nNote\n\n\nThe following corresponding SQLConf properties for \nCOLUMN_BATCH_SIZE\n and \nCOLUMN_MAX_DELTA_ROWS\n are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL):\n\n\n\n\nsnappydata.column.batchSize\n - Explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit \nCOLUMN_BATCH_SIZE\n specification, then this is inherited for that table property.\n\n\nsnappydata.column.maxDeltaRows\n - The maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit COLUMN_MAX_DELTA_ROWS specification, then this is inherited for that table property.\n\n\n\n\n\n\nTables created using the standard SQL syntax without any of SnappyData specific extensions are created as row-oriented replicated tables. Thus, each data server node in the cluster hosts a consistent replica of the table. All tables are also registered in the Spark catalog and hence visible as DataFrames.\n\n\nFor example, \ncreate table if not exists Table1 (a int)\n is equivalent to \ncreate table if not exists Table1 (a int) using row\n.\n\n\nExamples\n\n\nExample: Column Table Partitioned on a Single Column\n\n\nsnappy\nCREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL)\n    USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY');\n\n\n\n\nExample: Column Table Partitioned with 10 Buckets and Persistence Enabled\n\n\nsnappy\nCREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL)\n    USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY', PERSISTENCE 'SYNCHRONOUS');\n\n\n\n\nExample: Replicated, Persistent Row Table\n\n\nsnappy\nCREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (PARTITION_BY 'S_SUPPKEY', BUCKETS '10', PERSISTENCE 'ASYNCHRONOUS');\n\n\n\n\nExample: Row Table Partitioned with 10 Buckets and Overflow Enabled\n\n\nsnappy\nCREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (BUCKETS '10',\n      PARTITION_BY 'S_SUPPKEY',\n      PERSISTENCE 'ASYNCHRONOUS',\n      EVICTION_BY 'LRUCOUNT 3',\n      OVERFLOW 'true');\n\n\n\n\nExample: Create Table using Select Query\n\n\nCREATE TABLE CUSTOMER_STAGING USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') AS SELECT * FROM CUSTOMER ;\n\n\n\n\nWith this alternate form of the CREATE TABLE statement, you specify the column names and/or the column data types with a query. The columns in the query result are used as a model for creating the columns in the new table.\n\n\nIf no column names are specified for the new table, then all the columns in the result of the query expression are used to create same-named columns in the new table, of the corresponding data type(s). If one or more column names are specified for the new table, the same number of columns must be present in the result of the query expression; the data types of those columns are used for the corresponding columns of the new table.\n\n\nNote that only the column names and data types from the queried table are used when creating the new table. Additional settings in the queried table, such as partitioning, replication, and persistence, are not duplicated. You can optionally specify partitioning, replication, and persistence configuration settings for the new table and those settings need not match the settings of the queried table.\n\n\nExample: Create Table using Spark DataFrame API\n\n\nFor information on using the Apache Spark API, refer to \nUsing the Spark DataFrame API\n.\n\n\nExample: Create Column Table with PUT INTO\n\n\nsnappy\n CREATE TABLE COL_TABLE (\n      PRSN_EVNT_ID BIGINT NOT NULL,\n      VER bigint NOT NULL,\n      CLIENT_ID BIGINT NOT NULL,\n      SRC_TYP_ID BIGINT NOT NULL)\n      USING COLUMN OPTIONS(PARTITION_BY 'PRSN_EVNT_ID,CLIENT_ID', BUCKETS '64', KEY_COLUMNS 'PRSN_EVNT_ID, CLIENT_ID');\n\n\n\n\nExample: Create Table with Eviction Settings\n\n\nUse eviction settings to keep your table within a specified limit, either by removing evicted data completely or by creating an overflow table that persists the evicted data to a disk store.\n\n\n\n\n\n\nDecide whether to evict based on:\n\n\n\n\n\n\nEntry count (useful if table row sizes are relatively uniform).\n\n\n\n\n\n\nTotal bytes used.\n\n\n\n\n\n\nPercentage of JVM heap used. This uses the SnappyData resource manager. When the manager determines that eviction is required, the manager orders the eviction controller to start evicting from all tables where the eviction criterion is set to\u00a0LRUHEAPPERCENT.\n\n\n\n\n\n\n\n\n\n\nDecide what action to take when the limit is reached:\n\n\n\n\n\n\nLocally destroy the row (partitioned tables only).\n\n\n\n\n\n\nOverflow the row data to disk.\n\n\n\n\n\n\n\n\n\n\nIf you want to overflow data to disk (or persist the entire table to disk), configure a named disk store to use for the overflow data. If you do not specify a disk store when creating an overflow table, SnappyData stores the overflow data in the default disk store.\n\n\n\n\n\n\nCreate the table with the required eviction configuration.\n\n\nFor example, to evict using LRU entry count and overflow evicted rows to a disk store (OverflowDiskStore):\n\n\nCREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (EVICTION_BY 'LRUCOUNT 2', OVERFLOW 'true', DISKSTORE 'OverflowDiskStore', PERSISTENCE 'async');\n\n\n\nTo create a table that simply removes evicted data from memory without persisting the evicted data, use the\u00a0\nDESTROY\n\u00a0eviction action. For example:\nDefault in SnappyData for \nsynchronous\n is \npersistence\n, \noverflow\n is \ntrue\n and \neviction_by\n is \nLRUHEAPPERCENT\n.\n\n\nCREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (PARTITION_BY 'OrderId', EVICTION_BY 'LRUMEMSIZE 1000');\n\n\n\n\n\n\n\n\n\nConstraint (only for Row Tables)\n\n\nA CONSTRAINT clause is an optional part of a CREATE TABLE statement that defines a rule to which table data must conform.\n\n\nThere are two types of constraints:\n\n\nColumn-level constraints\n: Refer to a single column in the table and do not specify a column name (except check constraints). They refer to the column that they follow.\n\n\nTable-level constraints\n: Refer to one or more columns in the table. Table-level constraints specify the names of the columns to which they apply. Table-level CHECK constraints can refer to 0 or more columns in the table.\n\n\nColumn and table constraints include:\n\n\n\n\n\n\nNOT NULL\u2014 Specifies that a column cannot hold NULL values (constraints of this type are not nameable).\n\n\n\n\n\n\nPRIMARY KEY\u2014 Specifies a column (or multiple columns if specified in a table constraint) that uniquely identifies a row in the table. The identified columns must be defined as NOT NULL.\n\n\n\n\n\n\nUNIQUE\u2014 Specifies that values in the column must be unique. NULL values are not allowed.\n\n\n\n\n\n\nFOREIGN KEY\u2014 Specifies that the values in the columns must correspond to values in the referenced primary key or unique columns or that they are NULL. \nIf the foreign key consists of multiple columns and any column is NULL, then the whole key is considered NULL. SnappyData permits the insert no matter what is in the non-null columns.\n\n\n\n\n\n\nCHECK\u2014 Specifies rules for values in a column, or specifies a wide range of rules for values when included as a table constraint. The CHECK constraint has the same format and restrictions for column and table constraints.\nColumn constraints and table constraints have the same function; the difference is where you specify them. Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition. \n\n\n\n\n\n\nColumn-level constraints (except for check constraints) refer to only one column.\nIf you do not specify a name for a column or table constraint, then SnappyData generates a unique name.\n\n\nExample\n: The following example demonstrates how to create a table with \nFOREIGN KEY\n: \n\n\nsnappy\n create table trading.customers (cid int not null, cust_name varchar(100), since date, addr varchar(100), tid int, primary key (cid));\n\nsnappy\n create table trading.networth (cid int not null, cash decimal (30, 20), securities decimal (30, 20), loanlimit int, availloan decimal (30, 20),  tid int, constraint netw_pk primary key (cid), constraint cust_newt_fk foreign key (cid) references trading.customers (cid));\n\nsnappy\n show importedkeys in trading;\nPKTABLE_NAME        |PKCOLUMN_NAME       |PK_NAME             |FKTABLE_SCHEM       |FKTABLE_NAME        |FKCOLUMN_NAME       |FK_NAME             |KEY_SEQ\n---------------------------------------------------------------------------------------------------------------------------------------------------------- \nCUSTOMERS           |CID                 |SQL180403162038220  |TRADING             |NETWORTH            |CID                 |CUST_NEWT_FK        |1  \n\n\n\n\n\n\nIdentity Columns (only for Row Tables)\n\n\n\nSnappyData supports both GENERATED ALWAYS and GENERATED BY DEFAULT identity columns only for BIGINT and INTEGER data types. The START WITH and INCREMENT BY clauses are supported only for GENERATED BY DEFAULT identity columns.\n\n\nFor a GENERATED ALWAYS identity column, SnappyData increments the default value on every insertion, and stores the incremented value in the column. You cannot insert a value directly into a GENERATED ALWAYS identity column, and you cannot update a value in a GENERATED ALWAYS identity column. Instead, you must either specify the DEFAULT keyword when inserting data into the table or you must leave the identity column out of the insertion column list.\n\n\nConsider a table with the following column definition:\n\n\ncreate table greetings (i int generated always as identity, ch char(50)) using row;\n\n\n\n\nYou can insert rows into the table using either the DEFAULT keyword or by omitting the identity column from the INSERT statement:\n\n\ninsert into greetings values (DEFAULT, 'hello');\n\n\n\n\ninsert into greetings(ch) values ('hi');\n\n\n\n\nThe values that SnappyData automatically generates for a GENERATED ALWAYS identity column are unique.\n\n\nFor a GENERATED BY DEFAULT identity column, SnappyData increments and uses a default value for an INSERT only when no explicit value is given. To use the generated default value, either specify the DEFAULT keyword when inserting into the identity column or leave the identity column out of the INSERT column list.\n\n\nIn contrast to GENERATED ALWAYS identity columns, with a GENERATED BY DEFAULT column you can specify an identity value to use instead of the generated default value. To specify a value, include it in the INSERT statement.\n\n\nFor example, consider a table created using the statement:\n\n\ncreate table greetings (i int generated by default as identity, ch char(50)); \n\n\n\n\nThe following statement specifies the value \u201c1\u201d for the identity column:\n\n\ninsert into greetings values (1, 'hi'); \n\n\n\n\nThese statements both use generated default values:\n\n\ninsert into greetings values (DEFAULT, 'hello');\ninsert into greetings(ch) values ('bye');\n\n\n\n\nAlthough the automatically-generated values in a GENERATED BY DEFAULT identity column are unique, a GENERATED BY DEFAULT column does not guarantee unique identity values for all rows in the table. For example, in the above statements, the rows containing \u201chi\u201d and \u201chello\u201d both have an identity value of \u201c1.\u201d This occurs because the generated column starts at \u201c1\u201d and the user-specified value was also \u201c1.\u201d\n\n\nTo avoid duplicating identity values (for example, during an import operation), you can use the START WITH clause to specify the first identity value that SnappyData should assign and increment. Or, you can use a primary key or a unique constraint on the GENERATED BY DEFAULT identity column to check for and disallow duplicates.\n\n\nBy default, the initial value of a GENERATED BY DEFAULT identity column is 1, and the value is incremented by 1 for each INSERT. Use the optional START WITH clause to specify a new initial value. Use the optional INCREMENT BY clause to change the increment value used during each INSERT.\n\n\nRelated Topics\n\n\n\n\n\n\nDROP TABLE\n\n\n\n\n\n\nDELETE TABLE\n\n\n\n\n\n\nSHOW TABLES\n\n\n\n\n\n\nTRUNCATE TABLE", 
            "title": "CREATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#create-table", 
            "text": "Following is the syntax used to create a Row/Column table:  CREATE TABLE [IF NOT EXISTS] table_name \n    ( column-definition [ , column-definition  ] * )    \n    USING [row | column] // If not specified, a row table is created.\n    OPTIONS (\n    COLOCATE_WITH 'table-name',  // Default none\n    PARTITION_BY 'column-name', // If not specified, replicated table for row tables, and partitioned internally for column tables.\n    BUCKETS  'num-partitions', // Default 128. Must be an integer.\n    COMPRESSION 'NONE', //By default COMPRESSION is 'ON'. \n    REDUNDANCY       'num-of-copies' , // Must be an integer. By default, REDUNDANCY is set to 0 (zero). '1' is recommended value. Maximum limit is '3'\n    EVICTION_BY 'LRUMEMSIZE integer-constant | LRUCOUNT interger-constant | LRUHEAPPERCENT',\n    PERSISTENCE  'ASYNCHRONOUS | ASYNC | SYNCHRONOUS | SYNC | NONE\u2019,\n    DISKSTORE 'DISKSTORE_NAME', //empty string maps to default diskstore\n    OVERFLOW 'true | false', // specifies the action to be executed upon eviction event, 'false' allowed only when EVCITON_BY is not set.\n    EXPIRE 'time_to_live_in_seconds',\n    COLUMN_BATCH_SIZE 'column-batch-size-in-bytes', // Must be an integer. Only for column table.\n    KEY_COLUMNS  'column_name,..', // Only for column table if putInto support is required\n    COLUMN_MAX_DELTA_ROWS 'number-of-rows-in-each-bucket', // Must be an integer   0 and   2GB. Only for column table.\n    )\n    [AS select_statement];  Refer to the following sections for:   Creating Sample Table  Creating External Table  Creating Temporary Table  Creating Stream Table .", 
            "title": "CREATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#column-definition", 
            "text": "The column definition defines the name of a column and its data type.   column-definition  (for Column Table)  column-definition: column-name column-data-type [NOT NULL]\n\ncolumn-name: 'unique column name'   column-definition  (for Row Table)  column-definition: column-definition-for-row-table | table-constraint\n\ncolumn-definition-for-row-table: column-name column-data-type [ column-constraint ] *\n    [ [ WITH ] DEFAULT { constant-expression | NULL } \n      | [ GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY\n          [ ( START WITH start-value-as-integer [, INCREMENT BY step-value-as-integer ] ) ] ] ]\n    [ column-constraint ] *   Refer to the  identity  section for more information on GENERATED.  Refer to the  constraint  section for more information on table-constraint and column-constraint.   column-data-type \nThe following data types are supported:  column-data-type: \n    BIGINT |\n    BINARY |\n    BLOB |\n    BOOLEAN |\n    BYTE |\n    CLOB |\n    DATE |\n    DECIMAL |\n    DOUBLE |\n    FLOAT |\n    INT |\n    INTEGER |\n    LONG |\n    NUMERIC |\n    REAL |\n    SHORT |\n    SMALLINT |\n    STRING |\n    TIMESTAMP |\n    TINYINT |\n    VARBINARY |\n    VARCHAR |  Column tables can also use  ARRAY ,  MAP  and  STRUCT  types.  Decimal and numeric has default precision of 38 and scale of 18.", 
            "title": "Column Definition"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#using", 
            "text": "You can specify if you want to create a row table or a column table. If this is not specified, a row table is created by default.", 
            "title": "Using"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#options", 
            "text": "You can specify the following options when you create a table:   COLOCATE_WITH  PARTITION_BY  BUCKETS  COMPRESSION  REDUNDANCY  EVICTION_BY  PERSISTENCE  DISKSTORE  OVERFLOW  EXPIRE  COLUMN_BATCH_SIZE  COLUMN_MAX_DELTA_ROWS    Note  If options are not specified, then the default values are used to create the table.     COLOCATE_WITH \nThe COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated.    PARTITION_BY \nUse the PARTITION_BY {COLUMN} clause to provide a set of column names that determine the partitioning.  \nIf not specified, for row table (mentioned further for the case of column table) it is a 'replicated row table'.  \nColumn and row tables support hash partitioning on one or more columns. These are specified as comma-separated column names in the PARTITION_BY option of the CREATE TABLE DDL or createTable API. The hashing scheme follows the Spark Catalyst Hash Partitioning to minimize shuffles in joins. If no PARTITION_BY option is specified for a column table, then, the table is still partitioned internally.  The default number of storage partitions (BUCKETS) is 128 in cluster mode for column and row tables, and 11 in local mode for column and partitioned row tables. This can be changed using the BUCKETS option in CREATE TABLE DDL or createTable API.   BUCKETS   \nThe optional BUCKETS attribute specifies the fixed number of \"buckets\" to use for the partitioned row or column tables. Each data server JVM manages one or more buckets. A bucket is a container of data and is the smallest unit of partitioning and migration in the system. For instance, in a cluster of five nodes and a bucket count of 25 would result in 5 buckets on each node. But, if you configured the reverse - 25 nodes and a bucket count of 5, only 5 data servers hosts all the data for this table. If not specified, the number of buckets defaults to 128. See  best practices  for more information. \nFor row tables,  BUCKETS  must be created with the  PARTITION_BY  clause, else an error is reported.   COMPRESSION \nColumn tables use compression of data by default. This reduces the total storage footprint for large tables. SnappyData column tables encode data for compression and hence require memory that is less than or equal to the on-disk size of the uncompressed data. By default, compression is on for column tables. To disable data compression, you can set the COMPRESSION option to  none  when you create a table. For example:  CREATE TABLE AIRLINE USING column OPTIONS(compression 'none')  AS (select * from STAGING_AIRLINE);  See  best practices  for more information.   REDUNDANCY \nUse the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail. It is important to note that redundancy of '1' implies two physical copies of data. By default, REDUNDANCY is set to 0 (zero). A REDUNDANCY value of '1' is recommended. A large value for REDUNDANCY clause has an adverse impact on performance, network usage, and memory usage. A maximum limit of '3' can be set for REDUNDANCY. See  best practices  for more information.   EVICTION_BY \nUse the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store. It is important to note that all tables (expected to host larger data sets) overflow to disk, by default. See  best practices  for more information. The value for this parameter is set in MB.\nFor column tables, the default eviction setting is  LRUHEAPPERCENT  and the default action is to overflow to disk. You can also specify the  OVERFLOW  parameter along with the  EVICTION_BY  clause.   Note    EVICTION_BY is not supported for replicated tables.    For column tables, you cannot use the LRUMEMSIZE or LRUCOUNT eviction settings. For row tables, no such defaults are set. Row tables allow all the eviction settings.      PERSISTENCE \nWhen you specify the PERSISTENCE keyword, SnappyData persists the in-memory table data to a local SnappyData disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.    Note    By default, both row and column tables are persistent.    The option  PERSISTENT  has been deprecated as of SnappyData 0.9. Although it does work, it is recommended to use  PERSISTENCE  instead.      DISKSTORE \nThe disk directories where you want to persist the table data. By default, SnappyData creates a \"default\" disk store on each member node. You can use this option to control the location where data is stored. For instance, you may decide to use a network file system or specify multiple disk mount points to uniformly scatter the data across disks. For more information, refer to  CREATE DISKSTORE .   OVERFLOW  \nUse the OVERFLOW clause to specify the action to be taken upon the eviction event. For persistent tables, setting this to 'true' overflows the table evicted rows to disk based on the EVICTION_BY criteria. Setting this to 'false' is not allowed except when EVICTION_BY is set. In such case, the eviction itself is disabled. \nWhen you configure an overflow table, only the evicted rows are written to disk. If you restart or shut down a member that hosts the overflow table, the table data that was in memory is not restored unless you explicitly configure persistence (or you configure one or more replicas with a partitioned table).   Note  The tables are evicted to disk by default, which means table data overflows to a local SnappyStore disk store.    EXPIRE \nUse the EXPIRE clause with tables to control the SnappyStore memory usage. It expires the rows after configured  time_to_live_in_seconds .   COLUMN_BATCH_SIZE \nThe default size of blocks to use for storage in the SnappyData column store. When inserting data into the column storage this is the unit (in bytes) that is used to split the data into chunks for efficient storage and retrieval. The default value is 25165824 (24M).   COLUMN_MAX_DELTA_ROWS \nThe maximum number of rows that can be in the delta buffer of a column table for each bucket, before it is flushed into the column store. Although the size of column batches is limited by COLUMN_BATCH_SIZE (and thus limits the size of row buffer for each bucket as well), this property allows a lower limit on the number of rows for better scan performance. The value should be   0 and   2GB. The default value is 10000.   Note  The following corresponding SQLConf properties for  COLUMN_BATCH_SIZE  and  COLUMN_MAX_DELTA_ROWS  are set if the table creation is done in that session (and the properties have not been explicitly specified in the DDL):   snappydata.column.batchSize  - Explicit batch size for this session for bulk insert operations. If a table is created in the session without any explicit  COLUMN_BATCH_SIZE  specification, then this is inherited for that table property.  snappydata.column.maxDeltaRows  - The maximum limit on rows in the delta buffer for each bucket of column table in this session. If a table is created in the session without any explicit COLUMN_MAX_DELTA_ROWS specification, then this is inherited for that table property.    Tables created using the standard SQL syntax without any of SnappyData specific extensions are created as row-oriented replicated tables. Thus, each data server node in the cluster hosts a consistent replica of the table. All tables are also registered in the Spark catalog and hence visible as DataFrames.  For example,  create table if not exists Table1 (a int)  is equivalent to  create table if not exists Table1 (a int) using row .", 
            "title": "Options"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-column-table-partitioned-on-a-single-column", 
            "text": "snappy CREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL)\n    USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY');", 
            "title": "Example: Column Table Partitioned on a Single Column"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-column-table-partitioned-with-10-buckets-and-persistence-enabled", 
            "text": "snappy CREATE TABLE CUSTOMER ( \n    C_CUSTKEY     INTEGER NOT NULL,\n    C_NAME        VARCHAR(25) NOT NULL,\n    C_ADDRESS     VARCHAR(40) NOT NULL,\n    C_NATIONKEY   INTEGER NOT NULL,\n    C_PHONE       VARCHAR(15) NOT NULL,\n    C_ACCTBAL     DECIMAL(15,2)   NOT NULL,\n    C_MKTSEGMENT  VARCHAR(10) NOT NULL,\n    C_COMMENT     VARCHAR(117) NOT NULL)\n    USING COLUMN OPTIONS (BUCKETS '10', PARTITION_BY 'C_CUSTKEY', PERSISTENCE 'SYNCHRONOUS');", 
            "title": "Example: Column Table Partitioned with 10 Buckets and Persistence Enabled"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-replicated-persistent-row-table", 
            "text": "snappy CREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (PARTITION_BY 'S_SUPPKEY', BUCKETS '10', PERSISTENCE 'ASYNCHRONOUS');", 
            "title": "Example: Replicated, Persistent Row Table"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-row-table-partitioned-with-10-buckets-and-overflow-enabled", 
            "text": "snappy CREATE TABLE SUPPLIER ( \n      S_SUPPKEY INTEGER NOT NULL PRIMARY KEY, \n      S_NAME STRING NOT NULL, \n      S_ADDRESS STRING NOT NULL, \n      S_NATIONKEY INTEGER NOT NULL, \n      S_PHONE STRING NOT NULL, \n      S_ACCTBAL DECIMAL(15, 2) NOT NULL,\n      S_COMMENT STRING NOT NULL)\n      USING ROW OPTIONS (BUCKETS '10',\n      PARTITION_BY 'S_SUPPKEY',\n      PERSISTENCE 'ASYNCHRONOUS',\n      EVICTION_BY 'LRUCOUNT 3',\n      OVERFLOW 'true');", 
            "title": "Example: Row Table Partitioned with 10 Buckets and Overflow Enabled"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-create-table-using-select-query", 
            "text": "CREATE TABLE CUSTOMER_STAGING USING COLUMN OPTIONS (PARTITION_BY 'C_CUSTKEY') AS SELECT * FROM CUSTOMER ;  With this alternate form of the CREATE TABLE statement, you specify the column names and/or the column data types with a query. The columns in the query result are used as a model for creating the columns in the new table.  If no column names are specified for the new table, then all the columns in the result of the query expression are used to create same-named columns in the new table, of the corresponding data type(s). If one or more column names are specified for the new table, the same number of columns must be present in the result of the query expression; the data types of those columns are used for the corresponding columns of the new table.  Note that only the column names and data types from the queried table are used when creating the new table. Additional settings in the queried table, such as partitioning, replication, and persistence, are not duplicated. You can optionally specify partitioning, replication, and persistence configuration settings for the new table and those settings need not match the settings of the queried table.", 
            "title": "Example: Create Table using Select Query"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-create-table-using-spark-dataframe-api", 
            "text": "For information on using the Apache Spark API, refer to  Using the Spark DataFrame API .", 
            "title": "Example: Create Table using Spark DataFrame API"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-create-column-table-with-put-into", 
            "text": "snappy  CREATE TABLE COL_TABLE (\n      PRSN_EVNT_ID BIGINT NOT NULL,\n      VER bigint NOT NULL,\n      CLIENT_ID BIGINT NOT NULL,\n      SRC_TYP_ID BIGINT NOT NULL)\n      USING COLUMN OPTIONS(PARTITION_BY 'PRSN_EVNT_ID,CLIENT_ID', BUCKETS '64', KEY_COLUMNS 'PRSN_EVNT_ID, CLIENT_ID');", 
            "title": "Example: Create Column Table with PUT INTO"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#example-create-table-with-eviction-settings", 
            "text": "Use eviction settings to keep your table within a specified limit, either by removing evicted data completely or by creating an overflow table that persists the evicted data to a disk store.    Decide whether to evict based on:    Entry count (useful if table row sizes are relatively uniform).    Total bytes used.    Percentage of JVM heap used. This uses the SnappyData resource manager. When the manager determines that eviction is required, the manager orders the eviction controller to start evicting from all tables where the eviction criterion is set to\u00a0LRUHEAPPERCENT.      Decide what action to take when the limit is reached:    Locally destroy the row (partitioned tables only).    Overflow the row data to disk.      If you want to overflow data to disk (or persist the entire table to disk), configure a named disk store to use for the overflow data. If you do not specify a disk store when creating an overflow table, SnappyData stores the overflow data in the default disk store.    Create the table with the required eviction configuration.  For example, to evict using LRU entry count and overflow evicted rows to a disk store (OverflowDiskStore):  CREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (EVICTION_BY 'LRUCOUNT 2', OVERFLOW 'true', DISKSTORE 'OverflowDiskStore', PERSISTENCE 'async');  To create a table that simply removes evicted data from memory without persisting the evicted data, use the\u00a0 DESTROY \u00a0eviction action. For example:\nDefault in SnappyData for  synchronous  is  persistence ,  overflow  is  true  and  eviction_by  is  LRUHEAPPERCENT .  CREATE TABLE Orders(OrderId INT NOT NULL,ItemId INT) USING row OPTIONS (PARTITION_BY 'OrderId', EVICTION_BY 'LRUMEMSIZE 1000');", 
            "title": "Example: Create Table with Eviction Settings"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#constraint-only-for-row-tables", 
            "text": "A CONSTRAINT clause is an optional part of a CREATE TABLE statement that defines a rule to which table data must conform.  There are two types of constraints:  Column-level constraints : Refer to a single column in the table and do not specify a column name (except check constraints). They refer to the column that they follow.  Table-level constraints : Refer to one or more columns in the table. Table-level constraints specify the names of the columns to which they apply. Table-level CHECK constraints can refer to 0 or more columns in the table.  Column and table constraints include:    NOT NULL\u2014 Specifies that a column cannot hold NULL values (constraints of this type are not nameable).    PRIMARY KEY\u2014 Specifies a column (or multiple columns if specified in a table constraint) that uniquely identifies a row in the table. The identified columns must be defined as NOT NULL.    UNIQUE\u2014 Specifies that values in the column must be unique. NULL values are not allowed.    FOREIGN KEY\u2014 Specifies that the values in the columns must correspond to values in the referenced primary key or unique columns or that they are NULL.  If the foreign key consists of multiple columns and any column is NULL, then the whole key is considered NULL. SnappyData permits the insert no matter what is in the non-null columns.    CHECK\u2014 Specifies rules for values in a column, or specifies a wide range of rules for values when included as a table constraint. The CHECK constraint has the same format and restrictions for column and table constraints.\nColumn constraints and table constraints have the same function; the difference is where you specify them. Table constraints allow you to specify more than one column in a PRIMARY KEY, UNIQUE, CHECK, or FOREIGN KEY constraint definition.     Column-level constraints (except for check constraints) refer to only one column.\nIf you do not specify a name for a column or table constraint, then SnappyData generates a unique name.  Example : The following example demonstrates how to create a table with  FOREIGN KEY :   snappy  create table trading.customers (cid int not null, cust_name varchar(100), since date, addr varchar(100), tid int, primary key (cid));\n\nsnappy  create table trading.networth (cid int not null, cash decimal (30, 20), securities decimal (30, 20), loanlimit int, availloan decimal (30, 20),  tid int, constraint netw_pk primary key (cid), constraint cust_newt_fk foreign key (cid) references trading.customers (cid));\n\nsnappy  show importedkeys in trading;\nPKTABLE_NAME        |PKCOLUMN_NAME       |PK_NAME             |FKTABLE_SCHEM       |FKTABLE_NAME        |FKCOLUMN_NAME       |FK_NAME             |KEY_SEQ\n---------------------------------------------------------------------------------------------------------------------------------------------------------- \nCUSTOMERS           |CID                 |SQL180403162038220  |TRADING             |NETWORTH            |CID                 |CUST_NEWT_FK        |1", 
            "title": "Constraint (only for Row Tables)"
        }, 
        {
            "location": "/reference/sql_reference/create-table/#identity-columns-only-for-row-tables", 
            "text": "SnappyData supports both GENERATED ALWAYS and GENERATED BY DEFAULT identity columns only for BIGINT and INTEGER data types. The START WITH and INCREMENT BY clauses are supported only for GENERATED BY DEFAULT identity columns.  For a GENERATED ALWAYS identity column, SnappyData increments the default value on every insertion, and stores the incremented value in the column. You cannot insert a value directly into a GENERATED ALWAYS identity column, and you cannot update a value in a GENERATED ALWAYS identity column. Instead, you must either specify the DEFAULT keyword when inserting data into the table or you must leave the identity column out of the insertion column list.  Consider a table with the following column definition:  create table greetings (i int generated always as identity, ch char(50)) using row;  You can insert rows into the table using either the DEFAULT keyword or by omitting the identity column from the INSERT statement:  insert into greetings values (DEFAULT, 'hello');  insert into greetings(ch) values ('hi');  The values that SnappyData automatically generates for a GENERATED ALWAYS identity column are unique.  For a GENERATED BY DEFAULT identity column, SnappyData increments and uses a default value for an INSERT only when no explicit value is given. To use the generated default value, either specify the DEFAULT keyword when inserting into the identity column or leave the identity column out of the INSERT column list.  In contrast to GENERATED ALWAYS identity columns, with a GENERATED BY DEFAULT column you can specify an identity value to use instead of the generated default value. To specify a value, include it in the INSERT statement.  For example, consider a table created using the statement:  create table greetings (i int generated by default as identity, ch char(50));   The following statement specifies the value \u201c1\u201d for the identity column:  insert into greetings values (1, 'hi');   These statements both use generated default values:  insert into greetings values (DEFAULT, 'hello');\ninsert into greetings(ch) values ('bye');  Although the automatically-generated values in a GENERATED BY DEFAULT identity column are unique, a GENERATED BY DEFAULT column does not guarantee unique identity values for all rows in the table. For example, in the above statements, the rows containing \u201chi\u201d and \u201chello\u201d both have an identity value of \u201c1.\u201d This occurs because the generated column starts at \u201c1\u201d and the user-specified value was also \u201c1.\u201d  To avoid duplicating identity values (for example, during an import operation), you can use the START WITH clause to specify the first identity value that SnappyData should assign and increment. Or, you can use a primary key or a unique constraint on the GENERATED BY DEFAULT identity column to check for and disallow duplicates.  By default, the initial value of a GENERATED BY DEFAULT identity column is 1, and the value is incremented by 1 for each INSERT. Use the optional START WITH clause to specify a new initial value. Use the optional INCREMENT BY clause to change the increment value used during each INSERT.  Related Topics    DROP TABLE    DELETE TABLE    SHOW TABLES    TRUNCATE TABLE", 
            "title": "Identity Columns (only for Row Tables)"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/", 
            "text": "CREATE TEMPORARY TABLE\n\n\nCREATE TEMPORARY TABLE table_name\n    USING datasource\n    [AS select_statement];\n\n\n\n\nFor more information on column-definition, refer to \nColumn Definition For Column Table\n.\n\n\nRefer to these sections for more information on \nCreating Table\n, \nCreating Sample Table\n, \nCreating External Table\n and \nCreating Stream Table\n.\n\n\nTEMPORARY\n\n\nTemporary tables are scoped to SQL connection or the Snappy Spark session that creates it. This table does not appear in the system catalog nor visible to other connections or sessions.\n\n\nUSING \n\n\nSpecify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister.\n\n\nAS \n\nPopulate the table with input data from the select statement. \n\n\nExamples\n\n\nsnappy\n CREATE TEMPORARY TABLE STAGING_AIRLINEREF USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData');\n\n\n\n\nsnappy\n CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;\n\n\n\n\n\n\nNote\n\n\nWhen creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results. \nEnsure that you create temporary tables with a unique name.\n\n\n\n\nCREATE GLOBAL TEMPORARY TABLE\n\n\nsnappy\n CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] USING PARQUET OPTIONS(path 'path-to-parquet');\n\nsnappy\n CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] AS SELECT [column-name], [column-name] FROM [table-name];\n\n\n\n\nDescription\n\n\nSpecifies a table definition that is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table.\n\n\nExamples\n\n\nsnappy\n CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINEREF1 USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData');\n\nsnappy\n CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINE2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;\n\n\n\n\n\n\nNote\n\n\nTemporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.", 
            "title": "CREATE TEMPORARY TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/#create-temporary-table", 
            "text": "CREATE TEMPORARY TABLE table_name\n    USING datasource\n    [AS select_statement];  For more information on column-definition, refer to  Column Definition For Column Table .  Refer to these sections for more information on  Creating Table ,  Creating Sample Table ,  Creating External Table  and  Creating Stream Table .  TEMPORARY  Temporary tables are scoped to SQL connection or the Snappy Spark session that creates it. This table does not appear in the system catalog nor visible to other connections or sessions.  USING   Specify the file format to use for this table. The data source may be one of TEXT, CSV, JSON, JDBC, PARQUET, ORC, and LIBSVM, or a fully qualified class name of a custom implementation of org.apache.spark.sql.sources.DataSourceRegister.  AS  \nPopulate the table with input data from the select statement.", 
            "title": "CREATE TEMPORARY TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/#examples", 
            "text": "snappy  CREATE TEMPORARY TABLE STAGING_AIRLINEREF USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData');  snappy  CREATE TEMPORARY TABLE STAGING_AIRLINE_TEMP2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;   Note  When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results.  Ensure that you create temporary tables with a unique name.", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/#create-global-temporary-table", 
            "text": "snappy  CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] USING PARQUET OPTIONS(path 'path-to-parquet');\n\nsnappy  CREATE GLOBAL TEMPORARY TABLE [global-temporary-table-name] AS SELECT [column-name], [column-name] FROM [table-name];", 
            "title": "CREATE GLOBAL TEMPORARY TABLE"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/#description", 
            "text": "Specifies a table definition that is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-temporary-table/#examples_1", 
            "text": "snappy  CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINEREF1 USING parquet OPTIONS(path '../../quickstart/data/airportcodeParquetData');\n\nsnappy  CREATE GLOBAL TEMPORARY TABLE STAGING_AIRLINE2 AS SELECT CODE, DESCRIPTION FROM STAGING_AIRLINEREF;   Note  Temporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/create-view/", 
            "text": "CREATE VIEW\n\n\nSYNTAX\n\n\nsnappy\n CREATE VIEW [view-name]  AS SELECT [column_name, column_name] FROM [table_name];\n\nsnappy\n CREATE VIEW [view-name] (column_name, column_name) AS SELECT column_name, column_name FROM [table_name];\n\n\n\n\nDescription\n\n\nThe View can be described as a virtual table that contains a set of definitions, built on top of the table(s) or the other view(s), but it does not physically store the data like a table.\nView is persistent and visible in system catalog and therefore shared between all connections.\n\n\nExamples \n\n\nsnappy\n CREATE VIEW TRADE.ORDERS1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;\n\nsnappy\n CREATE VIEW TRADE.ORDERS2 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;\n\n\n\n\nCREATE TEMPORARY VIEW\n\n\nsnappy\n CREATE TEMPORARY VIEW [temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name];\n\nsnappy\n CREATE TEMPORARY VIEW [temporary-view-name] USING PARQUET OPTIONS(PATH 'path-to-parquet');\n\n\n\n\nDescription\n\n\nCreates a session-specific temporary view, which is dropped when the session ends.\nTemporary views have the same restrictions as permanent views, so you cannot perform insert, update, delete, or copy operations on these views.\n\n\nLocal temporary views are session-scoped; the view drops automatically when the session ends. \n\n\nExamples\n\n\nsnappy\n CREATE TEMPORARY VIEW AIRLINEVIEW1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;\n\nsnappy\n CREATE TEMPORARY VIEW AIRLINEVIEW2 USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA');\n\n\n\n\nCREATE GLOBAL TEMPORARY VIEW\n\n\nsnappy\n CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name];\n\nsnappy\n CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] USING PARQUET OPTIONS(path 'path-to-parquet');\n\n\n\n\nDescription\n\n\nCreates a global temporary view this is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table.\n\n\nThe optional GLOBAL keyword allows the view to be shared among all connections but it is not persisted to system catalog so will disappear when lead/driver restarts or fails. Use CREATE VIEW for persistent views.\n\n\nExamples\n\n\nsnappy\n CREATE GLOBAL TEMPORARY VIEW ORDER AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;\n\nsnappy\n CREATE GLOBAL TEMPORARY VIEW AIRLINEVIEW USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA');\n\n\n\n\n\n\nNote\n\n\nTemporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with the former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.", 
            "title": "CREATE VIEW"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#create-view", 
            "text": "", 
            "title": "CREATE VIEW"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#syntax", 
            "text": "snappy  CREATE VIEW [view-name]  AS SELECT [column_name, column_name] FROM [table_name];\n\nsnappy  CREATE VIEW [view-name] (column_name, column_name) AS SELECT column_name, column_name FROM [table_name];", 
            "title": "SYNTAX"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#description", 
            "text": "The View can be described as a virtual table that contains a set of definitions, built on top of the table(s) or the other view(s), but it does not physically store the data like a table.\nView is persistent and visible in system catalog and therefore shared between all connections.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#examples", 
            "text": "snappy  CREATE VIEW TRADE.ORDERS1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;\n\nsnappy  CREATE VIEW TRADE.ORDERS2 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;", 
            "title": "Examples "
        }, 
        {
            "location": "/reference/sql_reference/create-view/#create-temporary-view", 
            "text": "snappy  CREATE TEMPORARY VIEW [temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name];\n\nsnappy  CREATE TEMPORARY VIEW [temporary-view-name] USING PARQUET OPTIONS(PATH 'path-to-parquet');", 
            "title": "CREATE TEMPORARY VIEW"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#description_1", 
            "text": "Creates a session-specific temporary view, which is dropped when the session ends.\nTemporary views have the same restrictions as permanent views, so you cannot perform insert, update, delete, or copy operations on these views.  Local temporary views are session-scoped; the view drops automatically when the session ends.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#examples_1", 
            "text": "snappy  CREATE TEMPORARY VIEW AIRLINEVIEW1 AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;\n\nsnappy  CREATE TEMPORARY VIEW AIRLINEVIEW2 USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA');", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#create-global-temporary-view", 
            "text": "snappy  CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] AS SELECT [column-name], [column-name] FROM [schema].[table-name];\n\nsnappy  CREATE GLOBAL TEMPORARY VIEW [global-temporary-view-name] USING PARQUET OPTIONS(path 'path-to-parquet');", 
            "title": "CREATE GLOBAL TEMPORARY VIEW"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#description_2", 
            "text": "Creates a global temporary view this is visible to all sessions. Temporary table data is visible only to the session that inserts the data into the table.  The optional GLOBAL keyword allows the view to be shared among all connections but it is not persisted to system catalog so will disappear when lead/driver restarts or fails. Use CREATE VIEW for persistent views.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/create-view/#examples_2", 
            "text": "snappy  CREATE GLOBAL TEMPORARY VIEW ORDER AS SELECT ORDERID, ITEMID FROM TRADE.ORDERS;\n\nsnappy  CREATE GLOBAL TEMPORARY VIEW AIRLINEVIEW USING PARQUET OPTIONS(PATH '../../QUICKSTART/DATA/AIRLINEPARQUETDATA');   Note  Temporary views/tables are scoped to SQL connection or the Snappy Spark session that creates it. VIEW or TABLE are synonyms in this context with the former being the preferred usage. This table does not appear in the system catalog nor visible to other connections or sessions.", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/delete/", 
            "text": "DELETE\n\n\nDelete rows from a table.\n\n\n{\nDELETE FROM table-name [ ]\n[ WHERE ]\n}\n\n\n\n\nDescription\n\n\nThis form is called a searched delete, removes all rows identified by the table name and WHERE clause.\n\n\nExample\n\n\n// Delete rows from the CUSTOMERS table where the CID is equal to 10.\nDELETE FROM TRADE.CUSTOMERS WHERE CID = 10;\n\n// Delete all rows from table T.\nDELETE FROM T;\n\n\n\n\nRelated Topics\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nDROP TABLE\n\n\n\n\n\n\nSHOW TABLES\n\n\n\n\n\n\nTRUNCATE TABLE", 
            "title": "DELETE"
        }, 
        {
            "location": "/reference/sql_reference/delete/#delete", 
            "text": "Delete rows from a table.  {\nDELETE FROM table-name [ ]\n[ WHERE ]\n}", 
            "title": "DELETE"
        }, 
        {
            "location": "/reference/sql_reference/delete/#description", 
            "text": "This form is called a searched delete, removes all rows identified by the table name and WHERE clause.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/delete/#example", 
            "text": "// Delete rows from the CUSTOMERS table where the CID is equal to 10.\nDELETE FROM TRADE.CUSTOMERS WHERE CID = 10;\n\n// Delete all rows from table T.\nDELETE FROM T;  Related Topics    CREATE TABLE    DROP TABLE    SHOW TABLES    TRUNCATE TABLE", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-statements/", 
            "text": "Drop Statements\n\n\nThe drop statements are used to drop diskstores, functions, indexes, schemas, and tables.\n\n\n\n\n\n\nDROP DISKSTORE\n\n\n\n\n\n\nDROP FUNCTION\n\n\n\n\n\n\nDROP INDEX\n\n\n\n\n\n\nDROP SCHEMA\n\n\n\n\n\n\nDROP TABLE", 
            "title": "DROP Statements"
        }, 
        {
            "location": "/reference/sql_reference/drop-statements/#drop-statements", 
            "text": "The drop statements are used to drop diskstores, functions, indexes, schemas, and tables.    DROP DISKSTORE    DROP FUNCTION    DROP INDEX    DROP SCHEMA    DROP TABLE", 
            "title": "Drop Statements"
        }, 
        {
            "location": "/reference/sql_reference/drop-diskstore/", 
            "text": "DROP DISKSTORE\n\n\nRemoves a disk store configuration from the SnappyData cluster.\n\n\nDROP DISKSTORE [ IF EXISTS ] store-name\n\n\n\n\nIF EXISTS\n \n\nInclude the \nIF EXISTS\n clause to execute the statement only if the specified disk store exists in SnappyData.\n\n\nstore-name\n\nUser-defined name of the disk store configuration that you want to remove. The available names are stored in the \nSYSDISKSTORES\n system table.\n\n\nExample\n\n\nThis command removes the disk store \"STORE1\" from the cluster:\n\n\nDROP DISKSTORE store1;\n\n\n\n\nRelated Topics\n\n\n\n\n\n\nCREATE DISKSTORE\n\n\n\n\n\n\nSYSDISKSTORES", 
            "title": "DROP DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/drop-diskstore/#drop-diskstore", 
            "text": "Removes a disk store configuration from the SnappyData cluster.  DROP DISKSTORE [ IF EXISTS ] store-name  IF EXISTS   \nInclude the  IF EXISTS  clause to execute the statement only if the specified disk store exists in SnappyData.  store-name \nUser-defined name of the disk store configuration that you want to remove. The available names are stored in the  SYSDISKSTORES  system table.", 
            "title": "DROP DISKSTORE"
        }, 
        {
            "location": "/reference/sql_reference/drop-diskstore/#example", 
            "text": "This command removes the disk store \"STORE1\" from the cluster:  DROP DISKSTORE store1;  Related Topics    CREATE DISKSTORE    SYSDISKSTORES", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/", 
            "text": "DROP FUNCTION\n\n\nDROP FUNCTION IF EXISTS udf_name\n\n\n\n\nDescription\n\n\nDrops an existing function. If the function to drop does not exist, an exception is reported.\n\n\nExample\n\n\nDROP FUNCTION IF EXISTS app.strnglen\n\n\n\n\nRelated Topics\n\n\n\n\nCREATE FUNCTION", 
            "title": "DROP FUNCTION"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/#drop-function", 
            "text": "DROP FUNCTION IF EXISTS udf_name", 
            "title": "DROP FUNCTION"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/#description", 
            "text": "Drops an existing function. If the function to drop does not exist, an exception is reported.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/drop-function/#example", 
            "text": "DROP FUNCTION IF EXISTS app.strnglen  Related Topics   CREATE FUNCTION", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/", 
            "text": "DROP INDEX\n\n\nDROP INDEX [ IF EXISTS ] [schema-name.]index-name;\n\n\n\n\nDescription\n\n\nDrops the index in the given schema (or current schema if none is provided). Include the \nIF EXISTS\n clause to execute the statement only if the specified index exists in SnappyData.\n\n\nExample\n\n\nDROP INDEX IF EXISTS app.idx;", 
            "title": "DROP INDEX"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/#drop-index", 
            "text": "DROP INDEX [ IF EXISTS ] [schema-name.]index-name;", 
            "title": "DROP INDEX"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/#description", 
            "text": "Drops the index in the given schema (or current schema if none is provided). Include the  IF EXISTS  clause to execute the statement only if the specified index exists in SnappyData.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/drop-index/#example", 
            "text": "DROP INDEX IF EXISTS app.idx;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/", 
            "text": "DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE\n\n\nDROP TABLE [ IF EXISTS ] [schema-name.]table-name\n\n\n\n\nDescription\n\n\nRemoves the specified table. Include the \nIF EXISTS\n clause to execute the statement only if the specified table exists in SnappyData. The \nschema-name.\n prefix is optional if you are currently using the schema that contains the table.\n\n\nExample\n\n\nDROP TABLE IF EXISTS app.customer\n\n\n\n\nRelated Topics\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nCREATE EXTERNAL TABLE\n\n\n\n\n\n\nCREATE SAMPLE TABLE\n\n\n\n\n\n\nDELETE TABLE\n\n\n\n\n\n\nSHOW TABLES\n\n\n\n\n\n\nTRUNCATE TABLE", 
            "title": "DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/#drop-tableexternal-tablesample-table", 
            "text": "DROP TABLE [ IF EXISTS ] [schema-name.]table-name", 
            "title": "DROP TABLE/EXTERNAL TABLE/SAMPLE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/#description", 
            "text": "Removes the specified table. Include the  IF EXISTS  clause to execute the statement only if the specified table exists in SnappyData. The  schema-name.  prefix is optional if you are currently using the schema that contains the table.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/drop-table/#example", 
            "text": "DROP TABLE IF EXISTS app.customer  Related Topics    CREATE TABLE    CREATE EXTERNAL TABLE    CREATE SAMPLE TABLE    DELETE TABLE    SHOW TABLES    TRUNCATE TABLE", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/drop-schema/", 
            "text": "DROP SCHEMA\n\n\nDROP TABLE [schema-name] restrict;\n\n\n\n\nDescription\n\n\nPermanently removes a schema from the database. Ensure that you delete all the objects that exist in a schema before you drop it. This is an irreversible process.\n\n\nExample\n\n\ndrop schema trade restrict;", 
            "title": "DROP SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/drop-schema/#drop-schema", 
            "text": "DROP TABLE [schema-name] restrict;", 
            "title": "DROP SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/drop-schema/#description", 
            "text": "Permanently removes a schema from the database. Ensure that you delete all the objects that exist in a schema before you drop it. This is an irreversible process.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/drop-schema/#example", 
            "text": "drop schema trade restrict;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/deploy/", 
            "text": "DEPLOY\n\n\nUse the DEPLOY statements to deploy packages and dependency jars. You can also undeploy the deployed jars.\n\n\n\n\nDEPLOY PACKAGE\n\n\nDEPLOY JAR\n\n\nUNDEPLOY", 
            "title": "DEPLOY Statements"
        }, 
        {
            "location": "/reference/sql_reference/deploy/#deploy", 
            "text": "Use the DEPLOY statements to deploy packages and dependency jars. You can also undeploy the deployed jars.   DEPLOY PACKAGE  DEPLOY JAR  UNDEPLOY", 
            "title": "DEPLOY"
        }, 
        {
            "location": "/reference/sql_reference/deploy_package/", 
            "text": "DEPLOY PACKAGES\n\n\nDeploys package in SnappyData.\n\n\nSyntax\n\n\ndeploy package \nunique-alias-name\n \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ]\n\n\n\n\n\n\n\n\nunique-alias-name\n - A name to identify a package. This name can be used to remove the package from the cluster.  You can use alphabets, numbers, and underscores to create the name.\n\n\n\n\n\n\npackages\n - Comma-delimited string of maven packages. \n\n\n\n\n\n\nrepos\n - Comma-delimited string of remote repositories other than the \nMaven Central\n and \nspark-package\n repositories. These two repositories are searched by default.  The format specified by Maven is: \ngroupId:artifactId:version\n.\n\n\n\n\n\n\npath\n - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the \nivy.home\n property. if even that is not specified, the \n.ivy2\n in the user\u2019s home directory is used.\n\n\n\n\n\n\nDescription\n\n\nPackages can be deployed in SnappyData using the \nDEPLOY PACKAGE\n SQL. You can pass the following through this SQL:\n\n\n\n\nName of the package.\n\n\nRepository where the package is located.\n\n\nPath to a local cache of jars.\n\n\n\n\n\n\nNote\n\n\nSnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise the resolution of the package fails.\n\n\n\n\nFor resolving the package, Maven Central and Spark packages, located at http://dl.bintray.com/spark-packages, are searched by default. Hence, you must specify the repository only if the package is not there at \nMaven Central\n or in the \nspark-package\n repository.\n\n\n\n\nTip\n\n\nUse \nspark-packages.org\n to search for Spark packages. Most of the popular Spark packages are listed here.\n\n\n\n\nExample\n\n\n\n\nDeploy packages from a default repository.\n\n\n\n\ndeploy package spark_deep_learning_0_3_0 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work'\n\n\n\n\ndeploy package spark_redshift_300 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work'\n\n\n\n\nDeploy packages from a non-default repository.\n\n\nRelated Topics\n\n\n\n\nDEPLOY JAR", 
            "title": "DEPLOY PACKAGE"
        }, 
        {
            "location": "/reference/sql_reference/deploy_package/#deploy-packages", 
            "text": "Deploys package in SnappyData.", 
            "title": "DEPLOY PACKAGES"
        }, 
        {
            "location": "/reference/sql_reference/deploy_package/#syntax", 
            "text": "deploy package  unique-alias-name  \u2018packages\u2019 [ repos \u2018repositories\u2019 ] [ path 'some path to cache resolved jars' ]    unique-alias-name  - A name to identify a package. This name can be used to remove the package from the cluster.  You can use alphabets, numbers, and underscores to create the name.    packages  - Comma-delimited string of maven packages.     repos  - Comma-delimited string of remote repositories other than the  Maven Central  and  spark-package  repositories. These two repositories are searched by default.  The format specified by Maven is:  groupId:artifactId:version .    path  - The path to the local repository. This path should be visible to all the lead nodes of the system. If this path is not specified then the system uses the path set in the  ivy.home  property. if even that is not specified, the  .ivy2  in the user\u2019s home directory is used.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/deploy_package/#description", 
            "text": "Packages can be deployed in SnappyData using the  DEPLOY PACKAGE  SQL. You can pass the following through this SQL:   Name of the package.  Repository where the package is located.  Path to a local cache of jars.    Note  SnappyData requires internet connectivity to connect to repositories which are hosted outside the network. Otherwise the resolution of the package fails.   For resolving the package, Maven Central and Spark packages, located at http://dl.bintray.com/spark-packages, are searched by default. Hence, you must specify the repository only if the package is not there at  Maven Central  or in the  spark-package  repository.   Tip  Use  spark-packages.org  to search for Spark packages. Most of the popular Spark packages are listed here.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/deploy_package/#example", 
            "text": "Deploy packages from a default repository.   deploy package spark_deep_learning_0_3_0 'databricks:spark-deep-learning:0.3.0-spark2.2-s_2.11' path '/home/snappydata/work'  deploy package spark_redshift_300 'com.databricks:spark-redshift_2.10:3.0.0-preview1' path '/home/snappydata/work'  Deploy packages from a non-default repository.  Related Topics   DEPLOY JAR", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/deploy_jar/", 
            "text": "DEPLOY JARS\n\n\nDeploys a jar in a running system.\n\n\nSyntax\n\n\ndeploy jar \nunique-alias-name\n \u2018jars\u2019\n\n\n\n\n\n\n\n\nunique-alias-name\n - A name to identify the jar. This name can be used to remove the jar from the cluster.  You can use alphabets, numbers and underscores to create the name.\n\n\n\n\n\n\njars\n - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData.\n\n\n\n\n\n\nDescription\n\n\nSnappyData provides a method to deploy a jar in a running system through SQL. You can execute the \ndeploy jar \n command to deploy dependency jars. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system.\n\n\nExample\n\n\ndeploy jar SparkDaria spark-daria_2.11.8-2.2.0_0.10.0.jar  \u2018jars\u2019\n\n\n\n\nRelated Topics\n\n\n\n\nDEPLOY PACKAGE\n\n\nUNDEPLOY", 
            "title": "DEPLOY JAR"
        }, 
        {
            "location": "/reference/sql_reference/deploy_jar/#deploy-jars", 
            "text": "Deploys a jar in a running system.", 
            "title": "DEPLOY JARS"
        }, 
        {
            "location": "/reference/sql_reference/deploy_jar/#syntax", 
            "text": "deploy jar  unique-alias-name  \u2018jars\u2019    unique-alias-name  - A name to identify the jar. This name can be used to remove the jar from the cluster.  You can use alphabets, numbers and underscores to create the name.    jars  - Comma-delimited string of jar paths. These paths are expected to be accessible from all the lead nodes in SnappyData.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/deploy_jar/#description", 
            "text": "SnappyData provides a method to deploy a jar in a running system through SQL. You can execute the  deploy jar   command to deploy dependency jars. In cases where the artifacts of the dependencies are not available in the provided cache path, then during restart, it automatically resolves all the packages and jars again and installs them in the system.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/deploy_jar/#example", 
            "text": "deploy jar SparkDaria spark-daria_2.11.8-2.2.0_0.10.0.jar  \u2018jars\u2019  Related Topics   DEPLOY PACKAGE  UNDEPLOY", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/undeploy/", 
            "text": "UNDEPLOY\n\n\nRemoves the jars that are directly installed and the jars that are associated with a package, from the system.\n\n\nSyntax\n\n\nundeploy \nunique-alias-name\n;\n\n\n\n\n\n\njar-name\n - Name of the jar that must be removed.  \n\n\n\n\nDescription\n\n\nThe command removes the jars that are directly installed and the jars that are associated with a package, from the system. \n\n\n\n\nNote\n\n\nThe removal is only captured when you use the \nundeploy\n command,  the jars are removed only when the system restarts.\n\n\n\n\nExample\n\n\nundeploy spark_deep_learning_0_3_0; \n\n\n\n\nRelated Topics\n\n\n\n\nDEPLOY PACKAGE\n\n\nDEPLOY JAR", 
            "title": "UNDEPLOY"
        }, 
        {
            "location": "/reference/sql_reference/undeploy/#undeploy", 
            "text": "Removes the jars that are directly installed and the jars that are associated with a package, from the system.", 
            "title": "UNDEPLOY"
        }, 
        {
            "location": "/reference/sql_reference/undeploy/#syntax", 
            "text": "undeploy  unique-alias-name ;   jar-name  - Name of the jar that must be removed.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/undeploy/#description", 
            "text": "The command removes the jars that are directly installed and the jars that are associated with a package, from the system.    Note  The removal is only captured when you use the  undeploy  command,  the jars are removed only when the system restarts.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/undeploy/#example", 
            "text": "undeploy spark_deep_learning_0_3_0;   Related Topics   DEPLOY PACKAGE  DEPLOY JAR", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/grant/", 
            "text": "GRANT\n\n\nSyntax\n\n\nThe syntax for the GRANT statement differs if you are granting privileges to a table or to a routine.\n\n\nSyntax for Tables\n\n\nGRANT privilege-type ON [ TABLE ] { table-name | view-name } TO grantees\n\n\n\n\nSyntax for Routines\n\n\nGRANT EXECUTE ON { FUNCTION | PROCEDURE } routine-designator TO grantees\n\n\n\n\nDescription\n\n\nThe GRANT statement enables permissions for a specific user or all users to perform actions on SQL objects.\n\n\nThe following types of permissions can be granted:\n\n\n\n\nPerform DML operations on a specific table.\n\n\nInsert/Delete rows from a table.\n\n\nSelect/Update data on a table or subset of columns in a table.\n\n\nCreate a foreign key reference to the named table or to a subset of columns from a table.\n\n\nRun a specified function or procedure.\n\n\n\n\n\n\nprivilege-type\n\n\nALL PRIVILEGES |  privilege-list\n\n\n\n\nUse the ALL PRIVILEGES privilege type to grant all of the permissions to the user for the specified table. You can also grant one or more table privileges by specifying a privilege-list.\n\n\n\n\nprivilege-list\n\n\ntable-privilege {, table-privilege }*\n\n\n\n\n\n\ntable-privilege\n\n\nALTER | DELETE | INSERT | REFERENCES [column-list] | SELECT [column-list] |\n UPDATE [ column-list ]\n\n\n\n\nUse the \nALTER\n privilege to grant permission to the command on the specified table.\n\n\nUse the \nDELETE\n privilege type to grant permission to delete rows from the specified table.\n\n\nUse the \nINSERT\n privilege type to grant permission to insert rows into the specified table.\n\n\n\n\n\nUse the \nSELECT\n privilege type to grant permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is valid on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table.\n\n\nUse the \nUPDATE\n privilege type to grant permission to use the UPDATE statement on the specified table. If a column list is specified, the permission applies only to the specified columns. To update a row using a statement that includes a WHERE clause, you must have \nSELECT\n permission on the columns in the row that you want to update.\n\n\n\n\ncolumn-list\n\n\n( column-identifier {, column-identifier }* )\n\n\n\n\n\n\ngrantees\n\n\n{ authorization ID | PUBLIC } [,{ authorization ID | PUBLIC } ] *\n\n\n\n\nYou can grant privileges for specific users or for all users. Use the keyword PUBLIC to specify all users. When PUBLIC is specified, the privileges affect all current and future users. The privileges granted to PUBLIC and to individual users are independent privileges. For example, a SELECT privilege on table t is granted to both PUBLIC and to the authorization ID harry. The SELECT privilege is later revoked from the authorization ID harry, but Harry can access the table t through the PUBLIC privilege.\n\n\n\n\nroutine-designator\n\n\n{ function-name | procedure-name }\n\n\n\n\nExample\n\n\nTo grant the SELECT privilege on table \"t\" to the authorization IDs \"sam\" and \"bob:\"\n\n\nGRANT SELECT ON TABLE t TO sam,bob;\n\n\n\n\nTo grant the UPDATE privileges on table \"t\" to the authorization IDs \"john\" and \"smith:\"\n\n\nGRANT UPDATE ON TABLE t TO john,smith;\n\n\n\n\nTo grant ALTER TABLE privileges on table \"t\" to the authorization ID \"adam:\"\n\n\nGRANT ALTER ON TABLE t TO adam;\n\n\n\n\nTo grant the SELECT privilege on table \"test.sample\" to all users:\n\n\nGRANT SELECT ON TABLE test.sample to PUBLIC;", 
            "title": "GRANT"
        }, 
        {
            "location": "/reference/sql_reference/grant/#grant", 
            "text": "", 
            "title": "GRANT"
        }, 
        {
            "location": "/reference/sql_reference/grant/#syntax", 
            "text": "The syntax for the GRANT statement differs if you are granting privileges to a table or to a routine.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/grant/#syntax-for-tables", 
            "text": "GRANT privilege-type ON [ TABLE ] { table-name | view-name } TO grantees", 
            "title": "Syntax for Tables"
        }, 
        {
            "location": "/reference/sql_reference/grant/#syntax-for-routines", 
            "text": "GRANT EXECUTE ON { FUNCTION | PROCEDURE } routine-designator TO grantees", 
            "title": "Syntax for Routines"
        }, 
        {
            "location": "/reference/sql_reference/grant/#description", 
            "text": "The GRANT statement enables permissions for a specific user or all users to perform actions on SQL objects.  The following types of permissions can be granted:   Perform DML operations on a specific table.  Insert/Delete rows from a table.  Select/Update data on a table or subset of columns in a table.  Create a foreign key reference to the named table or to a subset of columns from a table.  Run a specified function or procedure.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/grant/#privilege-type", 
            "text": "ALL PRIVILEGES |  privilege-list  Use the ALL PRIVILEGES privilege type to grant all of the permissions to the user for the specified table. You can also grant one or more table privileges by specifying a privilege-list.", 
            "title": "privilege-type"
        }, 
        {
            "location": "/reference/sql_reference/grant/#privilege-list", 
            "text": "table-privilege {, table-privilege }*", 
            "title": "privilege-list"
        }, 
        {
            "location": "/reference/sql_reference/grant/#table-privilege", 
            "text": "ALTER | DELETE | INSERT | REFERENCES [column-list] | SELECT [column-list] |\n UPDATE [ column-list ]  Use the  ALTER  privilege to grant permission to the command on the specified table.  Use the  DELETE  privilege type to grant permission to delete rows from the specified table.  Use the  INSERT  privilege type to grant permission to insert rows into the specified table.   Use the  SELECT  privilege type to grant permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is valid on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table.  Use the  UPDATE  privilege type to grant permission to use the UPDATE statement on the specified table. If a column list is specified, the permission applies only to the specified columns. To update a row using a statement that includes a WHERE clause, you must have  SELECT  permission on the columns in the row that you want to update.", 
            "title": "table-privilege"
        }, 
        {
            "location": "/reference/sql_reference/grant/#column-list", 
            "text": "( column-identifier {, column-identifier }* )", 
            "title": "column-list"
        }, 
        {
            "location": "/reference/sql_reference/grant/#grantees", 
            "text": "{ authorization ID | PUBLIC } [,{ authorization ID | PUBLIC } ] *  You can grant privileges for specific users or for all users. Use the keyword PUBLIC to specify all users. When PUBLIC is specified, the privileges affect all current and future users. The privileges granted to PUBLIC and to individual users are independent privileges. For example, a SELECT privilege on table t is granted to both PUBLIC and to the authorization ID harry. The SELECT privilege is later revoked from the authorization ID harry, but Harry can access the table t through the PUBLIC privilege.", 
            "title": "grantees"
        }, 
        {
            "location": "/reference/sql_reference/grant/#routine-designator", 
            "text": "{ function-name | procedure-name }", 
            "title": "routine-designator"
        }, 
        {
            "location": "/reference/sql_reference/grant/#example", 
            "text": "To grant the SELECT privilege on table \"t\" to the authorization IDs \"sam\" and \"bob:\"  GRANT SELECT ON TABLE t TO sam,bob;  To grant the UPDATE privileges on table \"t\" to the authorization IDs \"john\" and \"smith:\"  GRANT UPDATE ON TABLE t TO john,smith;  To grant ALTER TABLE privileges on table \"t\" to the authorization ID \"adam:\"  GRANT ALTER ON TABLE t TO adam;  To grant the SELECT privilege on table \"test.sample\" to all users:  GRANT SELECT ON TABLE test.sample to PUBLIC;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/insert/", 
            "text": "INSERT\n\n\nAn INSERT statement creates a row or rows and stores them in the named table. The number of values assigned in an INSERT statement must be the same as the number of specified or implied columns.\n\n\nINSERT INTO table-name\n    [ ( simple-column-name [ , simple-column-name ]* ) ]\n   Query\n\n\n\n\nDescription\n\n\nThe query can be:\n\n\n\n\na VALUES list\n\n\na multiple-row VALUES expression\n\n\n\n\n\n\nNote\n\n\nSnappyData does not support an INSERT with a subselect query if any subselect query requires aggregation. \n\n\n\n\nSingle-row and multiple-row lists can include the keyword DEFAULT. Specifying DEFAULT for a column inserts the column's default value into the column. Another way to insert the default value into the column is to omit the column from the column list and only insert values into other columns in the table.\n\n\nFor more information, refer to \nSELECT\n.\n\n\nExample\n\n\n\n--create table trade.customers\nCREATE TABLE TRADE.CUSTOMERS (CID INT, CUST_NAME VARCHAR(100), SINCE DATE, ADDR VARCHAR(100));\n\nINSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData');\n\n-- Insert a new customer into the CUSTOMERS  table,\n-- but do not assign  value to  'SINCE'  column\nINSERT INTO TRADE.CUSTOMERS(CID, CUST_NAME,  ADDR) VALUES (2, 'USER 2', 'SnappyData');\n\n-- Insert two new customers using one statement \n-- into the CUSTOMER table as in the previous example, \n-- but do not assign  value to 'SINCE'  field of the new customer.\nINSERT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR) VALUES (3, 'User 3' , 'SnappyData'), (4, 'User 4' , 'SnappyData');\n\n-- Insert the DEFAULT value for the SINCE column\nINSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', DEFAULT, 'SnappyData');\n\n-- Insert into another table using a select statement.\nINSERT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1';", 
            "title": "INSERT"
        }, 
        {
            "location": "/reference/sql_reference/insert/#insert", 
            "text": "An INSERT statement creates a row or rows and stores them in the named table. The number of values assigned in an INSERT statement must be the same as the number of specified or implied columns.  INSERT INTO table-name\n    [ ( simple-column-name [ , simple-column-name ]* ) ]\n   Query", 
            "title": "INSERT"
        }, 
        {
            "location": "/reference/sql_reference/insert/#description", 
            "text": "The query can be:   a VALUES list  a multiple-row VALUES expression    Note  SnappyData does not support an INSERT with a subselect query if any subselect query requires aggregation.    Single-row and multiple-row lists can include the keyword DEFAULT. Specifying DEFAULT for a column inserts the column's default value into the column. Another way to insert the default value into the column is to omit the column from the column list and only insert values into other columns in the table.  For more information, refer to  SELECT .", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/insert/#example", 
            "text": "--create table trade.customers\nCREATE TABLE TRADE.CUSTOMERS (CID INT, CUST_NAME VARCHAR(100), SINCE DATE, ADDR VARCHAR(100));\n\nINSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData');\n\n-- Insert a new customer into the CUSTOMERS  table,\n-- but do not assign  value to  'SINCE'  column\nINSERT INTO TRADE.CUSTOMERS(CID, CUST_NAME,  ADDR) VALUES (2, 'USER 2', 'SnappyData');\n\n-- Insert two new customers using one statement \n-- into the CUSTOMER table as in the previous example, \n-- but do not assign  value to 'SINCE'  field of the new customer.\nINSERT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR) VALUES (3, 'User 3' , 'SnappyData'), (4, 'User 4' , 'SnappyData');\n\n-- Insert the DEFAULT value for the SINCE column\nINSERT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', DEFAULT, 'SnappyData');\n\n-- Insert into another table using a select statement.\nINSERT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1';", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/lateral-view/", 
            "text": "LATERAL VIEW\n\n\nSupport for hive compatible LATERAL VIEW. It works with a table generating function like explode() and for each output row, joins it with the base table to create a view. Refer to \nthis document\n for details.\n\n\nSyntax\n\n\nFROM baseTable (lateralView)*\nlateralView: LATERAL VIEW function([expressions]) tableAlias [AS columnAlias (',' columnAlias)*]\n\n\n\n\n\nExample\n\n\nid\n: 1\n\npurpose\n: \nbusiness\n\n\ntype\n: sales\n\ncontact\n : [{ \nphone\n : \n555-1234\n, \nemail\n : \njsmth@company.com\n },\n           { \nphone\n : \n666-1234\n, \nemail\n : \nsmithj@company.com\n } ]\n\nSELECT id, part.phone, part.email FROM json\n  LATERAL VIEW explode(parts) partTable AS part\n\n1, \n555-1234\n, \njsmith@company.com\n\n1, \n666-1234\n, \nsmithj@company.com", 
            "title": "LATERAL VIEW"
        }, 
        {
            "location": "/reference/sql_reference/lateral-view/#lateral-view", 
            "text": "Support for hive compatible LATERAL VIEW. It works with a table generating function like explode() and for each output row, joins it with the base table to create a view. Refer to  this document  for details.", 
            "title": "LATERAL VIEW"
        }, 
        {
            "location": "/reference/sql_reference/lateral-view/#syntax", 
            "text": "FROM baseTable (lateralView)*\nlateralView: LATERAL VIEW function([expressions]) tableAlias [AS columnAlias (',' columnAlias)*]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/lateral-view/#example", 
            "text": "id : 1 purpose :  business  type : sales contact  : [{  phone  :  555-1234 ,  email  :  jsmth@company.com  },\n           {  phone  :  666-1234 ,  email  :  smithj@company.com  } ]\n\nSELECT id, part.phone, part.email FROM json\n  LATERAL VIEW explode(parts) partTable AS part\n\n1,  555-1234 ,  jsmith@company.com \n1,  666-1234 ,  smithj@company.com", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/put-into/", 
            "text": "PUT INTO\n\n\nPUT INTO operates like a standard \nINSERT\n statement.\n\n\n\n\nNote\n\n\nInsert/PUT INTO with partial column specification is not supported in SnappyData.\n\n\n\n\nFor Column Tables\n\n\nIf you need the \nputInto()\n functionality for column tables, you must specify key_columns while defining the column table.\nPUT INTO updates the row if present, else, it inserts the row. \n\nAs column tables do not have the primary key functionality, you must specify key_columns when creating the table.\n\nThese columns are used to identify a row uniquely. PUT INTO is available by SQL as well APIs.\n\n\nSyntax\n\n\nPUT INTO \nTABLENAME\n SELECT V1, V2, V3,......,Vn;\n\nPUT INTO \nschema name\n.\ntable name2\n SELECT * from \nschema name\n.\ntable name1\n;\n\nPUT INTO \nschema name\n.\ntable name2\n SELECT * from \nschema name\n.\ntable name1\n WHERE \ncolumn name\n='\nValue\n'\n\nPUT INTO \nschema name\n.\ntable name2\n SELECT from \nschema name\n.\ntable name1\n WHERE \ncolumn name\n='\nValue\n'\n\n\n\n\n\nExamples\n\n\n\n\nFor SQL\n\n\n// Insert into another table using a select statement for column tables with key columns\n\nPUT INTO TRADE.CUSTOMERS SELECT '1','2','hello';\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS;\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS WHERE C_NAME='User 1';\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT from CUSTOMERS WHERE C_NAME='User 1';\n\n\n\n\n\n\n\nWarning\n\n\nPUT INTO VALUES (V1, V2,...Vn)\n syntax do not work for column tables. Instead use \nPUT INTO SELECT V1, V2,...,Vn\n.\n\n\n\n\nFor API\n\n\nputInto API\n is available from the DataFrameWriter extension.\n\n\nimport org.apache.spark.sql.snappy._\ndataFrame.write.putInto(\ncol_table\n)\n\n\n\n\nFor Row Tables\n\n\nPUT INTO uses a syntax similar to the INSERT statement, but SnappyData does not check the existing primary key values before executing the PUT INTO command. If a row with the same primary key exists in the table, PUT INTO overwrites the older row value. If no rows with the same primary key exist, PUT INTO operates like a standard INSERT. This behavior ensures that only the last primary key value inserted or updated remains in the system, which preserves the primary key constraint. Removing the primary key check speeds execution when importing bulk data.\n\n\nThe PUT INTO statement is similar to the \"UPSERT\" command or capability provided by other RDBMS to relax primary key checks. By default, the PUT INTO statement ignores only primary key constraints. \n\n\nSyntax\n\n\nPUT INTO \nschema name\n.\ntable name\n VALUES (V1, V2,... ,Vn);\n\n\n\n\nExample\n\n\nPUT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData', 1);\n\n\n\n\nWhen specifying columns with table, columns should not have any \nCONSTRAINT\n, as explained in the following example:\n\n\nPUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID) VALUES (1, 'User 1' , 'SnappyData', 1), (2, 'User 2' , 'SnappyData', 1);\n\n\n\n\nPUT into another table using a select statement\n\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS;\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1'", 
            "title": "PUT INTO"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#put-into", 
            "text": "PUT INTO operates like a standard  INSERT  statement.   Note  Insert/PUT INTO with partial column specification is not supported in SnappyData.", 
            "title": "PUT INTO"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#for-column-tables", 
            "text": "If you need the  putInto()  functionality for column tables, you must specify key_columns while defining the column table.\nPUT INTO updates the row if present, else, it inserts the row.  \nAs column tables do not have the primary key functionality, you must specify key_columns when creating the table. \nThese columns are used to identify a row uniquely. PUT INTO is available by SQL as well APIs.", 
            "title": "For Column Tables"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#syntax", 
            "text": "PUT INTO  TABLENAME  SELECT V1, V2, V3,......,Vn;\n\nPUT INTO  schema name . table name2  SELECT * from  schema name . table name1 ;\n\nPUT INTO  schema name . table name2  SELECT * from  schema name . table name1  WHERE  column name =' Value '\n\nPUT INTO  schema name . table name2  SELECT from  schema name . table name1  WHERE  column name =' Value '", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#examples", 
            "text": "For SQL  // Insert into another table using a select statement for column tables with key columns\n\nPUT INTO TRADE.CUSTOMERS SELECT '1','2','hello';\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS;\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT * from CUSTOMERS WHERE C_NAME='User 1';\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT from CUSTOMERS WHERE C_NAME='User 1';   Warning  PUT INTO VALUES (V1, V2,...Vn)  syntax do not work for column tables. Instead use  PUT INTO SELECT V1, V2,...,Vn .   For API  putInto API  is available from the DataFrameWriter extension.  import org.apache.spark.sql.snappy._\ndataFrame.write.putInto( col_table )", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#for-row-tables", 
            "text": "PUT INTO uses a syntax similar to the INSERT statement, but SnappyData does not check the existing primary key values before executing the PUT INTO command. If a row with the same primary key exists in the table, PUT INTO overwrites the older row value. If no rows with the same primary key exist, PUT INTO operates like a standard INSERT. This behavior ensures that only the last primary key value inserted or updated remains in the system, which preserves the primary key constraint. Removing the primary key check speeds execution when importing bulk data.  The PUT INTO statement is similar to the \"UPSERT\" command or capability provided by other RDBMS to relax primary key checks. By default, the PUT INTO statement ignores only primary key constraints.", 
            "title": "For Row Tables"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#syntax_1", 
            "text": "PUT INTO  schema name . table name  VALUES (V1, V2,... ,Vn);", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/put-into/#example", 
            "text": "PUT INTO TRADE.CUSTOMERS VALUES (1, 'User 1', '2001-10-12', 'SnappyData', 1);  When specifying columns with table, columns should not have any  CONSTRAINT , as explained in the following example:  PUT INTO TRADE.CUSTOMERS (CID ,CUST_NAME , ADDR ,TID) VALUES (1, 'User 1' , 'SnappyData', 1), (2, 'User 2' , 'SnappyData', 1);  PUT into another table using a select statement  PUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS;\n\nPUT INTO TRADE.NEWCUSTOMERS SELECT * from TRADE.CUSTOMERS WHERE CUST_NAME='User 1'", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/revoke/", 
            "text": "REVOKE\n\n\nSyntax\n\n\nThe syntax used for the REVOKE statement differs depending on whether you revoke privileges for a table or for a routine.\n\n\nREVOKE privilege-type ON [ TABLE ] { table-name | view-name } FROM grantees\n\n\n\n\nIf you do not specify a column list, the statement revokes the privilege for all of the columns in the table.\n\n\nREVOKE EXECUTE ON { FUNCTION | PROCEDURE } routine-designator FROM grantees RESTRICT\n\n\n\n\nYou must use the RESTRICT clause on REVOKE statements for routines. The RESTRICT clause specifies that the EXECUTE privilege cannot be revoked if the specified routine is used in a view or constraint, and the privilege is being revoked from the owner of the view or constraint.\n\n\n\n\nDescription\n\n\nThe REVOKE statement removes permissions from a specific user or from all users to perform actions on database objects.\n\n\nThe following types of permissions can be revoked:\n\n\n\n\nDML operations permissions on a specific table.\n\n\nInsert/Delete data to/from a specific table.\n\n\nSelect/Update data permissions on a table or a subset of columns in a table.\n\n\nCreate a foreign key reference to the named table or to a subset of columns from a table.\n\n\nExecute permission on a specified routine (function or procedure).\n\n\n\n\nYou can revoke privileges from an object if you are the owner of the object or the distributed member owner.\n\n\n\n\nprivilege-type\n\n\nUse the ALL PRIVILEGES privilege type to revoke all of the permissions from the user for the specified table. You can also revoke one or more table privileges by specifying a privilege-list.\n\n\nUse the DELETE privilege type to revoke permission to delete rows from the specified table.\n\n\nUse the INSERT privilege type to revoke permission to insert rows into the specified table.\n\n\nUse the REFERENCES privilege type to revoke permission to create a foreign key reference to the specified table. If a column list is specified with the REFERENCES privilege, the permission is revoked on only the foreign key reference to the specified columns.\n\n\nUse the SELECT privilege type to revoke permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is revoked on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table.\n\n\nUse the UPDATE privilege type to revoke permission to use the UPDATE statement on the specified table. If a column list is specified, the permission is revoked only on the specified columns.\n\n\n\n\ngrantees\n\n\nYou can revoke the privileges from specific users or from all users. Use the keyword PUBLIC to specify all users.\n\n\nThe privileges revoked from PUBLIC and from individual users are independent privileges. For example, consider the case where the SELECT privilege on table \"t\" is granted to both PUBLIC and to the authorization ID \"harry.\" If the SELECT privilege is later revoked from the authorization ID \"harry,\" harry can still access table \"t\" using the PUBLIC privilege.\n\n\n\n\nNote\n\n\nThe privileges of the owner (distributed member) of an object cannot be revoked.\n\n\n\n\n\n\nroutine-designator\n\n\n{ qualified-name [ signature ] }\n\n\n\n\n\n\nCascading Object Dependencies\n\n\nFor views and constraints, if the privilege on which the object depends on is revoked, the object is automatically dropped. SnappyData does not try to determine if you have other privileges that can replace the privileges that are being revoked.\n\n\n\n\nTable-Level Privilege Limitations\n\n\nAll of the table-level privilege types for a specified grantee and table ID are stored in one row in the SYSTABLEPERMS system table. For example, when user2 is granted the SELECT and DELETE privileges on table user1.t1, a row is added to the SYSTABLEPERMS table. The GRANTEE field contains user2 and the TABLEID contains user1.t1. The SELECTPRIV and DELETEPRIV fields are set to Y. The remaining privilege type fields are set to N.\n\n\nWhen a grantee creates an object that relies on one of the privilege types, the engine tracks the dependency of the object on the specific row in the SYSTABLEPERMS table. For example, user2 creates the view v1 by using the statement SELECT * FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1). The dependency manager knows only that the view is dependent on a privilege type in that specific row, but does not track exactly which privilege type the view is dependent on.\n\n\nWhen a REVOKE statement for a table-level privilege is issued for a grantee and table ID, all of the objects that are dependent on the grantee and table ID are dropped. For example, if user1 revokes the DELETE privilege on table t1 from user2, the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the DELETE privilege for GRANTEE(user2), TABLEID(user1.t1).\n\n\n\n\nColumn-Level Privilege Limitations\n\n\nOnly one type of privilege for a specified grantee and table ID are stored in one row in the SYSCOLPERMS system table. For example, when user2 is granted the SELECT privilege on table user1.t1 for columns c12 and c13, a row is added to the SYSCOLPERMS. The GRANTEE field contains user2, the TABLEID contains user1.t1, the TYPE field contains S, and the COLUMNS field contains c12, c13.\n\n\nWhen a grantee creates an object that relies on the privilege type and the subset of columns in a table ID, the engine tracks the dependency of the object on the specific row in the SYSCOLPERMS table. For example, user2 creates the view v1 by using the statement SELECT c11 FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S). The dependency manager knows that the view is dependent on the SELECT privilege type, but does not track exactly which columns the view is dependent on.\n\n\nWhen a REVOKE statement for a column-level privilege is issued for a grantee, table ID, and type, all of the objects that are dependent on the grantee, table ID, and type are dropped. For example, if user1 revokes the SELECT privilege on column c12 on table user1.t1 from user2, the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the column c12 for GRANTEE(user2), TABLEID(user1.t1), TYPE(S).\n\n\nExamples\n\n\nTo revoke the SELECT privilege on table t from the authorization IDs maria and harry:\n\n\nREVOKE SELECT ON TABLE t FROM sam,bob;\n\n\n\n\nTo revoke the UPDATE privileges on table t from the authorization IDs john and smith:\n\n\nREVOKE UPDATE ON TABLE t FROM adam,richard;\n\n\n\n\nTo revoke the SELECT privilege on table s.v from all users:\n\n\nREVOKE SELECT ON TABLE test.sample FROM PUBLIC;\n\n\n\n\nTo revoke the UPDATE privilege on columns c1 and c2 of table s.v from all users:\n\n\nREVOKE UPDATE (c1,c2) ON TABLE test.sample FROM PUBLIC;\n\n\n\n\nTo revoke the EXECUTE privilege on procedure p from the authorization ID george:\n\n\nREVOKE EXECUTE ON PROCEDURE p FROM richard RESTRICT;", 
            "title": "REVOKE"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#revoke", 
            "text": "", 
            "title": "REVOKE"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#syntax", 
            "text": "The syntax used for the REVOKE statement differs depending on whether you revoke privileges for a table or for a routine.  REVOKE privilege-type ON [ TABLE ] { table-name | view-name } FROM grantees  If you do not specify a column list, the statement revokes the privilege for all of the columns in the table.  REVOKE EXECUTE ON { FUNCTION | PROCEDURE } routine-designator FROM grantees RESTRICT  You must use the RESTRICT clause on REVOKE statements for routines. The RESTRICT clause specifies that the EXECUTE privilege cannot be revoked if the specified routine is used in a view or constraint, and the privilege is being revoked from the owner of the view or constraint.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#description", 
            "text": "The REVOKE statement removes permissions from a specific user or from all users to perform actions on database objects.  The following types of permissions can be revoked:   DML operations permissions on a specific table.  Insert/Delete data to/from a specific table.  Select/Update data permissions on a table or a subset of columns in a table.  Create a foreign key reference to the named table or to a subset of columns from a table.  Execute permission on a specified routine (function or procedure).   You can revoke privileges from an object if you are the owner of the object or the distributed member owner.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#privilege-type", 
            "text": "Use the ALL PRIVILEGES privilege type to revoke all of the permissions from the user for the specified table. You can also revoke one or more table privileges by specifying a privilege-list.  Use the DELETE privilege type to revoke permission to delete rows from the specified table.  Use the INSERT privilege type to revoke permission to insert rows into the specified table.  Use the REFERENCES privilege type to revoke permission to create a foreign key reference to the specified table. If a column list is specified with the REFERENCES privilege, the permission is revoked on only the foreign key reference to the specified columns.  Use the SELECT privilege type to revoke permission to perform SELECT statements on a table or view. If a column list is specified with the SELECT privilege, the permission is revoked on only those columns. If no column list is specified, then the privilege is valid on all of the columns in the table.  Use the UPDATE privilege type to revoke permission to use the UPDATE statement on the specified table. If a column list is specified, the permission is revoked only on the specified columns.", 
            "title": "privilege-type"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#grantees", 
            "text": "You can revoke the privileges from specific users or from all users. Use the keyword PUBLIC to specify all users.  The privileges revoked from PUBLIC and from individual users are independent privileges. For example, consider the case where the SELECT privilege on table \"t\" is granted to both PUBLIC and to the authorization ID \"harry.\" If the SELECT privilege is later revoked from the authorization ID \"harry,\" harry can still access table \"t\" using the PUBLIC privilege.   Note  The privileges of the owner (distributed member) of an object cannot be revoked.", 
            "title": "grantees"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#routine-designator", 
            "text": "{ qualified-name [ signature ] }", 
            "title": "routine-designator"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#cascading-object-dependencies", 
            "text": "For views and constraints, if the privilege on which the object depends on is revoked, the object is automatically dropped. SnappyData does not try to determine if you have other privileges that can replace the privileges that are being revoked.", 
            "title": "Cascading Object Dependencies"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#table-level-privilege-limitations", 
            "text": "All of the table-level privilege types for a specified grantee and table ID are stored in one row in the SYSTABLEPERMS system table. For example, when user2 is granted the SELECT and DELETE privileges on table user1.t1, a row is added to the SYSTABLEPERMS table. The GRANTEE field contains user2 and the TABLEID contains user1.t1. The SELECTPRIV and DELETEPRIV fields are set to Y. The remaining privilege type fields are set to N.  When a grantee creates an object that relies on one of the privilege types, the engine tracks the dependency of the object on the specific row in the SYSTABLEPERMS table. For example, user2 creates the view v1 by using the statement SELECT * FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1). The dependency manager knows only that the view is dependent on a privilege type in that specific row, but does not track exactly which privilege type the view is dependent on.  When a REVOKE statement for a table-level privilege is issued for a grantee and table ID, all of the objects that are dependent on the grantee and table ID are dropped. For example, if user1 revokes the DELETE privilege on table t1 from user2, the row in SYSTABLEPERMS for GRANTEE(user2), TABLEID(user1.t1) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the DELETE privilege for GRANTEE(user2), TABLEID(user1.t1).", 
            "title": "Table-Level Privilege Limitations"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#column-level-privilege-limitations", 
            "text": "Only one type of privilege for a specified grantee and table ID are stored in one row in the SYSCOLPERMS system table. For example, when user2 is granted the SELECT privilege on table user1.t1 for columns c12 and c13, a row is added to the SYSCOLPERMS. The GRANTEE field contains user2, the TABLEID contains user1.t1, the TYPE field contains S, and the COLUMNS field contains c12, c13.  When a grantee creates an object that relies on the privilege type and the subset of columns in a table ID, the engine tracks the dependency of the object on the specific row in the SYSCOLPERMS table. For example, user2 creates the view v1 by using the statement SELECT c11 FROM user1.t1, the dependency manager tracks the dependency of view v1 on the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S). The dependency manager knows that the view is dependent on the SELECT privilege type, but does not track exactly which columns the view is dependent on.  When a REVOKE statement for a column-level privilege is issued for a grantee, table ID, and type, all of the objects that are dependent on the grantee, table ID, and type are dropped. For example, if user1 revokes the SELECT privilege on column c12 on table user1.t1 from user2, the row in SYSCOLPERMS for GRANTEE(user2), TABLEID(user1.t1), TYPE(S) is modified by the REVOKE statement. The dependency manager sends a revoke invalidation message to the view user2.v1 and the view is dropped even though the view is not dependent on the column c12 for GRANTEE(user2), TABLEID(user1.t1), TYPE(S).", 
            "title": "Column-Level Privilege Limitations"
        }, 
        {
            "location": "/reference/sql_reference/revoke/#examples", 
            "text": "To revoke the SELECT privilege on table t from the authorization IDs maria and harry:  REVOKE SELECT ON TABLE t FROM sam,bob;  To revoke the UPDATE privileges on table t from the authorization IDs john and smith:  REVOKE UPDATE ON TABLE t FROM adam,richard;  To revoke the SELECT privilege on table s.v from all users:  REVOKE SELECT ON TABLE test.sample FROM PUBLIC;  To revoke the UPDATE privilege on columns c1 and c2 of table s.v from all users:  REVOKE UPDATE (c1,c2) ON TABLE test.sample FROM PUBLIC;  To revoke the EXECUTE privilege on procedure p from the authorization ID george:  REVOKE EXECUTE ON PROCEDURE p FROM richard RESTRICT;", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/sql_reference/select/", 
            "text": "SELECT\n\n\nSELECT [DISTINCT] named_expression[, named_expression, ...]\n    FROM relation[, relation, ...]\n    [WHERE boolean_expression]\n    [aggregation [HAVING boolean_expression]]\n    [ORDER BY sort_expressions]\n    [CLUSTER BY expressions]\n    [DISTRIBUTE BY expressions]\n    [SORT BY sort_expressions]\n    [WINDOW named_window[, WINDOW named_window, ...]]\n    [LIMIT num_rows]\n\nnamed_expression:\n    : expression [AS alias]\n\nrelation:\n    | join_relation\n    | (table_name|query) [sample] [AS alias]\n\nexpressions:\n    : expression[, expression, ...]\n\nsort_expressions:\n    : expression [ASC|DESC][, expression [ASC|DESC], ...]\n\n\n\n\nFor information on executing queries on Synopsis Data Engine, refer to \nSDE\n.\n\n\nDescription\n\n\nOutput data from one or more relations.\n\n\nA relation here refers to any source of input data. It could be the contents of an existing table (or view), the joined result of two existing tables, or a subquery (the result of another select statement).\n\n\nDISTINCT\n\nSelect all matching rows from the relation then remove duplicate results.\n\n\nWHERE\n\nFilter rows by a predicate.\n\n\nHAVING\n\nFilter grouped result by a predicate.\n\n\nORDER BY\n\nImpose total ordering on a set of expressions. Default sort direction is ascending. This may not be used with SORT BY, CLUSTER BY, or DISTRIBUTE BY.\n\n\nDISTRIBUTE BY\n\nRepartition rows in the relation based on a set of expressions. Rows with the same expression values will be hashed to the same worker. This may not be used with ORDER BY or CLUSTER BY.\n\n\nSORT BY\n\nImpose ordering on a set of expressions within each partition. Default sort direction is ascending. This may not be used with ORDER BY or CLUSTER BY.\n\n\nCLUSTER BY\n\nRepartition rows in the relation based on a set of expressions and sort the rows in ascending order based on the expressions. In other words, this is a shorthand for DISTRIBUTE BY and SORT BY where all expressions are sorted in ascending order. This may not be used with ORDER BY, DISTRIBUTE BY, or SORT BY.\n\n\nWINDOW\n\nAssign an identifier to a window specification.\n\n\nLIMIT\n\nLimit the number of rows returned.\n\n\nExample\n\n\n    SELECT * FROM boxes\n    SELECT width, length FROM boxes WHERE height=3\n    SELECT DISTINCT width, length FROM boxes WHERE height=3 LIMIT 2\n    SELECT * FROM VALUES (1, 2, 3) AS (width, length, height)\n    SELECT * FROM VALUES (1, 2, 3), (2, 3, 4) AS (width, length, height)\n    SELECT * FROM boxes ORDER BY width\n    SELECT * FROM boxes DISTRIBUTE BY width SORT BY width\n    SELECT * FROM boxes CLUSTER BY length\n\n\n\n\nJOINS\n\n\n    join_relation:\n        | relation join_type JOIN relation (ON boolean_expression | USING (column_name[, column_name, ...]))\n        : relation NATURAL join_type JOIN relation\n    join_type:\n        | INNER\n        | (LEFT|RIGHT) SEMI\n        | (LEFT|RIGHT|FULL) [OUTER]\n        : [LEFT] ANTI\n\n\n\n\nINNER JOIN\n\nSelect all rows from both relations where there is match.\n\n\nOUTER JOIN\n\nSelect all rows from both relations, filling with null values on the side that does not have a match.\n\n\nSEMI JOIN\n\nSelect only rows from the side of the SEMI JOIN where there is a match. If one row matches multiple rows, only the first match is returned.\n\n\nLEFT ANTI JOIN\n\nSelect only rows from the left side that match no rows on the right side.\n\n\nExample\n:\n\n\n    SELECT * FROM boxes INNER JOIN rectangles ON boxes.width = rectangles.width\n    SELECT * FROM boxes FULL OUTER JOIN rectangles USING (width, length)\n    SELECT * FROM boxes NATURAL JOIN rectangles\n\n\n\n\nAGGREGATION\n\n\n    aggregation:\n        : GROUP BY expressions [(WITH ROLLUP | WITH CUBE | GROUPING SETS (expressions))]\n\n\n\n\nGroup by a set of expressions using one or more aggregate functions. Common built-in aggregate functions include count, avg, min, max, and sum.\n\n\nROLLUP\n\nCreate a grouping set at each hierarchical level of the specified expressions. For instance, GROUP BY a, b, c WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (a), ()). The total number of grouping sets will be N + 1, where N is the number of group expressions.\n\n\nCUBE\n\nCreate a grouping set for each possible combination of a set of the specified expressions. For instance, GROUP BY a, b, c WITH CUBE is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ()). The total number of grouping sets will be 2^N, where N is the number of group expressions.\n\n\nGROUPING SETS\n\nPerform a group by for each subset of the group expressions specified in the grouping sets. For instance, GROUP BY x, y GROUPING SETS (x, y) is equivalent to the result of GROUP BY x unioned with that of GROUP BY y.\n\n\nExample\n:\n\n\n    SELECT height, COUNT(*) AS num_rows FROM boxes GROUP BY height\n    SELECT width, AVG(length) AS average_length FROM boxes GROUP BY width\n    SELECT width, length, height FROM boxes GROUP BY width, length, height WITH ROLLUP\n    SELECT width, length, avg(height) FROM boxes GROUP BY width, length GROUPING SETS (width, length)\n\n\n\n\nWindow Functions\n\n\nwindow_expression:\n    : expression OVER window_spec\n\nnamed_window:\n    : window_identifier AS window_spec\n\nwindow_spec:\n    | window_identifier\n    : ((PARTITION|DISTRIBUTE) BY expressions\n          [(ORDER|SORT) BY sort_expressions] [window_frame])\n\nwindow_frame:\n    | (RANGE|ROWS) frame_bound\n    : (RANGE|ROWS) BETWEEN frame_bound AND frame_bound\n\nframe_bound:\n    | CURRENT ROW\n    | UNBOUNDED (PRECEDING|FOLLOWING)\n    : expression (PRECEDING|FOLLOWING)\n\n\n\n\nCompute a result over a range of input rows. A windowed expression is specified using the OVER keyword, which is followed by either an identifier to the window (defined using the WINDOW keyword) or the specification of a window.\n\n\nPARTITION BY\n\nSpecify which rows will be in the same partition, aliased by DISTRIBUTE BY.\n\n\nORDER BY\n\nSpecify how rows within a window partition are ordered, aliased by SORT BY.\n\n\nRANGE bound\n\nExpress the size of the window in terms of a value range for the expression.\n\n\nROWS bound\n\nExpress the size of the window in terms of the number of rows before and/or after the current row.\n\n\nCURRENT ROW\n\nUse the current row as a bound.\n\n\nUNBOUNDED\n\nUse negative infinity as the lower bound or infinity as the upper bound.\n\n\nPRECEDING\n\nIf used with a RANGE bound, this defines the lower bound of the value range. If used with a ROWS bound, this determines the number of rows before the current row to keep in the window.\n\n\nFOLLOWING\n\nIf used with a RANGE bound, this defines the upper bound of the value range. If used with a ROWS bound, this determines the number of rows after the current row to keep in the window.", 
            "title": "SELECT"
        }, 
        {
            "location": "/reference/sql_reference/select/#select", 
            "text": "SELECT [DISTINCT] named_expression[, named_expression, ...]\n    FROM relation[, relation, ...]\n    [WHERE boolean_expression]\n    [aggregation [HAVING boolean_expression]]\n    [ORDER BY sort_expressions]\n    [CLUSTER BY expressions]\n    [DISTRIBUTE BY expressions]\n    [SORT BY sort_expressions]\n    [WINDOW named_window[, WINDOW named_window, ...]]\n    [LIMIT num_rows]\n\nnamed_expression:\n    : expression [AS alias]\n\nrelation:\n    | join_relation\n    | (table_name|query) [sample] [AS alias]\n\nexpressions:\n    : expression[, expression, ...]\n\nsort_expressions:\n    : expression [ASC|DESC][, expression [ASC|DESC], ...]  For information on executing queries on Synopsis Data Engine, refer to  SDE .", 
            "title": "SELECT"
        }, 
        {
            "location": "/reference/sql_reference/select/#description", 
            "text": "Output data from one or more relations.  A relation here refers to any source of input data. It could be the contents of an existing table (or view), the joined result of two existing tables, or a subquery (the result of another select statement).  DISTINCT \nSelect all matching rows from the relation then remove duplicate results.  WHERE \nFilter rows by a predicate.  HAVING \nFilter grouped result by a predicate.  ORDER BY \nImpose total ordering on a set of expressions. Default sort direction is ascending. This may not be used with SORT BY, CLUSTER BY, or DISTRIBUTE BY.  DISTRIBUTE BY \nRepartition rows in the relation based on a set of expressions. Rows with the same expression values will be hashed to the same worker. This may not be used with ORDER BY or CLUSTER BY.  SORT BY \nImpose ordering on a set of expressions within each partition. Default sort direction is ascending. This may not be used with ORDER BY or CLUSTER BY.  CLUSTER BY \nRepartition rows in the relation based on a set of expressions and sort the rows in ascending order based on the expressions. In other words, this is a shorthand for DISTRIBUTE BY and SORT BY where all expressions are sorted in ascending order. This may not be used with ORDER BY, DISTRIBUTE BY, or SORT BY.  WINDOW \nAssign an identifier to a window specification.  LIMIT \nLimit the number of rows returned.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/select/#example", 
            "text": "SELECT * FROM boxes\n    SELECT width, length FROM boxes WHERE height=3\n    SELECT DISTINCT width, length FROM boxes WHERE height=3 LIMIT 2\n    SELECT * FROM VALUES (1, 2, 3) AS (width, length, height)\n    SELECT * FROM VALUES (1, 2, 3), (2, 3, 4) AS (width, length, height)\n    SELECT * FROM boxes ORDER BY width\n    SELECT * FROM boxes DISTRIBUTE BY width SORT BY width\n    SELECT * FROM boxes CLUSTER BY length", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/select/#joins", 
            "text": "join_relation:\n        | relation join_type JOIN relation (ON boolean_expression | USING (column_name[, column_name, ...]))\n        : relation NATURAL join_type JOIN relation\n    join_type:\n        | INNER\n        | (LEFT|RIGHT) SEMI\n        | (LEFT|RIGHT|FULL) [OUTER]\n        : [LEFT] ANTI  INNER JOIN \nSelect all rows from both relations where there is match.  OUTER JOIN \nSelect all rows from both relations, filling with null values on the side that does not have a match.  SEMI JOIN \nSelect only rows from the side of the SEMI JOIN where there is a match. If one row matches multiple rows, only the first match is returned.  LEFT ANTI JOIN \nSelect only rows from the left side that match no rows on the right side.  Example :      SELECT * FROM boxes INNER JOIN rectangles ON boxes.width = rectangles.width\n    SELECT * FROM boxes FULL OUTER JOIN rectangles USING (width, length)\n    SELECT * FROM boxes NATURAL JOIN rectangles", 
            "title": "JOINS"
        }, 
        {
            "location": "/reference/sql_reference/select/#aggregation", 
            "text": "aggregation:\n        : GROUP BY expressions [(WITH ROLLUP | WITH CUBE | GROUPING SETS (expressions))]  Group by a set of expressions using one or more aggregate functions. Common built-in aggregate functions include count, avg, min, max, and sum.  ROLLUP \nCreate a grouping set at each hierarchical level of the specified expressions. For instance, GROUP BY a, b, c WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (a), ()). The total number of grouping sets will be N + 1, where N is the number of group expressions.  CUBE \nCreate a grouping set for each possible combination of a set of the specified expressions. For instance, GROUP BY a, b, c WITH CUBE is equivalent to GROUP BY a, b, c GROUPING SETS ((a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ()). The total number of grouping sets will be 2^N, where N is the number of group expressions.  GROUPING SETS \nPerform a group by for each subset of the group expressions specified in the grouping sets. For instance, GROUP BY x, y GROUPING SETS (x, y) is equivalent to the result of GROUP BY x unioned with that of GROUP BY y.  Example :      SELECT height, COUNT(*) AS num_rows FROM boxes GROUP BY height\n    SELECT width, AVG(length) AS average_length FROM boxes GROUP BY width\n    SELECT width, length, height FROM boxes GROUP BY width, length, height WITH ROLLUP\n    SELECT width, length, avg(height) FROM boxes GROUP BY width, length GROUPING SETS (width, length)", 
            "title": "AGGREGATION"
        }, 
        {
            "location": "/reference/sql_reference/select/#window-functions", 
            "text": "window_expression:\n    : expression OVER window_spec\n\nnamed_window:\n    : window_identifier AS window_spec\n\nwindow_spec:\n    | window_identifier\n    : ((PARTITION|DISTRIBUTE) BY expressions\n          [(ORDER|SORT) BY sort_expressions] [window_frame])\n\nwindow_frame:\n    | (RANGE|ROWS) frame_bound\n    : (RANGE|ROWS) BETWEEN frame_bound AND frame_bound\n\nframe_bound:\n    | CURRENT ROW\n    | UNBOUNDED (PRECEDING|FOLLOWING)\n    : expression (PRECEDING|FOLLOWING)  Compute a result over a range of input rows. A windowed expression is specified using the OVER keyword, which is followed by either an identifier to the window (defined using the WINDOW keyword) or the specification of a window.  PARTITION BY \nSpecify which rows will be in the same partition, aliased by DISTRIBUTE BY.  ORDER BY \nSpecify how rows within a window partition are ordered, aliased by SORT BY.  RANGE bound \nExpress the size of the window in terms of a value range for the expression.  ROWS bound \nExpress the size of the window in terms of the number of rows before and/or after the current row.  CURRENT ROW \nUse the current row as a bound.  UNBOUNDED \nUse negative infinity as the lower bound or infinity as the upper bound.  PRECEDING \nIf used with a RANGE bound, this defines the lower bound of the value range. If used with a ROWS bound, this determines the number of rows before the current row to keep in the window.  FOLLOWING \nIf used with a RANGE bound, this defines the upper bound of the value range. If used with a ROWS bound, this determines the number of rows after the current row to keep in the window.", 
            "title": "Window Functions"
        }, 
        {
            "location": "/reference/sql_reference/set-isolation/", 
            "text": "SET ISOLATION\n\n\nChange the transaction isolation level for the connection.\n\n\nSyntax\n\n\nSET [ CURRENT ] ISOLATION [ = ]\n{ \nCS | READ COMMITTED\nRS | REPEATABLE READ\nRESET\n}\n\n\n\n\n\n\nDescription\n\n\nThe supported isolation levels in SnappyData are NONE, READ COMMITTED and REPEATABLE READ.\n\n\nIsolation level NONE indicates no transactional behavior. This is a special constant that indicates that transactions are not supported. \n\nThe RESET clause corresponds to the NONE isolation level. For more information, see \nOverview of SnappyData Distributed Transactions\n.\n\n\nThis statement behaves identically to the JDBC \njava.sql.Connection.setTransactionIsolation\n method and commits the current transaction if isolation level has changed.\n\n\nExample\n\n\nsnappy\n set ISOLATION READ COMMITTED;\n\nsnappy\n VALUES CURRENT ISOLATION;\n1\n----\nCS\n\n1 row selected", 
            "title": "SET ISOLATION"
        }, 
        {
            "location": "/reference/sql_reference/set-isolation/#set-isolation", 
            "text": "Change the transaction isolation level for the connection.", 
            "title": "SET ISOLATION"
        }, 
        {
            "location": "/reference/sql_reference/set-isolation/#syntax", 
            "text": "SET [ CURRENT ] ISOLATION [ = ]\n{ \nCS | READ COMMITTED\nRS | REPEATABLE READ\nRESET\n}", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/sql_reference/set-isolation/#description", 
            "text": "The supported isolation levels in SnappyData are NONE, READ COMMITTED and REPEATABLE READ.  Isolation level NONE indicates no transactional behavior. This is a special constant that indicates that transactions are not supported.  \nThe RESET clause corresponds to the NONE isolation level. For more information, see  Overview of SnappyData Distributed Transactions .  This statement behaves identically to the JDBC  java.sql.Connection.setTransactionIsolation  method and commits the current transaction if isolation level has changed.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/set-isolation/#example", 
            "text": "snappy  set ISOLATION READ COMMITTED;\n\nsnappy  VALUES CURRENT ISOLATION;\n1\n----\nCS\n\n1 row selected", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/", 
            "text": "SET SCHEMA\n\n\nSet or change the default schema for a connection's session.\n\n\nSET SCHEMA schema-name\n\n\n\n\nDescription\n\n\nThe SET SCHEMA statement sets or changes the default schema for a connection's session to the provided schema. This is then used as the schema for all statements issued from the connection that does not explicitly specify a schema name. \n\nThe default schema is APP.\n\n\nExample\n\n\n-- below are equivalent assuming a TRADE schema\nSET SCHEMA TRADE;\nSET SCHEMA trade;", 
            "title": "SET SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/#set-schema", 
            "text": "Set or change the default schema for a connection's session.  SET SCHEMA schema-name", 
            "title": "SET SCHEMA"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/#description", 
            "text": "The SET SCHEMA statement sets or changes the default schema for a connection's session to the provided schema. This is then used as the schema for all statements issued from the connection that does not explicitly specify a schema name.  \nThe default schema is APP.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/set-schema/#example", 
            "text": "-- below are equivalent assuming a TRADE schema\nSET SCHEMA TRADE;\nSET SCHEMA trade;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/", 
            "text": "TRUNCATE TABLE\n\n\nRemove all content from a table and return it to its initial, empty state. TRUNCATE TABLE clears all in-memory data for the specified table as well as any data that was persisted to SnappyData disk stores. \n\n\nTRUNCATE TABLE table-name\n\n\n\n\nDescription\n\n\nTo truncate a table, you must be the table's owner. You cannot use this command to truncate system tables.\n\n\nExample\n\n\nTo truncate the \"flights\" table in the current schema:\n\n\nTRUNCATE TABLE flights;\n\n\n\n\nRelated Topics\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nDROP TABLE\n\n\n\n\n\n\nDELETE TABLE\n\n\n\n\n\n\nSHOW TABLES", 
            "title": "TRUNCATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/#truncate-table", 
            "text": "Remove all content from a table and return it to its initial, empty state. TRUNCATE TABLE clears all in-memory data for the specified table as well as any data that was persisted to SnappyData disk stores.   TRUNCATE TABLE table-name", 
            "title": "TRUNCATE TABLE"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/#description", 
            "text": "To truncate a table, you must be the table's owner. You cannot use this command to truncate system tables.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/truncate-table/#example", 
            "text": "To truncate the \"flights\" table in the current schema:  TRUNCATE TABLE flights;  Related Topics    CREATE TABLE    DROP TABLE    DELETE TABLE    SHOW TABLES", 
            "title": "Example"
        }, 
        {
            "location": "/reference/sql_reference/update/", 
            "text": "UPDATE\n\n\nUpdate the value of one or more columns.\n\n\n{ UPDATE table-name [ [ AS ] correlation-name]\n        SET column-name = value\n        [, column-name = value} ]*\n        [ WHERE ]    \n}\n\n\n\n\nDescription\n\n\nThis form of the UPDATE statement is called a searched update. It updates the value of one or more columns for all rows of the table for which the WHERE clause evaluates to TRUE. Specifying DEFAULT for the update value sets the value of the column to the default defined for that table.\n\n\nThe UPDATE statement returns the number of rows that were updated.\n\n\n\n\nNote\n\n\n\n\n\n\nUpdates on partitioning column and primary key column is not supported.\n\n\n\n\n\n\nDelete/Update with a subquery does not work with row tables, if the row table does not have a primary key. An exception to this rule is, if the subquery contains another row table and a simple where clause.\n\n\n\n\n\n\n\n\nvalue\n\n\nexpression | DEFAULT\n\n\n\n\nExample\n\n\n// Change the ADDRESS and SINCE  fields to null for all customers with ID greater than 10.\nUPDATE TRADE.CUSTOMERS SET ADDR=NULL, SINCE=NULL  WHERE CID \n 10;\n\n// Set the ADDR of all customers to 'SnappyData' where the current address is NULL.\nUPDATE TRADE.CUSTOMERS SET ADDR = 'Snappydata' WHERE ADDR IS NULL;\n\n// Increase the  QTY field by 10 for all rows of SELLORDERS table.\nUPDATE TRADE.SELLORDERS SET QTY = QTY+10;\n\n// Change the  STATUS  field of  all orders of SELLORDERS table to DEFAULT  ( 'open') , for customer ID = 10\nUPDATE TRADE.SELLORDERS SET STATUS = DEFAULT  WHERE CID = 10;\n\n// Use multiple tables in UPDATE statement with JOIN\nUPDATE TRADE.SELLORDERS SET A.TID = B.TID FROM TRADE.SELLORDERS A JOIN TRADE.CUSTOMERS B ON A.CID = B.CID;", 
            "title": "UPDATE"
        }, 
        {
            "location": "/reference/sql_reference/update/#update", 
            "text": "Update the value of one or more columns.  { UPDATE table-name [ [ AS ] correlation-name]\n        SET column-name = value\n        [, column-name = value} ]*\n        [ WHERE ]    \n}", 
            "title": "UPDATE"
        }, 
        {
            "location": "/reference/sql_reference/update/#description", 
            "text": "This form of the UPDATE statement is called a searched update. It updates the value of one or more columns for all rows of the table for which the WHERE clause evaluates to TRUE. Specifying DEFAULT for the update value sets the value of the column to the default defined for that table.  The UPDATE statement returns the number of rows that were updated.   Note    Updates on partitioning column and primary key column is not supported.    Delete/Update with a subquery does not work with row tables, if the row table does not have a primary key. An exception to this rule is, if the subquery contains another row table and a simple where clause.     value  expression | DEFAULT", 
            "title": "Description"
        }, 
        {
            "location": "/reference/sql_reference/update/#example", 
            "text": "// Change the ADDRESS and SINCE  fields to null for all customers with ID greater than 10.\nUPDATE TRADE.CUSTOMERS SET ADDR=NULL, SINCE=NULL  WHERE CID   10;\n\n// Set the ADDR of all customers to 'SnappyData' where the current address is NULL.\nUPDATE TRADE.CUSTOMERS SET ADDR = 'Snappydata' WHERE ADDR IS NULL;\n\n// Increase the  QTY field by 10 for all rows of SELLORDERS table.\nUPDATE TRADE.SELLORDERS SET QTY = QTY+10;\n\n// Change the  STATUS  field of  all orders of SELLORDERS table to DEFAULT  ( 'open') , for customer ID = 10\nUPDATE TRADE.SELLORDERS SET STATUS = DEFAULT  WHERE CID = 10;\n\n// Use multiple tables in UPDATE statement with JOIN\nUPDATE TRADE.SELLORDERS SET A.TID = B.TID FROM TRADE.SELLORDERS A JOIN TRADE.CUSTOMERS B ON A.CID = B.CID;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/system-procedures/", 
            "text": "Built-in System Procedures and Built-in Functions\n\n\nSnappyData provides built-in system procedures to help you manage the distributed system. For example, you can use system procedures to install required JAR files.\n\n\nSnappyData also supports built-in function, which lets you perform various kinds of data transformations directly in SELECT statements.\n\n\n\n\nNote\n\n\nIf you enable SQL authorization, you must use the \nGRANT\n command to grant normal users permission to use these procedures. \n\n\n\n\nThe following built-in procedures are available:\n\n\n\n\n\n\nSYS.DUMP_STACKS\n\n\n\n\n\n\nSYS.REBALANCE_ALL_BUCKETS\n\n\n\n\n\n\nSYS.SET_CRITICAL_HEAP_PERCENTAGE\n\n\n\n\n\n\nSYS.SET_TRACE_FLAG\n\n\n\n\n\n\nThe following built-in function is available:\n\n\n\n\nDSID", 
            "title": "Built-in System Procedures and Built-in Functions"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/system-procedures/#built-in-system-procedures-and-built-in-functions", 
            "text": "SnappyData provides built-in system procedures to help you manage the distributed system. For example, you can use system procedures to install required JAR files.  SnappyData also supports built-in function, which lets you perform various kinds of data transformations directly in SELECT statements.   Note  If you enable SQL authorization, you must use the  GRANT  command to grant normal users permission to use these procedures.    The following built-in procedures are available:    SYS.DUMP_STACKS    SYS.REBALANCE_ALL_BUCKETS    SYS.SET_CRITICAL_HEAP_PERCENTAGE    SYS.SET_TRACE_FLAG    The following built-in function is available:   DSID", 
            "title": "Built-in System Procedures and Built-in Functions"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/", 
            "text": "SYS.DUMP_STACKS\n\n\nWrites thread stacks, locks, and transaction states to the SnappyData log file. You can write stack information either for the current SnappyData member or for all SnappyData members in the distributed system.\n\n\n\n\n\nSyntax\n\n\nSYS.DUMP_STACKS (\nIN ALL BOOLEAN\n)\n\n\n\n\nALL\n \n\nSpecifies boolean value: \ntrue\n to log stack trace information for all SnappyData members, or \nfalse\n to log information only for the local SnappyData member.\n\n\nExample\n\n\nThis command writes thread stack information only for the local SnappyData member. The stack information is written to the SnappyData log file (by default snappyserver.log in the member startup directory):\n\n\nsnappy\n call sys.dump_stacks('false');", 
            "title": "DUMP_STACKS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/#sysdump_stacks", 
            "text": "Writes thread stacks, locks, and transaction states to the SnappyData log file. You can write stack information either for the current SnappyData member or for all SnappyData members in the distributed system.", 
            "title": "SYS.DUMP_STACKS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/#syntax", 
            "text": "SYS.DUMP_STACKS (\nIN ALL BOOLEAN\n)  ALL   \nSpecifies boolean value:  true  to log stack trace information for all SnappyData members, or  false  to log information only for the local SnappyData member.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dump-stacks/#example", 
            "text": "This command writes thread stack information only for the local SnappyData member. The stack information is written to the SnappyData log file (by default snappyserver.log in the member startup directory):  snappy  call sys.dump_stacks('false');", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/", 
            "text": "SYS.REBALANCE_ALL_BUCKETS\n\n\nRebalance partitioned table data on available SnappyData members.\n\n\nSyntax\n\n\nSYS.REBALANCE_ALL_BUCKETS()\n\n\n\n\nRebalancing is a SnappyData member operation that affects partitioned tables created in the cluster. Rebalancing performs two tasks:\n\n\n\n\n\n\nIf the partitioned table's redundancy setting is not satisfied, rebalancing does what it can to recover redundancy. \n\n\n\n\n\n\nRebalancing moves the partitioned table's data buckets between host members as needed to establish the best balance of data across the distributed system.\n\n\n\n\n\n\nFor efficiency, when starting multiple members, trigger the rebalance a single time, after you have added all members.\n\n\n\nExample\n\n\nsnappy\n call sys.rebalance_all_buckets();", 
            "title": "REBALANCE_ALL_BUCKETS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/#sysrebalance_all_buckets", 
            "text": "Rebalance partitioned table data on available SnappyData members.", 
            "title": "SYS.REBALANCE_ALL_BUCKETS"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/#syntax", 
            "text": "SYS.REBALANCE_ALL_BUCKETS()  Rebalancing is a SnappyData member operation that affects partitioned tables created in the cluster. Rebalancing performs two tasks:    If the partitioned table's redundancy setting is not satisfied, rebalancing does what it can to recover redundancy.     Rebalancing moves the partitioned table's data buckets between host members as needed to establish the best balance of data across the distributed system.    For efficiency, when starting multiple members, trigger the rebalance a single time, after you have added all members.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/rebalance-all-buckets/#example", 
            "text": "snappy  call sys.rebalance_all_buckets();", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/", 
            "text": "SYS.SET_CRITICAL_HEAP_PERCENTAGE\n\n\nSets the percentage threshold of Java heap memory usage that triggers \nLowMemoryException\ns on a SnappyData data store. This procedure executes only on the local SnappyData data store member.\n\n\nThis procedure sets the percentage threshold of critical Java heap memory usage on the local SnappyData data store. If the amount of heap memory being used exceeds the percentage, the member will report LowMemoryExceptions during local or client put operations into heap tables. The member will also inform other members in the distributed system that it has reached the critical threshold. When a data store is started with the \n-heap-size\n option, the default critical threshold is 90%.\n\n\nSyntax\n\n\nSYS.SET_CRITICAL_HEAP_PERCENTAGE (\nIN PERCENTAGE REAL NOT NULL\n)\n\n\n\n\nPERCENTAGE\n \n\nThe percentage of used heap space that triggers \nLowMemoryException\ns on the local SnappyData data store.\n\n\nExample\n\n\nThis command sets the critical threshold for heap memory usage on the local SnappyData member to 99.9%:\n\n\nsnappy\ncall sys.set_critical_heap_percentage (99.9);", 
            "title": "SET_CRITICAL_HEAP_PERCENTAGE"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/#sysset_critical_heap_percentage", 
            "text": "Sets the percentage threshold of Java heap memory usage that triggers  LowMemoryException s on a SnappyData data store. This procedure executes only on the local SnappyData data store member.  This procedure sets the percentage threshold of critical Java heap memory usage on the local SnappyData data store. If the amount of heap memory being used exceeds the percentage, the member will report LowMemoryExceptions during local or client put operations into heap tables. The member will also inform other members in the distributed system that it has reached the critical threshold. When a data store is started with the  -heap-size  option, the default critical threshold is 90%.", 
            "title": "SYS.SET_CRITICAL_HEAP_PERCENTAGE"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/#syntax", 
            "text": "SYS.SET_CRITICAL_HEAP_PERCENTAGE (\nIN PERCENTAGE REAL NOT NULL\n)  PERCENTAGE   \nThe percentage of used heap space that triggers  LowMemoryException s on the local SnappyData data store.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set_critical_heap_percentage/#example", 
            "text": "This command sets the critical threshold for heap memory usage on the local SnappyData member to 99.9%:  snappy call sys.set_critical_heap_percentage (99.9);", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/", 
            "text": "SYS.SET_TRACE_FLAG\n\n\nThis procedure enables or disables a specific trace flag for the distributed system as a whole. You must be a system user to execute this procedure.\n\n\nSyntax\n\n\nCALL SYS.SET_TRACE_FLAG (\nIN TRACE_FLAG VARCHAR(256),\nIN ON BOOLEAN\n)\n\n\n\n\nTRACE_FLAG \n\nSpecifies name of the trace flag to enable or disable.\n\n\nON \n\nSpecifies boolean value: \ntrue\n or \n1\n to enable the trace flag, or \nfalse\n or \n0\n to disable it.\n\n\nExample\n\n\nThis command traces all JAR installation, update, and removal operations in the SnappyData distributed system:\n\n\nsnappy\n call sys.set_trace_flag ('TraceJars', 'true');", 
            "title": "SET_TRACE_FLAG"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/#sysset_trace_flag", 
            "text": "This procedure enables or disables a specific trace flag for the distributed system as a whole. You must be a system user to execute this procedure.", 
            "title": "SYS.SET_TRACE_FLAG"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/#syntax", 
            "text": "CALL SYS.SET_TRACE_FLAG (\nIN TRACE_FLAG VARCHAR(256),\nIN ON BOOLEAN\n)  TRACE_FLAG  \nSpecifies name of the trace flag to enable or disable.  ON  \nSpecifies boolean value:  true  or  1  to enable the trace flag, or  false  or  0  to disable it.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/set-trace-flag/#example", 
            "text": "This command traces all JAR installation, update, and removal operations in the SnappyData distributed system:  snappy  call sys.set_trace_flag ('TraceJars', 'true');", 
            "title": "Example"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dsid/", 
            "text": "DSID\n\n\nThe DSID function returns the string form of the distributed member process identity uniquely represented in the distributed system.\n\n\n\n\nNote\n\n\n\n\n\n\nThis function is not supported in the Smart Connector mode. It is only supported for embedded mode, JDBC and ODBC.\n\n\n\n\n\n\nIn some rare cases, if the bucket has just moved while the query was being scheduled, then remote bucket fetch cannot be performed by the query partition but it still displays the DSID() of the node where the partition was executed.\n\n\n\n\n\n\n\n\nExample\n\n\nsnappy\nselect count(*), dsid() from AIRLINE group by dsid();\n\ncount(1)            |DSID()                      \n-------------------------------------------------\n347749              |192.168.1.98(3625)\nv3\n:8739 \n348440              |192.168.1.98(3849)\nv4\n:50958\n303811              |192.168.1.98(3255)\nv1\n:2428 \n\n3 rows selected", 
            "title": "DSID"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dsid/#dsid", 
            "text": "The DSID function returns the string form of the distributed member process identity uniquely represented in the distributed system.   Note    This function is not supported in the Smart Connector mode. It is only supported for embedded mode, JDBC and ODBC.    In some rare cases, if the bucket has just moved while the query was being scheduled, then remote bucket fetch cannot be performed by the query partition but it still displays the DSID() of the node where the partition was executed.", 
            "title": "DSID"
        }, 
        {
            "location": "/reference/inbuilt_system_procedures/dsid/#example", 
            "text": "snappy select count(*), dsid() from AIRLINE group by dsid();\n\ncount(1)            |DSID()                      \n-------------------------------------------------\n347749              |192.168.1.98(3625) v3 :8739 \n348440              |192.168.1.98(3849) v4 :50958\n303811              |192.168.1.98(3255) v1 :2428 \n\n3 rows selected", 
            "title": "Example"
        }, 
        {
            "location": "/reference/system_tables/system_tables/", 
            "text": "System Tables\n\n\nTables that SnappyData uses to manage the distributed system and its data. All system tables are part of the SYS schema.\n\n\nThe following system tables are available:\n\n\n\n\n\n\nMEMBERS\n\n\n\n\n\n\nSYSDISKSTORES\n\n\n\n\n\n\nSYSDISKSTOREIDS\n\n\n\n\n\n\nSYSTABLES", 
            "title": "System Tables"
        }, 
        {
            "location": "/reference/system_tables/system_tables/#system-tables", 
            "text": "Tables that SnappyData uses to manage the distributed system and its data. All system tables are part of the SYS schema.  The following system tables are available:    MEMBERS    SYSDISKSTORES    SYSDISKSTOREIDS    SYSTABLES", 
            "title": "System Tables"
        }, 
        {
            "location": "/reference/system_tables/members/", 
            "text": "MEMBERS\n\n\nA SnappyData virtual table that contains information about each distributed system member.\n\n\n\n\nNote\n\n\nSnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.\n\n\n\n\n\n\n\n\n\n\nColumn Name\n\n\nType\n\n\nLength\n\n\nNullable\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nID\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nThe unique ID of the member. This ID has the format: \nhostname(process_id);member_number;:udp_port/tcp_port\nFor example:\n10.0.1.31(66878);v0;:41715/63386\n\n\n\n\n\n\nKIND\n\n\nVARCHAR\n\n\n24\n\n\nNo\n\n\nSpecifies the type of SnappyData member process: \n * datastore\u2014A member that hosts data.\n * peer\u2014A member that does not host data.\n * locator\u2014Provides discovery services for a cluster.\n Member types can also be qualified with additional keywords \n  * normal\u2014The member can communicate with other members in a cluster. \n * loner\u2014The member is standalone and cannot communicate with other members. Loners use no locators for discovery.\n * admin\u2014The member also acts as a JMX manager node.\n\n\n\n\n\n\nHOSTDATA\n\n\nBOOLEAN\n\n\n\n\nYes\n\n\nA value of \u20181\u2019 indicates that this member is a data store and can host data. Otherwise, the member is a peer client with no hosted data.\n\n\n\n\n\n\nISELDER\n\n\nBOOLEAN\n\n\n\n\nNo\n\n\nIs this the eldest member of the distributed system. Typically, this is the member who first joins the cluster.\n\n\n\n\n\n\nIPADDRESS\n\n\nVARCHAR\n\n\n64\n\n\nYes\n\n\nThe fully-qualified hostname/IP address of the member.\n\n\n\n\n\n\nHOST\n\n\nVARCHAR\n\n\n128\n\n\nYes\n\n\nThe fully-qualified hostname of the member.\n\n\n\n\n\n\nPID\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe member process ID.\n\n\n\n\n\n\nPORT\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe member UDP port.\n\n\n\n\n\n\nROLES\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nNot used.\n\n\n\n\n\n\nNETSERVERS\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nHost and port information for Network Servers that are running on SnappyData members.\n\n\n\n\n\n\nLOCATOR\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nHost and port information for locator members.\n\n\n\n\n\n\nSERVERGROUPS\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nA comma-separated list of server groups of which this member is a part. \n \nNote\n: SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.\n\n\n\n\n\n\nSYSTEMPROPS\n\n\nCLOB\n\n\n2147483647\n\n\nNo\n\n\nA list of all system properties used to start this member. This includes properties such as the classpath, JVM version, and so forth.\n\n\n\n\n\n\nGEMFIREPROPS\n\n\nCLOB\n\n\n2147483647\n\n\nNo\n\n\nThe names and values of GemFire core system properties that the member uses.\n\n\n\n\n\n\nBOOTPROPS\n\n\nCLOB\n\n\n2147483647\n\n\nNo\n\n\nAll of the SnappyData boot properties names and values that a member uses.\n\n\n\n\n\n\n\n\nExample\n \n\n\nsnappy\n select * from SYS.MEMBERS;\nID                           |KIND             |STATUS |HOSTDATA|ISELDER|IPADDRESS |HOST     |PID        |PORT       |ROLES|NETSERVERS               |THRIFTSERVERS            |LOCATOR         |SERVERGROUPS               |MANAGERINFO              |SYSTEMPROPS    |GEMFIREPROPS   |BOOTPROPS      \n---------------------------------------------------------------------------------------------------------------- \n127.0.0.1(5687)\nv1\n:47719    |datastore(normal)|RUNNING|true    |false  |/127.0.0.1|localhost|5687       |47719      |     |localhost/127.0.0.1[1528]|localhost/127.0.0.1[1528]|NULL            |                           |Managed Node             |\n--- System Pr\n|\n--- GemFire P\n|\n--- GemFireXD\n\n127.0.0.1(5877)\nv2\n:10769    |accessor(normal) |RUNNING|false   |false  |/127.0.0.1|localhost|5877       |10769      |     |                         |                         |NULL            |IMPLICIT_LEADER_SERVERGROUP|Managed Node             |\n--- System Pr\n|\n--- GemFire P\n|\n--- GemFireXD\n\n127.0.0.1(5548)\nec\nv0\n:21415|locator(normal)  |RUNNING|false   |true   |/127.0.0.1|localhost|5548       |21415      |     |localhost/127.0.0.1[1527]|localhost/127.0.0.1[1527]|127.0.0.1[10334]|                           |Manager Node: Not Running|\n--- System Pr\n|\n--- GemFire P\n|\n--- GemFireXD\n\n\n3 rows selected", 
            "title": "MEMBERS"
        }, 
        {
            "location": "/reference/system_tables/members/#members", 
            "text": "A SnappyData virtual table that contains information about each distributed system member.   Note  SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.      Column Name  Type  Length  Nullable  Contents      ID  VARCHAR  128  No  The unique ID of the member. This ID has the format:  hostname(process_id);member_number;:udp_port/tcp_port For example: 10.0.1.31(66878);v0;:41715/63386    KIND  VARCHAR  24  No  Specifies the type of SnappyData member process:   * datastore\u2014A member that hosts data.  * peer\u2014A member that does not host data.  * locator\u2014Provides discovery services for a cluster.  Member types can also be qualified with additional keywords    * normal\u2014The member can communicate with other members in a cluster.   * loner\u2014The member is standalone and cannot communicate with other members. Loners use no locators for discovery.  * admin\u2014The member also acts as a JMX manager node.    HOSTDATA  BOOLEAN   Yes  A value of \u20181\u2019 indicates that this member is a data store and can host data. Otherwise, the member is a peer client with no hosted data.    ISELDER  BOOLEAN   No  Is this the eldest member of the distributed system. Typically, this is the member who first joins the cluster.    IPADDRESS  VARCHAR  64  Yes  The fully-qualified hostname/IP address of the member.    HOST  VARCHAR  128  Yes  The fully-qualified hostname of the member.    PID  INTEGER  10  No  The member process ID.    PORT  INTEGER  10  No  The member UDP port.    ROLES  VARCHAR  128  No  Not used.    NETSERVERS  VARCHAR  32672  No  Host and port information for Network Servers that are running on SnappyData members.    LOCATOR  VARCHAR  32672  No  Host and port information for locator members.    SERVERGROUPS  VARCHAR  32672  No  A comma-separated list of server groups of which this member is a part.    Note : SnappyData converts server group names to all uppercase letters before storing the values in the SYS.MEMBERS table. DDL statements and procedures automatically convert any supplied server group values to all uppercase letters. However, you must specify uppercase values for server groups when you directly query the SYS.MEMBERS table.    SYSTEMPROPS  CLOB  2147483647  No  A list of all system properties used to start this member. This includes properties such as the classpath, JVM version, and so forth.    GEMFIREPROPS  CLOB  2147483647  No  The names and values of GemFire core system properties that the member uses.    BOOTPROPS  CLOB  2147483647  No  All of the SnappyData boot properties names and values that a member uses.     Example    snappy  select * from SYS.MEMBERS;\nID                           |KIND             |STATUS |HOSTDATA|ISELDER|IPADDRESS |HOST     |PID        |PORT       |ROLES|NETSERVERS               |THRIFTSERVERS            |LOCATOR         |SERVERGROUPS               |MANAGERINFO              |SYSTEMPROPS    |GEMFIREPROPS   |BOOTPROPS      \n---------------------------------------------------------------------------------------------------------------- \n127.0.0.1(5687) v1 :47719    |datastore(normal)|RUNNING|true    |false  |/127.0.0.1|localhost|5687       |47719      |     |localhost/127.0.0.1[1528]|localhost/127.0.0.1[1528]|NULL            |                           |Managed Node             |\n--- System Pr |\n--- GemFire P |\n--- GemFireXD \n127.0.0.1(5877) v2 :10769    |accessor(normal) |RUNNING|false   |false  |/127.0.0.1|localhost|5877       |10769      |     |                         |                         |NULL            |IMPLICIT_LEADER_SERVERGROUP|Managed Node             |\n--- System Pr |\n--- GemFire P |\n--- GemFireXD \n127.0.0.1(5548) ec v0 :21415|locator(normal)  |RUNNING|false   |true   |/127.0.0.1|localhost|5548       |21415      |     |localhost/127.0.0.1[1527]|localhost/127.0.0.1[1527]|127.0.0.1[10334]|                           |Manager Node: Not Running|\n--- System Pr |\n--- GemFire P |\n--- GemFireXD \n\n3 rows selected", 
            "title": "MEMBERS"
        }, 
        {
            "location": "/reference/system_tables/sysdiskstores/", 
            "text": "SYSDISKSTORES\n\n\nContains information about all disk stores created in the SnappyData distributed system.\n\n\nSee \nCREATE DISKSTORE\n.\n\n\n\n\n\n\n\n\nColumn Name\n\n\nType\n\n\nLength\n\n\nNullable\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nNAME\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nThe unique identifier of the disk store.\n\n\n\n\n\n\nMAXLOGSIZE\n\n\nBIGINT\n\n\n10\n\n\nNo\n\n\nThe maximum size, in megabytes, of a single oplog file in the disk store.\n\n\n\n\n\n\nAUTOCOMPACT\n\n\nCHAR\n\n\n6\n\n\nNo\n\n\nSpecifies whether SnappyData automatically compacts log files in this disk store.\n\n\n\n\n\n\nALLOWFORCECOMPACTION\n\n\nCHAR\n\n\n6\n\n\nNo\n\n\nSpecifies whether the disk store permits online compaction of log files using the \nsnappy\n utility.\n\n\n\n\n\n\nCOMPACTIONTHRESHOLD\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe threshold after which an oplog file is eligible for compaction. Specified as a percentage value from 0\u2013100.\n\n\n\n\n\n\nTIMEINTERVAL\n\n\nBIGINT\n\n\n10\n\n\nNo\n\n\nThe maximum number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk.\n\n\n\n\n\n\nWRITEBUFFERSIZE\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe size of the buffer SnappyData uses to store operations when writing to the disk store.\n\n\n\n\n\n\nQUEUESIZE\n\n\nINTEGER\n\n\n10\n\n\nNo\n\n\nThe maximum number of row operations that SnappyData can asynchronously queue for writing to the disk store.\n\n\n\n\n\n\nDIR_PATH_SIZE\n\n\nVARCHAR\n\n\n32672\n\n\nNo\n\n\nThe directory names that hold disk store oplog files, and the maximum size in megabytes that each directory can store.\n\n\n\n\n\n\n\n\nExample\n \n\n\nsnappy\n select * from sys.SYSDISKSTORES;\nNAME                       |MAXLOGSIZE          |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL        |WRITEBUFFERSIZE|QUEUESIZE  |DIR_PATH_SIZE              \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nSTORE2                     |456                 |true       |false               |50                 |1000                |19292393       |17374      |/build-artifacts/scala-2.11/snappy/\n\nSTORE1-SNAPPY-DELTA        |50                  |true       |false               |80                 |223344              |19292393       |17374      |/build-artifacts/scala-2.11/snappy/\n\nSTORE2-SNAPPY-DELTA        |50                  |true       |false               |50                 |1000                |19292393       |17374      |/build-artifacts/scala-2.11/snappy/\n\nGFXD-DEFAULT-DISKSTORE     |1024                |true       |false               |50                 |1000                |32768          |0          |/build-artifacts/scala-2.11/snappy/\n\nGFXD-DD-DISKSTORE          |10                  |true       |false               |50                 |1000                |32768          |0          |/build-artifacts/scala-2.11/snappy/\n\nSNAPPY-INTERNAL-DELTA      |50                  |true       |false               |50                 |1000                |32768          |0          |/build-artifacts/scala-2.11/snappy/\n\nSTORE1                     |1024                |true       |false               |80                 |223344              |19292393       |17374      |/build-artifacts/scala-2.11/snappy/\n\n\n7 rows selected\n\n\n\n\nsnappy\n select * from SYS.SYSDISKSTORES where NAME = 'STORE1';\nNAME                       |MAXLOGSIZE          |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL        |WRITEBUFFERSIZE|QUEUESIZE  |DIR_PATH_SIZE              \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nSTORE1                     |1024                |true       |false               |80                 |223344              |19292393       |17374      |/build-artifacts/scala-2.11/snappy/\n\n\n1 row selected\n\n\n\n\nsnappy\n select * from sys.sysdiskstores where DIR_PATH_SIZE like '%mytest%';\nNAME             |MAXLOGSIZE          |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL        |WRITEBUFFERSIZE|QUEUESIZE  |DIR_PATH_SIZE\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ \nTEST             |1024                |true       |false               |50                 |1000                |32768          |0          |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest                      \nTEST-SNAPPY-DELTA|50                  |true       |false               |50                 |1000                |32768          |0          |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest/snappy-internal-delta\n\n2 rows selected\n\n\n\n\nRelated Topics\n\n\n\n\n\n\nDROP DISKSTORE\n\n\n\n\n\n\nCREATE DISKSTORE", 
            "title": "SYSDISKSTORES"
        }, 
        {
            "location": "/reference/system_tables/sysdiskstores/#sysdiskstores", 
            "text": "Contains information about all disk stores created in the SnappyData distributed system.  See  CREATE DISKSTORE .     Column Name  Type  Length  Nullable  Contents      NAME  VARCHAR  128  No  The unique identifier of the disk store.    MAXLOGSIZE  BIGINT  10  No  The maximum size, in megabytes, of a single oplog file in the disk store.    AUTOCOMPACT  CHAR  6  No  Specifies whether SnappyData automatically compacts log files in this disk store.    ALLOWFORCECOMPACTION  CHAR  6  No  Specifies whether the disk store permits online compaction of log files using the  snappy  utility.    COMPACTIONTHRESHOLD  INTEGER  10  No  The threshold after which an oplog file is eligible for compaction. Specified as a percentage value from 0\u2013100.    TIMEINTERVAL  BIGINT  10  No  The maximum number of milliseconds that can elapse before SnappyData asynchronously flushes data to disk.    WRITEBUFFERSIZE  INTEGER  10  No  The size of the buffer SnappyData uses to store operations when writing to the disk store.    QUEUESIZE  INTEGER  10  No  The maximum number of row operations that SnappyData can asynchronously queue for writing to the disk store.    DIR_PATH_SIZE  VARCHAR  32672  No  The directory names that hold disk store oplog files, and the maximum size in megabytes that each directory can store.     Example    snappy  select * from sys.SYSDISKSTORES;\nNAME                       |MAXLOGSIZE          |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL        |WRITEBUFFERSIZE|QUEUESIZE  |DIR_PATH_SIZE              \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nSTORE2                     |456                 |true       |false               |50                 |1000                |19292393       |17374      |/build-artifacts/scala-2.11/snappy/ \nSTORE1-SNAPPY-DELTA        |50                  |true       |false               |80                 |223344              |19292393       |17374      |/build-artifacts/scala-2.11/snappy/ \nSTORE2-SNAPPY-DELTA        |50                  |true       |false               |50                 |1000                |19292393       |17374      |/build-artifacts/scala-2.11/snappy/ \nGFXD-DEFAULT-DISKSTORE     |1024                |true       |false               |50                 |1000                |32768          |0          |/build-artifacts/scala-2.11/snappy/ \nGFXD-DD-DISKSTORE          |10                  |true       |false               |50                 |1000                |32768          |0          |/build-artifacts/scala-2.11/snappy/ \nSNAPPY-INTERNAL-DELTA      |50                  |true       |false               |50                 |1000                |32768          |0          |/build-artifacts/scala-2.11/snappy/ \nSTORE1                     |1024                |true       |false               |80                 |223344              |19292393       |17374      |/build-artifacts/scala-2.11/snappy/ \n\n7 rows selected  snappy  select * from SYS.SYSDISKSTORES where NAME = 'STORE1';\nNAME                       |MAXLOGSIZE          |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL        |WRITEBUFFERSIZE|QUEUESIZE  |DIR_PATH_SIZE              \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nSTORE1                     |1024                |true       |false               |80                 |223344              |19292393       |17374      |/build-artifacts/scala-2.11/snappy/ \n\n1 row selected  snappy  select * from sys.sysdiskstores where DIR_PATH_SIZE like '%mytest%';\nNAME             |MAXLOGSIZE          |AUTOCOMPACT|ALLOWFORCECOMPACTION|COMPACTIONTHRESHOLD|TIMEINTERVAL        |WRITEBUFFERSIZE|QUEUESIZE  |DIR_PATH_SIZE\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ \nTEST             |1024                |true       |false               |50                 |1000                |32768          |0          |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest                      \nTEST-SNAPPY-DELTA|50                  |true       |false               |50                 |1000                |32768          |0          |build-artifacts/scala-2.11/snappy/work/localhost-server-1/mytest/snappy-internal-delta\n\n2 rows selected  Related Topics    DROP DISKSTORE    CREATE DISKSTORE", 
            "title": "SYSDISKSTORES"
        }, 
        {
            "location": "/reference/system_tables/sysdiskstoreids/", 
            "text": "SYSDISKSTOREIDS\n\n\nContains information about all disk stores IDs created in the SnappyData distributed system.\n\n\n\n\n\n\n\n\nColumn Name\n\n\nType\n\n\nLength\n\n\nNullable\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nMEMBERID\n\n\nVARCHAR\n\n\n128\n\n\nfalse\n\n\nThe ID of the cluster member.\n\n\n\n\n\n\nNAME\n\n\nVARCHAR\n\n\n128\n\n\nfalse\n\n\nThe diskstore name. Two inbuilt diskstores one for DataDictionary is named \nGFXD-DD-DISKSTORE\n while the default data diskstore is named \nGFXD-DEFAULT-DISKSTORE\n.\n\n\n\n\n\n\nID\n\n\nCHAR\n\n\n36\n\n\nfalse\n\n\nThe unique diskstore ID. This is what appears in log files or output of \nsnappy-status-all.sh\n script if a member is waiting for another node to start to sync its data (if it determines the other node may have more recent data).\n\n\n\n\n\n\nDIRS\n\n\nVARCHAR\n\n\n32672\n\n\nfalse\n\n\nComma-separated list of directories used for the diskstore. These are the ones provided in \nCREATE DISKSTORE\n or else if no explicit directory was provided, then the working directory of the node.\n\n\n\n\n\n\n\n\nExample\n \n\n\nsnappy\n create diskstore d1 ('D1');\nsnappy\n select * from sys.diskstoreids;\nMEMBERID                        |NAME                            |ID                              |DIRS                            \n-----------------------------------------------------------------------------------------------------------------------------------\n127.0.0.1(7794)\nv2\n:20960       |GFXD-DEFAULT-DISKSTORE          |a47f63ca-f128-4102-bcd6-51549fa\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |STORE1                          |88d7685c-a190-4711-b2ca-ec27ece\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |D1-SNAPPY-DELTA                 |7432070a-05e3-4303-b731-91abbe3\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |GFXD-DEFAULT-DISKSTORE          |d1024174-8ec8-47ac-8df8-4b7f63b\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |STORE1-SNAPPY-DELTA             |e252410f-1440-4b8e-9b84-343276f\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |D1                              |56a1f778-ba8c-41bd-8556-27c35fa\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |STORE2-SNAPPY-DELTA             |fc3a72c0-f70a-4fed-891f-f3521d4\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |STORE2                          |4b089853-3fb1-4fd7-ba81-e3e6fe3\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |GFXD-DD-DISKSTORE               |72350926-f5fa-415f-9124-24e0944\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7638)\nv1\n:24353       |SNAPPY-INTERNAL-DELTA           |6a20b9db-e5c6-4081-baa8-77da8b1\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7494)\nec\nv0\n:35874   |GFXD-DD-DISKSTORE               |da99f2a1-56fa-47a6-a31b-ea12e8a\n|/build-artifacts/scala-2.11/snappy\n\n127.0.0.1(7494)\nec\nv0\n:35874   |GFXD-DEFAULT-DISKSTORE          |5412d64a-dfd2-46dc-b3ba-09c2079\n|/build-artifacts/scala-2.11/snappy\n\n\n12 rows selected", 
            "title": "SYSDISKSTOREIDS"
        }, 
        {
            "location": "/reference/system_tables/sysdiskstoreids/#sysdiskstoreids", 
            "text": "Contains information about all disk stores IDs created in the SnappyData distributed system.     Column Name  Type  Length  Nullable  Contents      MEMBERID  VARCHAR  128  false  The ID of the cluster member.    NAME  VARCHAR  128  false  The diskstore name. Two inbuilt diskstores one for DataDictionary is named  GFXD-DD-DISKSTORE  while the default data diskstore is named  GFXD-DEFAULT-DISKSTORE .    ID  CHAR  36  false  The unique diskstore ID. This is what appears in log files or output of  snappy-status-all.sh  script if a member is waiting for another node to start to sync its data (if it determines the other node may have more recent data).    DIRS  VARCHAR  32672  false  Comma-separated list of directories used for the diskstore. These are the ones provided in  CREATE DISKSTORE  or else if no explicit directory was provided, then the working directory of the node.     Example    snappy  create diskstore d1 ('D1');\nsnappy  select * from sys.diskstoreids;\nMEMBERID                        |NAME                            |ID                              |DIRS                            \n-----------------------------------------------------------------------------------------------------------------------------------\n127.0.0.1(7794) v2 :20960       |GFXD-DEFAULT-DISKSTORE          |a47f63ca-f128-4102-bcd6-51549fa |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |STORE1                          |88d7685c-a190-4711-b2ca-ec27ece |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |D1-SNAPPY-DELTA                 |7432070a-05e3-4303-b731-91abbe3 |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |GFXD-DEFAULT-DISKSTORE          |d1024174-8ec8-47ac-8df8-4b7f63b |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |STORE1-SNAPPY-DELTA             |e252410f-1440-4b8e-9b84-343276f |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |D1                              |56a1f778-ba8c-41bd-8556-27c35fa |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |STORE2-SNAPPY-DELTA             |fc3a72c0-f70a-4fed-891f-f3521d4 |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |STORE2                          |4b089853-3fb1-4fd7-ba81-e3e6fe3 |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |GFXD-DD-DISKSTORE               |72350926-f5fa-415f-9124-24e0944 |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7638) v1 :24353       |SNAPPY-INTERNAL-DELTA           |6a20b9db-e5c6-4081-baa8-77da8b1 |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7494) ec v0 :35874   |GFXD-DD-DISKSTORE               |da99f2a1-56fa-47a6-a31b-ea12e8a |/build-artifacts/scala-2.11/snappy \n127.0.0.1(7494) ec v0 :35874   |GFXD-DEFAULT-DISKSTORE          |5412d64a-dfd2-46dc-b3ba-09c2079 |/build-artifacts/scala-2.11/snappy \n\n12 rows selected", 
            "title": "SYSDISKSTOREIDS"
        }, 
        {
            "location": "/reference/system_tables/systables/", 
            "text": "SYSTABLES\n\n\nDescribes the tables and views in the distributed system.\n\n\n\n\n\n\n\n\nColumn Name\n\n\nType\n\n\nLength\n\n\nNullable\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nTABLEID\n\n\nCHAR\n\n\n36\n\n\nNo\n\n\nUnique identifier for table or view\n\n\n\n\n\n\nTABLENAME\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nTable or view name\n\n\n\n\n\n\nTABLETYPE\n\n\nCHAR\n\n\n1\n\n\nNo\n\n\n'S' (system table), 'T' (user table), 'A' (synonym), or 'V' (view)\n\n\n\n\n\n\nSCHEMAID\n\n\nCHAR\n\n\n36\n\n\nNo\n\n\nSchema ID for the table or view\n\n\n\n\n\n\nTABLESCHEMANAME\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nThe table schema\n\n\n\n\n\n\nLOCKGRANULARITY\n\n\nCHAR\n\n\n1\n\n\nNo\n\n\nLock granularity for the table: 'T' (table level locking) or 'R' (row level locking, the default)\n\n\n\n\n\n\nSERVERGROUPS\n\n\nVARCHAR\n\n\n128\n\n\nNo\n\n\nThe server groups assigned to the table\n\n\n\n\n\n\nDATAPOLICY\n\n\nVARCHAR\n\n\n24\n\n\nNo\n\n\nTable partitioning and replication status\n\n\n\n\n\n\nPARTITIONATTRS\n\n\nLONG VARCHAR\n\n\n32,700\n\n\nYes\n\n\nFor partitioned tables, displays the additional partitioning attributes assigned with the CREATE TABLE statement, such as colocation, buckets, and redundancy values\n\n\n\n\n\n\nRESOLVER\n\n\nLONG VARCHAR\n\n\n32,700\n\n\nYes\n\n\nThe partitioning resolver (contains the partitioning clause).\n\n\n\n\n\n\nEXPIRATIONATTRS\n\n\nLONG VARCHAR\n\n\n32,700\n\n\nYes\n\n\nRow expiration settings\n\n\n\n\n\n\nEVICTIONATTRS\n\n\nLONG VARCHAR\n\n\n32,700\n\n\nYes\n\n\nRow eviction settings\n\n\n\n\n\n\nDISKATTRS\n\n\nLONG VARCHAR\n\n\n32,700\n\n\nYes\n\n\nTable persistence settings\n\n\n\n\n\n\nLOADER\n\n\nVARCHAR\n\n\n128\n\n\nYes\n\n\nNot available for this release\n\n\n\n\n\n\nWRITER\n\n\nVARCHAR\n\n\n128\n\n\nYes\n\n\nNot available for this release\n\n\n\n\n\n\nLISTENERS\n\n\nLONG VARCHAR\n\n\n32,700\n\n\nYes\n\n\nNot available for this release\n\n\n\n\n\n\nASYNCLISTENERS\n\n\nVARCHAR\n\n\n256\n\n\nYes\n\n\nNot available for this release\n\n\n\n\n\n\nGATEWAYENABLED\n\n\nBOOLEAN\n\n\n1\n\n\nNo\n\n\nNot available for this release\n\n\n\n\n\n\nGATEWAYSENDERS\n\n\nVARCHAR\n\n\n256\n\n\nYes\n\n\nNot available for this release\n\n\n\n\n\n\n\n\nExample\n \n\n\nsnappy\n select * from SYS.SYSTABLES;\nTABLEID                             |TABLENAME                |TABLETYPE|SCHEMAID                            |TABLESCHEMANAME      |LOCKGRANULARITY|SERVERGROUPS|DATAPOLICY          |PARTITIONATTRS|RESOLVER|EXPIRATIONATTRS|EVICTIONATTRS|DISKATTRS                                                 |LOADER|WRITER|LISTENERS|ASYNCLISTENERS|GATEWAYENABLED|GATEWAYSENDERS|OFFHEAPENABLED\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \ndf2180fd-0162-84af-a660-0000f2ef3af8|TBLS                     |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n0073400e-00b6-fdfc-71ce-000b0a763800|GATEWAYSENDERS           |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n80000000-00d1-15f7-ab70-000a0a0b1500|SYSSTATEMENTS            |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \na6c0c1fe-0162-84af-a660-0000f2ef3af8|PARTITION_KEY_VALS       |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n140d4147-0162-84af-a660-0000f2ef3af8|SORT_COLS                |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n9fc7c266-0162-84af-a660-0000f2ef3af8|FUNCS                    |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \nf33d40c7-0162-84af-a660-0000f2ef3af8|SYSXPLAIN_RESULTSETS     |C        |c013800d-00fb-2644-07ec-000000134f30|SYSSTAT              |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n80000039-00d0-fd77-3ed8-000a0a0b1900|SYSKEYS                  |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \ne03f4017-0115-382c-08df-ffffe275b270|SYSROLES                 |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n80000000-00d3-e222-873f-000a0a0b1900|SYSFILES                 |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n2057c01b-0103-0e39-b8e7-00000010f010|SYSROUTINEPERMS          |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n65548115-0162-84af-a660-0000f2ef3af8|SDS                      |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n96a2414f-0162-84af-a660-0000f2ef3af8|SD_PARAMS                |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false", 
            "title": "SYSTABLES"
        }, 
        {
            "location": "/reference/system_tables/systables/#systables", 
            "text": "Describes the tables and views in the distributed system.     Column Name  Type  Length  Nullable  Contents      TABLEID  CHAR  36  No  Unique identifier for table or view    TABLENAME  VARCHAR  128  No  Table or view name    TABLETYPE  CHAR  1  No  'S' (system table), 'T' (user table), 'A' (synonym), or 'V' (view)    SCHEMAID  CHAR  36  No  Schema ID for the table or view    TABLESCHEMANAME  VARCHAR  128  No  The table schema    LOCKGRANULARITY  CHAR  1  No  Lock granularity for the table: 'T' (table level locking) or 'R' (row level locking, the default)    SERVERGROUPS  VARCHAR  128  No  The server groups assigned to the table    DATAPOLICY  VARCHAR  24  No  Table partitioning and replication status    PARTITIONATTRS  LONG VARCHAR  32,700  Yes  For partitioned tables, displays the additional partitioning attributes assigned with the CREATE TABLE statement, such as colocation, buckets, and redundancy values    RESOLVER  LONG VARCHAR  32,700  Yes  The partitioning resolver (contains the partitioning clause).    EXPIRATIONATTRS  LONG VARCHAR  32,700  Yes  Row expiration settings    EVICTIONATTRS  LONG VARCHAR  32,700  Yes  Row eviction settings    DISKATTRS  LONG VARCHAR  32,700  Yes  Table persistence settings    LOADER  VARCHAR  128  Yes  Not available for this release    WRITER  VARCHAR  128  Yes  Not available for this release    LISTENERS  LONG VARCHAR  32,700  Yes  Not available for this release    ASYNCLISTENERS  VARCHAR  256  Yes  Not available for this release    GATEWAYENABLED  BOOLEAN  1  No  Not available for this release    GATEWAYSENDERS  VARCHAR  256  Yes  Not available for this release     Example    snappy  select * from SYS.SYSTABLES;\nTABLEID                             |TABLENAME                |TABLETYPE|SCHEMAID                            |TABLESCHEMANAME      |LOCKGRANULARITY|SERVERGROUPS|DATAPOLICY          |PARTITIONATTRS|RESOLVER|EXPIRATIONATTRS|EVICTIONATTRS|DISKATTRS                                                 |LOADER|WRITER|LISTENERS|ASYNCLISTENERS|GATEWAYENABLED|GATEWAYSENDERS|OFFHEAPENABLED\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \ndf2180fd-0162-84af-a660-0000f2ef3af8|TBLS                     |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n0073400e-00b6-fdfc-71ce-000b0a763800|GATEWAYSENDERS           |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n80000000-00d1-15f7-ab70-000a0a0b1500|SYSSTATEMENTS            |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \na6c0c1fe-0162-84af-a660-0000f2ef3af8|PARTITION_KEY_VALS       |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n140d4147-0162-84af-a660-0000f2ef3af8|SORT_COLS                |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n9fc7c266-0162-84af-a660-0000f2ef3af8|FUNCS                    |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \nf33d40c7-0162-84af-a660-0000f2ef3af8|SYSXPLAIN_RESULTSETS     |C        |c013800d-00fb-2644-07ec-000000134f30|SYSSTAT              |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n80000039-00d0-fd77-3ed8-000a0a0b1900|SYSKEYS                  |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \ne03f4017-0115-382c-08df-ffffe275b270|SYSROLES                 |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n80000000-00d3-e222-873f-000a0a0b1900|SYSFILES                 |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n2057c01b-0103-0e39-b8e7-00000010f010|SYSROUTINEPERMS          |S        |8000000d-00d0-fd77-3ed8-000a0a0b1900|SYS                  |R              |            |NORMAL              |NULL          |NULL    |NULL           |NULL         |NULL                                                      |NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n65548115-0162-84af-a660-0000f2ef3af8|SDS                      |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false         \n96a2414f-0162-84af-a660-0000f2ef3af8|SD_PARAMS                |T        |d46b80cd-0162-84af-a660-0000f2ef3af8|SNAPPY_HIVE_METASTORE|R              |            |PERSISTENT_REPLICATE|NULL          |NULL    |NULL           |NULL         |DiskStore is GFXD-DD-DISKSTORE; Synchronous writes to disk|NULL  |NULL  |NULL     |NULL          |false         |NULL          |false", 
            "title": "SYSTABLES"
        }, 
        {
            "location": "/reference/command_line_utilities/store-launcher/", 
            "text": "Command Line Utilities\n\n\nUse the \nsnappy\n command-line utility to launch SnappyData utilities.\n\n\nTo display a full list of snappy commands and options:\n\n\n./bin/snappy --help\n\n\n\n\nThe command form to display a particular utility's usage is:\n\n\n./bin/snappy \nutility\n --help\n\n\n\n\nWith no arguments, \nsnappy\n starts an \ninteractive SQL command shell\n:\n\n\n./bin/snappy\n\n\n\nTo specify a system property for an interactive `snappy` session, you must define the JAVA_ARGS environment variable before starting `snappy`. For example, `snappy` uses the `snappy.history` system property to define the file that stores a list of the commands that are executed during an interactive session. To change this property, you would define it as part of the JAVA_ARGS variable:\n\n```pre\n$ export JAVA_ARGS=\n-Dsnappy.history=/Users/user1/snappystore-history.sql\n\n$ snappy\n\n\n\n\nTo launch and exit a \nsnappy\n utility (rather than start an interactive \nsnappy\n shell) use the syntax:\n\n\n./bin/snappy \nutility\n \narguments for specified utility\n\n\n\n\n\nTo specify a system property when launching a \nsnappy\n utility, use -J-D\nproperty_name\n=\nproperty_value\n argument.\n\n\nIn addition to launching various utilities provided with SnappyData, when launched without any arguments, \nsnappy\n starts an interactive command shell that you can use to connect to a SnappyData system and execute various commands, including SQL statements.\n\n\n\n\n\n\nbackup and restore\n\n    Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.\n\n\n\n\n\n\ncompact-all-disk-stores\n Performs online compaction of SnappyData disk stores.\n\n\n\n\n\n\ncompact-disk-store\n Performs offline compaction of a single SnappyData disk store.\n\n\n\n\n\n\n\nrevoke-missing-disk-store\n\n    Instruct SnappyData members to stop waiting for a disk store to become available.\n\n\n\n\n\n\nrun\n\n    Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive SnappyData shell.\n\n\n\n\n\n\nversion\n\n    Prints information about the SnappyData product version", 
            "title": "Command Line Utilities"
        }, 
        {
            "location": "/reference/command_line_utilities/store-launcher/#command-line-utilities", 
            "text": "Use the  snappy  command-line utility to launch SnappyData utilities.  To display a full list of snappy commands and options:  ./bin/snappy --help  The command form to display a particular utility's usage is:  ./bin/snappy  utility  --help  With no arguments,  snappy  starts an  interactive SQL command shell :  ./bin/snappy\n\n\n\nTo specify a system property for an interactive `snappy` session, you must define the JAVA_ARGS environment variable before starting `snappy`. For example, `snappy` uses the `snappy.history` system property to define the file that stores a list of the commands that are executed during an interactive session. To change this property, you would define it as part of the JAVA_ARGS variable:\n\n```pre\n$ export JAVA_ARGS= -Dsnappy.history=/Users/user1/snappystore-history.sql \n$ snappy  To launch and exit a  snappy  utility (rather than start an interactive  snappy  shell) use the syntax:  ./bin/snappy  utility   arguments for specified utility   To specify a system property when launching a  snappy  utility, use -J-D property_name = property_value  argument.  In addition to launching various utilities provided with SnappyData, when launched without any arguments,  snappy  starts an interactive command shell that you can use to connect to a SnappyData system and execute various commands, including SQL statements.    backup and restore \n    Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.    compact-all-disk-stores  Performs online compaction of SnappyData disk stores.    compact-disk-store  Performs offline compaction of a single SnappyData disk store.    revoke-missing-disk-store \n    Instruct SnappyData members to stop waiting for a disk store to become available.    run \n    Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive SnappyData shell.    version \n    Prints information about the SnappyData product version", 
            "title": "Command Line Utilities"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/", 
            "text": "Backup and Restore\n\n\nCreates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.\n\n\nAn online backup saves the following:\n\n\n\n\nFor each member with persistent data, the backup includes disk store files for all stores containing persistent table data.\n\n\nConfiguration files from the member startup.\n\n\nA restore script (restore.sh) copies the files back to their original locations.\n\n\n\n\n\n\nNote\n\n\n\n\nSnappyData does not support backing up disk stores on systems with live transactions, or when concurrent DML statements are being executed. \nIf a backup of the live transaction or concurrent DML operations, is performed, there is a possibility of partial commits or partial changes of DML operations appearing in the backups.\n\n\nSnappyData does not support taking incremental backups on systems with live transactions, or when concurrent DML statements are being executed.\n\n\n\n\n\n\n\n\n\n\nGuidelines\n\n\n\n\n\n\nSpecifying the Backup Directory\n\n\n\n\n\n\nBackup Directory Structure and Contents\n\n\n\n\n\n\nPerforming a Full Backup\n\n\n\n\n\n\nPerforming an Incremental Backup\n\n\n\n\n\n\nList of Properties\n\n\n\n\n\n\nRestoring your Backup\n\n\n\n\n\n\nVerify the Backup is Successful\n\n\n\n\n\n\n\n\nGuidelines\n\n\n\n\n\n\nRun this command during a period of low activity in your system. The backup does not block system activities, but it uses file system resources on all hosts in your distributed system and can affect performance.\n\n\n\n\n\n\nIf you try to create backup files from a running system using file copy commands, you can get incomplete and unusable copies.\n\n\n\n\n\n\nMake sure the target backup directory exists and has the proper permissions for your members to write to it and create subdirectories.\n\n\n\n\n\n\nIt is recommended to \ncompact your disk store\n before running the backup.\n\n\n\n\n\n\nMake sure that those SnappyData members that host persistent data are running in the distributed system. Offline members cannot back up their disk stores. (A complete backup can still be performed if all table data is available in the running members).\n\n\n\n\n\n\n\n\nSpecifying the Backup Directory\n\n\nThe directory you specify for backup can be used multiple times. Each backup first creates a top-level directory for the backup, under the directory you specify, identified to the minute.\n\nYou can use one of two formats:\n\n\n\n\n\n\nUse a single physical location, such as a network file server. (For example, \nfileServerDirectory\n/\nSnappyBackupLocation\n).\n\n\n\n\n\n\nUse a directory that is local to all host machines in the system. (For example, \nSnappyBackupLocation\n).\n\n\n\n\n\n\n\n\nBackup Directory Structure and Contents\n\n\nThe backup directory contains a backup of the persistent data.  Below is the structure of files and directories backed up in a distributed system:\n\n\n2018-03-15-05-31-46:\n10_80_141_112_10715_ec_v0_7393 10_80_141_112_10962_v1_57099\n\n2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393:\nconfig diskstores README.txt restore.sh user\n2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/config:\n2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/user:\n2018-03-15-05-31-46/2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/diskstores/\nGFXD-DD-DISKSTORE_4d9fa95e-7746-4d4d-b404-2648d64cf35e GFXD-DEFAULT-DISKSTORE_3c446ce4-43e4-4c14-bce5-e4336b6570e5\n\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099:\nconfig diskstores README.txt restore.sh user\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/config\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/user\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/diskstores:\nGFXD-DD-DISKSTORE_76705038-10de-4b3e-955b-446546fe4036 GFXD-DEFAULT-DISKSTORE_157fa93d-c8a9-4585-ba78-9c10eb9c2ab6 USERDISKSTORE_216d5484-86f7-4e82-be81-d5bf7c2ba59f USERDISKSTORE-SNAPPY-DELTA_e7e12e86-3907-49e6-8f4c-f8a7a0d4156c\n\n\n\n\n\n\n\n\n\n\nDirectory\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nconfig\n\n\nFor internal use\n\n\n\n\n\n\ndiskstores\n\n\n- GFXD-DD-DISKSTORE: Diskstores created for DataDictionary  \n - GFXD-DEFAULT-DISKSTORE: The default diskstore. \n- USERDISKSTORE: Generated for diskstores created by users using \nCREATE DISKSTORE\n command.\n- USERDISKSTORE-SNAPPY-DELTA: Created for delta regions.\n\n\n\n\n\n\nuser\n\n\nFor internal use\n\n\n\n\n\n\nREADME.txt\n\n\nThe file contains information about other files in a directory.\n\n\n\n\n\n\nrestore.sh\n\n\nScript that copies files back to their original locations.\n\n\n\n\n\n\n\n\n\n\nPerforming a Full Backup\n\n\nFor each member with persistent data, the backup includes:\n\n\n\n\n\n\nDisk store files for all stores containing persistent tables.\n\n\n\n\n\n\nBackup of all disk stores including the disk stores created for metadata as well as separate disk stores created for row buffer. \n\n\n\n\n\n\nConfiguration files from the member startup (snappydata.properties). These configuration files are not automatically restored, to avoid interfering with any more recent configurations. In particular, if these are extracted from a master jar file, copying the separate files into your working area could override the files in the jar.\n\n\n\n\n\n\nA restore script (restore.sh), written for the member\u2019s operating system, that copies the files back to their original locations.\n\n\n\n\n\n\nTo perform a full backup:\n\n\n\n\n\n\nStart the cluster\n.\n\n\n\n\n\n\nStart the snappy-shell and connect to the cluster\n.\n\n\n\n\n\n\nStop all transactions running in your distributed system, and do not execute DML statements during the backup. SnappyData does not support backing up a disk store while live transactions are taking place or when concurrent DML statements are being executed.\n\n\n\n\n\n\nRun the backup command, providing your backup directory location.\n\n\n./bin/snappy backup \nSnappyBackupLocation\n -locators=localhost:\npeer-discovery-address\n\n\n\n\n\n\nRead the message that reports on the success of the operation.\n\n\n\n\n\n\nIf the operation is successful, you see a message like this:\n\n\nThe following disk stores were backed up:\n    1f5dbd41-309b-4997-a50b-95890183f8ce [\nhostname\n:/\nLocatorLogDirectory\n/datadictionary]\n    5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [\nhostname\n:/\nLocatorLogDirectory\n]\n    da31492f-3234-4b7e-820b-30c6b85c19a2 [\nhostname\n:/\nServerLogDirectory\n/snappy-internal-delta]\n    5a5d7ab2-96cf-4a73-8106-7a816a67f098 [\nhostname\n:/\nServerLogDirectory\n/datadictionary]\n    42510800-40e3-4abf-bcc4-7b7e8c5af951 [\nhostname\n:/\nServerLogDirectory\n]\nBackup successful.\n\n\n\n\nIf the operation does not succeed, a message is displayed indicating that the backup was incomplete and is noted in the ending status message. It leaves the file INCOMPLETE_BACKUP in its highest level backup directory. \nOffline members leave nothing, so you only have this message from the backup operation itself. Although offline members cannot back up their disk stores, a complete backup can be obtained if at least one copy of the data is available in a running member.\n\n\nIf the cluster is secure, you also need to specify all the security properties as command-line arguments to the backup command. The security properties you need to provide are the same as those mentioned in the configuration files in the \nconf\n directory (locators, servers or leads) when the cluster is launched.\nThe only difference is that any valid user can run this command. That is, the user does not have to be a snappydata cluster administrator to run the backup command.\n\n\nFor example:\n\n\n./bin/snappy backup   /snappydata_backup_location/   -locators=locatorhostname:10334  -auth-provider=LDAP  -gemfirexd.auth-ldap-server=ldap://\nldap-server-host\n:389/  -user=\nusername\n  -password=\npassword\n  -gemfirexd.auth-ldap-search-base=\nsearch-base-values\n  -gemfirexd.auth-ldap-search-dn=\nsearch-dn-values\n -gemfirexd.auth-ldap-search-pw=\npassword\n\n\n\n\n\n\nOptionally, you can encrypt the user's password first and use it in the above command to explicitly avoid putting the password in plain text in the command-line. Here is \nhow you can encrypt the password\n\n\n\n\nPerforming an Incremental backup\n\n\nAn incremental backup saves the difference between the last backup and the current data. An incremental backup copies only operation logs that are not already present in the baseline directories for each member. For incremental backups, the restore script contains explicit references to operation logs in one or more previously-chained incremental backups. When the restore script is run from an incremental backup, it also restores the operation logs from previous incremental backups that are part of the backup chain.\n\n\nIf members are missing from the baseline directory because they were offline or did not exist at the time of the baseline backup, those members place full backups of all their files into the incremental backup directory.\n\n\nTo perform an incremental backup, execute the backup command but specify the baseline directory as well as your incremental backup directory (both can be the same directory). \nFor example:\n\n\n./bin/snappy backup -baseline=\nSnappyBackupLocation\n \nSnappyBackupLocation\n -locators=\npeer-discovery-address\n\n\n\n\n\nThe tool reports on the success of the operation. If the operation is successful, you see a message like this:\n\n\nThe following disk stores were backed up:\n    1f5dbd41-309b-4997-a50b-95890183f8ce [\nhostname\n:/\nLocatorLogDirectory\n/datadictionary]\n    5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [\nhostname\n:/\nLocatorLogDirectory\n]\n    da31492f-3234-4b7e-820b-30c6b85c19a2 [\nhostname\n:/\nServerLogDirectory\n/snappy-internal-delta]\n    5a5d7ab2-96cf-4a73-8106-7a816a67f098 [\nhostname\n:/\nServerLogDirectory\n/datadictionary]\n    42510800-40e3-4abf-bcc4-7b7e8c5af951 [\nhostname\n:/\nServerLogDirectory\n]\nBackup successful.\n\n\n\n\nA member that fails to complete its backup is noted in this ending status message and leaves the file INCOMPLETE_BACKUP. The next time you perform a backup operation a full backup is performed.\n\n\nTo make additional incremental backups, execute the same backup command described in this section by providing the incremental backup directory and the baseline directory.\n\n\n\n\nList of Properties\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-baseline\n\n\nThe directory that contains a baseline backup used for comparison during an incremental backup. The baseline directory corresponds to the backup location you specified when the last backup was performed. (For example, a baseline directory can resemble \nfileServerDirectory\n/\nSnappyDataBackupLocation\n.). \n An incremental backup operation backs up any data that is not already present in the specified \n-baseline\n directory. If the member cannot find previously backed up data or if the previously backed up data is corrupt, then command performs a full backup on that member. The command also performs a full backup if you omit the \n-baseline\n option. Optionally, you can provide the directory with the time stamp details, to perform an incremental backup (For example, \nfileServerDirectory\n/\nSnappyDataBackupLocation\n/\nTimeStamp\n).\n\n\n\n\n\n\n-target-directory\n\n\nThe directory in which SnappyData stores the backup content. See \nSpecifying the Backup Directory\n.\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. For example, \n-locators=localhost:10334\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-J-D=\n\n\nSets Java system property. For example: \n-J-Dgemfire.ack-wait-threshold=20\n\n\n\n\n\n\n-J\n\n\nPrefix for any JVM property. For example \n-J-Xmx4g\n\n\n\n\n\n\n\n\n\n\nRestoring your Backup\n\n\nThe restore.sh script is generated for each member in the cluster in the timestamp directory. The script (restore.sh) copies files back to their original locations.\n\n\n\n\n\n\nNavigate to the backup subdirectory with the timestamp of the backup that you want to restore. For example, if you performed multiple incremental backups, navigate to the latest backup directory in order to restore your system to the last available backup.\n\n\n\n\n\n\nRun each restore script on the host where the backup originated.\n\n\nSnappyBackupLocation\n/\nTimestampDirectory\n/restore.sh\n\n\n\n\n\n\n\nRepeat this procedure as necessary for other members of the distributed system.\n\n\n\n\n\n\nAfter all disk stores have been restored, restart all members of the original cluster.\n\n\n\n\n\n\nYou can also do this manually:\n\n\n\n\n\n\nRestore your disk stores when your members are offline and the system is down.\n\n\n\n\n\n\nRead the restore scripts to see where the files are placed and make sure the destination locations are ready. The restore scripts do not copy over files with the same names.\n\n\n\n\n\n\nRun the restore scripts. Run each script on the host where the backup originated.\n\n\n\n\n\n\nThe restore operation copies the files back to their original location. All the disk stores including users created one and disk stores for metadata are also restored.\n\n\n\n\nVerify the Backup is Successful\n\n\nTo ensure that your backup is successful, you can try the following options:\n\n\n\n\n\n\nExecute the \nselect count(*) from \nTableName\n;\n query and verify the total number of rows.\n\n\n\n\n\n\nVerify the table details in the \nSnappy Pulse UI\n.\n\n\n\n\n\n\nIf you have done updates, you can verify to see if those specific updates are available.", 
            "title": "backup and restore"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#backup-and-restore", 
            "text": "Creates a backup of operational disk stores for all members running in the distributed system. Each member with persistent data creates a backup of its own configuration and disk stores.  An online backup saves the following:   For each member with persistent data, the backup includes disk store files for all stores containing persistent table data.  Configuration files from the member startup.  A restore script (restore.sh) copies the files back to their original locations.    Note   SnappyData does not support backing up disk stores on systems with live transactions, or when concurrent DML statements are being executed.  If a backup of the live transaction or concurrent DML operations, is performed, there is a possibility of partial commits or partial changes of DML operations appearing in the backups.  SnappyData does not support taking incremental backups on systems with live transactions, or when concurrent DML statements are being executed.      Guidelines    Specifying the Backup Directory    Backup Directory Structure and Contents    Performing a Full Backup    Performing an Incremental Backup    List of Properties    Restoring your Backup    Verify the Backup is Successful", 
            "title": "Backup and Restore"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#guidelines", 
            "text": "Run this command during a period of low activity in your system. The backup does not block system activities, but it uses file system resources on all hosts in your distributed system and can affect performance.    If you try to create backup files from a running system using file copy commands, you can get incomplete and unusable copies.    Make sure the target backup directory exists and has the proper permissions for your members to write to it and create subdirectories.    It is recommended to  compact your disk store  before running the backup.    Make sure that those SnappyData members that host persistent data are running in the distributed system. Offline members cannot back up their disk stores. (A complete backup can still be performed if all table data is available in the running members).", 
            "title": "Guidelines"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#specifying-the-backup-directory", 
            "text": "The directory you specify for backup can be used multiple times. Each backup first creates a top-level directory for the backup, under the directory you specify, identified to the minute. \nYou can use one of two formats:    Use a single physical location, such as a network file server. (For example,  fileServerDirectory / SnappyBackupLocation ).    Use a directory that is local to all host machines in the system. (For example,  SnappyBackupLocation ).", 
            "title": "Specifying the Backup Directory"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#backup-directory-structure-and-contents", 
            "text": "The backup directory contains a backup of the persistent data.  Below is the structure of files and directories backed up in a distributed system:  2018-03-15-05-31-46:\n10_80_141_112_10715_ec_v0_7393 10_80_141_112_10962_v1_57099\n\n2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393:\nconfig diskstores README.txt restore.sh user\n2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/config:\n2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/user:\n2018-03-15-05-31-46/2018-03-15-05-31-46/10_80_141_112_10715_ec_v0_7393/diskstores/\nGFXD-DD-DISKSTORE_4d9fa95e-7746-4d4d-b404-2648d64cf35e GFXD-DEFAULT-DISKSTORE_3c446ce4-43e4-4c14-bce5-e4336b6570e5\n\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099:\nconfig diskstores README.txt restore.sh user\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/config\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/user\n2018-03-15-05-31-46/10_80_141_112_10962_v1_57099/diskstores:\nGFXD-DD-DISKSTORE_76705038-10de-4b3e-955b-446546fe4036 GFXD-DEFAULT-DISKSTORE_157fa93d-c8a9-4585-ba78-9c10eb9c2ab6 USERDISKSTORE_216d5484-86f7-4e82-be81-d5bf7c2ba59f USERDISKSTORE-SNAPPY-DELTA_e7e12e86-3907-49e6-8f4c-f8a7a0d4156c     Directory  Contents      config  For internal use    diskstores  - GFXD-DD-DISKSTORE: Diskstores created for DataDictionary    - GFXD-DEFAULT-DISKSTORE: The default diskstore.  - USERDISKSTORE: Generated for diskstores created by users using  CREATE DISKSTORE  command. - USERDISKSTORE-SNAPPY-DELTA: Created for delta regions.    user  For internal use    README.txt  The file contains information about other files in a directory.    restore.sh  Script that copies files back to their original locations.", 
            "title": "Backup Directory Structure and Contents"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#performing-a-full-backup", 
            "text": "For each member with persistent data, the backup includes:    Disk store files for all stores containing persistent tables.    Backup of all disk stores including the disk stores created for metadata as well as separate disk stores created for row buffer.     Configuration files from the member startup (snappydata.properties). These configuration files are not automatically restored, to avoid interfering with any more recent configurations. In particular, if these are extracted from a master jar file, copying the separate files into your working area could override the files in the jar.    A restore script (restore.sh), written for the member\u2019s operating system, that copies the files back to their original locations.    To perform a full backup:    Start the cluster .    Start the snappy-shell and connect to the cluster .    Stop all transactions running in your distributed system, and do not execute DML statements during the backup. SnappyData does not support backing up a disk store while live transactions are taking place or when concurrent DML statements are being executed.    Run the backup command, providing your backup directory location.  ./bin/snappy backup  SnappyBackupLocation  -locators=localhost: peer-discovery-address    Read the message that reports on the success of the operation.    If the operation is successful, you see a message like this:  The following disk stores were backed up:\n    1f5dbd41-309b-4997-a50b-95890183f8ce [ hostname :/ LocatorLogDirectory /datadictionary]\n    5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [ hostname :/ LocatorLogDirectory ]\n    da31492f-3234-4b7e-820b-30c6b85c19a2 [ hostname :/ ServerLogDirectory /snappy-internal-delta]\n    5a5d7ab2-96cf-4a73-8106-7a816a67f098 [ hostname :/ ServerLogDirectory /datadictionary]\n    42510800-40e3-4abf-bcc4-7b7e8c5af951 [ hostname :/ ServerLogDirectory ]\nBackup successful.  If the operation does not succeed, a message is displayed indicating that the backup was incomplete and is noted in the ending status message. It leaves the file INCOMPLETE_BACKUP in its highest level backup directory. \nOffline members leave nothing, so you only have this message from the backup operation itself. Although offline members cannot back up their disk stores, a complete backup can be obtained if at least one copy of the data is available in a running member.  If the cluster is secure, you also need to specify all the security properties as command-line arguments to the backup command. The security properties you need to provide are the same as those mentioned in the configuration files in the  conf  directory (locators, servers or leads) when the cluster is launched.\nThe only difference is that any valid user can run this command. That is, the user does not have to be a snappydata cluster administrator to run the backup command.  For example:  ./bin/snappy backup   /snappydata_backup_location/   -locators=locatorhostname:10334  -auth-provider=LDAP  -gemfirexd.auth-ldap-server=ldap:// ldap-server-host :389/  -user= username   -password= password   -gemfirexd.auth-ldap-search-base= search-base-values   -gemfirexd.auth-ldap-search-dn= search-dn-values  -gemfirexd.auth-ldap-search-pw= password   Optionally, you can encrypt the user's password first and use it in the above command to explicitly avoid putting the password in plain text in the command-line. Here is  how you can encrypt the password", 
            "title": "Performing a Full Backup"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#performing-an-incremental-backup", 
            "text": "An incremental backup saves the difference between the last backup and the current data. An incremental backup copies only operation logs that are not already present in the baseline directories for each member. For incremental backups, the restore script contains explicit references to operation logs in one or more previously-chained incremental backups. When the restore script is run from an incremental backup, it also restores the operation logs from previous incremental backups that are part of the backup chain.  If members are missing from the baseline directory because they were offline or did not exist at the time of the baseline backup, those members place full backups of all their files into the incremental backup directory.  To perform an incremental backup, execute the backup command but specify the baseline directory as well as your incremental backup directory (both can be the same directory).  For example:  ./bin/snappy backup -baseline= SnappyBackupLocation   SnappyBackupLocation  -locators= peer-discovery-address   The tool reports on the success of the operation. If the operation is successful, you see a message like this:  The following disk stores were backed up:\n    1f5dbd41-309b-4997-a50b-95890183f8ce [ hostname :/ LocatorLogDirectory /datadictionary]\n    5cb9afc3-12fd-4be8-9c0c-cc6c7fdec86e [ hostname :/ LocatorLogDirectory ]\n    da31492f-3234-4b7e-820b-30c6b85c19a2 [ hostname :/ ServerLogDirectory /snappy-internal-delta]\n    5a5d7ab2-96cf-4a73-8106-7a816a67f098 [ hostname :/ ServerLogDirectory /datadictionary]\n    42510800-40e3-4abf-bcc4-7b7e8c5af951 [ hostname :/ ServerLogDirectory ]\nBackup successful.  A member that fails to complete its backup is noted in this ending status message and leaves the file INCOMPLETE_BACKUP. The next time you perform a backup operation a full backup is performed.  To make additional incremental backups, execute the same backup command described in this section by providing the incremental backup directory and the baseline directory.", 
            "title": "Performing an Incremental backup"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#list-of-properties", 
            "text": "Option  Description      -baseline  The directory that contains a baseline backup used for comparison during an incremental backup. The baseline directory corresponds to the backup location you specified when the last backup was performed. (For example, a baseline directory can resemble  fileServerDirectory / SnappyDataBackupLocation .).   An incremental backup operation backs up any data that is not already present in the specified  -baseline  directory. If the member cannot find previously backed up data or if the previously backed up data is corrupt, then command performs a full backup on that member. The command also performs a full backup if you omit the  -baseline  option. Optionally, you can provide the directory with the time stamp details, to perform an incremental backup (For example,  fileServerDirectory / SnappyDataBackupLocation / TimeStamp ).    -target-directory  The directory in which SnappyData stores the backup content. See  Specifying the Backup Directory .    -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the peer-discovery-port used when starting the cluster (default 10334). This is a mandatory field. For example,  -locators=localhost:10334    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.    -J-D=  Sets Java system property. For example:  -J-Dgemfire.ack-wait-threshold=20    -J  Prefix for any JVM property. For example  -J-Xmx4g", 
            "title": "List of Properties"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#restoring-your-backup", 
            "text": "The restore.sh script is generated for each member in the cluster in the timestamp directory. The script (restore.sh) copies files back to their original locations.    Navigate to the backup subdirectory with the timestamp of the backup that you want to restore. For example, if you performed multiple incremental backups, navigate to the latest backup directory in order to restore your system to the last available backup.    Run each restore script on the host where the backup originated.  SnappyBackupLocation / TimestampDirectory /restore.sh    Repeat this procedure as necessary for other members of the distributed system.    After all disk stores have been restored, restart all members of the original cluster.    You can also do this manually:    Restore your disk stores when your members are offline and the system is down.    Read the restore scripts to see where the files are placed and make sure the destination locations are ready. The restore scripts do not copy over files with the same names.    Run the restore scripts. Run each script on the host where the backup originated.    The restore operation copies the files back to their original location. All the disk stores including users created one and disk stores for metadata are also restored.", 
            "title": "Restoring your Backup"
        }, 
        {
            "location": "/reference/command_line_utilities/store-backup/#verify-the-backup-is-successful", 
            "text": "To ensure that your backup is successful, you can try the following options:    Execute the  select count(*) from  TableName ;  query and verify the total number of rows.    Verify the table details in the  Snappy Pulse UI .    If you have done updates, you can verify to see if those specific updates are available.", 
            "title": "Verify the Backup is Successful"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-all-disk-stores/", 
            "text": "compact-all-disk-stores\n\n\nPerform online compaction of SnappyData disk stores.\n\n\nSyntax\n\n\nFor secured cluster\n\n\n./bin/snappy compact-all-disk-stores -locators==\naddresses\n -auth-provider=\nauthprovider\n -user=\nusername\n -password=\npassword\n -gemfirexd.auth-ldap-search-base=\nsearch-base-values\n -gemfirexd.auth-ldap-search-dn=\nsearch-dn-values\n -gemfirexd.auth-ldap-search-pw=\npassword\n\n\n\n\n\nFor non-secured cluster\n\n\n./bin/snappy compact-all-disk-stores==\n  \n-locators=\naddresses\n [-bind-address=\naddress\n] [-\nprop-name\n=\nprop-value\n]*\n\n\n\n\nThe table describes options for \nsnappy compact-all-disk-stores\n. \n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the \npeer-discovery-port\n used when starting the cluster (default 10334). This is a mandatory field.\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default \nsnappy\n uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-prop-name\n prop-value\n\n\nAny other SnappyData distributed system property.\n\n\n\n\n\n\nAuthentication properties\n\n\nRefer \nAuthentication Properites\n.\n\n\n\n\n\n\n\n\nDescription\n\n\nWhen a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files.\n\n\nManual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains.\n\n\nOffline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started.\n\n\nOnline Compaction\n\n\nTo run manual online compaction, ALLOWFORCECOMPACTION should be set to true while \ncreating a diskstore\n\nYou can run manual online compaction at any time while the system is running. Oplogs that are eligible for compaction, based on the COMPACTIONTHRESHOLD, are compacted into the current oplog.\n\n\nExample\n\n\nSecured cluster\n\n\n./bin/snappy compact-all-disk-stores -locators=locatorhostname:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123\n\n// The following output is displayed:\n\nConnecting to distributed system: locators=localhost[10334]\n18/11/15 17:54:02.964 IST main\ntid=0x1\n INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/15 17:54:03.757 IST main\ntid=0x1\n INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@11a82d0f configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nCompaction complete.\nThe following disk stores compacted some files:\n\n\n\n\n\nNon-secured cluster\n\n\n./bin/snappy compact-all-disk-stores==locators=locatorhostname:10334*", 
            "title": "compact-all-disk-stores"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-all-disk-stores/#compact-all-disk-stores", 
            "text": "Perform online compaction of SnappyData disk stores.", 
            "title": "compact-all-disk-stores"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-all-disk-stores/#syntax", 
            "text": "For secured cluster  ./bin/snappy compact-all-disk-stores -locators== addresses  -auth-provider= authprovider  -user= username  -password= password  -gemfirexd.auth-ldap-search-base= search-base-values  -gemfirexd.auth-ldap-search-dn= search-dn-values  -gemfirexd.auth-ldap-search-pw= password   For non-secured cluster  ./bin/snappy compact-all-disk-stores==\n   -locators= addresses  [-bind-address= address ] [- prop-name = prop-value ]*  The table describes options for  snappy compact-all-disk-stores .      Option  Description      -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the  peer-discovery-port  used when starting the cluster (default 10334). This is a mandatory field.    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default  snappy  uses the hostname, or localhost if the hostname points to a local loopback address.    -prop-name  prop-value  Any other SnappyData distributed system property.    Authentication properties  Refer  Authentication Properites .", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-all-disk-stores/#description", 
            "text": "When a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files.  Manual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains.  Offline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-all-disk-stores/#online-compaction", 
            "text": "To run manual online compaction, ALLOWFORCECOMPACTION should be set to true while  creating a diskstore \nYou can run manual online compaction at any time while the system is running. Oplogs that are eligible for compaction, based on the COMPACTIONTHRESHOLD, are compacted into the current oplog.", 
            "title": "Online Compaction"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-all-disk-stores/#example", 
            "text": "Secured cluster  ./bin/snappy compact-all-disk-stores -locators=locatorhostname:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123\n\n// The following output is displayed:\n\nConnecting to distributed system: locators=localhost[10334]\n18/11/15 17:54:02.964 IST main tid=0x1  INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/15 17:54:03.757 IST main tid=0x1  INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@11a82d0f configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nCompaction complete.\nThe following disk stores compacted some files:  Non-secured cluster  ./bin/snappy compact-all-disk-stores==locators=locatorhostname:10334*", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-disk-store/", 
            "text": "compact-disk-store\n\n\nPerform offline compaction of a single SnappyData disk store.\n\n\nSyntax\n\n\n./bin/snappy compact-disk-store \ndiskStoreName\n \ndirectory\n+ [-maxOplogSize=\nint\n]\n\n\n\n\nDescription\n\n\n\n\nNote\n\n\nDo not perform offline compaction on the baseline directory of an incremental backup.\n\n\n\n\nWhen a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files.\n\n\nManual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains.\n\n\nOffline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started.\n\n\n\n\nNote\n\n\nYou must provide all of the directories in the disk store. If no oplog max size is specified, SnappyData uses the system default.\n Offline compaction can consume a large amount of memory. If you get a java.lang.OutOfMemory error while running this command, you made need to increase the heap size by setting the \n-Xmx\n and \n-Xms\n options in the JAVA_ARGS environment variable. \nCommand Line Utilites\n provides more information about setting Java options.\n\n\n\n\n\n\n\nExample\n\n\n./bin/snappy compact-disk-store myDiskStoreName  /firstDir  /secondDir   \nmaxOplogSize=maxMegabytesForOplog\n\n\n\n\nThe output of this command is similar to:\n\n\nOffline compaction removed 12 records.\nTotal number of region entries in this disk store is: 7", 
            "title": "compact-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-disk-store/#compact-disk-store", 
            "text": "Perform offline compaction of a single SnappyData disk store.", 
            "title": "compact-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-disk-store/#syntax", 
            "text": "./bin/snappy compact-disk-store  diskStoreName   directory + [-maxOplogSize= int ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-disk-store/#description", 
            "text": "Note  Do not perform offline compaction on the baseline directory of an incremental backup.   When a CRUD operation is performed on a persistent/overflow table, the data is written to the log files. Any pre-existing operation record for the same row becomes obsolete, and SnappyData marks it as garbage. It compacts an old operation log by copying all non-garbage records into the current log and discarding the old files.  Manual compaction can be done for online and offline disk stores. For the online disk store, the current operation log is not available for compaction, no matter how much garbage it contains.  Offline compaction runs in the same way, but without the incoming CRUD operations. Also, because there is no current open log, the compaction creates a new one to get started.   Note  You must provide all of the directories in the disk store. If no oplog max size is specified, SnappyData uses the system default.  Offline compaction can consume a large amount of memory. If you get a java.lang.OutOfMemory error while running this command, you made need to increase the heap size by setting the  -Xmx  and  -Xms  options in the JAVA_ARGS environment variable.  Command Line Utilites  provides more information about setting Java options.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/command_line_utilities/store-compact-disk-store/#example", 
            "text": "./bin/snappy compact-disk-store myDiskStoreName  /firstDir  /secondDir   \nmaxOplogSize=maxMegabytesForOplog  The output of this command is similar to:  Offline compaction removed 12 records.\nTotal number of region entries in this disk store is: 7", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/validate_diskstore/", 
            "text": "validate-disk-store\n\n\nVerifies the health of an offline disk store and provides information about the tables using that disk store.\n\n\nSyntax\n\n\n./bin/snappypy validate-disk-store \ndiskStoreName\n \ndirectory\n+\n\n\n\n\nIn the syntax, you must provide the name of disk store to be validated and the path which stores the disk store files.\n\n\nDescription\n\n\nThe SnappyData \nvalidate-disk-store \ncommand verifies the health of your offline disk store and gives you information about the following:\n\n   Tables that are using that disk store\n\n   Total number of rows \n*   Number of records that would be removed, if you have compacted the store.\n\n\nYou can use this command:\n\n\n\n\nBefore compacting an offline disk store, to determine whether it is worthwhile.\n\n\nBefore restoring a disk store.\n\n\nAny time you want to ensure the disk store is in good shape.\n\n\n\n\nExample\n\n\n ./bin/snappy validate-disk-store GFXD-DEFAULT-DISKSTORE /home/xyz/\nsnappydata_install_dir\n/work/localhost-server-1\n\n\n\n\nThis command displays an output  as shown in the following example:\n\n\nlog4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n/__UUID_PERSIST: entryCount=42\n/APP/SNAPPYSYS_INTERNAL____AIRLINE_COLUMN_STORE_ entryCount=0 bucketCount=8\n/partitioned_region entryCount=6 bucketCount=10\nDisk store contains 1 compactable records.\nTotal number of region entries in this disk store is: 6", 
            "title": "validate-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/validate_diskstore/#validate-disk-store", 
            "text": "Verifies the health of an offline disk store and provides information about the tables using that disk store.", 
            "title": "validate-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/validate_diskstore/#syntax", 
            "text": "./bin/snappypy validate-disk-store  diskStoreName   directory +  In the syntax, you must provide the name of disk store to be validated and the path which stores the disk store files.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/validate_diskstore/#description", 
            "text": "The SnappyData  validate-disk-store  command verifies the health of your offline disk store and gives you information about the following:    Tables that are using that disk store    Total number of rows \n*   Number of records that would be removed, if you have compacted the store.  You can use this command:   Before compacting an offline disk store, to determine whether it is worthwhile.  Before restoring a disk store.  Any time you want to ensure the disk store is in good shape.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/command_line_utilities/validate_diskstore/#example", 
            "text": "./bin/snappy validate-disk-store GFXD-DEFAULT-DISKSTORE /home/xyz/ snappydata_install_dir /work/localhost-server-1  This command displays an output  as shown in the following example:  log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n/__UUID_PERSIST: entryCount=42\n/APP/SNAPPYSYS_INTERNAL____AIRLINE_COLUMN_STORE_ entryCount=0 bucketCount=8\n/partitioned_region entryCount=6 bucketCount=10\nDisk store contains 1 compactable records.\nTotal number of region entries in this disk store is: 6", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/", 
            "text": "list-missing-disk-stores\n\n\nLists all disk stores with the most recent data for which other members are waiting.\n\n\nSyntax\n\n\nSecured cluster\n\n\n./bin/snappy list-missing-disk-stores -locators=\naddresses\n -auth-provider=\nauth-provider\n -user=\nusername\n -password=\npassword\n -gemfirexd.auth-ldap-server=ldap://\nldap-server-host\n:\nldap-server-port\n/  -gemfirexd.auth-ldap-search-base=\nsearch-base-values\n -gemfirexd.auth-ldap-search-dn=\nsearch-dn-values\n -gemfirexd.auth-ldap-search-pw=\npassword\n\n\n\n\n\nNon-secured cluster\n\n\n./bin/snappy list-missing-disk-stores -locators=localhost:bind address\n\n\n\n\nIf no locator option is specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.\n\n\nThe table describes options for snappy list-missing-disk-stores.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the \npeer-discovery-port\n used when starting the cluster (default 10334). This is a mandatory field.\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-prop-name\n\n\nAny other SnappyData distributed system property.\n\n\n\n\n\n\nAuthentication properties\n\n\nRefer \nAuthentication Properites\n.\n\n\n\n\n\n\n\n\nExample\n\n\nSecured cluster\n\n\n./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123\n\nConnecting to distributed system: locators=localhost[10334]\n18/11/15 18:08:26.802 IST main\ntid=0x1\n INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/15 18:08:27.575 IST main\ntid=0x1\n INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nThe distributed system did not have any missing disk stores\n\n\n\n\nNon-secured Cluster\n\n\n./bin/snappy list-missing-disk-stores -locators=localhost:10334\n\nConnecting to distributed system: locators=localhost[10334]\nlog4j:WARN No appenders could be found for logger (com.gemstone.org.jgroups.util.GemFireTracer).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n18/11/19 14:08:33 INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@33a053d configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nThe distributed system did not have any missing disk stores", 
            "title": "list-missing-disk-stores"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/#list-missing-disk-stores", 
            "text": "Lists all disk stores with the most recent data for which other members are waiting.", 
            "title": "list-missing-disk-stores"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/#syntax", 
            "text": "Secured cluster  ./bin/snappy list-missing-disk-stores -locators= addresses  -auth-provider= auth-provider  -user= username  -password= password  -gemfirexd.auth-ldap-server=ldap:// ldap-server-host : ldap-server-port /  -gemfirexd.auth-ldap-search-base= search-base-values  -gemfirexd.auth-ldap-search-dn= search-dn-values  -gemfirexd.auth-ldap-search-pw= password   Non-secured cluster  ./bin/snappy list-missing-disk-stores -locators=localhost:bind address  If no locator option is specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.  The table describes options for snappy list-missing-disk-stores.     Option  Description      -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the  peer-discovery-port  used when starting the cluster (default 10334). This is a mandatory field.    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.    -prop-name  Any other SnappyData distributed system property.    Authentication properties  Refer  Authentication Properites .", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-list-missing-disk-stores/#example", 
            "text": "Secured cluster  ./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123\n\nConnecting to distributed system: locators=localhost[10334]\n18/11/15 18:08:26.802 IST main tid=0x1  INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/15 18:08:27.575 IST main tid=0x1  INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nThe distributed system did not have any missing disk stores  Non-secured Cluster  ./bin/snappy list-missing-disk-stores -locators=localhost:10334\n\nConnecting to distributed system: locators=localhost[10334]\nlog4j:WARN No appenders could be found for logger (com.gemstone.org.jgroups.util.GemFireTracer).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n18/11/19 14:08:33 INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@33a053d configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nThe distributed system did not have any missing disk stores", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/", 
            "text": "revoke-missing-disk-store\n\n\nInstruct SnappyData members to stop waiting for a disk store to become available.\n\n\nSyntax\n\n\nSecured cluster\n\n\n./bin/snappy revoke-missing-disk-store -locators=\naddresses\n -auth-provider=\nauth-provider\n -user=\nusername\n -password=\npassword\n -gemfirexd.auth-ldap-server=ldap://\nldap-server-host\n:\nldap-server-port\n/ -gemfirexd.auth-ldap-search-base=\nsearch-base-values\n -gemfirexd.auth-ldap-search-dn=\nsearch-dn-values\n -gemfirexd.auth-ldap-search-pw=\npassword\n\n\n\n\n\n\nNon-secured cluster\n\n\n./bin/snappy revoke-missing-disk-store \ndisk-store-id\n\n   \n-locators=\naddresses\n \n        [-bind-address=\naddress\n] \n  [-\nprop-name\n=\nprop-value\n]*\n\n\n\n\nThe table describes options and arguments for snappy \nrevoke-missing-disk-store\n. If no multicast or locator options are specified on the command-line, the command uses the \ngemfirexd.properties\n file (if available) to determine the distributed system to which it should connect.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-disk-store-id\n\n\n(Required.) Specifies the unique ID of the disk store to revoke.\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the \npeer-discovery-port\n used when starting the cluster (default 10334). This is a mandatory field.\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-prop-name\n\n\nAny other SnappyData distributed system property.\n\n\n\n\n\n\nAuthentication properties\n\n\nRefer \nAuthentication Properites\n.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\nSecured cluster\n\n\nThe following example depicts how to revoke the missing disk stores in a secured cluster:\n\n\nUsing the following command, you must first list the missing disk stores:\n\n\n./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123\n\n\n\n\nNext, run the \nrevoke-missing-disk-store\n command to revoke the missing disk stores in case more recent data is available:\n\n\n./bin/snappy revoke-missing-disk-store -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://\nldap-server-host\n:389/ -user=\nusername\n -password=\npassword\n -gemfirexd.auth-ldap-search-base=\nsearch-base-values\n -gemfirexd.auth-ldap-search-dn=\nsearch-dn-values\n -gemfirexd.auth-ldap-search-pw=\npassword\n\n\nConnecting to distributed system: locators=localhost[10334]\n18/11/16 16:24:37.187 IST main\ntid=0x1\n INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/16 16:24:38.025 IST main\ntid=0x1\n INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nrevocation was successful and no disk stores are now missing\n\n\n\n\n\nFinally, you can use the same \nlist-missing-disk-stores\ncommand to confirm that no disk stores are missing.\n\n\nNon-secured cluster\n\n\nThe following example depicts how to revoke the missing disk stores in a non-secured cluster:\n\n\nUsing the following command, you must first list the missing disk stores:\n\n\n./bin/snappy list-missing-disk-stores -locators=localhost:10334\n\nConnecting to distributed system: -locators=localhost:10334\n1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary]\n\n\n\n\nNext, run the \nrevoke-missing-disk-store\n command to revoke the missing disk stores in case more recent data is available:\n\n\n./bin/snappy revoke-missing-disk-store 1f811502-f126-4ce4-9839-9549335b734d -locators=localhost:10334\n\nConnecting to distributed system: -locators=localhost:10334\nrevocation was successful and no disk stores are now missing\n\n\n\n\nFinally, use the \nlist-missing-disk-stores\n command to confirm that none of the disk stores are missing.", 
            "title": "revoke-missing-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/#revoke-missing-disk-store", 
            "text": "Instruct SnappyData members to stop waiting for a disk store to become available.", 
            "title": "revoke-missing-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/#syntax", 
            "text": "Secured cluster  ./bin/snappy revoke-missing-disk-store -locators= addresses  -auth-provider= auth-provider  -user= username  -password= password  -gemfirexd.auth-ldap-server=ldap:// ldap-server-host : ldap-server-port / -gemfirexd.auth-ldap-search-base= search-base-values  -gemfirexd.auth-ldap-search-dn= search-dn-values  -gemfirexd.auth-ldap-search-pw= password   Non-secured cluster  ./bin/snappy revoke-missing-disk-store  disk-store-id \n    -locators= addresses  \n        [-bind-address= address ] \n  [- prop-name = prop-value ]*  The table describes options and arguments for snappy  revoke-missing-disk-store . If no multicast or locator options are specified on the command-line, the command uses the  gemfirexd.properties  file (if available) to determine the distributed system to which it should connect.     Option  Description      -disk-store-id  (Required.) Specifies the unique ID of the disk store to revoke.    -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the  peer-discovery-port  used when starting the cluster (default 10334). This is a mandatory field.    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.    -prop-name  Any other SnappyData distributed system property.    Authentication properties  Refer  Authentication Properites .", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-revoke-missing-disk-stores/#example", 
            "text": "Secured cluster  The following example depicts how to revoke the missing disk stores in a secured cluster:  Using the following command, you must first list the missing disk stores:  ./bin/snappy list-missing-disk-stores -locators=localhost:10334 -auth-provider=LDAP -user=snappy1 -password=snappy1  -J-Dgemfirexd.auth-ldap-server=ldap://localhost:389/ -J-Dgemfirexd.auth-ldap-search-base=cn=sales-group,ou=sales,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-dn=cn=admin,dc=example,dc=com -J-Dgemfirexd.auth-ldap-search-pw=user123  Next, run the  revoke-missing-disk-store  command to revoke the missing disk stores in case more recent data is available:  ./bin/snappy revoke-missing-disk-store -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap:// ldap-server-host :389/ -user= username  -password= password  -gemfirexd.auth-ldap-search-base= search-base-values  -gemfirexd.auth-ldap-search-dn= search-dn-values  -gemfirexd.auth-ldap-search-pw= password \n\nConnecting to distributed system: locators=localhost[10334]\n18/11/16 16:24:37.187 IST main tid=0x1  INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/16 16:24:38.025 IST main tid=0x1  INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nrevocation was successful and no disk stores are now missing  Finally, you can use the same  list-missing-disk-stores command to confirm that no disk stores are missing.  Non-secured cluster  The following example depicts how to revoke the missing disk stores in a non-secured cluster:  Using the following command, you must first list the missing disk stores:  ./bin/snappy list-missing-disk-stores -locators=localhost:10334\n\nConnecting to distributed system: -locators=localhost:10334\n1f811502-f126-4ce4-9839-9549335b734d [curwen.local:/Users/user1/snappydata/rowstore/SnappyData_RowStore_13_bNNNNN_platform/server2/./datadictionary]  Next, run the  revoke-missing-disk-store  command to revoke the missing disk stores in case more recent data is available:  ./bin/snappy revoke-missing-disk-store 1f811502-f126-4ce4-9839-9549335b734d -locators=localhost:10334\n\nConnecting to distributed system: -locators=localhost:10334\nrevocation was successful and no disk stores are now missing  Finally, use the  list-missing-disk-stores  command to confirm that none of the disk stores are missing.", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-unblock-disk-stores/", 
            "text": "unblock-disk-store\n\n\nIndicates a member waiting for other diskStoreID to go ahead with the initialization. When a member recovers from a set of persistent files, it waits for other members that were also persisting the same region to start up. If the persistent files for those other members were lost or not available, this method can be used to tell the members to stop waiting for that data and consider its own data as latest.\n\n\nSyntax\n\n\nSecured cluster\n\n\n./bin/snappy unblock-disk-store\ndisk-store-id\n -locators=localhost:\naddresses\n  -auth-provider=\nauth-provider\n -user=\nusername\n -password=\npassword\n -gemfirexd.auth-ldap-server=ldap://\nldap-server-host\n:\nldap-server-port\n/ -gemfirexd.auth-ldap-search-base=\nsearch-base-values\n -gemfirexd.auth-ldap-search-dn=\nsearch-dn-values\n -gemfirexd.auth-ldap-search-pw=\npassword\n\n\n\n\n\nNon-secured cluster\n\n\n./bin/snappy unblock-disk-store \ndisk-store-id\n\n   \n-locators=\naddresses\n \n        [-bind-address=\naddress\n] \n  [-\nprop-name\n=\nprop-value\n]*\n\n\n\n\nThe table describes options and arguments for snappy unblock-disk-store. If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-disk-store-id\n\n\n(Required.) Specifies the unique ID of the disk store to unblock.\n\n\n\n\n\n\n-locators\n\n\nList of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the \npeer-discovery-port\n used when starting the cluster (default 10334). This is a mandatory field.\n\n\n\n\n\n\n-bind-address\n\n\nThe address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.\n\n\n\n\n\n\n-prop-name\n\n\nAny other SnappyData distributed system property.\n\n\n\n\n\n\nAuthentication properties\n\n\nRefer \nAuthentication Properites\n.\n\n\n\n\n\n\n\n\nExample\n\n\nSecured cluster\n\n\n./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap://\nldap-server-host\n:389/ -user=\nusername\n -password=\npassword\n -gemfirexd.auth-ldap-search-base=\nsearch-base-values\n -gemfirexd.auth-ldap-search-dn=\nsearch-dn-values\n -gemfirexd.auth-ldap-search-pw=\npassword\n\n\nConnecting to distributed system: locators=localhost[10334]\n18/11/16 16:26:56.050 IST main\ntid=0x1\n INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/16 16:26:56.863 IST main\ntid=0x1\n INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nUnblock was successful and no disk stores are now waiting\n\n\n\n\nNon-secured cluster\n\n\n./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\nUnblock was successful and no disk stores are now waiting", 
            "title": "unblock-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-unblock-disk-stores/#unblock-disk-store", 
            "text": "Indicates a member waiting for other diskStoreID to go ahead with the initialization. When a member recovers from a set of persistent files, it waits for other members that were also persisting the same region to start up. If the persistent files for those other members were lost or not available, this method can be used to tell the members to stop waiting for that data and consider its own data as latest.", 
            "title": "unblock-disk-store"
        }, 
        {
            "location": "/reference/command_line_utilities/store-unblock-disk-stores/#syntax", 
            "text": "Secured cluster  ./bin/snappy unblock-disk-store disk-store-id  -locators=localhost: addresses   -auth-provider= auth-provider  -user= username  -password= password  -gemfirexd.auth-ldap-server=ldap:// ldap-server-host : ldap-server-port / -gemfirexd.auth-ldap-search-base= search-base-values  -gemfirexd.auth-ldap-search-dn= search-dn-values  -gemfirexd.auth-ldap-search-pw= password   Non-secured cluster  ./bin/snappy unblock-disk-store  disk-store-id \n    -locators= addresses  \n        [-bind-address= address ] \n  [- prop-name = prop-value ]*  The table describes options and arguments for snappy unblock-disk-store. If no multicast or locator options are specified on the command-line, the command uses the gemfirexd.properties file (if available) to determine the distributed system to which it should connect.     Option  Description      -disk-store-id  (Required.) Specifies the unique ID of the disk store to unblock.    -locators  List of locators used to discover members of the distributed system. Supply all locators as comma-separated host:port values. The port is the  peer-discovery-port  used when starting the cluster (default 10334). This is a mandatory field.    -bind-address  The address to which this peer binds for receiving peer-to-peer messages. By default SnappyData uses the hostname, or localhost if the hostname points to a local loopback address.    -prop-name  Any other SnappyData distributed system property.    Authentication properties  Refer  Authentication Properites .", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-unblock-disk-stores/#example", 
            "text": "Secured cluster  ./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334 -auth-provider=LDAP -gemfirexd.auth-ldap-server=ldap:// ldap-server-host :389/ -user= username  -password= password  -gemfirexd.auth-ldap-search-base= search-base-values  -gemfirexd.auth-ldap-search-dn= search-dn-values  -gemfirexd.auth-ldap-search-pw= password \n\nConnecting to distributed system: locators=localhost[10334]\n18/11/16 16:26:56.050 IST main tid=0x1  INFO SNAPPY: TraceAuthentication: Enabling authorization for auth provider LDAP\n18/11/16 16:26:56.863 IST main tid=0x1  INFO SnappyUnifiedMemoryManager: BootTimeMemoryManager org.apache.spark.memory.SnappyUnifiedMemoryManager@16943e88 configuration:\n        Total Usable Heap = 786.2 MB (824374722)\n        Storage Pool = 393.1 MB (412187361)\n        Execution Pool = 393.1 MB (412187361)\n        Max Storage Pool Size = 628.9 MB (659499777)\nUnblock was successful and no disk stores are now waiting  Non-secured cluster  ./bin/snappy unblock-disk-store a395f237-c5e5-4e76-8024-353272e86f28 -locators=localhost:10334\nConnecting to distributed system: -locators=localhost:10334\nUnblock was successful and no disk stores are now waiting", 
            "title": "Example"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/", 
            "text": "run\n\n\nConnects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell.\n\n\nSyntax\n\n\n./bin/snappy run -file=\npath or URL\n\n     [-auth-provider=\nname\n]\n     [-client-bind-address=\naddress\n]\n     [-client-port=\nport\n]\n     [-encoding=\ncharset\n]\n     [-extra-conn-props=\nproperties\n] \n     [-help] \n     [-ignore-errors]\n     [-J-D\nproperty=value\n]\n     [-password[=\npassword\n]]\n     [-path=\npath\n]\n     [-user=\nusername\n]\n\n\n\n\nThis table describes options for the \nsnappy run\n command. Default values are used if you do not specify an option.\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-file\n\n\nThe local path of a SQL command file to execute, or a URL that links to the SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell.\nThis argument is required.\n\n\n\n\n\n\n-auth-provider\n\n\nSets the authentication provider to use for peer-to-peer connections as well as client-server connections. Valid values are BUILTIN and LDAP. All other members of the SnappyData distributed system must use the same authentication provider and user definitions. If you omit this option, the connection uses no authentication mechanism.\n\n\n\n\n\n\n-client-bind-address\n\n\nSet the hostname or IP address to which the locator or server listens on for JDBC/ODBC/thrift client connections.\n\n\n\n\n\n\n-client-port\n\n\nThe port on which a SnappyData locator listens for client connections. The default is 1527.\nUse this option with \n-client-bind-address\n to attach to a SnappyData cluster as a thin client and perform the command.\n\n\n\n\n\n\n-encoding\n\n\nThe character set encoding of the SQL script file (\n-file\n argument). The default is UTF-8. Other possible values are: US-ASCII, ISO-8859-1, UTF-8, UTF-16BE, UTF-16LE, UTF-16. See the \njava.nio.charset.Charset\n reference for more information.\n\n\n\n\n\n\n-extra-conn-props\n\n\nA semicolon-separated list of properties to use when connecting to the SnappyData distributed system.\n\n\n\n\n\n\nhelp, --help\n\n\nDisplay the help message for this snappy command.\n\n\n\n\n\n\n-ignore-errors\n\n\nInclude this option to ignore any errors that may occur while executing statements in the file, and continue executing the remaining statements. If you omit this option, then snappy immediately terminates the script's execution if an exception occurs.\n\n\n\n\n\n\n-J-D;property=value;\n\n\nSets Java system property to the specified value.\n\n\n\n\n\n\n-password\n\n\nIf the servers or locators have been configured to use authentication, this option specifies the password for the user (specified with the -user option) to use for booting the server and joining the distributed system.\nThe password value is optional. If you omit the password, you are prompted to enter a password from the console.\n\n\n\n\n\n\n-path\n\n\nConfigures the working directory for any other SQL command files executed from within the script. The \n-path\n entry is prepended to any SQL script file name executed that the script executes in a \nrun\n command.\n\n\n\n\n\n\n-user\n\n\nIf the servers or locators have been configured to use authentication, this option specifies the username to use for booting the server and joining the distributed system.\n\n\n\n\n\n\n\n\nDescription\n\n\nSpecify the below command to connect to a SnappyData Distributed system and execute a SQL command file:\n\n\nUse both \n-client-bind-address\n and \n-client-port\n to connect to a SnappyData cluster as a thin client and perform the command.\n\n\nThe \n-file\n argument specifies the location of the SQL script file to execute. If the script file itself calls other script files using \nrun 'filename'\n, also consider using the \n-path\n option to specify the location of the embedded script files. If an exception occurs while executing the script, SnappyData immediately stops executing script commands, unless you include the \n-ignore-errors\n option.\n\n\nExamples\n\n\nThis command connects to a SnappyData network server running on localhost:1527 and executes commands in the create_and_load_column_table.sql file:\n\n\n./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 \n\n\n\n\nIf the script calls for dependent scripts (for example load_countries.sql, load_cities.sql) and if the command is executed outside the directory in which the dependent scripts are located, specify the working directory using the \n-path\n option.\n\n\n./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -path=/home/user1/snappydata/examples/quickstart -client-bind-address=localhost -client-port=1527\n\n\n\n\nYou can also run the command by providing the username and password.\n\n\n./bin/snappy run -file=/home/supriya/snappy/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 -user=user1 -password=user123", 
            "title": "run"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#run", 
            "text": "Connects to a SnappyData distributed system and executes the contents of a SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell.", 
            "title": "run"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#syntax", 
            "text": "./bin/snappy run -file= path or URL \n     [-auth-provider= name ]\n     [-client-bind-address= address ]\n     [-client-port= port ]\n     [-encoding= charset ]\n     [-extra-conn-props= properties ] \n     [-help] \n     [-ignore-errors]\n     [-J-D property=value ]\n     [-password[= password ]]\n     [-path= path ]\n     [-user= username ]  This table describes options for the  snappy run  command. Default values are used if you do not specify an option.     Option  Description      -file  The local path of a SQL command file to execute, or a URL that links to the SQL command file. All commands in the specified file must be compatible with the interactive snappy SQL shell. This argument is required.    -auth-provider  Sets the authentication provider to use for peer-to-peer connections as well as client-server connections. Valid values are BUILTIN and LDAP. All other members of the SnappyData distributed system must use the same authentication provider and user definitions. If you omit this option, the connection uses no authentication mechanism.    -client-bind-address  Set the hostname or IP address to which the locator or server listens on for JDBC/ODBC/thrift client connections.    -client-port  The port on which a SnappyData locator listens for client connections. The default is 1527. Use this option with  -client-bind-address  to attach to a SnappyData cluster as a thin client and perform the command.    -encoding  The character set encoding of the SQL script file ( -file  argument). The default is UTF-8. Other possible values are: US-ASCII, ISO-8859-1, UTF-8, UTF-16BE, UTF-16LE, UTF-16. See the  java.nio.charset.Charset  reference for more information.    -extra-conn-props  A semicolon-separated list of properties to use when connecting to the SnappyData distributed system.    help, --help  Display the help message for this snappy command.    -ignore-errors  Include this option to ignore any errors that may occur while executing statements in the file, and continue executing the remaining statements. If you omit this option, then snappy immediately terminates the script's execution if an exception occurs.    -J-D;property=value;  Sets Java system property to the specified value.    -password  If the servers or locators have been configured to use authentication, this option specifies the password for the user (specified with the -user option) to use for booting the server and joining the distributed system. The password value is optional. If you omit the password, you are prompted to enter a password from the console.    -path  Configures the working directory for any other SQL command files executed from within the script. The  -path  entry is prepended to any SQL script file name executed that the script executes in a  run  command.    -user  If the servers or locators have been configured to use authentication, this option specifies the username to use for booting the server and joining the distributed system.", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#description", 
            "text": "Specify the below command to connect to a SnappyData Distributed system and execute a SQL command file:  Use both  -client-bind-address  and  -client-port  to connect to a SnappyData cluster as a thin client and perform the command.  The  -file  argument specifies the location of the SQL script file to execute. If the script file itself calls other script files using  run 'filename' , also consider using the  -path  option to specify the location of the embedded script files. If an exception occurs while executing the script, SnappyData immediately stops executing script commands, unless you include the  -ignore-errors  option.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/command_line_utilities/store-run/#examples", 
            "text": "This command connects to a SnappyData network server running on localhost:1527 and executes commands in the create_and_load_column_table.sql file:  ./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527   If the script calls for dependent scripts (for example load_countries.sql, load_cities.sql) and if the command is executed outside the directory in which the dependent scripts are located, specify the working directory using the  -path  option.  ./bin/snappy run -file=/home/user1/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -path=/home/user1/snappydata/examples/quickstart -client-bind-address=localhost -client-port=1527  You can also run the command by providing the username and password.  ./bin/snappy run -file=/home/supriya/snappy/snappydata/examples/quickstart/scripts/create_and_load_column_table.sql -client-bind-address=localhost -client-port=1527 -user=user1 -password=user123", 
            "title": "Examples"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/", 
            "text": "version\n\n\nPrints information about the SnappyData product version.\n\n\nSyntax\n\n\n./bin/snappy version\n\n\n\n\nExample\n\n\n./bin/snappy version\nSnappyData Platform Version 1.0.2.1\n    SnappyData RowStore 1.6.2.1\n    SnappyData Column Store 1.0.2.1", 
            "title": "version"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/#version", 
            "text": "Prints information about the SnappyData product version.", 
            "title": "version"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/#syntax", 
            "text": "./bin/snappy version", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/command_line_utilities/store-version/#example", 
            "text": "./bin/snappy version\nSnappyData Platform Version 1.0.2.1\n    SnappyData RowStore 1.6.2.1\n    SnappyData Column Store 1.0.2.1", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/store_command_reference/", 
            "text": "Snappy-SQL Shell Interactive Commands\n\n\nsnappy\n implements an interactive command-line tool that is based on the Apache Derby \nij\n tool. Use \nsnappy\n to run scripts or interactive queries against a SnappyData cluster.\n\n\nStart the interactive \nsnappy\n command prompt by using the snappy script without supplying any other options:\n\n\nsnappy\n\n\n\n\nThe system property \nsnappy.history\n specifies a file in which to store all of the commands executed during an interactive \nsnappy\n session. For example:\n\n\n$ export JAVA_ARGS=\n-Dsnappy.history=/Users/user1/snappydata-history.sql\n\n$ snappy\n\n\n\n\nBy default the history file is named .snappy.history, and it is stored in the current user's home directory.\n\n\nsnappy\n accepts several commands to control its use of JDBC. It recognizes a semicolon as the end of a \nsnappy\n or SQL command. It treats semicolons within SQL comments, strings, and delimited identifiers as part of those constructs and not as the end of the command. Semicolons are required at the end of a \nsnappy\n or SQL statement.\n\n\nAll \nsnappy\n commands, identifiers, and keywords are case-insensitive.\n\n\nCommands can span multiple lines without using any special escape character for ends of lines. This means that if a string spans a line, the new line contents show up in the value in the string.\n\n\nsnappy\n treats any command that it does not recognize as a SQL command that is passed to the underlying connection. This means that any syntactic errors in \nsnappy\n commands are handed to the SQL engine and generally result in SQL parsing errors.\n\n\n\n\n\n\nautocommit\n\n    Turns the connection's auto-commit mode on or off.\n\n\n\n\n\n\ncommit\n\n    Issues a \njava.sql.Connection.commit\n request.\n\n\n\n\n\n\nconnect client\n\n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the \nhost:port\n values.\n\n\n\n\n\n\nconnect\n\n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the \nhost:port\n values.\n\n\n\n\n\n\ndescribe\n\n    Provides a description of the specified table or view.\n\n\n\n\n\n\ndisconnect\n\n    Disconnects from the database.\n\n\n\n\n\n\nelapsedtime\n\n    Displays the total time elapsed during statement execution.\n\n\n\n\n\n\nexit\n\n    Completes the \nsnappy\n application and halts processing.\n\n\n\n\n\n\nMaximumDisplayWidth\n\n    Sets the largest display width for columns to the specified value.\n\n\n\n\n\n\nrollback\n\n    Issues a \njava.sql.Connection.rollback\n request.\n\n\n\n\n\n\nrun\n\n    Treats the value of the string as a valid file name, and redirects \nsnappy\n processing to read from that file until it ends or an exit command is executed.\n\n\n\n\n\n\nset connection\n\n    Specifies which connection to make current when more than one connection is open.\n\n\n\n\n\n\nshow\n\n    Displays information about active connections and database objects.", 
            "title": "Snappy-SQL Shell Interactive Commands"
        }, 
        {
            "location": "/reference/interactive_commands/store_command_reference/#snappy-sql-shell-interactive-commands", 
            "text": "snappy  implements an interactive command-line tool that is based on the Apache Derby  ij  tool. Use  snappy  to run scripts or interactive queries against a SnappyData cluster.  Start the interactive  snappy  command prompt by using the snappy script without supplying any other options:  snappy  The system property  snappy.history  specifies a file in which to store all of the commands executed during an interactive  snappy  session. For example:  $ export JAVA_ARGS= -Dsnappy.history=/Users/user1/snappydata-history.sql \n$ snappy  By default the history file is named .snappy.history, and it is stored in the current user's home directory.  snappy  accepts several commands to control its use of JDBC. It recognizes a semicolon as the end of a  snappy  or SQL command. It treats semicolons within SQL comments, strings, and delimited identifiers as part of those constructs and not as the end of the command. Semicolons are required at the end of a  snappy  or SQL statement.  All  snappy  commands, identifiers, and keywords are case-insensitive.  Commands can span multiple lines without using any special escape character for ends of lines. This means that if a string spans a line, the new line contents show up in the value in the string.  snappy  treats any command that it does not recognize as a SQL command that is passed to the underlying connection. This means that any syntactic errors in  snappy  commands are handed to the SQL engine and generally result in SQL parsing errors.    autocommit \n    Turns the connection's auto-commit mode on or off.    commit \n    Issues a  java.sql.Connection.commit  request.    connect client \n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the  host:port  values.    connect \n    Using the JDBC SnappyData thin client driver, connects to a SnappyData member indicated by the  host:port  values.    describe \n    Provides a description of the specified table or view.    disconnect \n    Disconnects from the database.    elapsedtime \n    Displays the total time elapsed during statement execution.    exit \n    Completes the  snappy  application and halts processing.    MaximumDisplayWidth \n    Sets the largest display width for columns to the specified value.    rollback \n    Issues a  java.sql.Connection.rollback  request.    run \n    Treats the value of the string as a valid file name, and redirects  snappy  processing to read from that file until it ends or an exit command is executed.    set connection \n    Specifies which connection to make current when more than one connection is open.    show \n    Displays information about active connections and database objects.", 
            "title": "Snappy-SQL Shell Interactive Commands"
        }, 
        {
            "location": "/reference/interactive_commands/autocommit/", 
            "text": "autocommit\n\n\nSyntax\n\n\nAUTOCOMMIT { ON | OFF }\n\n\n\n\n\n\nDescription\n\n\nTurns the connection's auto-commit mode on or off. JDBC specifies that the default auto-commit mode is \nON\n. Certain types of processing require that auto-commit mode be \nOFF\n.\n\n\nIf auto-commit mode is changed from \noff\n to \non\n when a transaction is outstanding, that work is committed when the current transaction commits, not at the time auto-commit is turned on. Use \nCommit\n or \nRollback\n before turning on auto-commit when there is a transaction outstanding, so that all prior work is completed before the return to auto-commit mode.\n\n\nExample\n\n\nsnappy\n AUTOCOMMIT off;\nsnappy\n INSERT INTO greetings values (DEFAULT, 'hello');\n1 row inserted/updated/deleted\nsnappy\n COMMIT;", 
            "title": "autocommit"
        }, 
        {
            "location": "/reference/interactive_commands/autocommit/#autocommit", 
            "text": "", 
            "title": "autocommit"
        }, 
        {
            "location": "/reference/interactive_commands/autocommit/#syntax", 
            "text": "AUTOCOMMIT { ON | OFF }", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/autocommit/#description", 
            "text": "Turns the connection's auto-commit mode on or off. JDBC specifies that the default auto-commit mode is  ON . Certain types of processing require that auto-commit mode be  OFF .  If auto-commit mode is changed from  off  to  on  when a transaction is outstanding, that work is committed when the current transaction commits, not at the time auto-commit is turned on. Use  Commit  or  Rollback  before turning on auto-commit when there is a transaction outstanding, so that all prior work is completed before the return to auto-commit mode.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/autocommit/#example", 
            "text": "snappy  AUTOCOMMIT off;\nsnappy  INSERT INTO greetings values (DEFAULT, 'hello');\n1 row inserted/updated/deleted\nsnappy  COMMIT;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/commit/", 
            "text": "commit\n\n\nSyntax\n\n\nCOMMIT\n\n\n\n\n\n\nDescription\n\n\nIssues a \njava.sql.Connection.commit\n request. Use this command only if auto-commit is \noff\n. A \njava.sql.Connection.commit\n request commits the currently active transaction and initiates a new transaction.\n\n\nExample\n\n\nsnappy\n AUTOCOMMIT off;\nsnappy\n INSERT INTO greetings values (DEFAULT, 'hello');\n1 row inserted/updated/deleted\nsnappy\n COMMIT;", 
            "title": "commit"
        }, 
        {
            "location": "/reference/interactive_commands/commit/#commit", 
            "text": "", 
            "title": "commit"
        }, 
        {
            "location": "/reference/interactive_commands/commit/#syntax", 
            "text": "COMMIT", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/commit/#description", 
            "text": "Issues a  java.sql.Connection.commit  request. Use this command only if auto-commit is  off . A  java.sql.Connection.commit  request commits the currently active transaction and initiates a new transaction.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/commit/#example", 
            "text": "snappy  AUTOCOMMIT off;\nsnappy  INSERT INTO greetings values (DEFAULT, 'hello');\n1 row inserted/updated/deleted\nsnappy  COMMIT;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/", 
            "text": "connect client\n\n\nSyntax\n\n\nCONNECT CLIENT 'host:port[;property=value]*' [ AS connectionName ]\n\n\n\n\nDescription\n\n\nUses the JDBC SnappyData thin client driver to connect to a SnappyData member indicated by the \nhost:port\n values. You can specify an optional name for your connection. Use the \nset connection\n to switch between multiple connections. If you do not name a connection, the system generates a name automatically.\n\n\nIf the connection requires a user name and password, supply those with the optional properties.\n\n\nIf the connect succeeds, the connection becomes the current one and \nsnappy\n displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.\n\n\nAll further commands are processed against the new, current connection.\n\n\nExample\n\n\nSnappyData version 1.0.2.1 \nsnappy\n connect client 'localhost:1527' as clientConnection;\nsnappy\n show connections;\nCLIENTCONNECTION* -     jdbc:snappydata:thrift://localhost[1527]\n* = current connection", 
            "title": "connect client"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#connect-client", 
            "text": "", 
            "title": "connect client"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#syntax", 
            "text": "CONNECT CLIENT 'host:port[;property=value]*' [ AS connectionName ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#description", 
            "text": "Uses the JDBC SnappyData thin client driver to connect to a SnappyData member indicated by the  host:port  values. You can specify an optional name for your connection. Use the  set connection  to switch between multiple connections. If you do not name a connection, the system generates a name automatically.  If the connection requires a user name and password, supply those with the optional properties.  If the connect succeeds, the connection becomes the current one and  snappy  displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.  All further commands are processed against the new, current connection.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/connect_client/#example", 
            "text": "SnappyData version 1.0.2.1 \nsnappy  connect client 'localhost:1527' as clientConnection;\nsnappy  show connections;\nCLIENTCONNECTION* -     jdbc:snappydata:thrift://localhost[1527]\n* = current connection", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/connect/", 
            "text": "connect\n\n\nConnects to the database indicated by the \nConnectionURLString\n.\n\n\nSyntax\n\n\nCONNECT ConnectionURLString [ PROTOCOL Identifier ]\n    [ AS Identifier ]\n\n\n\n\nDescription\n\n\nConnects to the database indicated by the \nConnectionURLString\n. You have the option of specifying a name for your connection. Use the \nSet Connection\n command to switch between connections. If you do not name a connection, the system generates a name automatically.\n\n\n\n\n\n\n\nNote\n\n\nIf the connection requires a user name and password, supply those in the connection URL string, as shown in the example. \n\n\n\n\nIf the connect succeeds, the connection becomes the current one and \nsnappy\n displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.\n\n\nAll further commands are processed against the new, current connection.\n\n\nExample\n\n\nsnappy\n protocol 'jdbc:derby:';\nsnappy\n connect '//armenia:29303/myDB;user=a;password=a' as db5Connection; \nsnappy\n show connections;\nDB5CONNECTION* -        jdbc:derby://armenia:29303/myDB\n* = current connection", 
            "title": "connect"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#connect", 
            "text": "Connects to the database indicated by the  ConnectionURLString .", 
            "title": "connect"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#syntax", 
            "text": "CONNECT ConnectionURLString [ PROTOCOL Identifier ]\n    [ AS Identifier ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#description", 
            "text": "Connects to the database indicated by the  ConnectionURLString . You have the option of specifying a name for your connection. Use the  Set Connection  command to switch between connections. If you do not name a connection, the system generates a name automatically.    Note  If the connection requires a user name and password, supply those in the connection URL string, as shown in the example.    If the connect succeeds, the connection becomes the current one and  snappy  displays a new prompt for the next command to be entered. If you have more than one open connection, the name of the connection appears in the prompt.  All further commands are processed against the new, current connection.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/connect/#example", 
            "text": "snappy  protocol 'jdbc:derby:';\nsnappy  connect '//armenia:29303/myDB;user=a;password=a' as db5Connection; \nsnappy  show connections;\nDB5CONNECTION* -        jdbc:derby://armenia:29303/myDB\n* = current connection", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/describe/", 
            "text": "describe\n\n\nProvides a description of the specified table or view.\n\n\nSyntax\n\n\nDESCRIBE { table-Name | view-Name }\n\n\n\n\nDescription\n\n\nProvides a description of the specified table or view. For a list of tables in the current schema, use the Show Tables command. For a list of views in the current schema, use the Show Views command. For a list of available schemas, use the Show Schemas command.\n\n\nIf the table or view is in a particular schema, qualify it with the schema name. If the table or view name is case-sensitive, enclose it in single quotes. You can display all the columns from all the tables and views in a single schema in a single display by using the wildcard character '*'. \n\n\nExample\n\n\nsnappy\n describe maps;\nCOLUMN_NAME         |TYPE_NAME|DEC\n|NUM\n|COLUM\n|COLUMN_DEF|CHAR_OCTE\n|IS_NULL\n\n------------------------------------------------------------------------------\nMAP_ID              |INTEGER  |0   |10  |10    |AUTOINCRE\n|NULL      |NO\nMAP_NAME            |VARCHAR  |NULL|NULL|24    |NULL      |48        |NO\nREGION              |VARCHAR  |NULL|NULL|26    |NULL      |52        |YES\nAREA                |DECIMAL  |4   |10  |8     |NULL      |NULL      |NO\nPHOTO_FORMAT        |VARCHAR  |NULL|NULL|26    |NULL      |52        |NO\nPICTURE             |BLOB     |NULL|NULL|102400|NULL      |NULL      |YES\n\n6 rows selected\nsnappy", 
            "title": "describe"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#describe", 
            "text": "Provides a description of the specified table or view.", 
            "title": "describe"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#syntax", 
            "text": "DESCRIBE { table-Name | view-Name }", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#description", 
            "text": "Provides a description of the specified table or view. For a list of tables in the current schema, use the Show Tables command. For a list of views in the current schema, use the Show Views command. For a list of available schemas, use the Show Schemas command.  If the table or view is in a particular schema, qualify it with the schema name. If the table or view name is case-sensitive, enclose it in single quotes. You can display all the columns from all the tables and views in a single schema in a single display by using the wildcard character '*'.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/describe/#example", 
            "text": "snappy  describe maps;\nCOLUMN_NAME         |TYPE_NAME|DEC |NUM |COLUM |COLUMN_DEF|CHAR_OCTE |IS_NULL \n------------------------------------------------------------------------------\nMAP_ID              |INTEGER  |0   |10  |10    |AUTOINCRE |NULL      |NO\nMAP_NAME            |VARCHAR  |NULL|NULL|24    |NULL      |48        |NO\nREGION              |VARCHAR  |NULL|NULL|26    |NULL      |52        |YES\nAREA                |DECIMAL  |4   |10  |8     |NULL      |NULL      |NO\nPHOTO_FORMAT        |VARCHAR  |NULL|NULL|26    |NULL      |52        |NO\nPICTURE             |BLOB     |NULL|NULL|102400|NULL      |NULL      |YES\n\n6 rows selected\nsnappy", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/", 
            "text": "disconnect\n\n\nDisconnects from the database.\n\n\nSyntax\n\n\nDISCONNECT [ ALL | CURRENT | ConnectionIdentifier ]\n\n\n\n\nDescription\n\n\nDisconnects from the database. Specifically, issues a \njava.sql.Connection.close\n request against the connection indicated on the command line. There must be a current connection at the time the request is made.\n\n\nIf ALL is specified, all known connections are closed and there will be no current connection.\n\n\nDisconnect CURRENT is the same as Disconnect without indicating a connection; the default connection is closed.\n\n\nIf a connection name is specified with an identifier, the command disconnects the named connection. The name must be the name of a connection in the current session provided with the \nConnect\n command.\n\n\nIf the \nConnect\n command without the AS clause was used, you can supply the name the system generated for the connection. If the current connection is the named connection when the command completes, there will be no current connection and you must issue a \nConnect\n command.\n\n\nExample\n\n\nsnappy\n DISCONNECT CONNECTION1;", 
            "title": "disconnect"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#disconnect", 
            "text": "Disconnects from the database.", 
            "title": "disconnect"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#syntax", 
            "text": "DISCONNECT [ ALL | CURRENT | ConnectionIdentifier ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#description", 
            "text": "Disconnects from the database. Specifically, issues a  java.sql.Connection.close  request against the connection indicated on the command line. There must be a current connection at the time the request is made.  If ALL is specified, all known connections are closed and there will be no current connection.  Disconnect CURRENT is the same as Disconnect without indicating a connection; the default connection is closed.  If a connection name is specified with an identifier, the command disconnects the named connection. The name must be the name of a connection in the current session provided with the  Connect  command.  If the  Connect  command without the AS clause was used, you can supply the name the system generated for the connection. If the current connection is the named connection when the command completes, there will be no current connection and you must issue a  Connect  command.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/disconnect/#example", 
            "text": "snappy  DISCONNECT CONNECTION1;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/", 
            "text": "elapsedtime\n\n\nDisplays the total time elapsed during statement execution.\n\n\nSyntax\n\n\nELAPSEDTIME { ON | OFF }\n\n\n\n\nDescription\n\n\nWhen \nelapsedtime\n is turned on, \nsnappy\n displays the total time elapsed during statement execution. The default value is OFF.\n\n\nExample\n\n\nsnappy\n elapsedtime on;\nsnappy\n select * from airlines;\nA\n|AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT\n|ECONOMY_SE\n|BUSINESS_S\n|FIRSTCLASS\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\n\n3 rows selected\nELAPSED TIME = 2 milliseconds", 
            "title": "elapsedtime"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#elapsedtime", 
            "text": "Displays the total time elapsed during statement execution.", 
            "title": "elapsedtime"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#syntax", 
            "text": "ELAPSEDTIME { ON | OFF }", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#description", 
            "text": "When  elapsedtime  is turned on,  snappy  displays the total time elapsed during statement execution. The default value is OFF.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/elapsedtime/#example", 
            "text": "snappy  elapsedtime on;\nsnappy  select * from airlines;\nA |AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT |ECONOMY_SE |BUSINESS_S |FIRSTCLASS \n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\n\n3 rows selected\nELAPSED TIME = 2 milliseconds", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/exit/", 
            "text": "exit\n\n\nCompletes the \nsnappy\n application and halts processing.\n\n\nSyntax\n\n\nEXIT\n\n\n\n\nDescription\n\n\nCauses the \nsnappy\n application to complete and processing to halt. Issuing this command from within a file started with the \nRun\n command or on the command line causes the outermost input loop to halt.\n\n\nsnappy\n exits when the Exit command is entered or if given a command file on the Java invocation line, when the end of the command file is reached.\n\n\nExample\n\n\nsnappy\n DISCONNECT CONNECTION1;\nsnappy\n EXIT;", 
            "title": "exit"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#exit", 
            "text": "Completes the  snappy  application and halts processing.", 
            "title": "exit"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#syntax", 
            "text": "EXIT", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#description", 
            "text": "Causes the  snappy  application to complete and processing to halt. Issuing this command from within a file started with the  Run  command or on the command line causes the outermost input loop to halt.  snappy  exits when the Exit command is entered or if given a command file on the Java invocation line, when the end of the command file is reached.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/exit/#example", 
            "text": "snappy  DISCONNECT CONNECTION1;\nsnappy  EXIT;", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/", 
            "text": "MaximumDisplayWidth\n\n\nSets the largest display width for columns to the specified value.\n\n\nSyntax\n\n\nMAXIMUMDISPLAYWIDTH integer_value\n\n\n\n\nDescription\n\n\nSets the largest display width for columns to the specified value. You generally use this command to increase the default value in order to display large blocks of text.\n\n\nExample\n\n\nsnappy\n insert into airlineref values('A-1', 'NOW IS THE TIME');\n1 row inserted/updated/deleted\nsnappy\n maximumdisplaywidth 4;\nsnappy\n select * from AIRLINEREF where code='A-1';\nCODE|DES\n\n---------\nA-1 |NOW\n\n\n1 row selected\nsnappy\n  maximumdisplaywidth 30;\nsnappy\n select * from AIRLINEREF where code='A-1';\nCODE |DESCRIPTION                   \n------------------------------------\nA-1  |NOW IS THE TIME               \n\n1 row selected", 
            "title": "MaximumDisplayWidth"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#maximumdisplaywidth", 
            "text": "Sets the largest display width for columns to the specified value.", 
            "title": "MaximumDisplayWidth"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#syntax", 
            "text": "MAXIMUMDISPLAYWIDTH integer_value", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#description", 
            "text": "Sets the largest display width for columns to the specified value. You generally use this command to increase the default value in order to display large blocks of text.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/maximumdisplaywidth/#example", 
            "text": "snappy  insert into airlineref values('A-1', 'NOW IS THE TIME');\n1 row inserted/updated/deleted\nsnappy  maximumdisplaywidth 4;\nsnappy  select * from AIRLINEREF where code='A-1';\nCODE|DES \n---------\nA-1 |NOW \n\n1 row selected\nsnappy   maximumdisplaywidth 30;\nsnappy  select * from AIRLINEREF where code='A-1';\nCODE |DESCRIPTION                   \n------------------------------------\nA-1  |NOW IS THE TIME               \n\n1 row selected", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/rollback/", 
            "text": "rollback\n\n\nIssues a \njava.sql.Connection.rollback\n request.\n\n\nSyntax\n\n\nROLLBACK \n\n\n\n\n\n\nDescription\n\n\nIssues a \njava.sql.Connection.rollback\n request. Use only if auto-commit is off. A \njava.sql.Connection.rollback\n request undoes the currently active transaction and initiates a new transaction.\n\n\nExample\n\n\nsnappy\n SET ISOLATION read committed;\n0 rows inserted/updated/deleted\nsnappy\n VALUES CURRENT ISOLATION;\n1\n----\nCS\n\n1 row selected\nsnappy\n AUTOCOMMIT off;\nsnappy\n insert into airlines VALUES ('AN', 'Another New Airline', 0.20, 0.07, 0.6, 1.7, 20, 10, 5);\n1 row inserted/updated/deleted\nsnappy\n select * from airlines;\nA\n|AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT\n|ECONOMY_SE\n|BUSINESS_S\n|FIRSTCLASS\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\nAN|Another New Airline     |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\n\n4 rows selected\nsnappy\n ROLLBACK;\nsnappy\n select * from airlines;\nA\n|AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT\n|ECONOMY_SE\n|BUSINESS_S\n|FIRSTCLASS\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\n\n3 rows selected", 
            "title": "rollback"
        }, 
        {
            "location": "/reference/interactive_commands/rollback/#rollback", 
            "text": "Issues a  java.sql.Connection.rollback  request.", 
            "title": "rollback"
        }, 
        {
            "location": "/reference/interactive_commands/rollback/#syntax", 
            "text": "ROLLBACK", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/rollback/#description", 
            "text": "Issues a  java.sql.Connection.rollback  request. Use only if auto-commit is off. A  java.sql.Connection.rollback  request undoes the currently active transaction and initiates a new transaction.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/rollback/#example", 
            "text": "snappy  SET ISOLATION read committed;\n0 rows inserted/updated/deleted\nsnappy  VALUES CURRENT ISOLATION;\n1\n----\nCS\n\n1 row selected\nsnappy  AUTOCOMMIT off;\nsnappy  insert into airlines VALUES ('AN', 'Another New Airline', 0.20, 0.07, 0.6, 1.7, 20, 10, 5);\n1 row inserted/updated/deleted\nsnappy  select * from airlines;\nA |AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT |ECONOMY_SE |BUSINESS_S |FIRSTCLASS \n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\nAN|Another New Airline     |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\n\n4 rows selected\nsnappy  ROLLBACK;\nsnappy  select * from airlines;\nA |AIRLINE_FULL            |BASIC_RATE            |DISTANCE_DISCOUNT     |BUSINESS_LEVEL_FACTOR |FIRSTCLASS_LEVEL_FACT |ECONOMY_SE |BUSINESS_S |FIRSTCLASS \n-----------------------------------------------------------------------------------------------------------------------------------------------------------\nNA|New Airline             |0.2                   |0.07                  |0.6                   |1.7                   |20         |10         |5\nUS|Union Standard Airlines |0.19                  |0.05                  |0.4                   |1.6                   |20         |10         |5\nAA|Amazonian Airways       |0.18                  |0.03                  |0.5                   |1.5                   |20         |10         |5\n\n3 rows selected", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/run/", 
            "text": "run\n\n\nTreats the value of the string as a valid file name, and redirects \nsnappy\n processing to read from that file until it ends or an exit command is executed.\n\n\nSyntax\n\n\nRUN String\n\n\n\n\nDescription\n\n\nTreats the value of the string as a valid file name, and redirects \nsnappy\n processing to read from that file until it ends or an \nExit\n command is executed. If the end of the file is reached without \nsnappy\n exiting, reading continues from the previous input source once the end of the file is reached. Files can contain Run commands.\n\n\nsnappy\n prints out the statements in the file as it executes them.\n\n\nAny changes made to the \nsnappy\n environment by the file are visible in the environment when processing resumes.\n\n\nExample\n\n\nsnappy\n run 'ToursDB_schema.sql';", 
            "title": "run"
        }, 
        {
            "location": "/reference/interactive_commands/run/#run", 
            "text": "Treats the value of the string as a valid file name, and redirects  snappy  processing to read from that file until it ends or an exit command is executed.", 
            "title": "run"
        }, 
        {
            "location": "/reference/interactive_commands/run/#syntax", 
            "text": "RUN String", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/run/#description", 
            "text": "Treats the value of the string as a valid file name, and redirects  snappy  processing to read from that file until it ends or an  Exit  command is executed. If the end of the file is reached without  snappy  exiting, reading continues from the previous input source once the end of the file is reached. Files can contain Run commands.  snappy  prints out the statements in the file as it executes them.  Any changes made to the  snappy  environment by the file are visible in the environment when processing resumes.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/run/#example", 
            "text": "snappy  run 'ToursDB_schema.sql';", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/", 
            "text": "set connection\n\n\nSpecifies which connection to make current when more than one connection is open.\n\n\nSyntax\n\n\nSET CONNECTION Identifier;\n\n\n\n\nDescription\n\n\nAllows you to specify which connection to make current when you have more than one connection open. Use the \nShow Connections\n command to display open connections.\n\n\nIf there is no such connection, an error results and the current connection is unchanged.\n\n\nExample\n\n\nsnappy(CONNECTION0)\n set connection CONNECTION1;\nsnappy(CONNECTION1)\n show connections;\nCONNECTION0 -   jdbc:snappydata:thrift://127.0.0.1[1527]\nCONNECTION1* -  jdbc:snappydata:thrift://127.0.0.1[1527]\n* = current connection", 
            "title": "set connection"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#set-connection", 
            "text": "Specifies which connection to make current when more than one connection is open.", 
            "title": "set connection"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#syntax", 
            "text": "SET CONNECTION Identifier;", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#description", 
            "text": "Allows you to specify which connection to make current when you have more than one connection open. Use the  Show Connections  command to display open connections.  If there is no such connection, an error results and the current connection is unchanged.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/interactive_commands/set_connection/#example", 
            "text": "snappy(CONNECTION0)  set connection CONNECTION1;\nsnappy(CONNECTION1)  show connections;\nCONNECTION0 -   jdbc:snappydata:thrift://127.0.0.1[1527]\nCONNECTION1* -  jdbc:snappydata:thrift://127.0.0.1[1527]\n* = current connection", 
            "title": "Example"
        }, 
        {
            "location": "/reference/interactive_commands/show/", 
            "text": "SHOW\n\n\nDisplays information about active connections and database objects.\n\n\nSyntax\n\n\nSHOW\n{\n   CONNECTIONS |\n   FUNCTIONS   |\n   IMPORTEDKEYS [ IN schemaName | FROM table-Name ] |\n   INDEXES [ IN schemaName | FROM table-Name ] |\n   PROCEDURES [ IN schemaName ] |\n   SCHEMAS |\n   TABLES [ IN schemaName ] |\n   VIEWS [ IN schemaName ] |\n}\n\n\n\n\nThe following are covered in this section:\n\n\n\n\n\n\nSHOW CONNECTIONS\n\n\n\n\n\n\nSHOW FUNCTIONS\n\n\n\n\n\n\nSHOW IMPORTEDKEYS\n\n\n\n\n\n\nSHOW INDEXES\n\n\n\n\n\n\nSHOW PROCEDURES\n\n\n\n\n\n\nSHOW SCHEMAS\n\n\n\n\n\n\nSHOW TABLES\n\n\n\n\n\n\nSHOW VIEWS\n\n\n\n\n\n\nDescription\n\n\nDisplays information about active connections and database objects.\n\n\n \n\n\nSHOW CONNECTIONS\n\n\nIf there are no connections, the SHOW CONNECTIONS command returns \"No connections available\".\n\n\nOtherwise, the command displays a list of connection names and the URLs used to connect to them. The currently active connection is marked with an * after its name.\n\n\nExample\n\n\nsnappy\n show connections;\nCONNECTION0* -  jdbc:snappydata:thrift://127.0.0.1[1527]\n* = current connection\n\n\n\n\n \n\n\nSHOW FUNCTIONS\n\n\nDisplays the details of the default system functions.\n\n\nCurrently, UDF functions are not displayed in the list. This will be available in the future releases.\n\n\nExample\n\n\nsnappy\n show functions;\nFUNCTION_SCHEM      |FUNCTION_NAME                 |REMARKS             \n------------------------------------------------------------------------ \nSNAPPY_HIVE_METASTO\n|NUCLEUS_ASCII                 |org.datanucleus.sto\n\nSNAPPY_HIVE_METASTO\n|NUCLEUS_MATCHES               |org.datanucleus.sto\n\nSYS                 |CHECK_TABLE_EX                |com.pivotal.gemfire\n\nSYS                 |GET_CRITICAL_HEAP_PERCENTAGE  |com.pivotal.gemfire\n\nSYS                 |GET_CRITICAL_OFFHEAP_PERCENTA\n|com.pivotal.gemfire\n\nSYS                 |GET_EVICTION_HEAP_PERCENTAGE  |com.pivotal.gemfire\n\nSYS                 |GET_EVICTION_OFFHEAP_PERCENTA\n|com.pivotal.gemfire\n\nSYS                 |GET_IS_NATIVE_NANOTIMER       |com.pivotal.gemfire\n\nSYS                 |GET_NATIVE_NANOTIMER_TYPE     |com.pivotal.gemfire\n\nSYS                 |GET_SNAPSHOT_TXID_AND_HOSTURL |com.pivotal.gemfire\n\nSYS                 |GET_TABLE_VERSION             |com.pivotal.gemfire\n\nSYS                 |HDFS_LAST_MAJOR_COMPACTION    |com.pivotal.gemfire\n\nSYSCS_UTIL          |CHECK_TABLE                   |com.pivotal.gemfire\n\nSYSCS_UTIL          |GET_DATABASE_PROPERTY         |com.pivotal.gemfire\n\nSYSCS_UTIL          |GET_EXPLAIN_CONNECTION        |com.pivotal.gemfire\n\nSYSCS_UTIL          |GET_RUNTIMESTATISTICS         |com.pivotal.gemfire\n\nSYSCS_UTIL          |GET_USER_ACCESS               |com.pivotal.gemfire\n\nSYSIBM              |BLOBCREATELOCATOR             |com.pivotal.gemfire\n\nSYSIBM              |BLOBGETBYTES                  |com.pivotal.gemfire\n\nSYSIBM              |BLOBGETLENGTH                 |com.pivotal.gemfire\n\nSYSIBM              |BLOBGETPOSITIONFROMBYTES      |com.pivotal.gemfire\n\nSYSIBM              |BLOBGETPOSITIONFROMLOCATOR    |com.pivotal.gemfire\n\nSYSIBM              |CLOBCREATELOCATOR             |com.pivotal.gemfire\n\nSYSIBM              |CLOBGETLENGTH                 |com.pivotal.gemfire\n\nSYSIBM              |CLOBGETPOSITIONFROMLOCATOR    |com.pivotal.gemfire\n\nSYSIBM              |CLOBGETPOSITIONFROMSTRING     |com.pivotal.gemfire\n\nSYSIBM              |CLOBGETSUBSTRING              |com.pivotal.gemfire\n\n\n\n\n\n \n\n\nSHOW IMPORTEDKEYS\n\n\nDisplays all foreign keys in the specified schema or table. If you omit the schema and table clauses, SnappyData displays all foreign keys for all tables in the current schema.\n\n\nExample\n\n\nsnappy\n show importedkeys in app;\nPKTABLE_NAME   |PKCOLUMN_NAME   |PK_NAME              |FKTABLE_SCHEM   |FKTABLE_NAME   |FKCOLUMN_NAME   |FK_NAME        |KEY_SEQ\n-------------------------------------------------------------------------------------------------------------------------------- \nCUSTOMERS      |CID             |SQL180328162510710   |TRADE           |BUYORDERS      |CID             |BO_CUST_FK     |1\nSECURITIES     |SEC_ID          |SEC_PK               |TRADE           |BUYORDERS      |SID             |BO_SEC_FK      |1\nCUSTOMERS      |CID             |SQL180328162510710   |TRADE           |NETWORTH       |CID             |CUST_NEWT_FK   |1\nPORTFOLIO      |CID             |PORTF_PK             |TRADE           |SELLORDERS     |CID             |PORTF_FK       |1\nPORTFOLIO      |SID             |PORTF_PK             |TRADE           |SELLORDERS     |SID             |PORTF_FK       |2\nCUSTOMERS      |CID             |SQL180328162510710   |TRADE           |PORTFOLIO      |CID             |CUST_FK        |1\nSECURITIES     |SEC_ID          |SEC_PK               |TRADE           |PORTFOLIO      |SID             |SEC_FK         |1\nEMPLOYEES      |EID             |EMPLOYEES_PK         |TRADE           |TRADES         |EID             |EMP_FK         |1\nCUSTOMERS      |CID             |SQL180328162510710   |TRADE           |TRADES         |CID             |SQL18032816254\n|1\n\nsnappy\n show importedkeys from app.BUYORDERS;\nPKTABLE_NAME   |PKCOLUMN_NAME   |PK_NAME              |FKTABLE_SCHEM   |FKTABLE_NAME   |FKCOLUMN_NAME   |FK_NAME       |KEY_SEQ\n---------------------------------------------------------------------------------------------------------------- \nCUSTOMERS      |CID             |SQL180328162510710   |TRADE           |BUYORDERS      |CID             |BO_CUST_FK    |1\nSECURITIES     |SEC_ID          |SEC_PK               |TRADE           |BUYORDERS      |SID             |BO_SEC_FK     |1\n\n2 rows selected\n\n\n\n\n \n\n\nSHOW INDEXES\n\n\nDisplays all the indexes in the database.\n\n\nIf \nIN schemaName\n is specified, only the indexes in the specified schema are displayed. If \nFROM table-Name\n is specified, only the indexes on the specified table are displayed.\n\n\nExample\n\n\nsnappy\n show indexes in app;\nTABLE_NAME          |COLUMN_NAME         |NON_U\n|TYPE|ASC\n|CARDINA\n|PAGES\n---------------------------------------------------------------------------- \nAIRLINES            |AIRLINE             |false |3   |A   |NULL    |NULL\nCITIES              |CITY_ID             |false |3   |A   |NULL    |NULL\nCITIES              |COUNTRY_ISO_CODE    |true  |3   |A   |NULL    |NULL\nCOUNTRIES           |COUNTRY_ISO_CODE    |false |3   |A   |NULL    |NULL\nCOUNTRIES           |COUNTRY             |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_DATE         |false |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |FLIGHT_ID           |true  |3   |A   |NULL    |NULL\nFLIGHTAVAILABILITY  |SEGMENT_NUMBER      |true  |3   |A   |NULL    |NULL\nFLIGHTS             |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTS             |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTS             |DEST_AIRPORT        |true  |3   |A   |NULL    |NULL\nFLIGHTS             |ORIG_AIRPORT        |true  |3   |A   |NULL    |NULL\nMAPS                |MAP_ID              |false |3   |A   |NULL    |NULL\nMAPS                |MAP_NAME            |false |3   |A   |NULL    |NULL\n\n16 rows selected\nsnappy\n show indexes from flights;\nTABLE_NAME          |COLUMN_NAME         |NON_U\n|TYPE|ASC\n|CARDINA\n|PAGES\n---------------------------------------------------------------------------- \nFLIGHTS             |FLIGHT_ID           |false |3   |A   |NULL    |NULL\nFLIGHTS             |SEGMENT_NUMBER      |false |3   |A   |NULL    |NULL\nFLIGHTS             |DEST_AIRPORT        |true  |3   |A   |NULL    |NULL\nFLIGHTS             |ORIG_AIRPORT        |true  |3   |A   |NULL    |NULL\n\n4 rows selected\n\n\n\n\n \n\n\nSHOW PROCEDURES\n\n\nSHOW PROCEDURES displays all the procedures in the database that have been created with the CREATE PROCEDURE statement, as well as system procedures.\n\n\nIf \nIN schemaName\n is specified, only procedures in the specified schema are displayed.\n\n\nExample\n\n\nsnappy\n show procedures in syscs_util;\nPROCEDURE_SCHEM     |PROCEDURE_NAME                |REMARKS             \n------------------------------------------------------------------------ \nSYSCS_UTIL          |BACKUP_DATABASE               |com.pivotal.gemfire\n\nSYSCS_UTIL          |BACKUP_DATABASE_AND_ENABLE_LO\n|com.pivotal.gemfire\n\nSYSCS_UTIL          |BACKUP_DATABASE_AND_ENABLE_LO\n|com.pivotal.gemfire\n\nSYSCS_UTIL          |BACKUP_DATABASE_NOWAIT        |com.pivotal.gemfire\n\nSYSCS_UTIL          |BULK_INSERT                   |com.pivotal.gemfire\n\nSYSCS_UTIL          |CHECKPOINT_DATABASE           |com.pivotal.gemfire\n\nSYSCS_UTIL          |COMPRESS_TABLE                |com.pivotal.gemfire\n\nSYSCS_UTIL          |DISABLE_LOG_ARCHIVE_MODE      |com.pivotal.gemfire\n\nSYSCS_UTIL          |EMPTY_STATEMENT_CACHE         |com.pivotal.gemfire\n\nSYSCS_UTIL          |EXPORT_QUERY                  |com.pivotal.gemfire\n\nSYSCS_UTIL          |EXPORT_QUERY_LOBS_TO_EXTFILE  |com.pivotal.gemfire\n\nSYSCS_UTIL          |EXPORT_TABLE                  |com.pivotal.gemfire\n\nSYSCS_UTIL          |EXPORT_TABLE_LOBS_TO_EXTFILE  |com.pivotal.gemfire\n\nSYSCS_UTIL          |FREEZE_DATABASE               |com.pivotal.gemfire\n\nSYSCS_UTIL          |IMPORT_DATA                   |com.pivotal.gemfire\n\nSYSCS_UTIL          |IMPORT_DATA_EX                |com.pivotal.gemfire\n\nSYSCS_UTIL          |IMPORT_DATA_LOBS_FROM_EXTFILE |com.pivotal.gemfire\n\nSYSCS_UTIL          |IMPORT_TABLE                  |com.pivotal.gemfire\n\nSYSCS_UTIL          |IMPORT_TABLE_EX               |com.pivotal.gemfire\n\nSYSCS_UTIL          |IMPORT_TABLE_LOBS_FROM_EXTFILE|com.pivotal.gemfire\n\nSYSCS_UTIL          |INPLACE_COMPRESS_TABLE        |com.pivotal.gemfire\n\nSYSCS_UTIL          |RELOAD_SECURITY_POLICY        |com.pivotal.gemfire\n\nSYSCS_UTIL          |SET_DATABASE_PROPERTY         |com.pivotal.gemfire\n\nSYSCS_UTIL          |SET_EXPLAIN_CONNECTION        |com.pivotal.gemfire\n\nSYSCS_UTIL          |SET_RUNTIMESTATISTICS         |com.pivotal.gemfire\n\nSYSCS_UTIL          |SET_STATEMENT_STATISTICS      |com.pivotal.gemfire\n\nSYSCS_UTIL          |SET_STATISTICS_TIMING         |com.pivotal.gemfire\n\nSYSCS_UTIL          |SET_USER_ACCESS               |com.pivotal.gemfire\n\nSYSCS_UTIL          |UNFREEZE_DATABASE             |com.pivotal.gemfire\n\n\n\n29 rows selected\n\n\n\n\n\n \n\n\nSHOW SCHEMAS\n\n\nSHOW SCHEMAS displays all of the schemas in the current connection.\n\n\nExample\n\n\nsnappy\n create schema sample;\n\nsnappy\n show schemas;\nTABLE_SCHEM\n------------------------------ \nAPP\nNULLID\nSAMPLE\nSQLJ\nSYS\nSYSCAT\nSYSCS_DIAG\nSYSCS_UTIL\nSYSFUN\nSYSIBM\nSYSPROC\nSYSSTAT\n\n12 rows selected\n\n\n\n\n \n\n\nSHOW TABLES\n\n\nSHOW TABLES displays all of the tables in the current schema.\n\n\nIf \nIN schemaName\n is specified, the tables in the given schema are displayed.\n\n\nExample\n\n\nsnappy\n show tables in app;\nTABLE_SCHEM         |TABLE_NAME                    |TABLE_TYPE     |REMARKS\n------------------------------------------------------------------------ \nAPP                 |AIRLINES                      |COLUMN TABLE    |\nAPP                 |CITIES                        |COLUMN TABLE    |\nAPP                 |COUNTRIES                     |COLUMN TABLE    |\nAPP                 |FLIGHTAVAILABILITY            |EXTERNAL TABLE  |\nAPP                 |FLIGHTS                       |EXTERNAL TABLE  |\nAPP                 |FLIGHTS_HISTORY               |ROW TABLE       |\nAPP                 |MAPS                          |ROW TABLE       |\n\n7 rows selected\nsnappy\n\n\n\n\n\n \n\n\nSHOW VIEWS\n\n\nSHOW VIEWS displays all of the views in the current schema.\n\n\nIf \nIN schemaName\n is specified, the views in the given schema are displayed.\n\n\nExample\n\n\nsnappy\n create view v1 as select * from maps;\n\nsnappy\n show views;\nTABLE_SCHEM         |TABLE_NAME                    |TABLE_TYPE|REMARKS             \n----------------------------------------------------------------------------------- \nAPP                 |V1                            |VIEW      |                    \n\n1 row selected\n\nsnappy\n show views in APP;\nTABLE_SCHEM         |TABLE_NAME                    |TABLE_TYPE|REMARKS             \n----------------------------------------------------------------------------------- \nAPP                 |V1                            |VIEW      |                    \nAPP                 |V2                            |VIEW      |                    \n\n2 rows selected\n\n\n\n\n\n\nNote\n\n\nSHOW VIEWS do not display the temporary and global temporary views.\n\n\n\n\nRelated Topics\n\n\n\n\n\n\nSET CONNECTION\n\n\n\n\n\n\nCREATE FUNCTION\n\n\n\n\n\n\nCREATE IMPORTEDKEY\n\n\n\n\n\n\nCREATE INDEXES\n\n\n\n\n\n\nCREATE PROCEDURE\n\n\n\n\n\n\nCREATE SCHEMA\n\n\n\n\n\n\nCREATE TABLE\n\n\n\n\n\n\nCREATE VIEW", 
            "title": "show"
        }, 
        {
            "location": "/reference/interactive_commands/show/#show", 
            "text": "Displays information about active connections and database objects.", 
            "title": "SHOW"
        }, 
        {
            "location": "/reference/interactive_commands/show/#syntax", 
            "text": "SHOW\n{\n   CONNECTIONS |\n   FUNCTIONS   |\n   IMPORTEDKEYS [ IN schemaName | FROM table-Name ] |\n   INDEXES [ IN schemaName | FROM table-Name ] |\n   PROCEDURES [ IN schemaName ] |\n   SCHEMAS |\n   TABLES [ IN schemaName ] |\n   VIEWS [ IN schemaName ] |\n}  The following are covered in this section:    SHOW CONNECTIONS    SHOW FUNCTIONS    SHOW IMPORTEDKEYS    SHOW INDEXES    SHOW PROCEDURES    SHOW SCHEMAS    SHOW TABLES    SHOW VIEWS", 
            "title": "Syntax"
        }, 
        {
            "location": "/reference/interactive_commands/show/#description", 
            "text": "Displays information about active connections and database objects.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/", 
            "text": "Configuration Properties\n\n\nYou use JDBC connection properties, connection boot properties, and Java system properties to configure SnappyData members and connections.\n\n\n\n\n\n\nProperty Types\n\n\n\n\n\n\nProperty Prefixes\n\n\n\n\n\n\nUsing Non-ASCII Strings in SnappyData Property Files\n\n\n\n\n\n\nList of Property Names\n\n\n\n\n\n\n\n\nProperty Types\n\n\nSnappyData configuration properties are divided into the following property types:\n\n\n\n\n\n\nConnection properties\n. Connection properties configure the features of a SnappyData member or a SnappyData client connection when you start or connect to a SnappyData member. You can define connection properties directly in the JDBC connection URL (or in the \"connect\" command in an interactive SnappyData session). You can also define connection properties in the \ngemfirexd.properties\n file or as Java system properties. For example, use -J-D\nproperty_name\n=\nproperty_value\n when you execute a \nsnappy\n utility. Or, use the JAVA_ARGS environment variable to define a Java system property for an interactive \nsnappy\n session (JAVA_ARGS=\"-D\nproperty_name\n=\nproperty_value\n\"). \n\n\n\n\nNote\n\n\nYou must add a prefix to certain connection property names in order to specify those properties as Java system properties. See \nProperty Prefixes\n.\n\n\n\n\nConnection properties can be further categorized as either \nboot properties\n or \nclient properties\n:\n\n\n\n\n\n\nBoot properties\n. A boot connection property configures features of a SnappyData member, and can only be applied along with the first connection that starts a SnappyData member. Boot properties have no effect when they are specified on connections to a member after the member has started. Boot properties have no effect when they are specified on a thin client connection.\n\n\n\n\n\n\nClient properties\n. A client connection property configures features of the client connection itself and can be used with the JDBC thin client drive (for example, using a JDBC thin client connection URL or the \nconnect client\n command from an interactive \nsnappy\n session).\n\n\n\n\n\n\n\n\n\n\nSystem properties\n. Certain SnappyData configuration properties \nmust\n be specified either as Java system properties (using -J-D\nproperty_name\n=\nproperty_value\n with a \nsnappy\n utility or setting JAVA_ARGS=\"-D\nproperty_name\n=\nproperty_value\n\" for an interactive \nsnappy\n session). You cannot define these properties in a JDBC URL connection. Many of SnappyData system properties affect features of the SnappyData member at boot time and can be optionally defined in the gemfirexd.properties file. See the property description to determine whether or not a system property can be defined in gemfirexd.properties.\n\n\nThe names of SnappyData system properties always include the \nsnappydata.\n prefix. For example, all properties that configure LDAP server information for user authentication must be specified as Java system properties, rather than JDBC properties, when you boot a server.\n\n\n\n\n\n\nCertain properties have additional behaviors or restrictions. See the individual property descriptions for more information.\n\n\n\n\nProperty Prefixes\n\n\nYou must add a prefix to connection and boot property names when you define those properties as Java system properties. The \nPrefix\n row in each property table lists a prefix value (\nsnappydata.\n or \ngemfire.\n) when one is required. Do not use an indicated prefix when you specify the property in a connection string.\n\n\nIf no prefix is specified, use only the indicated property name in all circumstances. For example, use \"host-data\" whether you define this property in gemfirexd.properties, as a Java system property, or as a property definition for FabricServer.\n\n\n\n\nUsing Non-ASCII Strings in SnappyData Property Files\n\n\nYou can specify Unicode (non-ASCII) characters in SnappyData property files by using a \n\\uXXXX\n escape sequence. For a supplementary character, you need two escape sequences, one for each of the two UTF-16 code units. The XXXX denotes the 4 hexadecimal digits for the value of the UTF-16 code unit. For example, a properties file might have the following entries:\n\n\ns1=hello there\ns2=\\u3053\\u3093\\u306b\\u3061\\u306f\npre\n\nFor example, in `gemfirexd.properties`, you might write:\n\n```pre\nlog-file=my\\u00df.log\n\n\n\n\nto indicate the desired property definition of \nlog-file=my.log\n.\n\n\nIf you have edited and saved the file in a non-ASCII encoding, you can convert it to ASCII with the \nnative2ascii\n tool included in your Oracle Java distribution. For example, you might want to do this when editing a properties file in Shift_JIS, a popular Japanese encoding.\n\n\n\n\nList of Property Names\n\n\nBelow is the list of all the configuration properties and links for each property reference page.\n\n\n\n\n\n\nack-severe-alert-threshold\n\n\n\n\n\n\nack-wait-threshold\n\n\n\n\n\n\narchive-disk-space-limit\n\n\n\n\n\n\narchive-file-size-limit\n\n\n\n\n\n\nbind-address\n\n\n\n\n\n\nenable-network-partition-detection\n\n\n\n\n\n\nenable-stats\n\n\n\n\n\n\nenable-time-statistics\n\n\n\n\n\n\nenable-timestats\n\n\n\n\n\n\nenforce-unique-host\n\n\n\n\n\n\ninit-scripts\n\n\n\n\n\n\nload-balance\n\n\n\n\n\n\nlocators\n\n\n\n\n\n\nlog-file\n\n\n\n\n\n\nlog-level\n\n\n\n\n\n\nmember-timeout\n\n\n\n\n\n\nmembership-port-range\n\n\n\n\n\n\npassword\n\n\n\n\n\n\nread-timeout\n\n\n\n\n\n\nredundancy-zone\n\n\n\n\n\n\nsecondary-locators\n\n\n\n\n\n\nskip-constraint-checks\n\n\n\n\n\n\nskip-locks\n\n\n\n\n\n\nsocket-buffer-size\n\n\n\n\n\n\nsocket-lease-time\n\n\n\n\n\n\ngemfirexd.datadictionary.allow-startup-errors\n\n\n\n\n\n\ngemfirexd.default-startup-recovery-delay\n\n\n\n\n\n\nsnappy.history\n\n\n\n\n\n\ngemfirexd.max-lock-wait\n\n\n\n\n\n\ngemfirexd.query-cancellation-interval\n\n\n\n\n\n\ngemfirexd.query-timeout\n\n\n\n\n\n\nssl-enabled\n\n\n\n\n\n\nssl-ciphers\n\n\n\n\n\n\nssl-protocols\n\n\n\n\n\n\nssl-require-authentication\n\n\n\n\n\n\nstart-locator\n\n\n\n\n\n\nstatistic-archive-file\n\n\n\n\n\n\nstatistic-sample-rate\n\n\n\n\n\n\nstatistic-sampling-enabled\n\n\n\n\n\n\nsync-commits\n\n\n\n\n\n\nsys-disk-dir\n\n\n\n\n\n\nuser", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#configuration-properties", 
            "text": "You use JDBC connection properties, connection boot properties, and Java system properties to configure SnappyData members and connections.    Property Types    Property Prefixes    Using Non-ASCII Strings in SnappyData Property Files    List of Property Names", 
            "title": "Configuration Properties"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#property-types", 
            "text": "SnappyData configuration properties are divided into the following property types:    Connection properties . Connection properties configure the features of a SnappyData member or a SnappyData client connection when you start or connect to a SnappyData member. You can define connection properties directly in the JDBC connection URL (or in the \"connect\" command in an interactive SnappyData session). You can also define connection properties in the  gemfirexd.properties  file or as Java system properties. For example, use -J-D property_name = property_value  when you execute a  snappy  utility. Or, use the JAVA_ARGS environment variable to define a Java system property for an interactive  snappy  session (JAVA_ARGS=\"-D property_name = property_value \").    Note  You must add a prefix to certain connection property names in order to specify those properties as Java system properties. See  Property Prefixes .   Connection properties can be further categorized as either  boot properties  or  client properties :    Boot properties . A boot connection property configures features of a SnappyData member, and can only be applied along with the first connection that starts a SnappyData member. Boot properties have no effect when they are specified on connections to a member after the member has started. Boot properties have no effect when they are specified on a thin client connection.    Client properties . A client connection property configures features of the client connection itself and can be used with the JDBC thin client drive (for example, using a JDBC thin client connection URL or the  connect client  command from an interactive  snappy  session).      System properties . Certain SnappyData configuration properties  must  be specified either as Java system properties (using -J-D property_name = property_value  with a  snappy  utility or setting JAVA_ARGS=\"-D property_name = property_value \" for an interactive  snappy  session). You cannot define these properties in a JDBC URL connection. Many of SnappyData system properties affect features of the SnappyData member at boot time and can be optionally defined in the gemfirexd.properties file. See the property description to determine whether or not a system property can be defined in gemfirexd.properties.  The names of SnappyData system properties always include the  snappydata.  prefix. For example, all properties that configure LDAP server information for user authentication must be specified as Java system properties, rather than JDBC properties, when you boot a server.    Certain properties have additional behaviors or restrictions. See the individual property descriptions for more information.", 
            "title": "Property Types"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#property-prefixes", 
            "text": "You must add a prefix to connection and boot property names when you define those properties as Java system properties. The  Prefix  row in each property table lists a prefix value ( snappydata.  or  gemfire. ) when one is required. Do not use an indicated prefix when you specify the property in a connection string.  If no prefix is specified, use only the indicated property name in all circumstances. For example, use \"host-data\" whether you define this property in gemfirexd.properties, as a Java system property, or as a property definition for FabricServer.", 
            "title": "Property Prefixes"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#using-non-ascii-strings-in-snappydata-property-files", 
            "text": "You can specify Unicode (non-ASCII) characters in SnappyData property files by using a  \\uXXXX  escape sequence. For a supplementary character, you need two escape sequences, one for each of the two UTF-16 code units. The XXXX denotes the 4 hexadecimal digits for the value of the UTF-16 code unit. For example, a properties file might have the following entries:  s1=hello there\ns2=\\u3053\\u3093\\u306b\\u3061\\u306f\npre\n\nFor example, in `gemfirexd.properties`, you might write:\n\n```pre\nlog-file=my\\u00df.log  to indicate the desired property definition of  log-file=my.log .  If you have edited and saved the file in a non-ASCII encoding, you can convert it to ASCII with the  native2ascii  tool included in your Oracle Java distribution. For example, you might want to do this when editing a properties file in Shift_JIS, a popular Japanese encoding.", 
            "title": "Using Non-ASCII Strings in SnappyData Property Files"
        }, 
        {
            "location": "/reference/configuration_parameters/config_parameters/#list-of-property-names", 
            "text": "Below is the list of all the configuration properties and links for each property reference page.    ack-severe-alert-threshold    ack-wait-threshold    archive-disk-space-limit    archive-file-size-limit    bind-address    enable-network-partition-detection    enable-stats    enable-time-statistics    enable-timestats    enforce-unique-host    init-scripts    load-balance    locators    log-file    log-level    member-timeout    membership-port-range    password    read-timeout    redundancy-zone    secondary-locators    skip-constraint-checks    skip-locks    socket-buffer-size    socket-lease-time    gemfirexd.datadictionary.allow-startup-errors    gemfirexd.default-startup-recovery-delay    snappy.history    gemfirexd.max-lock-wait    gemfirexd.query-cancellation-interval    gemfirexd.query-timeout    ssl-enabled    ssl-ciphers    ssl-protocols    ssl-require-authentication    start-locator    statistic-archive-file    statistic-sample-rate    statistic-sampling-enabled    sync-commits    sys-disk-dir    user", 
            "title": "List of Property Names"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/", 
            "text": "ack-severe-alert-threshold\n\n\nDescription\n\n\nThe number of seconds the distributed system waits after the \nack-wait-threshold\n for a message to be acknowledged before it issues an alert at a severe level. A value of zero disables this feature.\n\n\nDefault Value\n\n\n0 (disabled)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "ack-severe-alert-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#ack-severe-alert-threshold", 
            "text": "", 
            "title": "ack-severe-alert-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#description", 
            "text": "The number of seconds the distributed system waits after the  ack-wait-threshold  for a message to be acknowledged before it issues an alert at a severe level. A value of zero disables this feature.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#default-value", 
            "text": "0 (disabled)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-severe-alert-threshold/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/", 
            "text": "ack-wait-threshold\n\n\nDescription\n\n\nThe number of seconds a distributed message waits for an acknowledgment before it sends an alert to signal that something might be wrong with the system member that is unresponsive. After sending this alert the waiter continues to wait. The alerts are logged in the system members log as warnings.\n\n\nValid values are in the range 0...2147483647\n\n\nDefault Value\n\n\n15\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "ack-wait-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#ack-wait-threshold", 
            "text": "", 
            "title": "ack-wait-threshold"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#description", 
            "text": "The number of seconds a distributed message waits for an acknowledgment before it sends an alert to signal that something might be wrong with the system member that is unresponsive. After sending this alert the waiter continues to wait. The alerts are logged in the system members log as warnings.  Valid values are in the range 0...2147483647", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#default-value", 
            "text": "15", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ack-wait-threshold/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/", 
            "text": "archive-disk-space-limit\n\n\nDescription\n\n\nThe maximum size in megabytes of all inactive statistic archive files combined. If this limit is exceeded, inactive archive files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited.\n\n\nDefault Value\n\n\n0 (unlimited)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "archive-disk-space-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#archive-disk-space-limit", 
            "text": "", 
            "title": "archive-disk-space-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#description", 
            "text": "The maximum size in megabytes of all inactive statistic archive files combined. If this limit is exceeded, inactive archive files are deleted, oldest first, until the total size is within the limit. If set to zero, disk space use is unlimited.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#default-value", 
            "text": "0 (unlimited)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-disk-space-limit/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/", 
            "text": "archive-file-size-limit\n\n\nDescription\n\n\nThe maximum size in megabytes of a single statistic archive file. Once this limit is exceeded, a new statistic archive file is created, and the current archive file becomes inactive. If set to zero, the file size is unlimited.\n\n\nDefault Value\n\n\n0 (unlimited)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "archive-file-size-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#archive-file-size-limit", 
            "text": "", 
            "title": "archive-file-size-limit"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#description", 
            "text": "The maximum size in megabytes of a single statistic archive file. Once this limit is exceeded, a new statistic archive file is created, and the current archive file becomes inactive. If set to zero, the file size is unlimited.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#default-value", 
            "text": "0 (unlimited)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/archive-file-size-limit/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/", 
            "text": "bind-address\n\n\nDescription\n\n\n\n\nNote\n\n\nThis setting is relevant only for multi-homed hosts (machines with multiple network interface cards). \n\n\n\n\nAdapter card the cache binds to for peer-to-peer communication. It also specifies the default location for SnappyData servers to listen on, which is used unless overridden by the \nserver-bind-address\n.\n\n\nSpecify the IP address, not the hostname, because each network card may not have a unique hostname. An empty string (the default) causes the member to listen on the default card for the machine.\n\n\nThis attribute is a machine-wide attribute used for system member and client/server communication. It has no effect on the locator location unless the locator is embedded in a member process.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "bind-address"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#bind-address", 
            "text": "", 
            "title": "bind-address"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#description", 
            "text": "Note  This setting is relevant only for multi-homed hosts (machines with multiple network interface cards).    Adapter card the cache binds to for peer-to-peer communication. It also specifies the default location for SnappyData servers to listen on, which is used unless overridden by the  server-bind-address .  Specify the IP address, not the hostname, because each network card may not have a unique hostname. An empty string (the default) causes the member to listen on the default card for the machine.  This attribute is a machine-wide attribute used for system member and client/server communication. It has no effect on the locator location unless the locator is embedded in a member process.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/bind-address/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/", 
            "text": "enable-network-partition-detection\n\n\nDescription\n\n\nBoolean instructing the system to detect and handle splits in the distributed system, typically caused by a partitioning of the network (split brain) where the distributed system is running. \n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "enable-network-partition-detection"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#enable-network-partition-detection", 
            "text": "", 
            "title": "enable-network-partition-detection"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#description", 
            "text": "Boolean instructing the system to detect and handle splits in the distributed system, typically caused by a partitioning of the network (split brain) where the distributed system is running.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-network-partition-detection/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/", 
            "text": "enable-stats\n\n\nDescription\n\n\nThis property can only be used with a peer client connection; you cannot use it from a thin client.\n\n\nEnables statistics collection at the statement level for the associated connection. \n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nsnappydata.", 
            "title": "enable-stats"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#enable-stats", 
            "text": "", 
            "title": "enable-stats"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#description", 
            "text": "This property can only be used with a peer client connection; you cannot use it from a thin client.  Enables statistics collection at the statement level for the associated connection.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-stats/#prefix", 
            "text": "snappydata.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/", 
            "text": "enable-time-statistics\n\n\nDescription\n\n\nBoolean instructing the system to track time-based statistics for the distributed system. Disabled by default for performance.\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "enable-time-statistics"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#enable-time-statistics", 
            "text": "", 
            "title": "enable-time-statistics"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#description", 
            "text": "Boolean instructing the system to track time-based statistics for the distributed system. Disabled by default for performance.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-time-statistics/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-timestats/", 
            "text": "enable-timestats\n\n\nDescription\n\n\nBoolean instructing the system to track time-based statistics for the current connection. Disabled by default for performance. See \nEvaluating Statistics for the System and Applications\n.\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\ngemfire.", 
            "title": "enable-timestats"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-timestats/#enable-timestats", 
            "text": "", 
            "title": "enable-timestats"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-timestats/#description", 
            "text": "Boolean instructing the system to track time-based statistics for the current connection. Disabled by default for performance. See  Evaluating Statistics for the System and Applications .", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-timestats/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-timestats/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enable-timestats/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/", 
            "text": "enforce-unique-host\n\n\nDescription\n\n\nDetermines whether SnappyData puts redundant copies of the same data in different members running on the same physical machine. By default, SnappyData tries to put redundant copies on different machines, but it puts them on the same machine if no other machines are available. \n\nSetting this property to \ntrue\n prevents this and requires different machines for redundant copies.\n\n\nUsage\n\n\nIn the \nconf/servers\n file you can set this as:\n\n\nlocalhost -locators=localhost:3241,localhost:3242 -gemfire.enforce-unique-host=true\n\n\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "enforce-unique-host"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#enforce-unique-host", 
            "text": "", 
            "title": "enforce-unique-host"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#description", 
            "text": "Determines whether SnappyData puts redundant copies of the same data in different members running on the same physical machine. By default, SnappyData tries to put redundant copies on different machines, but it puts them on the same machine if no other machines are available.  \nSetting this property to  true  prevents this and requires different machines for redundant copies.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#usage", 
            "text": "In the  conf/servers  file you can set this as:  localhost -locators=localhost:3241,localhost:3242 -gemfire.enforce-unique-host=true", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/enforce-unique-host/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/", 
            "text": "init-scripts\n\n\nDescription\n\n\nOne or more SQL script files to execute after loading DDL from the data dictionary. Use a comma-separated list of files to supply multiple values.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nsnappydata.", 
            "title": "init-scripts"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#init-scripts", 
            "text": "", 
            "title": "init-scripts"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#description", 
            "text": "One or more SQL script files to execute after loading DDL from the data dictionary. Use a comma-separated list of files to supply multiple values.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/init-scripts/#prefix", 
            "text": "snappydata.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/load-balance/", 
            "text": "load-balance\n\n\nDescription\n\n\nSpecifies whether load balancing is performed for the JDBC client connection. \n\n\nWith the default value (\"true\") clients are automatically connected to a less-loaded server if locators are used for member discovery. \n\n\n\n\nNote\n\n\n\n\n\n\nLoad balancing is provided only for SnappyData distributed systems that use locators for member discovery.\n\n\n\n\n\n\nWhen load balancing is enabled, clients may not be able to connect to a specific server even if they provide that server's unique port number for client connections. As a best practice, clients should always request connections using a locator address and port when load balancing is enabled.\n\n\n\n\n\n\n\n\nIf a JDBC client needs to connect to a specific member, set \nload-balance\n to \"false\" in the connection string and specify the connection details for a specific SnappyData member, rather than a locator. \n\n\nFor example:\n\n\nsnappy\n connect client 'server_hostname:server_port/;load-balance=false'\n\n\n\n\nDefault Value\n\n\ntrue\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "load-balance"
        }, 
        {
            "location": "/reference/configuration_parameters/load-balance/#load-balance", 
            "text": "", 
            "title": "load-balance"
        }, 
        {
            "location": "/reference/configuration_parameters/load-balance/#description", 
            "text": "Specifies whether load balancing is performed for the JDBC client connection.   With the default value (\"true\") clients are automatically connected to a less-loaded server if locators are used for member discovery.    Note    Load balancing is provided only for SnappyData distributed systems that use locators for member discovery.    When load balancing is enabled, clients may not be able to connect to a specific server even if they provide that server's unique port number for client connections. As a best practice, clients should always request connections using a locator address and port when load balancing is enabled.     If a JDBC client needs to connect to a specific member, set  load-balance  to \"false\" in the connection string and specify the connection details for a specific SnappyData member, rather than a locator.   For example:  snappy  connect client 'server_hostname:server_port/;load-balance=false'", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/load-balance/#default-value", 
            "text": "true", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/load-balance/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/load-balance/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/", 
            "text": "locators\n\n\nDescription\n\n\nList of locators used by system members. The list must be configured consistently for every member of the distributed system. If the list is empty, locators are not used.\n\n\nFor each locator, provide a host name and/or address (separated by '@', if you use both), followed by a port number in brackets. Examples:\n\n\nlocators=address1[port1],address2[port2]\n\n\n\n\nlocators=hostName1@address1[port1],name2@address2[port2]\n\n\n\n\nlocators=hostName1[port1],name2[port2]\n\n\n\n\n\n\nNote\n\n\nOn multi-homed hosts, this last notation uses the default address. If you use bind addresses for your locators, explicitly specify the addresses in the locator's list - do not use just the hostname. \n\n\n\n\nUsage\n\n\nTo start multiple locators in a cluster modify the following files: \n\n\n\n\n\n\nconf/locators\n\n\nlocalhost -peer-discovery-address=localhost -peer-discovery-port=3241 -locators=localhost:3242\nlocalhost -peer-discovery-address=localhost -peer-discovery-port=3242 -locators=localhost:3241\n\n\n\n\n\n\n\nconf/servers\n\n\nlocalhost -locators=localhost:3241,localhost:3242\n\n\n\n\n\n\n\nconf/leads\n\n\nlocalhost -locators=localhost:3241,localhost:3242\n\n\n\n\n\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "locators"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#locators", 
            "text": "", 
            "title": "locators"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#description", 
            "text": "List of locators used by system members. The list must be configured consistently for every member of the distributed system. If the list is empty, locators are not used.  For each locator, provide a host name and/or address (separated by '@', if you use both), followed by a port number in brackets. Examples:  locators=address1[port1],address2[port2]  locators=hostName1@address1[port1],name2@address2[port2]  locators=hostName1[port1],name2[port2]   Note  On multi-homed hosts, this last notation uses the default address. If you use bind addresses for your locators, explicitly specify the addresses in the locator's list - do not use just the hostname.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#usage", 
            "text": "To start multiple locators in a cluster modify the following files:     conf/locators  localhost -peer-discovery-address=localhost -peer-discovery-port=3241 -locators=localhost:3242\nlocalhost -peer-discovery-address=localhost -peer-discovery-port=3242 -locators=localhost:3241    conf/servers  localhost -locators=localhost:3241,localhost:3242    conf/leads  localhost -locators=localhost:3241,localhost:3242", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/locators/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/", 
            "text": "log-file\n\n\nDescription\n\n\nFile to use for writing log messages. If this property is not set, the default is used.\n\n\nEach member type has its own default output:\n\n\n\n\nleader: \nsnappyleader.log\n\n\nlocator: \nsnappylocator.log\n\n\nserver: \nsnappyserver.log\n\n\n\n\nUsage\n\n\nlocalhost -log-file=/home/user1/snappy/server/snappy-server.log\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "log-file"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#log-file", 
            "text": "", 
            "title": "log-file"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#description", 
            "text": "File to use for writing log messages. If this property is not set, the default is used.  Each member type has its own default output:   leader:  snappyleader.log  locator:  snappylocator.log  server:  snappyserver.log", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#usage", 
            "text": "localhost -log-file=/home/user1/snappy/server/snappy-server.log", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/log-file/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/", 
            "text": "log-level\n\n\nDescription\n\n\nThe level of detail of the messages written to the system member's log. Setting log-level to one of the ordered levels causes all messages of that level and greater severity to be printed.\n\n\nValid values from lowest to highest are fine, config, info, warning, error, severe, and none.\n\n\nUsage\n\n\nlocalhost -log-level=fine\n\n\nDefault Value\n\n\nconfig\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "log-level"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#log-level", 
            "text": "", 
            "title": "log-level"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#description", 
            "text": "The level of detail of the messages written to the system member's log. Setting log-level to one of the ordered levels causes all messages of that level and greater severity to be printed.  Valid values from lowest to highest are fine, config, info, warning, error, severe, and none.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#usage", 
            "text": "localhost -log-level=fine", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#default-value", 
            "text": "config", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/log-level/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/", 
            "text": "member-timeout\n\n\nDescription\n\n\nInterval, in milliseconds, between two attempts to determine whether another system member is alive. When another member appears to be gone, SnappyData tries to contact it twice before quitting.\n\n\nValid values are in the range 1000..600000.\n\n\nDefault Value\n\n\n5000\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "member-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#member-timeout", 
            "text": "", 
            "title": "member-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#description", 
            "text": "Interval, in milliseconds, between two attempts to determine whether another system member is alive. When another member appears to be gone, SnappyData tries to contact it twice before quitting.  Valid values are in the range 1000..600000.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#default-value", 
            "text": "5000", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/member-timeout/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/membership-port-range/", 
            "text": "membership-port-range\n\n\nDescription\n\n\nThe range of ports available for unicast UDP messaging and for TCP failure detection. Specify this value as two integers separated by a minus sign. Different members can use different ranges.\n\n\nSnappyData randomly chooses two unique integers from this range for the member, one for UDP unicast messaging and the other for TCP failure detection messaging. Additionally, the system uniquely identifies the member using the combined host IP address and UDP port number.\n\n\nYou may want to restrict the range of ports that SnappyData uses so the product can run in an environment where routers only allow traffic on certain ports.\n\n\nDefault Value\n\n\n1024-65535\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "membership-port-range"
        }, 
        {
            "location": "/reference/configuration_parameters/membership-port-range/#membership-port-range", 
            "text": "", 
            "title": "membership-port-range"
        }, 
        {
            "location": "/reference/configuration_parameters/membership-port-range/#description", 
            "text": "The range of ports available for unicast UDP messaging and for TCP failure detection. Specify this value as two integers separated by a minus sign. Different members can use different ranges.  SnappyData randomly chooses two unique integers from this range for the member, one for UDP unicast messaging and the other for TCP failure detection messaging. Additionally, the system uniquely identifies the member using the combined host IP address and UDP port number.  You may want to restrict the range of ports that SnappyData uses so the product can run in an environment where routers only allow traffic on certain ports.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/membership-port-range/#default-value", 
            "text": "1024-65535", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/membership-port-range/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/membership-port-range/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/password/", 
            "text": "password\n\n\nDescription\n\n\nA password for the user name given at boot or connection time.\nUse this attribute in conjunction with the user attribute.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "password"
        }, 
        {
            "location": "/reference/configuration_parameters/password/#password", 
            "text": "", 
            "title": "password"
        }, 
        {
            "location": "/reference/configuration_parameters/password/#description", 
            "text": "A password for the user name given at boot or connection time.\nUse this attribute in conjunction with the user attribute.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/password/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/password/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/password/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/read-timeout/", 
            "text": "read-timeout\n\n\nDescription\n\n\nDefines the read-timeout for a thin-client connection, in seconds. If no response is received after this time, the connection is dropped and SnappyData can attempt to reconnect.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "read-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/read-timeout/#read-timeout", 
            "text": "", 
            "title": "read-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/read-timeout/#description", 
            "text": "Defines the read-timeout for a thin-client connection, in seconds. If no response is received after this time, the connection is dropped and SnappyData can attempt to reconnect.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/read-timeout/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/read-timeout/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/read-timeout/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/", 
            "text": "redundancy-zone\n\n\nDescription\n\n\nDefines this member's redundancy zone. Used to separate members into different groups for satisfying partitioned table redundancy. If this property is set, SnappyData does not put redundant copies of data in members with the same redundancy zone setting.\n\n\nFor example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack. \n\n\nYou set one redundancy zone in the \nconf/servers\n file for all members that run on one rack:\n\n\n-gemfire.redundancy-zone=rack1\n\n\n\n\nYou can also set another redundancy zone in the \nconf/servers\n file for all members that run on another rack:\n\n\n-gemfire.redundancy-zone=rack2\n\n\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.\n\n\nExample\n\n\nlocalhost1 -gemfire.redundancy-zone=rack1\nlocalhost1 -gemfire.redundancy-zone=rack1\nlocalhost2 -gemfire.redundancy-zone=rack2\nlocalhost2 -gemfire.redundancy-zone=rack2", 
            "title": "redundancy-zone"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#redundancy-zone", 
            "text": "", 
            "title": "redundancy-zone"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#description", 
            "text": "Defines this member's redundancy zone. Used to separate members into different groups for satisfying partitioned table redundancy. If this property is set, SnappyData does not put redundant copies of data in members with the same redundancy zone setting.  For example, if you had redundancy set to 1, so you have one primary and one secondary copy of each data entry, you could split primary and secondary data copies between two machine racks by defining one redundancy zone for each rack.   You set one redundancy zone in the  conf/servers  file for all members that run on one rack:  -gemfire.redundancy-zone=rack1  You can also set another redundancy zone in the  conf/servers  file for all members that run on another rack:  -gemfire.redundancy-zone=rack2", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/redundancy-zone/#example", 
            "text": "localhost1 -gemfire.redundancy-zone=rack1\nlocalhost1 -gemfire.redundancy-zone=rack1\nlocalhost2 -gemfire.redundancy-zone=rack2\nlocalhost2 -gemfire.redundancy-zone=rack2", 
            "title": "Example"
        }, 
        {
            "location": "/reference/configuration_parameters/secondary-locators/", 
            "text": "secondary-locators\n\n\nDescription\n\n\nProvides the address and port number of one or more secondary SnappyData locator members to use for connecting to the distributed system. The secondary locator addresses are used to establish an initial connection to SnappyData if the primary locator is not available. Specify the address and port number of each locator using either the \nlocator[port]\n or \nlocator:port\n format. Use a comma-separated list to specify multiple secondary locators.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "secondary-locators"
        }, 
        {
            "location": "/reference/configuration_parameters/secondary-locators/#secondary-locators", 
            "text": "", 
            "title": "secondary-locators"
        }, 
        {
            "location": "/reference/configuration_parameters/secondary-locators/#description", 
            "text": "Provides the address and port number of one or more secondary SnappyData locator members to use for connecting to the distributed system. The secondary locator addresses are used to establish an initial connection to SnappyData if the primary locator is not available. Specify the address and port number of each locator using either the  locator[port]  or  locator:port  format. Use a comma-separated list to specify multiple secondary locators.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/secondary-locators/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/secondary-locators/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/secondary-locators/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-constraint-checks/", 
            "text": "skip-constraint-checks\n\n\nDescription\n\n\nWhen \nskip-constraint-checks\n is set to true, SnappyData ignores all primary key, foreign key, and unique constraints for all SQL statements that are executed over the connection. This connection property is typically used only when importing data into a SnappyData system, in order to speed the execution of insert statements. \n\n\n\n\nNote\n\n\nWhen you set this property on a connection, you must ensure that no SQL operations violate the foreign key and unique constraints defined in the system. For primary key constraints, SnappyData uses the \nPUT INTO\n DML syntax to ensure that only the last primary key value inserted or updated remains in the system; this preserves primary key constraints. However, foreign key and unique constraints \ncan\n be violated when this property is set. This can lead to undefined behavior in the system, because other connections that do not enable \nskip-constraint-checks\n still require constraint checks for correct operation.\n\n\n\n\nOne exception to the \nskip-constraint-checks\n behavior is that SnappyData will throw a constraint violation error if a SQL statement would cause a local unique index to have duplicate values. This type of local index is created when you specify a unique index on a replicated table, or on partitioned tables where the number unique index columns is greater than or equal to the table's partitioning columns. The exception does not apply to updating global indexes, because SnappyData uses the \nPUT INTO\n DML syntax to update global indexes when \nskip-constraint-checks\n is enabled. Using \nPUT INTO\n ensures that only the last index entry for a given value remains in the index, which preserves uniqueness.\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "skip-constraint-checks"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-constraint-checks/#skip-constraint-checks", 
            "text": "", 
            "title": "skip-constraint-checks"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-constraint-checks/#description", 
            "text": "When  skip-constraint-checks  is set to true, SnappyData ignores all primary key, foreign key, and unique constraints for all SQL statements that are executed over the connection. This connection property is typically used only when importing data into a SnappyData system, in order to speed the execution of insert statements.    Note  When you set this property on a connection, you must ensure that no SQL operations violate the foreign key and unique constraints defined in the system. For primary key constraints, SnappyData uses the  PUT INTO  DML syntax to ensure that only the last primary key value inserted or updated remains in the system; this preserves primary key constraints. However, foreign key and unique constraints  can  be violated when this property is set. This can lead to undefined behavior in the system, because other connections that do not enable  skip-constraint-checks  still require constraint checks for correct operation.   One exception to the  skip-constraint-checks  behavior is that SnappyData will throw a constraint violation error if a SQL statement would cause a local unique index to have duplicate values. This type of local index is created when you specify a unique index on a replicated table, or on partitioned tables where the number unique index columns is greater than or equal to the table's partitioning columns. The exception does not apply to updating global indexes, because SnappyData uses the  PUT INTO  DML syntax to update global indexes when  skip-constraint-checks  is enabled. Using  PUT INTO  ensures that only the last index entry for a given value remains in the index, which preserves uniqueness.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-constraint-checks/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-constraint-checks/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-constraint-checks/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-locks/", 
            "text": "skip-locks\n\n\nDescription\n\n\n\n\nNote\n\n\nThis property is provided only for the purpose of cancelling a long-running query in cases where the query causes a DDL operation to hold a DataDictionary lock, preventing new logins to the system. Using this property outside of its intended purpose can lead to data corruption, especially if DDL is performed while the property is enabled. \n\n\n\n\nskip-locks\n forces the associated connection to avoid acquiring DataDictionary and table locks, enabling a JVM owner user to log into a system where a blocked DDL operation holds a DataDictionary lock and prevents new connections. Any operation that attempts to acquire a table or DataDictionary lock from the connection logs a warning and sends a SQLWarning in the statement. Transaction locks are still obtained as usual.\n\n\nUse this property to connect directly to a SnappyData server, rather than a locator. (The property disables the load-balance property by default, as load balancing can cause local deadlocks even when \nskip-locks\n is enabled.) \n\n\nThis property is restricted to JVM owners. Attempting to set the property without JVM owner credentials fails with the error, \"Connection refused: administrator access required for skip-locks.\" If authorization is disabled, the default user \"APP\" is the JVM owner.\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "skip-locks"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-locks/#skip-locks", 
            "text": "", 
            "title": "skip-locks"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-locks/#description", 
            "text": "Note  This property is provided only for the purpose of cancelling a long-running query in cases where the query causes a DDL operation to hold a DataDictionary lock, preventing new logins to the system. Using this property outside of its intended purpose can lead to data corruption, especially if DDL is performed while the property is enabled.    skip-locks  forces the associated connection to avoid acquiring DataDictionary and table locks, enabling a JVM owner user to log into a system where a blocked DDL operation holds a DataDictionary lock and prevents new connections. Any operation that attempts to acquire a table or DataDictionary lock from the connection logs a warning and sends a SQLWarning in the statement. Transaction locks are still obtained as usual.  Use this property to connect directly to a SnappyData server, rather than a locator. (The property disables the load-balance property by default, as load balancing can cause local deadlocks even when  skip-locks  is enabled.)   This property is restricted to JVM owners. Attempting to set the property without JVM owner credentials fails with the error, \"Connection refused: administrator access required for skip-locks.\" If authorization is disabled, the default user \"APP\" is the JVM owner.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-locks/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-locks/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/skip-locks/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-buffer-size/", 
            "text": "socket-buffer-size\n\n\nDescription\n\n\nReceive buffer sizes in bytes of the TCP/IP connections used for data transmission. To minimize the buffer size allocation needed for distributing large, serializable messages, the messages are sent in chunks. This setting determines the size of the chunks. Larger buffers can handle large messages more quickly, but take up more memory.\n\n\nDefault Value\n\n\n32768\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "socket-buffer-size"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-buffer-size/#socket-buffer-size", 
            "text": "", 
            "title": "socket-buffer-size"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-buffer-size/#description", 
            "text": "Receive buffer sizes in bytes of the TCP/IP connections used for data transmission. To minimize the buffer size allocation needed for distributing large, serializable messages, the messages are sent in chunks. This setting determines the size of the chunks. Larger buffers can handle large messages more quickly, but take up more memory.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-buffer-size/#default-value", 
            "text": "32768", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-buffer-size/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-buffer-size/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-lease-time/", 
            "text": "socket-lease-time\n\n\nDescription\n\n\nThe time, in milliseconds, that a thread can have exclusive access to a socket it is not actively using. A value of zero causes socket leases to never expire. This property is ignored if conserve-sockets is true.\n\n\nValid values are in the range 0..600000.\n\n\nDefault Value\n\n\n60000\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "socket-lease-time"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-lease-time/#socket-lease-time", 
            "text": "", 
            "title": "socket-lease-time"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-lease-time/#description", 
            "text": "The time, in milliseconds, that a thread can have exclusive access to a socket it is not actively using. A value of zero causes socket leases to never expire. This property is ignored if conserve-sockets is true.  Valid values are in the range 0..600000.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-lease-time/#default-value", 
            "text": "60000", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-lease-time/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/socket-lease-time/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/", 
            "text": "gemfirexd.datadictionary.allow-startup-errors\n\n\nDescription\n\n\nEnables a SnappyData member to start up, ignoring DDL statements that fail during member initialization. This property enables you to resolve startup problems manually, after forcing the member to start. Typical DDL initialization problems occur when a required disk store file is unavailable, or when SnappyData cannot initialize a DBSynchronizer configuration due to the external RDBMS being unavailable. Use \ngemfirexd.datadictionary.allow-startup-errors\n to drop and recreate the disk store or DBSynchronizer configuration after startup. \n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nsystem\n\n\n\n\nNote\n\n\nYou must define this property as a Java system property (for example by using -J-D\nproperty_name\n=\nproperty_value\n with a \nsnappy\n utility, or by setting JAVA_ARGS=\"-D\nproperty_name\n=\nproperty_value\n\").\n\n\n\n\nPrefix\n\n\nn/a", 
            "title": "gemfirexd.datadictionary.allow-startup-errors"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#gemfirexddatadictionaryallow-startup-errors", 
            "text": "", 
            "title": "gemfirexd.datadictionary.allow-startup-errors"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#description", 
            "text": "Enables a SnappyData member to start up, ignoring DDL statements that fail during member initialization. This property enables you to resolve startup problems manually, after forcing the member to start. Typical DDL initialization problems occur when a required disk store file is unavailable, or when SnappyData cannot initialize a DBSynchronizer configuration due to the external RDBMS being unavailable. Use  gemfirexd.datadictionary.allow-startup-errors  to drop and recreate the disk store or DBSynchronizer configuration after startup.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#property-type", 
            "text": "system   Note  You must define this property as a Java system property (for example by using -J-D property_name = property_value  with a  snappy  utility, or by setting JAVA_ARGS=\"-D property_name = property_value \").", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.datadictionary.allow-startup-errors/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.default-startup-recovery-delay/", 
            "text": "gemfirexd.default-startup-recovery-delay\n\n\nDescription\n\n\nThe number of milliseconds to wait after a member joins the distributed system before recovering redundancy for partitioned tables having redundant copies. The default of 120000 (2 minutes) configures redundancy recovery to be started after 2 minutes whenever a new partitioned table host joins. A value of 0 configures immediate redundancy recovery while a value of -1 disables automatic recovery after a new member joins the distributed system. \n\n\nDefault Value\n\n\n120000\n\n\nProperty Type\n\n\nsystem\n\n\nPrefix\n\n\nn/a", 
            "title": "gemfirexd.default-startup-recovery-delay"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.default-startup-recovery-delay/#gemfirexddefault-startup-recovery-delay", 
            "text": "", 
            "title": "gemfirexd.default-startup-recovery-delay"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.default-startup-recovery-delay/#description", 
            "text": "The number of milliseconds to wait after a member joins the distributed system before recovering redundancy for partitioned tables having redundant copies. The default of 120000 (2 minutes) configures redundancy recovery to be started after 2 minutes whenever a new partitioned table host joins. A value of 0 configures immediate redundancy recovery while a value of -1 disables automatic recovery after a new member joins the distributed system.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.default-startup-recovery-delay/#default-value", 
            "text": "120000", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.default-startup-recovery-delay/#property-type", 
            "text": "system", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.default-startup-recovery-delay/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/snappy.history/", 
            "text": "snappy.history\n\n\nDescription\n\n\nThe path and filename in which the \nsnappy\n utility stores a list of the commands executed during an interactive \nsnappy\n session. Specify this system property in the JAVA_ARGS environment variable before you execute \nsnappy\n (for example, JAVA_ARGS=\"-Dsnappy.history=\npath-to-file\n\"). Specify an empty string value to disable recording a history of commands. See \nSnappy-SQL Shell Interactive Commands\n.\n\n\nDefault Value\n\n\n%UserProfile%\\.snappy.history (Windows)\n\n\n$HOME/.snappy.history (Linux)\n\n\nProperty Type\n\n\nsystem\n\n\nPrefix\n\n\nn/a\n\n\nExample\n\n\nexport JAVA_ARGS=\n-Dsnappy.history=*path-to-file*", 
            "title": "snappy.history"
        }, 
        {
            "location": "/reference/configuration_parameters/snappy.history/#snappyhistory", 
            "text": "", 
            "title": "snappy.history"
        }, 
        {
            "location": "/reference/configuration_parameters/snappy.history/#description", 
            "text": "The path and filename in which the  snappy  utility stores a list of the commands executed during an interactive  snappy  session. Specify this system property in the JAVA_ARGS environment variable before you execute  snappy  (for example, JAVA_ARGS=\"-Dsnappy.history= path-to-file \"). Specify an empty string value to disable recording a history of commands. See  Snappy-SQL Shell Interactive Commands .", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/snappy.history/#default-value", 
            "text": "%UserProfile%\\.snappy.history (Windows)  $HOME/.snappy.history (Linux)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/snappy.history/#property-type", 
            "text": "system", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/snappy.history/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/snappy.history/#example", 
            "text": "export JAVA_ARGS= -Dsnappy.history=*path-to-file*", 
            "title": "Example"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.max-lock-wait/", 
            "text": "gemfirexd.max-lock-wait\n\n\nDescription\n\n\nMaximum time in milliseconds that a DDL statement waits for a distributed lock on the data dictionary or a table.\n\n\nDefault Value\n\n\n300000\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nn/a", 
            "title": "gemfirexd.max-lock-wait"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.max-lock-wait/#gemfirexdmax-lock-wait", 
            "text": "", 
            "title": "gemfirexd.max-lock-wait"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.max-lock-wait/#description", 
            "text": "Maximum time in milliseconds that a DDL statement waits for a distributed lock on the data dictionary or a table.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.max-lock-wait/#default-value", 
            "text": "300000", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.max-lock-wait/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.max-lock-wait/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-cancellation-interval/", 
            "text": "gemfirexd.query-cancellation-interval\n\n\nDescription\n\n\nAfter used memory used passes a critical limit, SnappyData begins cancelling queries to free memory. This attribute specifies the period in milliseconds after which SnappyData cancels a query during periods of critical memory usage. With the default value, SnappyData cancels a query every 100 milliseconds when necessary to free memory.\n\n\nDefault Value\n\n\n100\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nn/a", 
            "title": "gemfirexd.query-cancellation-interval"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-cancellation-interval/#gemfirexdquery-cancellation-interval", 
            "text": "", 
            "title": "gemfirexd.query-cancellation-interval"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-cancellation-interval/#description", 
            "text": "After used memory used passes a critical limit, SnappyData begins cancelling queries to free memory. This attribute specifies the period in milliseconds after which SnappyData cancels a query during periods of critical memory usage. With the default value, SnappyData cancels a query every 100 milliseconds when necessary to free memory.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-cancellation-interval/#default-value", 
            "text": "100", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-cancellation-interval/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-cancellation-interval/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-timeout/", 
            "text": "gemfirexd.query-timeout\n\n\nDescription\n\n\nAfter a query has run for longer than the specified amount of time in seconds, SnappyData automatically cancels the query. \n\n\nDefault Value\n\n\nn/a\n\n\nProperty Type\n\n\nsystem\n\n\nPrefix\n\n\nn/a", 
            "title": "gemfirexd.query-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-timeout/#gemfirexdquery-timeout", 
            "text": "", 
            "title": "gemfirexd.query-timeout"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-timeout/#description", 
            "text": "After a query has run for longer than the specified amount of time in seconds, SnappyData automatically cancels the query.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-timeout/#default-value", 
            "text": "n/a", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-timeout/#property-type", 
            "text": "system", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/snappydata.query-timeout/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_enabled/", 
            "text": "ssl-enabled\n\n\nDescription\n\n\nUsed for configuring SSL encryption between SnappyData members. Boolean indicates whether to use SSL for member connections. The default is set to false. To enable the SSL encryption, you must set it to true. This attribute must be consistent across the distributed system members.\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nNA", 
            "title": "ssl-enabled"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_enabled/#ssl-enabled", 
            "text": "", 
            "title": "ssl-enabled"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_enabled/#description", 
            "text": "Used for configuring SSL encryption between SnappyData members. Boolean indicates whether to use SSL for member connections. The default is set to false. To enable the SSL encryption, you must set it to true. This attribute must be consistent across the distributed system members.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_enabled/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_enabled/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_enabled/#prefix", 
            "text": "NA", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_protocols/", 
            "text": "ssl-protocols\n\n\nDescription\n\n\nUsed for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL protocols for member connections. A setting of 'any' uses any protocol that is enabled by default in the configured JSSE provider.\n\n\nDefault Value\n\n\nany\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nNA", 
            "title": "ssl-protocols"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_protocols/#ssl-protocols", 
            "text": "", 
            "title": "ssl-protocols"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_protocols/#description", 
            "text": "Used for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL protocols for member connections. A setting of 'any' uses any protocol that is enabled by default in the configured JSSE provider.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_protocols/#default-value", 
            "text": "any", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_protocols/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_protocols/#prefix", 
            "text": "NA", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_ciphers/", 
            "text": "ssl-ciphers\n\n\nDescription\n\n\nUsed for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL ciphers for member connections. A setting of 'any' uses any ciphers that are enabled by default in the configured JSSE provider.\n\n\nDefault Value\n\n\nany\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nNA", 
            "title": "ssl-ciphers"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_ciphers/#ssl-ciphers", 
            "text": "", 
            "title": "ssl-ciphers"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_ciphers/#description", 
            "text": "Used for configuring SSL encryption between SnappyData members. A space-separated list of the valid SSL ciphers for member connections. A setting of 'any' uses any ciphers that are enabled by default in the configured JSSE provider.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_ciphers/#default-value", 
            "text": "any", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_ciphers/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_ciphers/#prefix", 
            "text": "NA", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_require_auth/", 
            "text": "ssl-require-authentication\n\n\nDescription\n\n\nUsed for configuring SSL encryption between SnappyData members. Boolean indicating whether to require authentication for member connections.\n\n\nDefault Value\n\n\ntrue\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nNA", 
            "title": "ssl-require-authentication"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_require_auth/#ssl-require-authentication", 
            "text": "", 
            "title": "ssl-require-authentication"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_require_auth/#description", 
            "text": "Used for configuring SSL encryption between SnappyData members. Boolean indicating whether to require authentication for member connections.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_require_auth/#default-value", 
            "text": "true", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_require_auth/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/ssl_require_auth/#prefix", 
            "text": "NA", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/", 
            "text": "start-locator\n\n\nDescription\n\n\nIf set, automatically starts a locator in the current process when the member connects to the distributed system and stops the locator when the member disconnects.\n\n\nTo use, specify the locator with an optional address or host specification and a required port number, in one of these formats:\n\n\nstart-locator=address[port1] \n\n\n\n\nstart-locator=port1\n\n\n\n\n\n\n\n\nIf you only specify the port, the address assigned to the member is used for the locator.\n\n\n\n\n\n\nIf not already there, this locator is automatically added to the list of locators in this set of SnappyData properties.\n\n\n\n\n\n\n\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "start-locator"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#start-locator", 
            "text": "", 
            "title": "start-locator"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#description", 
            "text": "If set, automatically starts a locator in the current process when the member connects to the distributed system and stops the locator when the member disconnects.  To use, specify the locator with an optional address or host specification and a required port number, in one of these formats:  start-locator=address[port1]   start-locator=port1    If you only specify the port, the address assigned to the member is used for the locator.    If not already there, this locator is automatically added to the list of locators in this set of SnappyData properties.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/start-locator/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/", 
            "text": "statistic-archive-file\n\n\nDescription\n\n\nThe file to which the running system member writes statistic samples. An empty string disables archiving. Adding .gz suffix to the file name causes it to be compressed. This property is commonly used with \narchive-disk-space-limit\n and \narchive-file-size-limit\n.\n\n\nDefault Value\n\n\nnot set (this disables statistic archiving)\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "statistic-archive-file"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#statistic-archive-file", 
            "text": "", 
            "title": "statistic-archive-file"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#description", 
            "text": "The file to which the running system member writes statistic samples. An empty string disables archiving. Adding .gz suffix to the file name causes it to be compressed. This property is commonly used with  archive-disk-space-limit  and  archive-file-size-limit .", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#default-value", 
            "text": "not set (this disables statistic archiving)", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-archive-file/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/", 
            "text": "statistic-sample-rate\n\n\nDescription\n\n\nBoot property that specifies how often to sample statistics, in milliseconds.\n\nValid values are in the range 1000..60000.\n\n\n\n\nNote\n\n\nIf the value is set to less than 1000, the rate will be set to 1000 because the VSD tool does not support sub-second sampling.\n\n\n\n\nDefault Value\n\n\n1000\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "statistic-sample-rate"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#statistic-sample-rate", 
            "text": "", 
            "title": "statistic-sample-rate"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#description", 
            "text": "Boot property that specifies how often to sample statistics, in milliseconds. \nValid values are in the range 1000..60000.   Note  If the value is set to less than 1000, the rate will be set to 1000 because the VSD tool does not support sub-second sampling.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#default-value", 
            "text": "1000", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sample-rate/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/", 
            "text": "statistic-sampling-enabled\n\n\nDescription\n\n\nDetermines whether to collect and archive statistics on the member.\n\nTurning statistics sampling off saves on resources, but it also takes away potentially valuable information for ongoing system tuning and about unexpected system problems.\n\n\nDefault Value\n\n\ntrue\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\ngemfire.", 
            "title": "statistic-sampling-enabled"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#statistic-sampling-enabled", 
            "text": "", 
            "title": "statistic-sampling-enabled"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#description", 
            "text": "Determines whether to collect and archive statistics on the member. \nTurning statistics sampling off saves on resources, but it also takes away potentially valuable information for ongoing system tuning and about unexpected system problems.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#default-value", 
            "text": "true", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/statistic-sampling-enabled/#prefix", 
            "text": "gemfire.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/sync-commits/", 
            "text": "sync-commits\n\n\nDescription\n\n\nDetermines whether second-phase commit actions occur in the background for the current connection, or whether the connection waits for second-phase commit actions to complete. By default (sync-commits=false) SnappyData performs second-phase commits in the background, but ensures that the connection that issued the transaction only sees completed results. This means that other threads or connections may see different results until the second-phase commit actions complete.\n\nUsing \nsync-commits=true\n ensures that the current thin client or peer client connection waits until all second-phase commit actions complete.\n\n\nDefault Value\n\n\nfalse\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nn/a", 
            "title": "sync-commits"
        }, 
        {
            "location": "/reference/configuration_parameters/sync-commits/#sync-commits", 
            "text": "", 
            "title": "sync-commits"
        }, 
        {
            "location": "/reference/configuration_parameters/sync-commits/#description", 
            "text": "Determines whether second-phase commit actions occur in the background for the current connection, or whether the connection waits for second-phase commit actions to complete. By default (sync-commits=false) SnappyData performs second-phase commits in the background, but ensures that the connection that issued the transaction only sees completed results. This means that other threads or connections may see different results until the second-phase commit actions complete. \nUsing  sync-commits=true  ensures that the current thin client or peer client connection waits until all second-phase commit actions complete.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/sync-commits/#default-value", 
            "text": "false", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/sync-commits/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/sync-commits/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/", 
            "text": "sys-disk-dir\n\n\nDescription\n\n\nSpecifies the base path of the default disk store. This directory also holds the data dictionary subdirectory, which stores the persistent data dictionary.\n\n\nOther SnappyData features also use this directory for storing files. For example, gateway queue overflow and overflow tables use this attribute by default. You can override \nsys-disk-dir\n for table overflow using options in a table's \nCREATE TABLE\n statement.\n\n\nUsage\n\n\n-spark.snappydata.store.sys-disk-dir=\n\n\n\n\nDefault Value\n\n\nThe SnappyData working directory.\n\n\nProperty Type\n\n\nconnection (boot)\n\n\nPrefix\n\n\nsnappydata.", 
            "title": "sys-disk-dir"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#sys-disk-dir", 
            "text": "", 
            "title": "sys-disk-dir"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#description", 
            "text": "Specifies the base path of the default disk store. This directory also holds the data dictionary subdirectory, which stores the persistent data dictionary.  Other SnappyData features also use this directory for storing files. For example, gateway queue overflow and overflow tables use this attribute by default. You can override  sys-disk-dir  for table overflow using options in a table's  CREATE TABLE  statement.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#usage", 
            "text": "-spark.snappydata.store.sys-disk-dir=", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#default-value", 
            "text": "The SnappyData working directory.", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#property-type", 
            "text": "connection (boot)", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/sys-disk-dir/#prefix", 
            "text": "snappydata.", 
            "title": "Prefix"
        }, 
        {
            "location": "/reference/configuration_parameters/user/", 
            "text": "user\n\n\nDescription\n\n\nThe user name for the member or connection. A valid username and password are required when user authentication is turned on.\n\n\nUse this attribute in conjunction with the \npassword\n attribute.\n\n\nDefault Value\n\n\nnot set\n\n\nProperty Type\n\n\nconnection\n\n\nPrefix\n\n\nn/a", 
            "title": "user"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#user", 
            "text": "", 
            "title": "user"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#description", 
            "text": "The user name for the member or connection. A valid username and password are required when user authentication is turned on.  Use this attribute in conjunction with the  password  attribute.", 
            "title": "Description"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#default-value", 
            "text": "not set", 
            "title": "Default Value"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#property-type", 
            "text": "connection", 
            "title": "Property Type"
        }, 
        {
            "location": "/reference/configuration_parameters/user/#prefix", 
            "text": "n/a", 
            "title": "Prefix"
        }, 
        {
            "location": "/apidocsintro/", 
            "text": "API Documentation\n\n\n\n\n\n\nDetails about \nSnappyData Spark Extension APIs\n can be found \nhere\n.\n\n\n\n\n\n\nDetails of all the \nother API reference for SnappyData \ncan be found \nhere\n.", 
            "title": "API Reference"
        }, 
        {
            "location": "/apidocsintro/#api-documentation", 
            "text": "Details about  SnappyData Spark Extension APIs  can be found  here .    Details of all the  other API reference for SnappyData  can be found  here .", 
            "title": "API Documentation"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/", 
            "text": "SnappyData Spark Extension API Reference Guide\n\n\nThis guide gives details of Spark extension APIs that are provided by SnappyData. The following APIs are included:\n\n\n\n\n\n\n\n\nSnappySession APIs\n\n\nDataFrameWriter APIs\n\n\nSnappySessionCatalog APIs\n\n\n\n\n\n\n\n\n\n\nsql\n   \n Query Using Cached Plan\n\n\nputInto\nPut Dataframe Content into Table\n\n\ngetKeyColumns\n \nGet Key Columns of SnappyData table\n\n\n\n\n\n\nsqlUncached\nQuery Using Fresh Plan\n\n\ndeleteFrom\nDelete DataFrame Content from Table\n\n\ngetTableType\n \nGet Table Type\n\n\n\n\n\n\ncreateTable\nCreate SnappyData Managed Table\n\n\n\n\n\n\n\n\n\n\ncreateTable\nCreate SnappyData Managed JDBC Table\n\n\n\n\n\n\n\n\n\n\ntruncateTable\n Empty Contents of Table\n\n\n\n\n\n\n\n\n\n\ndropTable\n \nDrop SnappyData Table\n\n\n\n\n\n\n\n\n\n\ncreateSampleTable\nCreate Stratified Sample Table\n\n\n\n\n\n\n\n\n\n\ncreateApproxTSTopK\nCreate Structure to Query Top-K\n\n\n\n\n\n\n\n\n\n\nsetSchema\nSet Current Database/schema\n\n\n\n\n\n\n\n\n\n\ngetCurrentSchema\nGet Current Schema of Session\n\n\n\n\n\n\n\n\n\n\ninsert\nInsert Row into an Existing Table\n\n\n\n\n\n\n\n\n\n\nput\nUpsert Row into an Existing Table\n\n\n\n\n\n\n\n\n\n\nupdate\nUpdate all Rows in Table\n\n\n\n\n\n\n\n\n\n\ndelete\nDelete all Rows in Table\n\n\n\n\n\n\n\n\n\n\nqueryApproxTSTopK\nFetch the TopK Entries\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nSnappySession APIs\n\n\nThe following APIs are available for SnappySession.\n\n\n\n\nsql\n\n\nsqlUncached\n\n\ncreateTable\n\n\ntruncateTable\n\n\ndropTable\n\n\ncreateSampleTable\n\n\ncreateApproxTSTopK\n\n\nsetSchema\n\n\ngetCurrentSchema\n\n\ninsert\n\n\nput\n\n\ndelete\n\n\nqueryApproxTSTopK\n\n\n\n\n \n\n\nsql\n\n\nYou can use this API to run a query with a cached plan for a given SQL.\n\n\nSyntax\n\n\nsql(sqlText : String)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsqlText\n\n\nThe SQL string required to execute.\n\n\n\n\n\n\nReturns\n\n\nDataframe\n\n\n\n\n\n\n\n\nExample\n \n\n\nsnappySession.sql(\u201cselect * from t1\u201d)\n\n\n\n\n \n\n\nsqlUncached\n\n\nYou can use this API to run a query using a fresh plan for a given SQL String.\n\n\nSyntax\n\n\nsqlUncached(sqlText : String)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsqlText\n\n\nThe SQL string required to execute.\n\n\n\n\n\n\nReturns\n\n\nDataframe\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.sqlUncached(\u201cselect * from t1\u201d)\n\n\n\n\n \n\n\ncreateTable\n\n\nCreates a SnappyData managed table. Any relation providers, that is the row, column etc., which are supported by SnappyData can be created here.\n\n\nSyntax\n\n\ncreateTable(\n      tableName: String,\n      provider: String,\n      schemaDDL: String,\n      options: Map[String, String],\n      allowExisting: Boolean)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nprovider\n\n\nProvider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc.\n\n\n\n\n\n\nschema\n\n\nThe table schema.\n\n\n\n\n\n\noptions\n\n\nProperties for table creation. For example, partition_by, buckets etc.\n\n\n\n\n\n\nallowExisting\n\n\nWhen set to \ntrue\n, tables with the same name are ignored, else an \nAnalysisException\n is thrown stating that the table already exists.\n\n\n\n\n\n\nReturns\n\n\nDataframe\n\n\n\n\n\n\n\n\nExample\n\n\ncase class Data(col1: Int, col2: Int, col3: Int)\nval props = Map.empty[String, String]\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sc.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\nval dataDF = snappySession.createDataFrame(rdd)\n\nsnappySession.createTable(tableName, \ncolumn\n, dataDF.schema, props)\n\n\n\n\n\n \n\n\ncreateTable\n\n\nCreates a SnappyData managed JDBC table which takes a free format DDL string. The DDL string should adhere to the syntax of the underlying JDBC store. SnappyData ships with an inbuilt JDBC store, which can be accessed by the data store of Row format. The options parameter can take connection details.\n\n\nSyntax\n\n\nSyntax: \n  createTable(\n      tableName: String,\n      provider: String,\n      schemaDDL: String,\n      options: Map[String, String],\n      allowExisting: Boolean)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nprovider\n\n\nProvider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc.\n\n\n\n\n\n\nschemaDDL\n\n\nThe table schema as a string interpreted by the provider.\n\n\n\n\n\n\noptions\n\n\nProperties for table creation. For example, partition_by, buckets etc.\n\n\n\n\n\n\nallowExisting\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n{{{\n   val props = Map(\n      \nurl\n -\n s\njdbc:derby:$path\n,\n      \ndriver\n -\n \norg.apache.derby.jdbc.EmbeddedDriver\n,\n      \npoolImpl\n -\n \ntomcat\n,\n      \nuser\n -\n \napp\n,\n       \npassword\n -\n \napp\n\n       )\n\n    val schemaDDL = \n(OrderId INT NOT NULL PRIMARY KEY,ItemId INT, ITEMREF INT)\n\n    snappySession.createTable(\njdbcTable\n, \njdbc\n, schemaDDL, props)\n\n\n\n\n\n \n\n\ntruncateTable\n\n\nEmpties the contents of the table without deleting the catalog entry.\n\n\nSyntax\n\n\ntruncateTable(tableName: String, ifExists: Boolean = false)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nifExists\n\n\nAttempt truncate only if the table exists.\n\n\n\n\n\n\nReturns\n\n\nDataframe\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.truncateTable(\u201ct1\u201d, true)\n\n\n\n\n \n\n\ndropTable\n\n\nDrop a SnappyData table created by a call to \nSnappySession.createTable\n, \nCatalog.createExternalTable\n or \nDataset.createOrReplaceTempView\n. \n\n\nSyntax\n\n\ndropTable(tableName: String, ifExists: Boolean = false)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nifExists\n\n\nAttempts drop only if the table exists.\n\n\n\n\n\n\nReturns\n\n\nUnit\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.dropTable(\u201ct1\u201d, true)\n\n\n\n\n \n\n\ncreateSampleTable\n\n\nCreates a stratified sample table.\n\n\nSyntax\n\n\ncreateSampleTable(tableName: String,\n      baseTable: Option[String],\n      samplingOptions: Map[String, String],\n      allowExisting: Boolean)\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nThe qualified name of the table.\n\n\n\n\n\n\nbaseTable\n\n\nThe base table of the sample table, if any.\n\n\n\n\n\n\nsamplingOptions\n\n\nsampling options such as QCS, reservoir size etc.\n\n\n\n\n\n\nallowExisting\n\n\nWhen set to \ntrue\n,  tables with the same name are ignored, else a \ntable exist\n exception is shown.\n\n\n\n\n\n\nReturns\n\n\nDataframe\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.createSampleTable(\nairline_sample\n,   Some(\nairline\n), Map(\nqcs\n -\n \nUniqueCarrier ,Year_ ,Month_\n,  \nfraction\n -\n \n0.05\n,  \nstrataReservoirSize\n -\n \n25\n, \nbuckets\n -\n \n57\n),\n allowExisting = false)\n\n\n\n\n \n\n\ncreateApproxTSTopK\n\n\nCreates an approximate structure to query top-K with time series support.\n\n\nSyntax\n\n\ncreateApproxTSTopK(topKName: String, baseTable: Option[String],  keyColumnName: String, inputDataSchema: StructType,       topkOptions: Map[String, String], allowExisting: Boolean = false)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntopKName\n\n\nThe qualified name of the top-K structure.\n\n\n\n\n\n\nbaseTable\n\n\nThe base table of the top-K structure, if any.\n\n\n\n\n\n\nkeyColumnName\n\n\n\n\n\n\n\n\ninputDataSchema\n\n\n\n\n\n\n\n\ntopkOptions\n\n\n\n\n\n\n\n\nallowExisting\n\n\nWhen set to \ntrue\n,  tables with the same name are ignored, else a \ntable exist\n exception is shown.\n\n\n\n\n\n\nReturns\n\n\nDataframe\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.createApproxTSTopK(\ntopktable\n, Some(\nhashtagTable\n), \nhashtag\n, schema, topKOption)\n\n\n\n\n \n\n\nsetSchema\n\n\nSets the current database/schema.\n\n\nSyntax\n\n\nsetSchema(schemaName: String)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nschemaName\n\n\nschema name which goes into the catalog.\n\n\n\n\n\n\nReturns\n\n\nUnit\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.setSchema(\u201cAPP\u201d)\n\n\n\n\n \n\n\ngetCurrentSchema\n\n\nGets the current schema of the session.\n\n\nSyntax\n\n\ngetCurrentSchema\n\n\n\n\n\nExample \n\n\nsnappySession.getCurrentSchema\n\n\n\n\nReturns\n\n\nString\n\n\n \n\n\ninsert\n\n\nInserts one or more row into an existing table.\n\n\nSyntax\n\n\ninsert(tableName: String, rows: Row*)\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nTable name for the insert operation.\n\n\n\n\n\n\nRows\n\n\nList of rows to be inserted into the table.\n\n\n\n\n\n\nReturns\n\n\nInt\n\n\n\n\n\n\n\n\nExample \n\n\nval row = Row(i, i, i)\nsnappySession.insert(\nt1\n, row)\n\n\n\n\n\n \n\n\nput\n\n\nUpserts one or more row into an existing table. Only works for row tables.\n\n\nSyntax\n\n\nput(tableName: String, rows: Row*)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nTable name for the put operation\n\n\n\n\n\n\nrows\n\n\nList of rows to be put into the table.\n\n\n\n\n\n\nReturns\n\n\nInt\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.put(tableName, dataDF.collect(): _*)\n\n\n\n\n \n\n\nupdate\n\n\nUpdates all the rows in the table that match passed filter expression. This works only for row tables.\n\n\nSyntax\n\n\nupdate(tableName: String, filterExpr: String, newColumnValues: Row,  updateColumns: String*)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nTh table name which needs to be updated.\n\n\n\n\n\n\nfilterExpr\n\n\nSQL WHERE criteria to select rows that will be updated.\n\n\n\n\n\n\nnewColumnValues\n\n\nA single row containing all the updated column values. They MUST match the \nupdateColumn: list passed\n.\n\n\n\n\n\n\nupdateColumns\n\n\nList of all column names that are updated.\n\n\n\n\n\n\nReturns\n\n\nInt\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.update(\nt1\n, \nITEMREF = 3\n , Row(99) , \nITEMREF\n )\n\n\n\n\n \n\n\ndelete\n\n\nDeletes all the rows in the table that match passed filter expression. This works only for row tables.\n\n\nSyntax\n\n\ndelete(tableName: String, filterExpr: String)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nfilterExpr\n\n\nSSQL WHERE criteria to select rows that will be updated.\n\n\n\n\n\n\nReturns\n\n\nInt\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.delete(\u201ct1\u201d, s\ncol1=$i\n))\n\n\n\n\n \n\n\nqueryApproxTSTopK\n\n\nFetches the topK entries in the\n Approx TopK\n synopsis for the specified time interval. The time interval specified here should not be less than the minimum time interval used when creating the TopK synopsis.\n\n\nSyntax\n\n\nqueryApproxTSTopK(topKName: String,\n      startTime: String = null, endTime: String = null,\n      k: Int = -1)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntopKName\n\n\nThe topK structure that is to be queried.\n\n\n\n\n\n\nstartTime\n\n\nStart time as string in the format \nyyyy-mm-dd hh:mm:ss\n.  If passed as \nnull\n, the oldest interval is considered as the start interval.\n\n\n\n\n\n\nendTime\n\n\nEnd time as string in the format \nyyyy-mm-dd hh:mm:ss\n. If passed as \nnull\n, the newest interval is considered as the last interval.\n\n\n\n\n\n\nk\n\n\nOptional. The number of elements to be queried. This is to be passed only for stream summary\n\n\n\n\n\n\nReturns\n\n\nDataframe\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.queryApproxTSTopK(\ntopktable\n)\n\n\n\n\n \n\n\nDataFrameWriter APIs\n\n\nThe following APIs are available for DataFrameWriter:\n\n\n\n\nputInto\n\n\ndeleteFrom\n\n\n\n\n \n\n\nputInto\n\n\nPuts the content of the DataFrame into the specified table. It requires that the schema of the DataFrame is the same as the schema of the table. Column names are ignored while matching the schemas and \nput into\n operation is performed using position based resolution.\nIf some rows are already present in the table, then they are updated. Also, the table on which \nputInto\n is implemented should have defined key columns, if its a column table. If it is a row table, then it should have defined a primary key.\n\n\nSyntax\n\n\nputInto(tableName: String)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nReturns\n\n\nUnit\n\n\n\n\n\n\n\n\nExample \n\n\nimport org.apache.spark.sql.snappy._\ndf.write.putInto(\u201csnappy_table\u201d)\n\n\n\n\n \n\n\ndeleteFrom\n\n\nThe \ndeleteFrom\n API deletes all those records from given snappy table which exists in the input Dataframe. Existence of the record is checked by comparing the key columns (or the primary keys) values. \n\n\nTo use this API, key columns(for column table) or primary keys(for row tables) must be defined in the SnappyData table.\n\n\nAlso, the source DataFrame must contain all the key columns or primary keys (depending upon the type of snappy table). The column existence is checked using a case-insensitive match of column names. If the source DataFrame contains columns other than the key columns, it will be ignored by the \ndeleteFrom\n API.\n\n\nSyntax\n\n\ndeleteFrom(tableName: String)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nReturns\n\n\nUnit\n\n\n\n\n\n\n\n\nExample \n\n\nimport org.apache.spark.sql.snappy._\ndf.write.deleteFrom(\u201csnappy_table\u201d)\n\n\n\n\n\nSnappySessionCatalog APIs\n\n\nThe following APIs are available for SnappySessionCatalog:\n\n\n\n\ngetKeyColumns\n\n\ngetTableType\n \n\n\n\n\n\n\nNote\n\n\nThese are developer APIs and are subject to change in the future.\n\n\n\n\n \n\n\ngetKeyColumns\n\n\nGets primary key or key columns of a SnappyData table.\n\n\nSyntax\n\n\ngetKeyColumns(tableName: String)\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nReturns\n\n\nSequence of key columns (for column tables) or sequence of primary keys (for row tables).\n\n\n\n\n\n\nExample \n\n\n\n\n\n\n\n\n\n\nsnappySession.sessionCatalog.getKeyColumns(\nt1\n)\n\n\n\n\n \n\n\ngetTableType\n\n\nGets the table type (row, column etc.) of a SnappyData table. \n\n\nSyntax\n\n\ngetTableType(tableName: String)\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntableName\n\n\nName of the table.\n\n\n\n\n\n\nReturns\n\n\nType of the table. Row, Column, Index, Stream, External, None etc.\n\n\n\n\n\n\n\n\nExample \n\n\nsnappySession.sessionCatalog.getTableType(\nt1\n)", 
            "title": "SnappyData Spark extension API Reference Guide"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#snappydata-spark-extension-api-reference-guide", 
            "text": "This guide gives details of Spark extension APIs that are provided by SnappyData. The following APIs are included:     SnappySession APIs  DataFrameWriter APIs  SnappySessionCatalog APIs      sql      Query Using Cached Plan  putInto Put Dataframe Content into Table  getKeyColumns   Get Key Columns of SnappyData table    sqlUncached Query Using Fresh Plan  deleteFrom Delete DataFrame Content from Table  getTableType   Get Table Type    createTable Create SnappyData Managed Table      createTable Create SnappyData Managed JDBC Table      truncateTable  Empty Contents of Table      dropTable   Drop SnappyData Table      createSampleTable Create Stratified Sample Table      createApproxTSTopK Create Structure to Query Top-K      setSchema Set Current Database/schema      getCurrentSchema Get Current Schema of Session      insert Insert Row into an Existing Table      put Upsert Row into an Existing Table      update Update all Rows in Table      delete Delete all Rows in Table      queryApproxTSTopK Fetch the TopK Entries", 
            "title": "SnappyData Spark Extension API Reference Guide"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#snappysession-apis", 
            "text": "The following APIs are available for SnappySession.   sql  sqlUncached  createTable  truncateTable  dropTable  createSampleTable  createApproxTSTopK  setSchema  getCurrentSchema  insert  put  delete  queryApproxTSTopK", 
            "title": "SnappySession APIs"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#sql", 
            "text": "You can use this API to run a query with a cached plan for a given SQL.  Syntax  sql(sqlText : String)  Parameters     Parameter  Description      sqlText  The SQL string required to execute.    Returns  Dataframe     Example    snappySession.sql(\u201cselect * from t1\u201d)", 
            "title": "sql"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#sqluncached", 
            "text": "You can use this API to run a query using a fresh plan for a given SQL String.  Syntax  sqlUncached(sqlText : String)  Parameters     Parameter  Description      sqlText  The SQL string required to execute.    Returns  Dataframe     Example   snappySession.sqlUncached(\u201cselect * from t1\u201d)", 
            "title": "sqlUncached"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#createtable", 
            "text": "Creates a SnappyData managed table. Any relation providers, that is the row, column etc., which are supported by SnappyData can be created here.  Syntax  createTable(\n      tableName: String,\n      provider: String,\n      schemaDDL: String,\n      options: Map[String, String],\n      allowExisting: Boolean)  Parameters     Parameter  Description      tableName  Name of the table.    provider  Provider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc.    schema  The table schema.    options  Properties for table creation. For example, partition_by, buckets etc.    allowExisting  When set to  true , tables with the same name are ignored, else an  AnalysisException  is thrown stating that the table already exists.    Returns  Dataframe     Example  case class Data(col1: Int, col2: Int, col3: Int)\nval props = Map.empty[String, String]\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sc.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\nval dataDF = snappySession.createDataFrame(rdd)\n\nsnappySession.createTable(tableName,  column , dataDF.schema, props)", 
            "title": "createTable"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#createtable_1", 
            "text": "Creates a SnappyData managed JDBC table which takes a free format DDL string. The DDL string should adhere to the syntax of the underlying JDBC store. SnappyData ships with an inbuilt JDBC store, which can be accessed by the data store of Row format. The options parameter can take connection details.  Syntax  Syntax: \n  createTable(\n      tableName: String,\n      provider: String,\n      schemaDDL: String,\n      options: Map[String, String],\n      allowExisting: Boolean)  Parameters     Parameter  Description      tableName  Name of the table.    provider  Provider name such as \u2018ROW\u2019, \u2018COLUMN\u2019' etc.    schemaDDL  The table schema as a string interpreted by the provider.    options  Properties for table creation. For example, partition_by, buckets etc.    allowExisting      Example  {{{\n   val props = Map(\n       url  -  s jdbc:derby:$path ,\n       driver  -   org.apache.derby.jdbc.EmbeddedDriver ,\n       poolImpl  -   tomcat ,\n       user  -   app ,\n        password  -   app \n       )\n\n    val schemaDDL =  (OrderId INT NOT NULL PRIMARY KEY,ItemId INT, ITEMREF INT) \n    snappySession.createTable( jdbcTable ,  jdbc , schemaDDL, props)", 
            "title": "createTable"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#truncatetable", 
            "text": "Empties the contents of the table without deleting the catalog entry.  Syntax  truncateTable(tableName: String, ifExists: Boolean = false)  Parameters     Parameter  Description      tableName  Name of the table.    ifExists  Attempt truncate only if the table exists.    Returns  Dataframe     Example   snappySession.truncateTable(\u201ct1\u201d, true)", 
            "title": "truncateTable"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#droptable", 
            "text": "Drop a SnappyData table created by a call to  SnappySession.createTable ,  Catalog.createExternalTable  or  Dataset.createOrReplaceTempView .   Syntax  dropTable(tableName: String, ifExists: Boolean = false)  Parameters     Parameter  Description      tableName  Name of the table.    ifExists  Attempts drop only if the table exists.    Returns  Unit     Example   snappySession.dropTable(\u201ct1\u201d, true)", 
            "title": "dropTable"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#createsampletable", 
            "text": "Creates a stratified sample table.  Syntax  createSampleTable(tableName: String,\n      baseTable: Option[String],\n      samplingOptions: Map[String, String],\n      allowExisting: Boolean)  Parameters     Parameter  Description      tableName  The qualified name of the table.    baseTable  The base table of the sample table, if any.    samplingOptions  sampling options such as QCS, reservoir size etc.    allowExisting  When set to  true ,  tables with the same name are ignored, else a  table exist  exception is shown.    Returns  Dataframe     Example   snappySession.createSampleTable( airline_sample ,   Some( airline ), Map( qcs  -   UniqueCarrier ,Year_ ,Month_ ,   fraction  -   0.05 ,   strataReservoirSize  -   25 ,  buckets  -   57 ),\n allowExisting = false)", 
            "title": "createSampleTable"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#createapproxtstopk", 
            "text": "Creates an approximate structure to query top-K with time series support.  Syntax  createApproxTSTopK(topKName: String, baseTable: Option[String],  keyColumnName: String, inputDataSchema: StructType,       topkOptions: Map[String, String], allowExisting: Boolean = false)  Parameters     Parameter  Description      topKName  The qualified name of the top-K structure.    baseTable  The base table of the top-K structure, if any.    keyColumnName     inputDataSchema     topkOptions     allowExisting  When set to  true ,  tables with the same name are ignored, else a  table exist  exception is shown.    Returns  Dataframe     Example   snappySession.createApproxTSTopK( topktable , Some( hashtagTable ),  hashtag , schema, topKOption)", 
            "title": "createApproxTSTopK"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#setschema", 
            "text": "Sets the current database/schema.  Syntax  setSchema(schemaName: String)  Parameters     Parameter  Description      schemaName  schema name which goes into the catalog.    Returns  Unit     Example   snappySession.setSchema(\u201cAPP\u201d)", 
            "title": "setSchema"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#getcurrentschema", 
            "text": "Gets the current schema of the session.  Syntax  getCurrentSchema  Example   snappySession.getCurrentSchema  Returns  String", 
            "title": "getCurrentSchema"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#insert", 
            "text": "Inserts one or more row into an existing table.  Syntax  insert(tableName: String, rows: Row*)  Parameters     Parameter  Description      tableName  Table name for the insert operation.    Rows  List of rows to be inserted into the table.    Returns  Int     Example   val row = Row(i, i, i)\nsnappySession.insert( t1 , row)", 
            "title": "insert"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#put", 
            "text": "Upserts one or more row into an existing table. Only works for row tables.  Syntax  put(tableName: String, rows: Row*)  Parameters     Parameter  Description      tableName  Table name for the put operation    rows  List of rows to be put into the table.    Returns  Int     Example   snappySession.put(tableName, dataDF.collect(): _*)", 
            "title": "put"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#update", 
            "text": "Updates all the rows in the table that match passed filter expression. This works only for row tables.  Syntax  update(tableName: String, filterExpr: String, newColumnValues: Row,  updateColumns: String*)  Parameters     Parameter  Description      tableName  Th table name which needs to be updated.    filterExpr  SQL WHERE criteria to select rows that will be updated.    newColumnValues  A single row containing all the updated column values. They MUST match the  updateColumn: list passed .    updateColumns  List of all column names that are updated.    Returns  Int     Example   snappySession.update( t1 ,  ITEMREF = 3  , Row(99) ,  ITEMREF  )", 
            "title": "update"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#delete", 
            "text": "Deletes all the rows in the table that match passed filter expression. This works only for row tables.  Syntax  delete(tableName: String, filterExpr: String)  Parameters     Parameter  Description      tableName  Name of the table.    filterExpr  SSQL WHERE criteria to select rows that will be updated.    Returns  Int     Example   snappySession.delete(\u201ct1\u201d, s col1=$i ))", 
            "title": "delete"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#queryapproxtstopk", 
            "text": "Fetches the topK entries in the  Approx TopK  synopsis for the specified time interval. The time interval specified here should not be less than the minimum time interval used when creating the TopK synopsis.  Syntax  queryApproxTSTopK(topKName: String,\n      startTime: String = null, endTime: String = null,\n      k: Int = -1)  Parameters     Parameter  Description      topKName  The topK structure that is to be queried.    startTime  Start time as string in the format  yyyy-mm-dd hh:mm:ss .  If passed as  null , the oldest interval is considered as the start interval.    endTime  End time as string in the format  yyyy-mm-dd hh:mm:ss . If passed as  null , the newest interval is considered as the last interval.    k  Optional. The number of elements to be queried. This is to be passed only for stream summary    Returns  Dataframe     Example   snappySession.queryApproxTSTopK( topktable )", 
            "title": "queryApproxTSTopK"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#dataframewriter-apis", 
            "text": "The following APIs are available for DataFrameWriter:   putInto  deleteFrom", 
            "title": "DataFrameWriter APIs"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#putinto", 
            "text": "Puts the content of the DataFrame into the specified table. It requires that the schema of the DataFrame is the same as the schema of the table. Column names are ignored while matching the schemas and  put into  operation is performed using position based resolution.\nIf some rows are already present in the table, then they are updated. Also, the table on which  putInto  is implemented should have defined key columns, if its a column table. If it is a row table, then it should have defined a primary key.  Syntax  putInto(tableName: String)  Parameters     Parameter  Description      tableName  Name of the table.    Returns  Unit     Example   import org.apache.spark.sql.snappy._\ndf.write.putInto(\u201csnappy_table\u201d)", 
            "title": "putInto"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#deletefrom", 
            "text": "The  deleteFrom  API deletes all those records from given snappy table which exists in the input Dataframe. Existence of the record is checked by comparing the key columns (or the primary keys) values.   To use this API, key columns(for column table) or primary keys(for row tables) must be defined in the SnappyData table.  Also, the source DataFrame must contain all the key columns or primary keys (depending upon the type of snappy table). The column existence is checked using a case-insensitive match of column names. If the source DataFrame contains columns other than the key columns, it will be ignored by the  deleteFrom  API.  Syntax  deleteFrom(tableName: String)  Parameters     Parameter  Description      tableName  Name of the table.    Returns  Unit     Example   import org.apache.spark.sql.snappy._\ndf.write.deleteFrom(\u201csnappy_table\u201d)", 
            "title": "deleteFrom"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#snappysessioncatalog-apis", 
            "text": "The following APIs are available for SnappySessionCatalog:   getKeyColumns  getTableType      Note  These are developer APIs and are subject to change in the future.", 
            "title": "SnappySessionCatalog APIs"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#getkeycolumns", 
            "text": "Gets primary key or key columns of a SnappyData table.  Syntax  getKeyColumns(tableName: String)  Parameters     Parameter  Description      tableName  Name of the table.    Returns  Sequence of key columns (for column tables) or sequence of primary keys (for row tables).    Example       snappySession.sessionCatalog.getKeyColumns( t1 )", 
            "title": "getKeyColumns"
        }, 
        {
            "location": "/reference/API_Reference/apireference_guide/#gettabletype", 
            "text": "Gets the table type (row, column etc.) of a SnappyData table.   Syntax  getTableType(tableName: String)  Parameters     Parameter  Description      tableName  Name of the table.    Returns  Type of the table. Row, Column, Index, Stream, External, None etc.     Example   snappySession.sessionCatalog.getTableType( t1 )", 
            "title": "getTableType"
        }, 
        {
            "location": "/reference/misc/passwordless_ssh/", 
            "text": "Configuring SSH Login without Password\n\n\n\n\nNote\n\n\nBefore you begin ensure that you have configured SSH login without password.\n\n\n\n\nBy default, Secure Socket Shell (SSH) requires a password for authentication on a remote server.\nThis setting needs to be modified to allow you to login to the remote host through the SSH protocol, without having to enter your SSH password multiple times when working with SnappyData.\n\n\nTo install and configure SSH, do the following:\n\n\n\n\n\n\nInstall SSH\n \n\n    To install SSH,  type: \nsudo apt-get install ssh\n \n\n    Mac OS X has a built-in SSH client.\n\n\n\n\n\n\nGenerate an RSA key pair\n\n    To generate an RSA key pair run the following command on the client computer, \n\n    \nssh-keygen -t rsa\n \n\n    Press \nEnter\n when prompted to enter the file in which to save the key, and for the pass phrase.\n\n\n\n\n\n\nCopy the Public Key\n\n    Once the key pair is generated, copy the contents of the public key file, to the authorized key on the remote site, by typing \ncat ~/.ssh/id_rsa.pub \n ~/.ssh/authorized_keys", 
            "title": "Configuring SSH Login without Password"
        }, 
        {
            "location": "/reference/misc/passwordless_ssh/#configuring-ssh-login-without-password", 
            "text": "Note  Before you begin ensure that you have configured SSH login without password.   By default, Secure Socket Shell (SSH) requires a password for authentication on a remote server.\nThis setting needs to be modified to allow you to login to the remote host through the SSH protocol, without having to enter your SSH password multiple times when working with SnappyData.  To install and configure SSH, do the following:    Install SSH   \n    To install SSH,  type:  sudo apt-get install ssh   \n    Mac OS X has a built-in SSH client.    Generate an RSA key pair \n    To generate an RSA key pair run the following command on the client computer,  \n     ssh-keygen -t rsa   \n    Press  Enter  when prompted to enter the file in which to save the key, and for the pass phrase.    Copy the Public Key \n    Once the key pair is generated, copy the contents of the public key file, to the authorized key on the remote site, by typing  cat ~/.ssh/id_rsa.pub   ~/.ssh/authorized_keys", 
            "title": "Configuring SSH Login without Password"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/", 
            "text": "Setting Up SnappyData ODBC Driver\n\n\nThis feature is available only in the Enterprise version of SnappyData. \n\n\n\n\nNote\n\n\n\n\n\n\nThis is currently tested and supported only on Windows 10 (32-bit and 64-bit systems).\n\n\n\n\n\n\nDownload and Install Visual C++ Redistributable for Visual Studio 2013\n \n\n\n\n\n\n\n\n\nStep 1: Install the SnappyData ODBC Driver\n\n\n\n\n\n\nDownload the SnappyData 1.0.2.1 Enterprise Edition\n.\n\n\n\n\n\n\nClick \nODBC INSTALLERS\n to download the \nsnappydata-odbc-1.0.2.zip\n file.\n\n\n\n\n\n\nFollow \nsteps 1 and 2\n to install the  SnappyData ODBC driver.\n\n\n\n\n\n\nStep 2: Create SnappyData DSN from ODBC Data Sources 64-bit/32-bit\n\n\nTo create SnappyData DSN from ODBC Data Sources:\n\n\n\n\n\n\nOpen the \nODBC Data Source Administrator\n window:\n\n\na. On the \nStart\n page, type ODBC Data Sources, and select \nSet up ODBC data sources\n from the list or select \nODBC Data Sources\n in the \nAdministrative Tools\n.\n\n\nb. Based on your Windows installation, open \nODBC Data Sources (64-bit)\n or \nODBC Data Sources (32-bit)\n\n\n\n\n\n\nIn the \nODBC Data Source Administrator\n window, select either the \nUser DSN\n or \nSystem DSN\n tab. \n\n\n\n\n\n\nClick \nAdd\n to view the list of installed ODBC drivers on your machine.\n\n\n\n\n\n\nFrom the list of drivers, select \nSnappyData ODBC Driver\n and click \nFinish\n.\n\n\n\n\n\n\nThe \nSnappyData ODBC Configuration\n dialog is displayed. \nEnter the following details to create a DSN:\n\n\n\n\n\n\nData Source Name\n: Name of the Data Source. For example, \nsnappydsn\n.  \n\n\n\n\n\n\nServer (Hostname or IP)\n: IP address of the data server which is running in the SnappyData cluster.\n\n\n\n\n\n\nPort\n: Port number of the server. By default, it is \n1528\n for the first data server in the cluster, if all the nodes in the cluster are started on the same machine.\n\n\n\n\n\n\n\n\nNote\n\n\nODBC driver cannot connect to the locator and must connect directly to one of the servers. Therefore, in cases where you start a cluster with multiple nodes on different machines and if the server and locator are collocated on a specific machine, then the port number of the server would be higher than that of the locator port which will be 1528. In case the locator is not collocated with the server on a machine then the server port will be 1527.\n\n\n\n\n\n\n\n\nLogin ID\n: The login ID required to connect to the server. For example, \napp\n\n\n\n\n\n\nPassword\n: The password required to connect to the server. For example, \napp\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nEnsure that you provide the IP Address/Host Name and Port number of the data server. If you provide the details of the locator, the connection fails. \n\n\n\n\nFor information about connecting Tableau using SnappyData ODBC Driver, refer to \nConnect Tableau using ODBC Driver", 
            "title": "Setting Up SnappyData ODBC Driver"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#setting-up-snappydata-odbc-driver", 
            "text": "This feature is available only in the Enterprise version of SnappyData.    Note    This is currently tested and supported only on Windows 10 (32-bit and 64-bit systems).    Download and Install Visual C++ Redistributable for Visual Studio 2013", 
            "title": "Setting Up SnappyData ODBC Driver"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#step-1-install-the-snappydata-odbc-driver", 
            "text": "Download the SnappyData 1.0.2.1 Enterprise Edition .    Click  ODBC INSTALLERS  to download the  snappydata-odbc-1.0.2.zip  file.    Follow  steps 1 and 2  to install the  SnappyData ODBC driver.", 
            "title": "Step 1: Install the SnappyData ODBC Driver"
        }, 
        {
            "location": "/setting_up_odbc_driver-tableau_desktop/#step-2-create-snappydata-dsn-from-odbc-data-sources-64-bit32-bit", 
            "text": "To create SnappyData DSN from ODBC Data Sources:    Open the  ODBC Data Source Administrator  window:  a. On the  Start  page, type ODBC Data Sources, and select  Set up ODBC data sources  from the list or select  ODBC Data Sources  in the  Administrative Tools .  b. Based on your Windows installation, open  ODBC Data Sources (64-bit)  or  ODBC Data Sources (32-bit)    In the  ODBC Data Source Administrator  window, select either the  User DSN  or  System DSN  tab.     Click  Add  to view the list of installed ODBC drivers on your machine.    From the list of drivers, select  SnappyData ODBC Driver  and click  Finish .    The  SnappyData ODBC Configuration  dialog is displayed.  Enter the following details to create a DSN:    Data Source Name : Name of the Data Source. For example,  snappydsn .      Server (Hostname or IP) : IP address of the data server which is running in the SnappyData cluster.    Port : Port number of the server. By default, it is  1528  for the first data server in the cluster, if all the nodes in the cluster are started on the same machine.     Note  ODBC driver cannot connect to the locator and must connect directly to one of the servers. Therefore, in cases where you start a cluster with multiple nodes on different machines and if the server and locator are collocated on a specific machine, then the port number of the server would be higher than that of the locator port which will be 1528. In case the locator is not collocated with the server on a machine then the server port will be 1527.     Login ID : The login ID required to connect to the server. For example,  app    Password : The password required to connect to the server. For example,  app       Note  Ensure that you provide the IP Address/Host Name and Port number of the data server. If you provide the details of the locator, the connection fails.    For information about connecting Tableau using SnappyData ODBC Driver, refer to  Connect Tableau using ODBC Driver", 
            "title": "Step 2: Create SnappyData DSN from ODBC Data Sources 64-bit/32-bit"
        }, 
        {
            "location": "/setting_up_jdbc_driver_qlikview/", 
            "text": "Setting Up SnappyData JDBC Client and QlikView\n\n\n\n\nNote\n\n\nBefore using SnappyData JDBC Client, make sure \nJava 8\n is installed.\n\n\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nStep 1: Download SnappyData JDBC Client\n\n\n\n\n\n\nStep 2: Download and Install QlikView\n\n\n\n\n\n\nStep 3: Download and Install JDBCConnector for QlikView\n\n\n\n\n\n\nStep 4: Configure the JDBCConnector to connect to SnappyData\n\n\n\n\n\n\nStep 5: Connecting from QlikView to SnappyData\n\n\n\n\n\n\n \n\n\nStep 1: Download SnappyData JDBC Client\n\n\n\n\nDownload the SnappyData JDBC Client JAR\n.\n    \nSee also: \nHow to connect using JDBC driver\n\n\n\n\n \n\n\nStep 2: Download and Install QlikView\n\n\n\n\n\n\nDownload a QlikView installation package\n.\n\n\n\n\n\n\nDouble-click the \nSetup.exe\n file to start the installation. For installation instructions refer to the QlikView  documentation.\n\n\n\n\n\n\n \n\n\nStep 3: Download and Install JDBCConnector for QlikView\n\n\nTo connect to SnappyData using JDBC Client from QlikView application, install the JDBCConnector. This connector integrates into the QlikView application\n\n\n\n\n\n\nDownload JDBCConnector installer\n.\n\n\n\n\n\n\nExtract the contents of the compressed file, and double-clik on the installer to start the installation process. For installation instructions, refer to the documentation provided for the QlikView JDBC Connector. \nYou may need to activate the product.\n\n\n\n\n\n\n \n\n\nStep 4: Configure the JDBCConnector to Connect to SnappyData\n\n\nAfter installing the JDBCConnector application, add the SnappyData profile in the JDBCConnector. \n\n\n\n\nTip\n\n\nYou can also create a profile from the QlikView application.\n\n\n\n\n\n\n\n\nOpen JDBCConnector Application.\n\n\n\n\n\n\nIn the \nProfiles\n tab, click \nCreate Profile\n.\n\n\n\n\n\n\nEnter a profile name. For example, SnappyData. \n\n\n\n\n\n\nClick \nSet As Default\n to set it as the default profile.\n\n\n\n\n\n\nIn the \nJava VM Options\n tab, click \nSelect JVM\n, to set the path for the \njvm.dll\n file. \n For example, C:\\Program Files\\Java\\jre1.8.0_121\\bi\\server\\jvm.dll.\n\n\n\n\n\n\nClick \nAdd\n, to add/update option \n-Xmx1024M\n.\n\n\n\n\n\n\nIn the \nJDBC Driver\n tab, select the path to the \nsnappydata-client-1.6.0.jar\n file.\n\n\n\n\n\n\nIn the \nAdvanced\n tab, add the JDBC Driver Classname \nio.snappydata.jdbc.ClientDriver\n.\n\n\n\n\n\n\nClick \nOK\n to save and apply your changes.\n\n\n\n\n\n\n \n\n\nStep 5: Connecting from QlikView to SnappyData\n\n\n\n\n\n\nOpen the QlikView desktop application.\n\n\n\n\n\n\nClick \nFile \n New\n from the menu bar to create a QlikView workbook.\n The Getting Started Wizard is displayed. Close it to continue.\n\n\n\n\n\n\nClick \nFile \n Edit Script\n from the menu bar.\n\n\n\n\n\n\nIn the \nData\n tab, select \nJDBCConnector_x64.dll\n from the \nDatabase\n drop down.\n\n\n\n\n\n\nClick \nConfigure\n. Verfiy that the following configuration is displayed:\n\n\n\n\nIn the Java VM Options tab, the path to \njvm.dll\n file is correct and also the add/update option displays \n-Xmx1024M\n.\n\n\nIn the JDBC Driver tab, the path to the \nsnappydata-client-1.6.0.jar\n file is correct.\n\n\nIn the Advanced tab, JDBC Driver class name is displayed as \nio.snappydata.jdbc.ClientDriver\n.\n\n\n\n\n\n\n\n\nClick \nConnect\n. The Connect through QlikView JDBC Connector window is displayed.\n\n\n\n\n\n\nIn \nURL\n field enter the SnappyData JDBC URL in the format \njdbc:snappydata://\n:\n \n For example, jdbc:snappydata://192.168.1.200:1527. \n\n\n\n\n\n\nEnter both the Username and Password as \napp\n.\n\n\n\n\n\n\n\n\n\n\nClick \nOK\nto apply the changes.\n\n\n\n\n\n\nWhen the connection is successful, \nCONNECT TO\n script is added to the list of scripts in \nEdit Script\n panel.\n\n\n\n\n\n\nClick the \nSelect\n button to add the Data Source Table (SELECT sql script) OR you can manually add the \nSELECT\n sql script.\n\n\n\n\n\n\nThe \nSELECT\n script is added to the list of scripts.\n\n\n\n\n\n\nClick \nOK\n.\n\n\n\n\n\n\nAfter adding the desired Data Source Table, click \nOK\n on the \nEdit Script\n panel.\n\n\n\n\n\n\nFrom the menu bar, click \nFile \n Reload\n to load data from the data source.\n\n\n\n\n\n\nFrom the menu bar, click \nTools \n Quick Chart Wizard\n to add the required charts.\n\n\n\n\n\n\nThe \nSelected charts\n wizard guides you for generating data visualizations. \nRefer to the QlikView documentation for more information on data visualization.", 
            "title": "Setting Up SnappyData JDBC Client and QlikView"
        }, 
        {
            "location": "/setting_up_jdbc_driver_qlikview/#setting-up-snappydata-jdbc-client-and-qlikview", 
            "text": "Note  Before using SnappyData JDBC Client, make sure  Java 8  is installed.   The following topics are covered in this section:    Step 1: Download SnappyData JDBC Client    Step 2: Download and Install QlikView    Step 3: Download and Install JDBCConnector for QlikView    Step 4: Configure the JDBCConnector to connect to SnappyData    Step 5: Connecting from QlikView to SnappyData", 
            "title": "Setting Up SnappyData JDBC Client and QlikView"
        }, 
        {
            "location": "/setting_up_jdbc_driver_qlikview/#step-1-download-snappydata-jdbc-client", 
            "text": "Download the SnappyData JDBC Client JAR .\n     See also:  How to connect using JDBC driver", 
            "title": "Step 1: Download SnappyData JDBC Client"
        }, 
        {
            "location": "/setting_up_jdbc_driver_qlikview/#step-2-download-and-install-qlikview", 
            "text": "Download a QlikView installation package .    Double-click the  Setup.exe  file to start the installation. For installation instructions refer to the QlikView  documentation.", 
            "title": "Step 2: Download and Install QlikView"
        }, 
        {
            "location": "/setting_up_jdbc_driver_qlikview/#step-3-download-and-install-jdbcconnector-for-qlikview", 
            "text": "To connect to SnappyData using JDBC Client from QlikView application, install the JDBCConnector. This connector integrates into the QlikView application    Download JDBCConnector installer .    Extract the contents of the compressed file, and double-clik on the installer to start the installation process. For installation instructions, refer to the documentation provided for the QlikView JDBC Connector.  You may need to activate the product.", 
            "title": "Step 3: Download and Install JDBCConnector for QlikView"
        }, 
        {
            "location": "/setting_up_jdbc_driver_qlikview/#step-4-configure-the-jdbcconnector-to-connect-to-snappydata", 
            "text": "After installing the JDBCConnector application, add the SnappyData profile in the JDBCConnector.    Tip  You can also create a profile from the QlikView application.     Open JDBCConnector Application.    In the  Profiles  tab, click  Create Profile .    Enter a profile name. For example, SnappyData.     Click  Set As Default  to set it as the default profile.    In the  Java VM Options  tab, click  Select JVM , to set the path for the  jvm.dll  file.   For example, C:\\Program Files\\Java\\jre1.8.0_121\\bi\\server\\jvm.dll.    Click  Add , to add/update option  -Xmx1024M .    In the  JDBC Driver  tab, select the path to the  snappydata-client-1.6.0.jar  file.    In the  Advanced  tab, add the JDBC Driver Classname  io.snappydata.jdbc.ClientDriver .    Click  OK  to save and apply your changes.", 
            "title": "Step 4: Configure the JDBCConnector to Connect to SnappyData"
        }, 
        {
            "location": "/setting_up_jdbc_driver_qlikview/#step-5-connecting-from-qlikview-to-snappydata", 
            "text": "Open the QlikView desktop application.    Click  File   New  from the menu bar to create a QlikView workbook.  The Getting Started Wizard is displayed. Close it to continue.    Click  File   Edit Script  from the menu bar.    In the  Data  tab, select  JDBCConnector_x64.dll  from the  Database  drop down.    Click  Configure . Verfiy that the following configuration is displayed:   In the Java VM Options tab, the path to  jvm.dll  file is correct and also the add/update option displays  -Xmx1024M .  In the JDBC Driver tab, the path to the  snappydata-client-1.6.0.jar  file is correct.  In the Advanced tab, JDBC Driver class name is displayed as  io.snappydata.jdbc.ClientDriver .     Click  Connect . The Connect through QlikView JDBC Connector window is displayed.    In  URL  field enter the SnappyData JDBC URL in the format  jdbc:snappydata:// :    For example, jdbc:snappydata://192.168.1.200:1527.     Enter both the Username and Password as  app .      Click  OK to apply the changes.    When the connection is successful,  CONNECT TO  script is added to the list of scripts in  Edit Script  panel.    Click the  Select  button to add the Data Source Table (SELECT sql script) OR you can manually add the  SELECT  sql script.    The  SELECT  script is added to the list of scripts.    Click  OK .    After adding the desired Data Source Table, click  OK  on the  Edit Script  panel.    From the menu bar, click  File   Reload  to load data from the data source.    From the menu bar, click  Tools   Quick Chart Wizard  to add the required charts.    The  Selected charts  wizard guides you for generating data visualizations.  Refer to the QlikView documentation for more information on data visualization.", 
            "title": "Step 5: Connecting from QlikView to SnappyData"
        }, 
        {
            "location": "/troubleshooting/troubleshooting/", 
            "text": "Troubleshooting Common Problems\n\n\nThe goal of this section is to determine why something does not work as expected and explain how to resolve the problem.\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nMember Startup Problems\n\n\n\n\n\n\nRecovering from a ConflictingPersistentDataException\n\n\n\n\n\n\nPreventing disk full errors\n\n\n\n\n\n\nRecovering from disk full errors\n\n\n\n\n\n\nResolving Catalog Inconsistency Issues\n\n\n\n\n\n\nCollecting logs, stats and dumps using the collect-debug-artifacts script\n\n\n\n\n\n\nTroubleshooting Error Messages", 
            "title": "Troubleshooting Common Problems"
        }, 
        {
            "location": "/troubleshooting/troubleshooting/#troubleshooting-common-problems", 
            "text": "The goal of this section is to determine why something does not work as expected and explain how to resolve the problem.  The following topics are covered in this section:    Member Startup Problems    Recovering from a ConflictingPersistentDataException    Preventing disk full errors    Recovering from disk full errors    Resolving Catalog Inconsistency Issues    Collecting logs, stats and dumps using the collect-debug-artifacts script    Troubleshooting Error Messages", 
            "title": "Troubleshooting Common Problems"
        }, 
        {
            "location": "/troubleshooting/member_startup_problems/", 
            "text": "Member Startup Problems\n\n\nThis section provides information and resolutions for the issues faced during the startup of cluster members. \n\n\nThe following issues are included here:\n\n\n\n\nDelayed startup due to unavailable disk stores\n\n\nDelayed startup due to missing disk stores\n \n\n\n\n\nTo avoid delayed startup and recovery, the following actions are recommended:\n\n\n\n\n\n\nUse the built-in \nsnappy-start-all.sh\n and \nsnappy-stop-all.sh\n scripts to start and stop the cluster. If for some reason those scripts are not used, then when possible, first shut down the data store members after disk stores have been synchronized in the system.\n Shut down remaining locator members after the data stores have stopped.\n\n\n\n\n\n\nEnsure that all persistent members are restarted properly. See \nRecovering from a ConflictingPersistentDataException\n for more information.\n\n\n\n\n\n\n \n\n\nDelayed Startup Due to Unavailable Disk Stores\n\n\nWhen you start SnappyData members, startup delays can occur if specific disk store files on other members are unavailable. This is part of the normal startup behavior and is designed to help ensure data consistency. For example, consider the following startup message for a locator (\nlocator2\n):\n\n\nSnappyData Locator pid: 23537 status: waiting\nWaiting for DataDictionary (DiskId: 531fc5bb-1720-4836-a468-3d738a21af63, Location: /snappydata/locator2/./datadictionary) on: \n [DiskId: aa77785a-0f03-4441-84f7-6eb6547d7833, Location: /snappydata/server1/./datadictionary]\n [DiskId: f417704b-fff4-4b99-81a2-75576d673547, Location: /snappydata/locator1/./datadictionary]\n\n\n\n\nHere, the startup messages indicate that \nlocator2\n is waiting for the persistent datadictionary files on \nlocator1\n and \nserver1\n to become available. SnappyData always persists the data dictionary for indexes and tables that you create, even if you do not configure those tables to persist their stored data. The startup messages above indicate that \nlocator1\n or \nlocator2\n might potentially store a newer copy of the data dictionary for the distributed system.\n\n\nContinuing the startup by booting the \nserver1\n data store yields:\n\n\nStarting SnappyData Server using locators for peer discovery: localhost[10337],localhost[10338]\nStarting network server for SnappyData Server at address localhost/127.0.0.1[1529]\nLogs generated in /snappydata/server1/gfxdserver.log\nThe server is still starting. 15 seconds have elapsed since the last log message: \n Region /_DDL_STMTS_META_REGION has potentially stale data. It is waiting for another member to recover the latest data.\nMy persistent id:\n\n  DiskStore ID: aa77785a-0f03-4441-84f7-6eb6547d7833\n  Name: \n  Location: /10.0.1.31:/snappydata/server1/./datadictionary\nup\nMembers with potentially new data:\n[\n  DiskStore ID: f417704b-fff4-4b99-81a2-75576d673547\n  Name: \n  Location: /10.0.1.31:/snappydata/locator1/./datadictionary\n]\nUse the \nsnappy-shell list-missing-disk-stores\n command to see all disk stores that are being waited on by other members.\n\n\n\n\nThe data store startup messages indicate that \nlocator1\n has \"potentially new data\" for the data dictionary. In this case, both \nlocator2\n and \nserver1\n were shut down before \nlocator1\n in the system, so those members are waiting on \nlocator1\n to ensure that they have the latest version of the data dictionary.\n\n\nThe above messages for data stores and locators may indicate that some members were not started. If the indicated disk store persistence files are available on the missing member, simply start that member and allow the running members to recover. For example, in the above system you would simply start locator1 and allow \nlocator2\n and \nserver1\n to synchronize their data.\n\n\n \n\n\nDelayed Startup Due to Missing Disk Stores\n\n\nSometimes a cluster does not get started, if the disk store files are missing from one of servers in the cluster. \nFor example, you start a cluster that consists of \nserver1\n and \nserver2\n. Suppose the disk store files in \nserver1\n are unavailable due to corruption or deletion. \nserver1\n, where the files were missing, attempts to start up as a new member, but it cannot due to InternalGemFireError and \nserver2\n cannot start because it is waiting for the missing disk stores on \nserver1\n. \nIn such a case, you can unblock the waiting server.\nIn case of more than two servers, despite of unblocking the waiting diskstores, one server can be still waiting upon the dependent server to come up. In such a case, change the order of the servers in the \nconf\n file and then restart the cluster.\n\n\nUnblocking the Disk Store\n\n\nIn the following sample startup log message that is displayed, you are notified that the \nserver1\n which has process ID number (PID) 21474 cannot come up because it joined the cluster as a new member and \nserver2\n with PID 21582 is waiting for the missing disk stores on \nserver1\n.\n\n\nSnappyData Server pid: 21474 status: stopped\nError starting server process: \nInternalGemFireError: None of the previous persistent node is up. - See log file for details.\nSnappyData Server pid: 21582 status: waiting\nMember disk state is not the most current. Table __PR._B__APP_ADJUSTMENT_4 at location /home/xyz/snappy/snappydata/server1/snappy-internal-delta is waiting for disk recovery from following members: \n[/127.0.0.1] [DiskId: 6190c93b-158f-40f1-8251-1e9c58e320c2, Location: /home/xyz/snappy/snappydata/server0/snappy-internal-delta]\n\n\n\n\nExecute the  \n./bin/snappy list-missing-disk-stores locators=\nhost\n:\nport\n command to view all the disk stores that are missing and awaited upon by other servers in the cluster. Check the messages in the \nstart_snappyserver.log\n file of the server which is waiting for the missing disk stores. \n\n\nThe following sample message is displayed in the \nstart_snappyserver.log\n file for the servers:\n\n\n[info 2018/07/10 12:00:16.302 IST \nRecovery thread for bucket _B__APP_ADJUSTMENT_4\n tid=0x4c] Region /APP/SNAPPYSYS_INTERNAL____ADJUSTMENT_COLUMN_STORE_, bucket 4 has potentially stale data.  It is waiting for another member to recover the latest data.\n  My persistent id:\n    DiskStore ID: 93e7e9cf-a513-4c67-89c3-da7e94a08efb\n    Location: /127.0.0.1:/home/xyz/snappy/snappydata/server2\n  Members with potentially new data:\n  [\n    DiskStore ID: 9791b9ff-7df3-44e8-99c8-7d62a3387002\n    Location: /127.0.0.1:/home/xyz/snappy/snappydata/server1\n  ]\n\n\n\n\nIn this sample log message, the diskID of \nserver2\n is waiting upon the diskID of \nserver1\n, which is missing. To start \nserver2\n, the waiting disk store in that server must be unblocked.\nHere it is shown that the diskID \n93e7e9cf-a513-4c67-89c3-da7e94a08efb\n of \nserver2\n is waiting upon the diskID \n9791b9ff-7df3-44e8-99c8-7d62a3387002\n of \nserver1\n, which is missing.\n\n\nRun the \nunblock-disk-store\n utility, in the following format, to unblock the waiting disk store:\n\n\n./bin/snappy unblock-disk-store \ndiskID of the waiting server\nlocators=localhost:10334\n\nFor example, \n./bin/snappy unblock-disk-store 93e7e9cf-a513-4c67-89c3-da7e94a08efb -locators=localhost:10334\n\n\nRestart the cluster and keep unblocking such disk stores that are displayed in the logs until all the servers reach the running status.\n\n\n\n\nNote\n\n\nThere is no loss of data when you unblock the disk stores.\n\n\n\n\nRebalancing Data on Servers\n\n\nAfter unblocking the disk stores, if you notice that one of the server in the cluster has more data as compared to the other servers, you can distribute the data among the servers. This ensures that each server carries almost equal data. To balance the data equally on the servers, do the following:\n\n\n\n\nConnect to snappy shell and obtain the jdbc client connection.\n\n\nRun the rebalance command.\n\n\nsnappy\n call sys.rebalance_all_buckets();\n\n\n\n\nRevoking Disk Stores that Prevent Startup\n\n\nIf a member cannot be restarted even after unblocking the disk store and restarting after re-ordering the servers in the \nconf\n file,  only then use the \nrevoke-missing-disk-store\n command.\n\n\n\n\nCaution\nThis can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the \nrevoke-missing-disk-store\n command as a last resort.  Contact \nsupport@snappydata.io\n if you need to use the \nrevoke-missing-disk-store\n command.", 
            "title": "Member Startup Problems"
        }, 
        {
            "location": "/troubleshooting/member_startup_problems/#member-startup-problems", 
            "text": "This section provides information and resolutions for the issues faced during the startup of cluster members.   The following issues are included here:   Delayed startup due to unavailable disk stores  Delayed startup due to missing disk stores     To avoid delayed startup and recovery, the following actions are recommended:    Use the built-in  snappy-start-all.sh  and  snappy-stop-all.sh  scripts to start and stop the cluster. If for some reason those scripts are not used, then when possible, first shut down the data store members after disk stores have been synchronized in the system.  Shut down remaining locator members after the data stores have stopped.    Ensure that all persistent members are restarted properly. See  Recovering from a ConflictingPersistentDataException  for more information.", 
            "title": "Member Startup Problems"
        }, 
        {
            "location": "/troubleshooting/member_startup_problems/#delayed-startup-due-to-unavailable-disk-stores", 
            "text": "When you start SnappyData members, startup delays can occur if specific disk store files on other members are unavailable. This is part of the normal startup behavior and is designed to help ensure data consistency. For example, consider the following startup message for a locator ( locator2 ):  SnappyData Locator pid: 23537 status: waiting\nWaiting for DataDictionary (DiskId: 531fc5bb-1720-4836-a468-3d738a21af63, Location: /snappydata/locator2/./datadictionary) on: \n [DiskId: aa77785a-0f03-4441-84f7-6eb6547d7833, Location: /snappydata/server1/./datadictionary]\n [DiskId: f417704b-fff4-4b99-81a2-75576d673547, Location: /snappydata/locator1/./datadictionary]  Here, the startup messages indicate that  locator2  is waiting for the persistent datadictionary files on  locator1  and  server1  to become available. SnappyData always persists the data dictionary for indexes and tables that you create, even if you do not configure those tables to persist their stored data. The startup messages above indicate that  locator1  or  locator2  might potentially store a newer copy of the data dictionary for the distributed system.  Continuing the startup by booting the  server1  data store yields:  Starting SnappyData Server using locators for peer discovery: localhost[10337],localhost[10338]\nStarting network server for SnappyData Server at address localhost/127.0.0.1[1529]\nLogs generated in /snappydata/server1/gfxdserver.log\nThe server is still starting. 15 seconds have elapsed since the last log message: \n Region /_DDL_STMTS_META_REGION has potentially stale data. It is waiting for another member to recover the latest data.\nMy persistent id:\n\n  DiskStore ID: aa77785a-0f03-4441-84f7-6eb6547d7833\n  Name: \n  Location: /10.0.1.31:/snappydata/server1/./datadictionary\nup\nMembers with potentially new data:\n[\n  DiskStore ID: f417704b-fff4-4b99-81a2-75576d673547\n  Name: \n  Location: /10.0.1.31:/snappydata/locator1/./datadictionary\n]\nUse the  snappy-shell list-missing-disk-stores  command to see all disk stores that are being waited on by other members.  The data store startup messages indicate that  locator1  has \"potentially new data\" for the data dictionary. In this case, both  locator2  and  server1  were shut down before  locator1  in the system, so those members are waiting on  locator1  to ensure that they have the latest version of the data dictionary.  The above messages for data stores and locators may indicate that some members were not started. If the indicated disk store persistence files are available on the missing member, simply start that member and allow the running members to recover. For example, in the above system you would simply start locator1 and allow  locator2  and  server1  to synchronize their data.", 
            "title": "Delayed Startup Due to Unavailable Disk Stores"
        }, 
        {
            "location": "/troubleshooting/member_startup_problems/#delayed-startup-due-to-missing-disk-stores", 
            "text": "Sometimes a cluster does not get started, if the disk store files are missing from one of servers in the cluster. \nFor example, you start a cluster that consists of  server1  and  server2 . Suppose the disk store files in  server1  are unavailable due to corruption or deletion.  server1 , where the files were missing, attempts to start up as a new member, but it cannot due to InternalGemFireError and  server2  cannot start because it is waiting for the missing disk stores on  server1 .  In such a case, you can unblock the waiting server.\nIn case of more than two servers, despite of unblocking the waiting diskstores, one server can be still waiting upon the dependent server to come up. In such a case, change the order of the servers in the  conf  file and then restart the cluster.", 
            "title": "Delayed Startup Due to Missing Disk Stores"
        }, 
        {
            "location": "/troubleshooting/member_startup_problems/#unblocking-the-disk-store", 
            "text": "In the following sample startup log message that is displayed, you are notified that the  server1  which has process ID number (PID) 21474 cannot come up because it joined the cluster as a new member and  server2  with PID 21582 is waiting for the missing disk stores on  server1 .  SnappyData Server pid: 21474 status: stopped\nError starting server process: \nInternalGemFireError: None of the previous persistent node is up. - See log file for details.\nSnappyData Server pid: 21582 status: waiting\nMember disk state is not the most current. Table __PR._B__APP_ADJUSTMENT_4 at location /home/xyz/snappy/snappydata/server1/snappy-internal-delta is waiting for disk recovery from following members: \n[/127.0.0.1] [DiskId: 6190c93b-158f-40f1-8251-1e9c58e320c2, Location: /home/xyz/snappy/snappydata/server0/snappy-internal-delta]  Execute the   ./bin/snappy list-missing-disk-stores locators= host : port  command to view all the disk stores that are missing and awaited upon by other servers in the cluster. Check the messages in the  start_snappyserver.log  file of the server which is waiting for the missing disk stores.   The following sample message is displayed in the  start_snappyserver.log  file for the servers:  [info 2018/07/10 12:00:16.302 IST  Recovery thread for bucket _B__APP_ADJUSTMENT_4  tid=0x4c] Region /APP/SNAPPYSYS_INTERNAL____ADJUSTMENT_COLUMN_STORE_, bucket 4 has potentially stale data.  It is waiting for another member to recover the latest data.\n  My persistent id:\n    DiskStore ID: 93e7e9cf-a513-4c67-89c3-da7e94a08efb\n    Location: /127.0.0.1:/home/xyz/snappy/snappydata/server2\n  Members with potentially new data:\n  [\n    DiskStore ID: 9791b9ff-7df3-44e8-99c8-7d62a3387002\n    Location: /127.0.0.1:/home/xyz/snappy/snappydata/server1\n  ]  In this sample log message, the diskID of  server2  is waiting upon the diskID of  server1 , which is missing. To start  server2 , the waiting disk store in that server must be unblocked.\nHere it is shown that the diskID  93e7e9cf-a513-4c67-89c3-da7e94a08efb  of  server2  is waiting upon the diskID  9791b9ff-7df3-44e8-99c8-7d62a3387002  of  server1 , which is missing.  Run the  unblock-disk-store  utility, in the following format, to unblock the waiting disk store:  ./bin/snappy unblock-disk-store  diskID of the waiting server locators=localhost:10334 \nFor example,  ./bin/snappy unblock-disk-store 93e7e9cf-a513-4c67-89c3-da7e94a08efb -locators=localhost:10334  Restart the cluster and keep unblocking such disk stores that are displayed in the logs until all the servers reach the running status.   Note  There is no loss of data when you unblock the disk stores.", 
            "title": "Unblocking the Disk Store"
        }, 
        {
            "location": "/troubleshooting/member_startup_problems/#rebalancing-data-on-servers", 
            "text": "After unblocking the disk stores, if you notice that one of the server in the cluster has more data as compared to the other servers, you can distribute the data among the servers. This ensures that each server carries almost equal data. To balance the data equally on the servers, do the following:   Connect to snappy shell and obtain the jdbc client connection.  Run the rebalance command.  snappy  call sys.rebalance_all_buckets();", 
            "title": "Rebalancing Data on Servers"
        }, 
        {
            "location": "/troubleshooting/member_startup_problems/#revoking-disk-stores-that-prevent-startup", 
            "text": "If a member cannot be restarted even after unblocking the disk store and restarting after re-ordering the servers in the  conf  file,  only then use the  revoke-missing-disk-store  command.   Caution This can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the  revoke-missing-disk-store  command as a last resort.  Contact  support@snappydata.io  if you need to use the  revoke-missing-disk-store  command.", 
            "title": "Revoking Disk Stores that Prevent Startup"
        }, 
        {
            "location": "/troubleshooting/recovering_from_a_conflictingpersistentdataexception/", 
            "text": "Recovering from a ConflictingPersistentDataException\n\n\nIf you receive a \nConflictingPersistentDataException\n during startup, it indicates that you have multiple copies of some persistent data and SnappyData cannot determine which copy to use. Normally SnappyData uses metadata to automatically determine which copy of persistent data to use. Each member persists, along with the data dictionary or table data, a list of other members that have the data and whether their data is up to date.\n\n\nA \nConflictingPersistentDataException\n happens when two members compare their metadata and find that it is inconsistent\u2014they either don\u2019t know about each other, or they both believe that the other member has stale data. The following are some scenarios that can cause a \nConflictingPersistentDataException\n.\n\n\nIndependently-created copies\n\n\nTrying to merge two independently-created distributed systems into a single distributed system causes a \nConflictingPersistentDataException\n. There are a few ways to end up with independently-created systems:\n\n\n\n\n\n\nConfiguration problems may cause SnappyData members connect to different locators that are not aware of each other. To avoid this problem, ensure that all locators and data stores always specify the same, complete list of locator addresses at startup (for example, \nlocators=locator1[10334],locator2[10334],locator3[10334]\n). \n\n\n\n\n\n\nAll persistent members in a system may be shut down, after which a brand new set of different persistent members attempts to start up.\n\n\n\n\n\n\nTrying to merge independent systems by pointing all members to the same set of locators then results in a \nConflictingPersistentDataException\n.\n\n\nSnappyData cannot merge independently-created data for the same table. Instead, you need to export the data from one of the systems and import it into the other system. See Exporting and Importing Data with SnappyData\n\n\nStarting new members first\n\n\nStarting a brand new member with no persistent data before starting older members that have persistent data can cause a \nConflictingPersistentDataException\n.\n\n\nThis can happen by accident if you shut down the system, then add a new member to the startup scripts, and finally start all members in parallel. In this case, the new member may start first. If this occurs, the new member creates an empty, independent copy of the data before the older members start up. When the older members start, the situation is similar to that described above in \u201cIndependently-created copies.\u201d\n\n\nIn this case, the fix is simply to move aside or delete the (empty) persistence files for the new member, shut down the new member, and finally restart the older members. After the older members have fully recovered, restart the new member.\n\n\nA network split, with enable-network-partition-detection set to false\n\n\nWith \nenable-network-partition-detection\n set to true, SnappyData detects a network partition and shuts down members to prevent a \"split brain.\" In this case no conflicts should occur when the system is restored.\n\n\nHowever, if \nenable-network-partition-detection\n is false, SnappyData cannot prevent a \"split brain\" after a network partition. Instead, each side of the network partition records that the other side of the partition has stale data. When the partition is healed and persistent members are restarted, they find a conflict because each side believes the other side's members are stale.\n\n\nIn some cases it may be possible to choose between sides of the network partition and keep only the data from one side of the partition. Otherwise you may need to salvage data and import it into a fresh system.\n\n\nResolving a ConflictingPersistentDataException\n\n\nIf you receive a \nConflictingPersistentDataException\n, you will not be able to start all of your members and have them join the same distributed system.\n\n\nFirst, determine if there is one part of the system that you can recover. For example, if you just added some new members to the system, try to start up without including those members. For the remaining members, use the data extractor tool to extract data from the persistence files and import it into a running system.", 
            "title": "Recovering from a ConflictingPersistentDataException"
        }, 
        {
            "location": "/troubleshooting/recovering_from_a_conflictingpersistentdataexception/#recovering-from-a-conflictingpersistentdataexception", 
            "text": "If you receive a  ConflictingPersistentDataException  during startup, it indicates that you have multiple copies of some persistent data and SnappyData cannot determine which copy to use. Normally SnappyData uses metadata to automatically determine which copy of persistent data to use. Each member persists, along with the data dictionary or table data, a list of other members that have the data and whether their data is up to date.  A  ConflictingPersistentDataException  happens when two members compare their metadata and find that it is inconsistent\u2014they either don\u2019t know about each other, or they both believe that the other member has stale data. The following are some scenarios that can cause a  ConflictingPersistentDataException .  Independently-created copies  Trying to merge two independently-created distributed systems into a single distributed system causes a  ConflictingPersistentDataException . There are a few ways to end up with independently-created systems:    Configuration problems may cause SnappyData members connect to different locators that are not aware of each other. To avoid this problem, ensure that all locators and data stores always specify the same, complete list of locator addresses at startup (for example,  locators=locator1[10334],locator2[10334],locator3[10334] ).     All persistent members in a system may be shut down, after which a brand new set of different persistent members attempts to start up.    Trying to merge independent systems by pointing all members to the same set of locators then results in a  ConflictingPersistentDataException .  SnappyData cannot merge independently-created data for the same table. Instead, you need to export the data from one of the systems and import it into the other system. See Exporting and Importing Data with SnappyData  Starting new members first  Starting a brand new member with no persistent data before starting older members that have persistent data can cause a  ConflictingPersistentDataException .  This can happen by accident if you shut down the system, then add a new member to the startup scripts, and finally start all members in parallel. In this case, the new member may start first. If this occurs, the new member creates an empty, independent copy of the data before the older members start up. When the older members start, the situation is similar to that described above in \u201cIndependently-created copies.\u201d  In this case, the fix is simply to move aside or delete the (empty) persistence files for the new member, shut down the new member, and finally restart the older members. After the older members have fully recovered, restart the new member.  A network split, with enable-network-partition-detection set to false  With  enable-network-partition-detection  set to true, SnappyData detects a network partition and shuts down members to prevent a \"split brain.\" In this case no conflicts should occur when the system is restored.  However, if  enable-network-partition-detection  is false, SnappyData cannot prevent a \"split brain\" after a network partition. Instead, each side of the network partition records that the other side of the partition has stale data. When the partition is healed and persistent members are restarted, they find a conflict because each side believes the other side's members are stale.  In some cases it may be possible to choose between sides of the network partition and keep only the data from one side of the partition. Otherwise you may need to salvage data and import it into a fresh system.  Resolving a ConflictingPersistentDataException  If you receive a  ConflictingPersistentDataException , you will not be able to start all of your members and have them join the same distributed system.  First, determine if there is one part of the system that you can recover. For example, if you just added some new members to the system, try to start up without including those members. For the remaining members, use the data extractor tool to extract data from the persistence files and import it into a running system.", 
            "title": "Recovering from a ConflictingPersistentDataException"
        }, 
        {
            "location": "/troubleshooting/preventing_disk_full_errors/", 
            "text": "Preventing Disk Full Errors\n\n\nIt is important to monitor the disk usage of SnappyData members. If a member lacks sufficient disk space for a disk store, the member attempts to shut down the disk store and its associated tables, and logs an error message. After you make sufficient disk space available to the member, you can restart the member. A shutdown due to a member running out of disk space can cause loss of data, data file corruption, log file corruption and other error conditions that can negatively impact your applications.\n\n\nYou can prevent disk file errors using the following techniques:\n\n\n\n\n\n\nUse default pre-allocation for disk store files and disk store metadata files. Pre-allocation reserves disk space for these files and leaves the member in a healthy state when the disk store is shut down, allowing you to restart the member once sufficient disk space has been made available. Pre-allocation is configured by default.\n\n\nPre-allocation is governed by the following system properties:\n\n\n\n\n\n\nDisk store files\n\u2014 set the \ngemfire.preAllocateDisk\n system property to true (the default).\n\n\n\n\n\n\nDisk store metadata files\n\u2014 set the \ngemfire.preAllocateIF\n system property to true (the default).\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nSnappyData recommends using ext4 filesystems on Linux platforms, because ext4 supports pre-allocation which speeds disk startup performance. If you are using ext3 filesystems in latency-sensitive environments with high write throughput, you can improve disk startup performance by setting the \nMAXLOGSIZE\n property of a disk store to a value lower than the default 1 GB. See \nCREATE DISKSTORE\n.\n\n\n\n\n\n\n\n\nMonitor SnappyData logs for low disk space warnings. SnappyData logs disk space warnings in the following situations:\n\n\n\n\n\n\nLog file directory\n\u2014logs a warning if the available space is less than 100 MB.\n\n\n\n\n\n\nDisk store directory\n\u2014logs a warning if the usable space is less than 1.15 times the space required to create a new oplog file.\n\n\n\n\n\n\nData dictionary\n\u2014logs a warning if the remaining space is less than 50 MB.", 
            "title": "Preventing Disk Full Errors"
        }, 
        {
            "location": "/troubleshooting/preventing_disk_full_errors/#preventing-disk-full-errors", 
            "text": "It is important to monitor the disk usage of SnappyData members. If a member lacks sufficient disk space for a disk store, the member attempts to shut down the disk store and its associated tables, and logs an error message. After you make sufficient disk space available to the member, you can restart the member. A shutdown due to a member running out of disk space can cause loss of data, data file corruption, log file corruption and other error conditions that can negatively impact your applications.  You can prevent disk file errors using the following techniques:    Use default pre-allocation for disk store files and disk store metadata files. Pre-allocation reserves disk space for these files and leaves the member in a healthy state when the disk store is shut down, allowing you to restart the member once sufficient disk space has been made available. Pre-allocation is configured by default.  Pre-allocation is governed by the following system properties:    Disk store files \u2014 set the  gemfire.preAllocateDisk  system property to true (the default).    Disk store metadata files \u2014 set the  gemfire.preAllocateIF  system property to true (the default).       Note  SnappyData recommends using ext4 filesystems on Linux platforms, because ext4 supports pre-allocation which speeds disk startup performance. If you are using ext3 filesystems in latency-sensitive environments with high write throughput, you can improve disk startup performance by setting the  MAXLOGSIZE  property of a disk store to a value lower than the default 1 GB. See  CREATE DISKSTORE .     Monitor SnappyData logs for low disk space warnings. SnappyData logs disk space warnings in the following situations:    Log file directory \u2014logs a warning if the available space is less than 100 MB.    Disk store directory \u2014logs a warning if the usable space is less than 1.15 times the space required to create a new oplog file.    Data dictionary \u2014logs a warning if the remaining space is less than 50 MB.", 
            "title": "Preventing Disk Full Errors"
        }, 
        {
            "location": "/troubleshooting/recovering_from_disk_full_errors/", 
            "text": "Recovering from Disk Full Errors\n\n\nIf a member of your SnappyData distributed system fails due to a disk full error condition, add or make additional disk capacity available and attempt to restart the member normally. If the member does not restart and there is a redundant copy of its tables in a disk store on another member, you can restore the member using the following steps:\n\n\n\n\n\n\nDelete or move the disk store files from the failed member.\n\n\n\n\n\n\n\nRevoke the member using the \nrevoke-disk-store\n command.\n\n\n\n\nNote\n\n\nThis can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the \nrevoke-missing-disk-store\n command as a last resort.  Contact \nsupport@snappydata.io\n if you need to use the \nrevoke-missing-disk-store\n command\n\n\n\n\n\n\n\n\nRestart the member.", 
            "title": "Recovering from Disk Full Errors"
        }, 
        {
            "location": "/troubleshooting/recovering_from_disk_full_errors/#recovering-from-disk-full-errors", 
            "text": "If a member of your SnappyData distributed system fails due to a disk full error condition, add or make additional disk capacity available and attempt to restart the member normally. If the member does not restart and there is a redundant copy of its tables in a disk store on another member, you can restore the member using the following steps:    Delete or move the disk store files from the failed member.    Revoke the member using the  revoke-disk-store  command.   Note  This can cause some loss of data if the revoked disk store actually contains recent changes to the data dictionary or to table data. The revoked disk stores cannot be added back to the system later. If you revoke a disk store on a member you need to delete the associated disk files from that member in order to start it again. Only use the  revoke-missing-disk-store  command as a last resort.  Contact  support@snappydata.io  if you need to use the  revoke-missing-disk-store  command     Restart the member.", 
            "title": "Recovering from Disk Full Errors"
        }, 
        {
            "location": "/troubleshooting/catalog_inconsistency/", 
            "text": "Resolving Catalog Inconsistency Issues\n\n\nA SnappyData catalog internally maintains table metadata in two catalogs:\n\n\n\n\nData dictionary\n required by SnappyData store\n\n\nHive metastore\n required by Spark\n\n\n\n\nIn rare conditions, SnappyData catalog may become inconsistent, if an entry for the table exists only in one of the catalogs instead of exisiting in both.  One of the symptoms for such an inconsistency is that you get an error that indicates that the table you are creating already exists. However, when you drop the same table, the table is not found.\n\n\nFor example:\n\n\nsnappy\n create table t1(col1 int);\nERROR 42000: (SQLState=42000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Syntax error or analysis exception: createTable: Table APP.T1 already exists.;\nsnappy\n drop table t1;\nERROR 42X05: (SQLState=42X05 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Table/View 'APP.T1' does not exist.\n\n\n\n\nIn such cases, you can use a system procedure, \nSYS.REPAIR_CATALOG\n (Boolean \nremoveInconsistentEntries\n, Boolean \nremoveTablesWithData\n) to remove catalog inconsistencies.\n\n\nThe following parameters are accepted by the \nSYS.REPAIR_CATALOG\n procedure:\n\n\n\n\n\n\nremoveInconsistentEntries\n - If \ntrue\n, removes inconsistent entries from the catalog.\n\n\n\n\n\n\nremoveTablesWithData\n - If \ntrue\n, removes entries for tables even if those tables contain data. By default the entries are not removed.\n\n\n\n\n\n\nExample: Resolving Catalog Inconsistency Issues\n\n\nWhen both parameters are set to \nfalse\n, the \nSYS.REPAIR_CATALOG\n procedure checks for any inconsistencies in the catalog and prints warning messages in the SnappyData system server log.\n\n\nsnappy\n call sys.repair_catalog('false', 'false');\n\n\n\n\nIn case of inconsistencies, the log will contain messages as shown in the following example:\n\n\n18/08/06 17:28:36.456 IST ThriftProcessor-3\ntid=0x82\n WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1]\n18/08/06 17:28:36.457 IST ThriftProcessor-3\ntid=0x82\n WARN snappystore: CATALOG: Use system procedure SYS.REPAIR_CATALOG() to remove inconsistency\n\n\n\n\n\nYou can then remove the catalog inconsistency by passing \nremoveInconsistentEntries\n parameter as \ntrue\n.\n\n\nsnappy\n call sys.repair_catalog('true', 'false');\n\n\n\n\nLater, you can examine the log to check which entries were removed. Following is a sample log for reference:\n\n\n18/08/06 17:34:26.548 IST ThriftProcessor-3\ntid=0x82\n WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1]\n18/08/06 17:34:26.548 IST ThriftProcessor-3\ntid=0x82\n WARN snappystore: CATALOG: Removing table APP.T1 from Hive metastore", 
            "title": "Resolving Catalog Inconsistency Issues"
        }, 
        {
            "location": "/troubleshooting/catalog_inconsistency/#resolving-catalog-inconsistency-issues", 
            "text": "A SnappyData catalog internally maintains table metadata in two catalogs:   Data dictionary  required by SnappyData store  Hive metastore  required by Spark   In rare conditions, SnappyData catalog may become inconsistent, if an entry for the table exists only in one of the catalogs instead of exisiting in both.  One of the symptoms for such an inconsistency is that you get an error that indicates that the table you are creating already exists. However, when you drop the same table, the table is not found.  For example:  snappy  create table t1(col1 int);\nERROR 42000: (SQLState=42000 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Syntax error or analysis exception: createTable: Table APP.T1 already exists.;\nsnappy  drop table t1;\nERROR 42X05: (SQLState=42X05 Severity=20000) (Server=localhost/127.0.0.1[1528] Thread=ThriftProcessor-3) Table/View 'APP.T1' does not exist.  In such cases, you can use a system procedure,  SYS.REPAIR_CATALOG  (Boolean  removeInconsistentEntries , Boolean  removeTablesWithData ) to remove catalog inconsistencies.  The following parameters are accepted by the  SYS.REPAIR_CATALOG  procedure:    removeInconsistentEntries  - If  true , removes inconsistent entries from the catalog.    removeTablesWithData  - If  true , removes entries for tables even if those tables contain data. By default the entries are not removed.", 
            "title": "Resolving Catalog Inconsistency Issues"
        }, 
        {
            "location": "/troubleshooting/catalog_inconsistency/#example-resolving-catalog-inconsistency-issues", 
            "text": "When both parameters are set to  false , the  SYS.REPAIR_CATALOG  procedure checks for any inconsistencies in the catalog and prints warning messages in the SnappyData system server log.  snappy  call sys.repair_catalog('false', 'false');  In case of inconsistencies, the log will contain messages as shown in the following example:  18/08/06 17:28:36.456 IST ThriftProcessor-3 tid=0x82  WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1]\n18/08/06 17:28:36.457 IST ThriftProcessor-3 tid=0x82  WARN snappystore: CATALOG: Use system procedure SYS.REPAIR_CATALOG() to remove inconsistency  You can then remove the catalog inconsistency by passing  removeInconsistentEntries  parameter as  true .  snappy  call sys.repair_catalog('true', 'false');  Later, you can examine the log to check which entries were removed. Following is a sample log for reference:  18/08/06 17:34:26.548 IST ThriftProcessor-3 tid=0x82  WARN snappystore: CATALOG: Catalog inconsistency detected: following tables in Hive metastore are not in datadictionary: schema = APP tables = [T1]\n18/08/06 17:34:26.548 IST ThriftProcessor-3 tid=0x82  WARN snappystore: CATALOG: Removing table APP.T1 from Hive metastore", 
            "title": "Example: Resolving Catalog Inconsistency Issues"
        }, 
        {
            "location": "/troubleshooting/collect_debug_artifacts/", 
            "text": "Collecting logs, stats and dumps using the collect-debug-artifacts script\n\n\nThis section uses the term 'node' frequently. A node denotes a server or a locator member when a purely SnappyData system is there. In a SnappyData distributed system a node can mean server, locator or lead member.\n\n\nThe script \ncollect-debug-artifacts\n enables you to collect the debug information like logs and stats. It also has an option to dump stacks of the running system. Details of all the options and capabilities of the script can be found below. The main purpose of this is to ease the collection of these information. The script collects all the artifacts node wise and outputs a tar file which contains member wise information.\n\n\nPre-requisites for running the script:\n\n\nThe script assumes certain conditions to be fulfilled before it is invoked. Please ensure that these requirements are fulfilled because the script does not validate these.\nThe conditions are:\n\n\n\n\n\n\nThis script is expected to be run by a user who has read and write permissions on the output directories of all the SnappyData nodes.\n\n\n\n\n\n\nThe user should have one way passwordless ssh setup from one machine to the other machines where the SnappyData nodes are running.\n\n\n\n\n\n\nBelow is the usage of the script\n\n\n\n      \nlinux-shell\n ./sbin/collect-debug-artifacts.sh -h\n\nUsage: collect-debug-artifacts\n       [ -c conffile|--conf=conffile|--config=conffile ]\n       [ -o resultdir|--out=resultdir|--outdir=resultdir ]\n       [ -h|--help ]\n       [ -a|--all ]\n       [ -d|--dump ]\n       [ -v|--verbose ]\n       [ -s starttimestamp|--start=starttimestamp ]\n       [ -e endtimestamp|--end=endtimestamp ]\n       [ -x debugtarfile|--extract=debugtarfile ]\n\n       Timestamp format: YYYY-MM-DD HH:MM[:SS]\n\n\n\n\nOptions:\n\n\nAll the options of the script are optional. By default the script tries to get the current logs. All the logs starting from the last restart and the last file before that. It also brings all the stat file in the output directory. However if you want to change this behavior of the script you can use the following options to collect the debug information as per your requirements. Please note that no stack dumps are collected by default. You need to use the '-d, --dump' option to get the stack dumps.\n\n\n-h, --help\n  Prints a usage message summary briefly summarizing the command line options\n\n\n-c, --conf \n  The script uses a configration file which has three configuration elements.\n  1. MEMBERS_FILE -- This is a text file which has member information. Each line has the host machine name followed by the full path to the run directory of the member. This file is generated automatically when the sbin/start-all-scripts.sh is used.\n  2. NO_OF_STACK_DUMPS -- This parameter tells the script that how many stack dumps will be attempted per member/node of the running system.\n  3. INTERVAL_BETWEEN_DUMPS -- The amount of time in seconds the script waits between registering stack dumps.\n\n\n-o, --out, --outdir\n  The directory where the output file in the form of tar, will be created.\n\n\n-a, --all\n  With '-a or --all' option all the logs and stats file are collected from each members output directory.\n\n\n-d, --dump\n  Stack dumps are not collected by default or with -a, --all option. The user need to explicitly provide this argument if the stack dumps need to be collected.\n\n\n-v, --verbose\n  verbose mode is on.\n\n\n-s, --start\n  The script can also be asked to collect log files for specified time interval. The time interval can be specified using the start time and an end time parameter. Both the parameter needs to be specified. The format in which the time stamp can be specified is 'YYYY-MM-DD HH:MM[:SS]'\n\n\n-x, --extract=debugtarfile \n  To extract the contents of the tar file.\n\n\n       Timestamp format: YYYY-MM-DD HH:MM[:SS]", 
            "title": "Collecting Logs, Stats and Dumps Using the collect-debug-artifacts Script"
        }, 
        {
            "location": "/troubleshooting/collect_debug_artifacts/#collecting-logs-stats-and-dumps-using-the-collect-debug-artifacts-script", 
            "text": "This section uses the term 'node' frequently. A node denotes a server or a locator member when a purely SnappyData system is there. In a SnappyData distributed system a node can mean server, locator or lead member.  The script  collect-debug-artifacts  enables you to collect the debug information like logs and stats. It also has an option to dump stacks of the running system. Details of all the options and capabilities of the script can be found below. The main purpose of this is to ease the collection of these information. The script collects all the artifacts node wise and outputs a tar file which contains member wise information.  Pre-requisites for running the script:  The script assumes certain conditions to be fulfilled before it is invoked. Please ensure that these requirements are fulfilled because the script does not validate these.\nThe conditions are:    This script is expected to be run by a user who has read and write permissions on the output directories of all the SnappyData nodes.    The user should have one way passwordless ssh setup from one machine to the other machines where the SnappyData nodes are running.    Below is the usage of the script  \n       linux-shell  ./sbin/collect-debug-artifacts.sh -h\n\nUsage: collect-debug-artifacts\n       [ -c conffile|--conf=conffile|--config=conffile ]\n       [ -o resultdir|--out=resultdir|--outdir=resultdir ]\n       [ -h|--help ]\n       [ -a|--all ]\n       [ -d|--dump ]\n       [ -v|--verbose ]\n       [ -s starttimestamp|--start=starttimestamp ]\n       [ -e endtimestamp|--end=endtimestamp ]\n       [ -x debugtarfile|--extract=debugtarfile ]\n\n       Timestamp format: YYYY-MM-DD HH:MM[:SS]  Options:  All the options of the script are optional. By default the script tries to get the current logs. All the logs starting from the last restart and the last file before that. It also brings all the stat file in the output directory. However if you want to change this behavior of the script you can use the following options to collect the debug information as per your requirements. Please note that no stack dumps are collected by default. You need to use the '-d, --dump' option to get the stack dumps.  -h, --help\n  Prints a usage message summary briefly summarizing the command line options  -c, --conf \n  The script uses a configration file which has three configuration elements.\n  1. MEMBERS_FILE -- This is a text file which has member information. Each line has the host machine name followed by the full path to the run directory of the member. This file is generated automatically when the sbin/start-all-scripts.sh is used.\n  2. NO_OF_STACK_DUMPS -- This parameter tells the script that how many stack dumps will be attempted per member/node of the running system.\n  3. INTERVAL_BETWEEN_DUMPS -- The amount of time in seconds the script waits between registering stack dumps.  -o, --out, --outdir\n  The directory where the output file in the form of tar, will be created.  -a, --all\n  With '-a or --all' option all the logs and stats file are collected from each members output directory.  -d, --dump\n  Stack dumps are not collected by default or with -a, --all option. The user need to explicitly provide this argument if the stack dumps need to be collected.  -v, --verbose\n  verbose mode is on.  -s, --start\n  The script can also be asked to collect log files for specified time interval. The time interval can be specified using the start time and an end time parameter. Both the parameter needs to be specified. The format in which the time stamp can be specified is 'YYYY-MM-DD HH:MM[:SS]'  -x, --extract=debugtarfile \n  To extract the contents of the tar file.         Timestamp format: YYYY-MM-DD HH:MM[:SS]", 
            "title": "Collecting logs, stats and dumps using the collect-debug-artifacts script"
        }, 
        {
            "location": "/troubleshooting/troubleshooting_error_messages/", 
            "text": "Troubleshooting Error Messages\n\n\nError messages provide information about problems that might occur when setting up the SnappyData cluster or when running queries. \nYou can use the following information to resolve such problems.\n\n\n\n\n\nThe following topics are covered in this section:\n\n\n\n\n\n\nRegion {0} has potentially stale data. It is waiting for another member to recover the latest data.\n\n\n\n\n\n\nXCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query\n\n\n\n\n\n\n{0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}]\n\n\n\n\n\n\nRegion {0} bucket {1} has persistent data that is no longer online stored at these locations: {2}\n\n\n\n\n\n\nForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\"\n\n\n\n\nNode went down or data no longer available while iterating the results\n.\n\n\n\n\n\n\n \nError Message:\n \n \n\n\nRegion {0} has potentially stale data. It is waiting for another member to recover the latest data.\nMy persistent id:\n\n{1}\n\nMembers with potentially new data:\n\n{2}Use the \"{3} list-missing-disk-stores\" command to see all disk stores that are being waited on by other members.\n\n\n\n\n \nDiagnosis:\n\nThe above message is typically displayed during start up when a member waits for other members in the cluster to be available, as the table data on disk is not the most current. \n\nThe status of the member is displayed as \nwaiting\n in such cases when you \ncheck the status\n of the cluster using the \nsnappy-status-all.sh\n command.\n\n\n\n \nSolution:\n \n\nThe status of the waiting members change to online once all the members are online and the status of the waiting members is updated. Users can check whether the status is changed from \nwaiting\n to \nonline\n by using the \nsnappy-status-all.sh\n command or by checking the \nSnappyData Pulse UI\n.\n\n\n\n\n\n\n\n\n \nError Message:\n \n \n\n\nXCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query\n\n\n\n \nDiagnosis:\n\nThis error message is reported when a system runs on low available memory. In such cases, the queries may get aborted and an error is reported to prevent the server from crashing due to low available memory.\n\nOnce the heap memory usage falls below \ncritical-heap-percentage\n the queries run successfully.\n\n\n\n \nSolution:\n \n\nTo avoid such issues, review your memory configuration and make sure that you have allocated enough heap memory. \n\nYou can also configure tables for eviction so that table rows are evicted from memory and overflow to disk when the system crosses eviction threshold. For more details refer to best practices for \nmemory management\n\n\n\n\n\n\n\n\n \nMessage:\n \n\n\n\n{0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}]\n\n\n\n \nDiagnosis:\n\nThe above warning message is displayed when a member is awaiting for a response from another member on the system and response has not been received for some time.\n\n\n\n \nSolution:\n \n\nThis generally means that there is a resource issue in (most likely) the member that is in \nwaiting\n status. Check whether there is a garbage collection activity going on in the member being waited for. \nDue of large GC pauses, the member may not be responding in the stipulated time. In such cases, review your memory configuration and consider whether you can configure to use \noff-heap memory\n.\n\n\n\n\n\n\n\n \nError Message:\n \n \n\n\nRegion {0} bucket {1} has persistent data that is no longer online stored at these locations: {2}\n\n\n\n \nDiagnosis:\n\nIn partitioned tables that are persisted to disk, if you have any of the members offline, the partitioned table is still available, but, may have some buckets represented only in offline disk stores. In this case, methods that access the bucket entries report a PartitionOfflineException error.\n\n\n \nSolution:\n \n\nIf possible, bring the missing member online. This restores the buckets to memory and you can work with them again.\n\n\n\n\n\n\n\n \nError Message:\n \n \n\n\nForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\"\n\n\n\n \nDiagnosis:\n\nA distributed system member\u2019s Cache and DistributedSystem are forcibly closed by the system membership coordinator if it becomes sick or too slow to respond to heartbeat requests. The log file for the member displays a ForcedDisconnectException with the message. \n\nOne possible reason for this could be that large GC pauses are causing the member to be unresponsive when the GC is in progress. \n\n\n\n \nSolution:\n \n\nTo minimize the chances of this happening, you can increase the DistributedSystem property \nmember-timeout\n. This setting also controls the length of time required to notice a network failure. Also, review your memory configuration and configure to use \noff-heap memory\n.\n\n\n\n\n\n\n\n\n \nError Message:\n \n \n\n\nNode went down or data no longer available while iterating the results.\n\n\n\n \nDiagnosis:\n\nIn cases where a node fails while a JDBC/ODBC client or job is consuming result of a query, then it can result in the query failing with such an exception. \n\n\n\n \nSolution:\n \n\nThis is expected behaviour where the product does not retry, since partial results are already consumed by the application. Application must retry the entire query after discarding any changes due to partial results that are consumed.", 
            "title": "Troubleshooting Error Messages"
        }, 
        {
            "location": "/troubleshooting/troubleshooting_error_messages/#troubleshooting-error-messages", 
            "text": "Error messages provide information about problems that might occur when setting up the SnappyData cluster or when running queries.  You can use the following information to resolve such problems.   The following topics are covered in this section:    Region {0} has potentially stale data. It is waiting for another member to recover the latest data.    XCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query    {0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}]    Region {0} bucket {1} has persistent data that is no longer online stored at these locations: {2}    ForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\"   Node went down or data no longer available while iterating the results .      Error Message:     \nRegion {0} has potentially stale data. It is waiting for another member to recover the latest data.\nMy persistent id: \n{1} \nMembers with potentially new data: \n{2}Use the \"{3} list-missing-disk-stores\" command to see all disk stores that are being waited on by other members.     Diagnosis: \nThe above message is typically displayed during start up when a member waits for other members in the cluster to be available, as the table data on disk is not the most current.  \nThe status of the member is displayed as  waiting  in such cases when you  check the status  of the cluster using the  snappy-status-all.sh  command.    Solution:   \nThe status of the waiting members change to online once all the members are online and the status of the waiting members is updated. Users can check whether the status is changed from  waiting  to  online  by using the  snappy-status-all.sh  command or by checking the  SnappyData Pulse UI .      Error Message:     \nXCL54.T Query/DML/DDL '{0}' canceled due to low memory on member '{1}'. Try reducing the search space by adding more filter conditions to the where clause. query    Diagnosis: \nThis error message is reported when a system runs on low available memory. In such cases, the queries may get aborted and an error is reported to prevent the server from crashing due to low available memory. \nOnce the heap memory usage falls below  critical-heap-percentage  the queries run successfully.    Solution:   \nTo avoid such issues, review your memory configuration and make sure that you have allocated enough heap memory.  \nYou can also configure tables for eviction so that table rows are evicted from memory and overflow to disk when the system crosses eviction threshold. For more details refer to best practices for  memory management       Message:    \n{0} seconds have elapsed while waiting for reply from {1} on {2} whose current membership list is: [{3}]    Diagnosis: \nThe above warning message is displayed when a member is awaiting for a response from another member on the system and response has not been received for some time.    Solution:   \nThis generally means that there is a resource issue in (most likely) the member that is in  waiting  status. Check whether there is a garbage collection activity going on in the member being waited for. \nDue of large GC pauses, the member may not be responding in the stipulated time. In such cases, review your memory configuration and consider whether you can configure to use  off-heap memory .      Error Message:     \nRegion {0} bucket {1} has persistent data that is no longer online stored at these locations: {2}    Diagnosis: \nIn partitioned tables that are persisted to disk, if you have any of the members offline, the partitioned table is still available, but, may have some buckets represented only in offline disk stores. In this case, methods that access the bucket entries report a PartitionOfflineException error.    Solution:   \nIf possible, bring the missing member online. This restores the buckets to memory and you can work with them again.      Error Message:     \nForcedDisconnectException Error: \"No Data Store found in the distributed system for: {0}\"    Diagnosis: \nA distributed system member\u2019s Cache and DistributedSystem are forcibly closed by the system membership coordinator if it becomes sick or too slow to respond to heartbeat requests. The log file for the member displays a ForcedDisconnectException with the message.  \nOne possible reason for this could be that large GC pauses are causing the member to be unresponsive when the GC is in progress.     Solution:   \nTo minimize the chances of this happening, you can increase the DistributedSystem property  member-timeout . This setting also controls the length of time required to notice a network failure. Also, review your memory configuration and configure to use  off-heap memory .      Error Message:     \nNode went down or data no longer available while iterating the results.    Diagnosis: \nIn cases where a node fails while a JDBC/ODBC client or job is consuming result of a query, then it can result in the query failing with such an exception.     Solution:   \nThis is expected behaviour where the product does not retry, since partial results are already consumed by the application. Application must retry the entire query after discarding any changes due to partial results that are consumed.", 
            "title": "Troubleshooting Error Messages"
        }, 
        {
            "location": "/additional_files/open_source_components/", 
            "text": "SnappyData Community Edition (Open Source) and Enterprise Edition\n\n\nSnappyData offers a fully functional core OSS distribution, which is the \nCommunity edition\n, that is Apache 2.0 licensed. The \nEnterprise edition \nof the product includes everything that is offered in the OSS version along with additional capabilities that are closed source and only available as part of a licensed subscription. You can download the Enterprise version for evaluation after registering on the \nSnappyData website\n.\n\n\nThe capabilities of the Community edition and the additional capabilities of the Enterprise edition are listed in the following table:\n\n\n\n\n\n\n\n\nFeature\n\n\nCommunity\n\n\nEnterprise\n\n\n\n\n\n\n\n\n\n\nMutable Row \n Column Store\n\n\nX\n\n\nX\n\n\n\n\n\n\nCompatibility with Spark\n\n\nX\n\n\nX\n\n\n\n\n\n\nShared Nothing Persistence and HA\n\n\nX\n\n\nX\n\n\n\n\n\n\nREST API for Spark Job Submission\n\n\nX\n\n\nX\n\n\n\n\n\n\nFault Tolerance for Driver\n\n\nX\n\n\nX\n\n\n\n\n\n\nAccess to the system using JDBC Driver\n\n\nX\n\n\nX\n\n\n\n\n\n\nCLI for backup, restore, and export data\n\n\nX\n\n\nX\n\n\n\n\n\n\nSpark console extensions\n\n\nX\n\n\nX\n\n\n\n\n\n\nSystem Perf/Behavior statistics\n\n\nX\n\n\nX\n\n\n\n\n\n\nSupport for transactions in Row tables\n\n\nX\n\n\nX\n\n\n\n\n\n\nSupport for indexing in Row Tables\n\n\nX\n\n\nX\n\n\n\n\n\n\nSQL extensions for stream processing\n\n\nX\n\n\nX\n\n\n\n\n\n\nRuntime deployment of packages and jars\n\n\nX\n\n\nX\n\n\n\n\n\n\nSynopsis Data Engine for Approximate Querying\n\n\n\n\nX\n\n\n\n\n\n\nODBC Driver with High Concurrency\n\n\n\n\nX\n\n\n\n\n\n\nOff-heap data storage for column tables\n\n\n\n\nX\n\n\n\n\n\n\nCDC Stream receiver for SQL Server into SnappyData\n\n\n\n\nX\n\n\n\n\n\n\nGemFire/Apache Geode connector\n\n\n\n\nX\n\n\n\n\n\n\nRow Level Security\n\n\n\n\nX\n\n\n\n\n\n\nUse encrypted password instead of clear text password\n\n\n\n\nX\n\n\n\n\n\n\nRestrict Table, View, Function creation even in user\u2019s own schema\n\n\n\n\nX\n\n\n\n\n\n\nLDAP security interface\n\n\n\n\nX", 
            "title": "Community Edition/ Enterprise Edition Features"
        }, 
        {
            "location": "/additional_files/open_source_components/#snappydata-community-edition-open-source-and-enterprise-edition", 
            "text": "SnappyData offers a fully functional core OSS distribution, which is the  Community edition , that is Apache 2.0 licensed. The  Enterprise edition  of the product includes everything that is offered in the OSS version along with additional capabilities that are closed source and only available as part of a licensed subscription. You can download the Enterprise version for evaluation after registering on the  SnappyData website .  The capabilities of the Community edition and the additional capabilities of the Enterprise edition are listed in the following table:     Feature  Community  Enterprise      Mutable Row   Column Store  X  X    Compatibility with Spark  X  X    Shared Nothing Persistence and HA  X  X    REST API for Spark Job Submission  X  X    Fault Tolerance for Driver  X  X    Access to the system using JDBC Driver  X  X    CLI for backup, restore, and export data  X  X    Spark console extensions  X  X    System Perf/Behavior statistics  X  X    Support for transactions in Row tables  X  X    Support for indexing in Row Tables  X  X    SQL extensions for stream processing  X  X    Runtime deployment of packages and jars  X  X    Synopsis Data Engine for Approximate Querying   X    ODBC Driver with High Concurrency   X    Off-heap data storage for column tables   X    CDC Stream receiver for SQL Server into SnappyData   X    GemFire/Apache Geode connector   X    Row Level Security   X    Use encrypted password instead of clear text password   X    Restrict Table, View, Function creation even in user\u2019s own schema   X    LDAP security interface   X", 
            "title": "SnappyData Community Edition (Open Source) and Enterprise Edition"
        }, 
        {
            "location": "/additional_files/license_model/", 
            "text": "Licensing Model\n\n\nUsers can download the fully functional OSS version or register on the site and download the Enterprise edition for evaluation purposes. Users can deploy the OSS version into production and choose to purchase support subscriptions for the same. This guarantees access to product support teams and any new releases that are delivered including patches, and hotfixes for critical issues with time bound SLAs.\n The alternative is to deploy the OSS version into production and use the various community channels for support.\n\n\nThe Enterprise edition of the product can be used for evaluation purposes free of charge, but the license expressly prohibits deploying the product into production without acquiring a license subscription from SnappyData. \nYou can reach out to \nsales@snappydata.io\n for more information on purchasing license subscriptions for the product. Subscriptions are priced per core per year with the option to upgrade to premium support if the user desires to do so. Both the open source and enterprise versions can be deployed on-premise or in the cloud. Web based deployment of clusters on AWS and Azure (future support) is available for the product.", 
            "title": "License Model"
        }, 
        {
            "location": "/additional_files/license_model/#licensing-model", 
            "text": "Users can download the fully functional OSS version or register on the site and download the Enterprise edition for evaluation purposes. Users can deploy the OSS version into production and choose to purchase support subscriptions for the same. This guarantees access to product support teams and any new releases that are delivered including patches, and hotfixes for critical issues with time bound SLAs.  The alternative is to deploy the OSS version into production and use the various community channels for support.  The Enterprise edition of the product can be used for evaluation purposes free of charge, but the license expressly prohibits deploying the product into production without acquiring a license subscription from SnappyData.  You can reach out to  sales@snappydata.io  for more information on purchasing license subscriptions for the product. Subscriptions are priced per core per year with the option to upgrade to premium support if the user desires to do so. Both the open source and enterprise versions can be deployed on-premise or in the cloud. Web based deployment of clusters on AWS and Azure (future support) is available for the product.", 
            "title": "Licensing Model"
        }, 
        {
            "location": "/release_notes/release_notes/", 
            "text": "Release Notes\n\n\nThe SnappyData team is pleased to announce the availability of version 1.0.2.1 of the platform. You can find the release artifacts of its Community Edition towards the end of this page.\n\n\nYou can also download the Enterprise Edition \nhere\n. The following table summarizes the features available in Enterprise and OSS (Community) editions.\n\n\n\n\n\n\n\n\nFeature\n\n\nCommunity\n\n\nEnterprise\n\n\n\n\n\n\n\n\n\n\nMutable Row \n Column Store\n\n\nX\n\n\nX\n\n\n\n\n\n\nCompatibility with Spark\n\n\nX\n\n\nX\n\n\n\n\n\n\nShared Nothing Persistence and HA\n\n\nX\n\n\nX\n\n\n\n\n\n\nREST API for Spark Job Submission\n\n\nX\n\n\nX\n\n\n\n\n\n\nFault Tolerance for Driver\n\n\nX\n\n\nX\n\n\n\n\n\n\nAccess to the system using JDBC Driver\n\n\nX\n\n\nX\n\n\n\n\n\n\nCLI for backup, restore, and export data\n\n\nX\n\n\nX\n\n\n\n\n\n\nSpark console extensions\n\n\nX\n\n\nX\n\n\n\n\n\n\nSystem Perf/Behavior statistics\n\n\nX\n\n\nX\n\n\n\n\n\n\nSupport for transactions in Row tables\n\n\nX\n\n\nX\n\n\n\n\n\n\nSupport for indexing in Row Tables\n\n\nX\n\n\nX\n\n\n\n\n\n\nSQL extensions for stream processing\n\n\nX\n\n\nX\n\n\n\n\n\n\nRuntime deployment of packages and jars\n\n\nX\n\n\nX\n\n\n\n\n\n\nSynopsis Data Engine for Approximate Querying\n\n\n\n\nX\n\n\n\n\n\n\nODBC Driver with High Concurrency\n\n\n\n\nX\n\n\n\n\n\n\nOff-heap data storage for column tables\n\n\n\n\nX\n\n\n\n\n\n\nCDC Stream receiver for SQL Server into SnappyData\n\n\n\n\nX\n\n\n\n\n\n\nGemFire/Apache Geode connector\n\n\n\n\nX\n\n\n\n\n\n\nRow Level Security\n\n\n\n\nX\n\n\n\n\n\n\nUse encrypted password instead of clear text password\n\n\n\n\nX\n\n\n\n\n\n\nRestrict Table, View, Function creation even in user\u2019s own schema\n\n\n\n\nX\n\n\n\n\n\n\nLDAP security interface\n\n\n\n\nX\n\n\n\n\n\n\n\n\nNew Features\n\n\nSnappyData 1.0.2.1 version includes the following new features:\n\n\n\n\nSupport Spark's HiveServer2 in SnappyData cluster. Enables starting an embedded Spark HiveServer2 on leads in embedded mode.\n\n\nProvided a default \nStructured Streaming Sink implementation\n for SnappyData column and row tables. A Sink property can enable conflation of events with the same key columns. \n\n\nAdded a \n-agent \nJVM argument in the launch commands to kill the JVM as soon as Out-of-Memory(OOM) occurs. This is important because the VM sometimes used to crash in unexpected ways later as a side effect of this corrupting internal metadata which later gave restart troubles. \nHandling Out-of-Memory Error\n\n\nAllow \nNONE\n as a valid policy for \nserver-auth-provider\n. Essentially, the cluster can now be configured only for user authentication, and mutual peer to peer authentication of cluster members can be disabled by specifying this property as NONE.\n\n\nAdd support for query hints to force a join type. This may be useful for cases where the result is known to be small, for example, but plan rules cannot determine it.\n\n\nAllow \ndeleteFrom\n API to work as long as the dataframe contains key columns.\n\n\n\n\nPerformance Enhancements\n\n\nThe following performance enhancements are included in SnappyData 1.0.2.1 version:\n\n\n\n\n\n\nAvoid shuffle when join key columns are a superset of child partitioning.\n\n\n\n\n\n\nAdded a pooled version of SnappyData JDBC driver for Spark to connect to SnappyData cluster as JDBC data source. \nConnecting with JDBC Client Pool Driver\n \n\n\n\n\n\n\nAdded caching for hive catalog lookups. Meta-data queries with large number of tables take quite long because of nested loop joins between \nSYSTABLES\n and \nHIVETABLES\n for most meta-data queries. Even if the table numbers were in hundreds, it used to take much time. [SNAP-2657]\n\n\n\n\n\n\nSelect Fixes and Performance Related Fixes\n\n\nThe following defect fixes are included in SnappyData 1.0.2.1 version:\n\n\n\n\n\n\nReset the pool at the end of collect to avoid spillover of low latency pool setting to the latter operations that may not use the CachedDataFrame execution paths. [SNAP-2659]\n\n\n\n\n\n\nFixed: Column added using 'ALTER TABLE ... ADD COLUMN ...' through SnappyData shell does not reflect in spark-shell. [SNAP-2491]  \n\n\n\n\n\n\nFixed the occasional failures in serialization using \nCachedDataFrame\n, if the node is just starting/stopping. Also, fixed a hang in shutdown for cases where hive client close is trying to boot up the node again, waiting on the locks that are taken during the shutdown.\n\n\n\n\n\n\nLead and Lag window functions were failing due to incorrect analysis error. [SNAP-2566]\n\n\n\n\n\n\nFixed the \nvalidate-disk-store\n tool. It was not getting initialized with registered types. This was required to deserialize byte arrays being read from persisted files.\n\n\n\n\n\n\nFix schema in ResultSet metadata. It used to show the default schema \nAPP\n always.\n\n\n\n\n\n\nSometimes a false unique constraint violation happened due to removed or destroyed AbstractRegionEntry. Now an attempt is made to remove it from the index and another try is made to put the new value against the index key. [SNAP-2627]\n\n\n\n\n\n\nFix for memory leak in oldEntrieMap leading to \nLowMemoryException\n and \nOutOfMemoryException\n. [SNAP-2654]\n\n\n\n\n\n\nDescription of Download Artifacts\n\n\nThe following table describes the download artifacts included in SnappyData 1.0.2.1 version:\n\n\n\n\n\n\n\n\nArtifact Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsnappydata-1.0.2.1-bin.tar.gz\n\n\nFull product binary (includes Hadoop 2.7)\n\n\n\n\n\n\nsnappydata-1.0.2.1-without-hadoop-bin.tar.gz\n\n\nProduct without the Hadoop dependency JARs\n\n\n\n\n\n\nsnappydata-jdbc_2.11-1.0.2.2.jar\n\n\nClient (JDBC) JAR\n\n\n\n\n\n\nsnappydata-zeppelin_2.11-0.7.3.4.jar\n\n\nThe Zeppelin interpreter jar for SnappyData, compatible with Apache Zeppelin 0.7.3\n\n\n\n\n\n\nsnappydata-ec2-0.8.2.tar.gz\n\n\nScript to Launch SnappyData cluster on AWS EC2 instances", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release_notes/release_notes/#release-notes", 
            "text": "The SnappyData team is pleased to announce the availability of version 1.0.2.1 of the platform. You can find the release artifacts of its Community Edition towards the end of this page.  You can also download the Enterprise Edition  here . The following table summarizes the features available in Enterprise and OSS (Community) editions.     Feature  Community  Enterprise      Mutable Row   Column Store  X  X    Compatibility with Spark  X  X    Shared Nothing Persistence and HA  X  X    REST API for Spark Job Submission  X  X    Fault Tolerance for Driver  X  X    Access to the system using JDBC Driver  X  X    CLI for backup, restore, and export data  X  X    Spark console extensions  X  X    System Perf/Behavior statistics  X  X    Support for transactions in Row tables  X  X    Support for indexing in Row Tables  X  X    SQL extensions for stream processing  X  X    Runtime deployment of packages and jars  X  X    Synopsis Data Engine for Approximate Querying   X    ODBC Driver with High Concurrency   X    Off-heap data storage for column tables   X    CDC Stream receiver for SQL Server into SnappyData   X    GemFire/Apache Geode connector   X    Row Level Security   X    Use encrypted password instead of clear text password   X    Restrict Table, View, Function creation even in user\u2019s own schema   X    LDAP security interface   X", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release_notes/release_notes/#new-features", 
            "text": "SnappyData 1.0.2.1 version includes the following new features:   Support Spark's HiveServer2 in SnappyData cluster. Enables starting an embedded Spark HiveServer2 on leads in embedded mode.  Provided a default  Structured Streaming Sink implementation  for SnappyData column and row tables. A Sink property can enable conflation of events with the same key columns.   Added a  -agent  JVM argument in the launch commands to kill the JVM as soon as Out-of-Memory(OOM) occurs. This is important because the VM sometimes used to crash in unexpected ways later as a side effect of this corrupting internal metadata which later gave restart troubles.  Handling Out-of-Memory Error  Allow  NONE  as a valid policy for  server-auth-provider . Essentially, the cluster can now be configured only for user authentication, and mutual peer to peer authentication of cluster members can be disabled by specifying this property as NONE.  Add support for query hints to force a join type. This may be useful for cases where the result is known to be small, for example, but plan rules cannot determine it.  Allow  deleteFrom  API to work as long as the dataframe contains key columns.", 
            "title": "New Features"
        }, 
        {
            "location": "/release_notes/release_notes/#performance-enhancements", 
            "text": "The following performance enhancements are included in SnappyData 1.0.2.1 version:    Avoid shuffle when join key columns are a superset of child partitioning.    Added a pooled version of SnappyData JDBC driver for Spark to connect to SnappyData cluster as JDBC data source.  Connecting with JDBC Client Pool Driver      Added caching for hive catalog lookups. Meta-data queries with large number of tables take quite long because of nested loop joins between  SYSTABLES  and  HIVETABLES  for most meta-data queries. Even if the table numbers were in hundreds, it used to take much time. [SNAP-2657]", 
            "title": "Performance Enhancements"
        }, 
        {
            "location": "/release_notes/release_notes/#select-fixes-and-performance-related-fixes", 
            "text": "The following defect fixes are included in SnappyData 1.0.2.1 version:    Reset the pool at the end of collect to avoid spillover of low latency pool setting to the latter operations that may not use the CachedDataFrame execution paths. [SNAP-2659]    Fixed: Column added using 'ALTER TABLE ... ADD COLUMN ...' through SnappyData shell does not reflect in spark-shell. [SNAP-2491]      Fixed the occasional failures in serialization using  CachedDataFrame , if the node is just starting/stopping. Also, fixed a hang in shutdown for cases where hive client close is trying to boot up the node again, waiting on the locks that are taken during the shutdown.    Lead and Lag window functions were failing due to incorrect analysis error. [SNAP-2566]    Fixed the  validate-disk-store  tool. It was not getting initialized with registered types. This was required to deserialize byte arrays being read from persisted files.    Fix schema in ResultSet metadata. It used to show the default schema  APP  always.    Sometimes a false unique constraint violation happened due to removed or destroyed AbstractRegionEntry. Now an attempt is made to remove it from the index and another try is made to put the new value against the index key. [SNAP-2627]    Fix for memory leak in oldEntrieMap leading to  LowMemoryException  and  OutOfMemoryException . [SNAP-2654]", 
            "title": "Select Fixes and Performance Related Fixes"
        }, 
        {
            "location": "/release_notes/release_notes/#description-of-download-artifacts", 
            "text": "The following table describes the download artifacts included in SnappyData 1.0.2.1 version:     Artifact Name  Description      snappydata-1.0.2.1-bin.tar.gz  Full product binary (includes Hadoop 2.7)    snappydata-1.0.2.1-without-hadoop-bin.tar.gz  Product without the Hadoop dependency JARs    snappydata-jdbc_2.11-1.0.2.2.jar  Client (JDBC) JAR    snappydata-zeppelin_2.11-0.7.3.4.jar  The Zeppelin interpreter jar for SnappyData, compatible with Apache Zeppelin 0.7.3    snappydata-ec2-0.8.2.tar.gz  Script to Launch SnappyData cluster on AWS EC2 instances", 
            "title": "Description of Download Artifacts"
        }, 
        {
            "location": "/release_notes/known_issues/", 
            "text": "Known Issues\n\n\nThe following key issues have been registered as bugs in the SnappyData bug tracking system:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBUG ID\n\n\nTitle\n\n\nDescription\n\n\nWorkaround\n\n\n\n\n\n\n\n\n\n\nSNAP-1375\n\n\nJVM crash reported\n\n\nThis was reported on: \n - RHEL kernel version: 3.10.0-327.13.1.el7.x86_64 \n - Java version: 1.8.0_121\n\n\nTo resolve this, use: \n - RHEL kernel version: 3.10.0-693.2.2.el7.x86_64 \n - Java version: 1.8.0_144\n\n\n\n\n\n\nSNAP-1422\n\n\nCatalog in smart connector inconsistent with servers\n\n\nCatalog in smart connector inconsistent with servers|When a table is queried from spark-shell (or from an application that uses smart connector mode) the table metadata is cached on the smart connector side. \nIf this table is dropped from SnappyData embedded cluster (by using snappy-shell, or JDBC application, or a Snappy job), the metadata on the smart connector side stays cached even though catalog has changed (table is dropped). \nIn such cases, the user may see unexpected errors like \"org.apache.spark.sql.AnalysisException: Table `SNAPPYTABLE` already exists\"  in the smart connector app side for example for `DataFrameWriter.saveAsTable()` API if the same table name that was dropped is used in `saveAsTable()`\n\n\n \n1. User may either create a new SnappySession in such scenarios \nOR \n \n2. Invalidate the cache on the Smart Connector mode, for example by calling \n  `snappy.sessionCatalog.invalidateAll()`\n\n\n\n\n\n\nSNAP-1634\n\n\nCreating a temporary table with the same name as an existing table in any schema should not be allowed\n\n\nWhen creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results. \n\n\n Ensure that you create temporary tables with a unique name. \n\n\n\n\n\n\nSNAP-1753\n\n\nTPCH Q19 execution performance degraded in 0.9\n\n\nA disjunctive query (that is, query with two or more predicates joined by an OR clause) with common filter predicates may report performance issues.\n\n\nTo resolve this, the query should be rewritten in the following manner to achieve better performance:\n\n  \nselect\n        sum(l_extendedprice) \n    from\n        LINEITEM,\n        PART\n    where\n        (\n       p_partkey = l_partkey\n       and p_size between 1 and 5\n and l_shipinstruct = 'DELIVER IN PERSON'\n        )\n        or\n        (\n       p_partkey = l_partkey\n       and p_brand = 'Brand#?'\n       and l_shipinstruct = 'DELIVER IN PERSON'\n        )\n\n\n\n\nselect\n        sum(l_extendedprice)\n    from\n        LINEITEM,\n        PART\n    where\n        ( p_partkey = l_partkey and l_shipinstruct = 'DELIVER IN PERSON') and \n        ( p_size between 1 and 5 or  p_brand = 'Brand#3')\n\n\n\n\n\n\n\n\n\nSNAP-1911\n\n\nJVM crash reported\n\n\nThis was reported on: \n -  RHEL kernel version: 3.10.0-327.13.1.el7.x86_64\n - Java version: 1.8.0_131\n\n\nTo resolve this, use: \n - RHEL kernel version: 3.10.0-693.2.2.el7.x86_64\n - Java version: 1.8.0_144\n\n\n\n\n\n\nSNAP-1999\n\n\nJVM crash reported\n\n\nThis was reported on: \n - RHEL kernel version: 3.10.0-327.13.1.el7.x86_64 \n - Java version: 1.8.0_131\n\n\nTo resolve this, use: \n - RHEL kernel version: 3.10.0-693.2.2.el7.x86_64 \n - Java version: 1.8.0_144\n\n\n\n\n\n\nSNAP-2017\n\n\nJVM crash reported\n\n\nThis was reported on: \n - RHEL kernel version: 3.10.0-514.10.2.el7.x86_64 \n - Java version: 1.8.0_144\n\n\nTo resolve this, use: \n -  RHEL kernel version:\u00a03.10.0-693.2.2.el7.x86_64 \n - Java version: 1.8.0_144\n\n\n\n\n\n\nSNAP-2436\n\n\nData mismatch in queries running on servers coming up after a failure\n\n\nData mismatch is observed in queries which are running when some servers are coming up after a failure. Also, the tables on which the queries are running must have set their redundancy to one or more for the issue to be observed. \n\n\nThis issue happens due to Spark retry mechanism with SnappyData tables. To avoid this issue, you can stop all the queries when one or more servers are coming up. If that is not feasible, you should configure the lead node with `spark.task.maxFailures = 0`; \n\n\n\n\n\n\nSNAP-2381\n\n\nData inconsistency due to concurrent putInto/update operations\n\n\nConcurrent putInto/update operations and inserts in column tables with overlapping keys may cause data inconsistency.  \n\n\nThis problem is not seen when all the concurrent operations deal with different sets of rows. You can either ensure serialized mutable operations on column tables or these should be working on a distinct set of key columns.\n\n\n\n\n\n\nSNAP-2457\n\n\nInconsistent results during further transformation when using snappySession.sql() from jobs, Zeppelin etc. \n\n\nWhen using snappySession.sql() from jobs, Zeppelin etc, if a further transformation is applied on the DataFrame, it may give incorrect results due to plan caching.  \n\n\nIf you are using SnappyJobs and using snappySession.sql(\"sql string\") you must ensure that further transformation is not done. For example:\n\n\n  \nval df1 = snappySession.sql(\"sql string\")\nval df2 = df1.repartition(12) // modifying df1\ndf2.collect()\n\n\n\nThe above operation will give inconsistent results, if you are using df2 further in your code.\nTo avoid this problem, you can use snappySession.sqlUncached(\"sql string\"). For example:\n\n\n \nval df1 = snappySession.sqlUncached(\"sql string\")\nval df2 = df1.repartition(12) // modifying df1\ndf2.collect()", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release_notes/known_issues/#known-issues", 
            "text": "The following key issues have been registered as bugs in the SnappyData bug tracking system:           BUG ID  Title  Description  Workaround      SNAP-1375  JVM crash reported  This was reported on:   - RHEL kernel version: 3.10.0-327.13.1.el7.x86_64   - Java version: 1.8.0_121  To resolve this, use:   - RHEL kernel version: 3.10.0-693.2.2.el7.x86_64   - Java version: 1.8.0_144    SNAP-1422  Catalog in smart connector inconsistent with servers  Catalog in smart connector inconsistent with servers|When a table is queried from spark-shell (or from an application that uses smart connector mode) the table metadata is cached on the smart connector side.  If this table is dropped from SnappyData embedded cluster (by using snappy-shell, or JDBC application, or a Snappy job), the metadata on the smart connector side stays cached even though catalog has changed (table is dropped).  In such cases, the user may see unexpected errors like \"org.apache.spark.sql.AnalysisException: Table `SNAPPYTABLE` already exists\"  in the smart connector app side for example for `DataFrameWriter.saveAsTable()` API if the same table name that was dropped is used in `saveAsTable()`   \n1. User may either create a new SnappySession in such scenarios  OR   \n2. Invalidate the cache on the Smart Connector mode, for example by calling    `snappy.sessionCatalog.invalidateAll()`    SNAP-1634  Creating a temporary table with the same name as an existing table in any schema should not be allowed  When creating a temporary table, the SnappyData catalog is not referred, which means, a temporary table with the same name as that of an existing SnappyData table can be created. Two tables with the same name lead to ambiguity during query execution and can either cause the query to fail or return wrong results.    Ensure that you create temporary tables with a unique name.     SNAP-1753  TPCH Q19 execution performance degraded in 0.9  A disjunctive query (that is, query with two or more predicates joined by an OR clause) with common filter predicates may report performance issues.  To resolve this, the query should be rewritten in the following manner to achieve better performance:   \nselect\n        sum(l_extendedprice) \n    from\n        LINEITEM,\n        PART\n    where\n        (\n       p_partkey = l_partkey\n       and p_size between 1 and 5\n and l_shipinstruct = 'DELIVER IN PERSON'\n        )\n        or\n        (\n       p_partkey = l_partkey\n       and p_brand = 'Brand#?'\n       and l_shipinstruct = 'DELIVER IN PERSON'\n        )  \nselect\n        sum(l_extendedprice)\n    from\n        LINEITEM,\n        PART\n    where\n        ( p_partkey = l_partkey and l_shipinstruct = 'DELIVER IN PERSON') and \n        ( p_size between 1 and 5 or  p_brand = 'Brand#3')     SNAP-1911  JVM crash reported  This was reported on:   -  RHEL kernel version: 3.10.0-327.13.1.el7.x86_64  - Java version: 1.8.0_131  To resolve this, use:   - RHEL kernel version: 3.10.0-693.2.2.el7.x86_64  - Java version: 1.8.0_144    SNAP-1999  JVM crash reported  This was reported on:   - RHEL kernel version: 3.10.0-327.13.1.el7.x86_64   - Java version: 1.8.0_131  To resolve this, use:   - RHEL kernel version: 3.10.0-693.2.2.el7.x86_64   - Java version: 1.8.0_144    SNAP-2017  JVM crash reported  This was reported on:   - RHEL kernel version: 3.10.0-514.10.2.el7.x86_64   - Java version: 1.8.0_144  To resolve this, use:   -  RHEL kernel version:\u00a03.10.0-693.2.2.el7.x86_64   - Java version: 1.8.0_144    SNAP-2436  Data mismatch in queries running on servers coming up after a failure  Data mismatch is observed in queries which are running when some servers are coming up after a failure. Also, the tables on which the queries are running must have set their redundancy to one or more for the issue to be observed.   This issue happens due to Spark retry mechanism with SnappyData tables. To avoid this issue, you can stop all the queries when one or more servers are coming up. If that is not feasible, you should configure the lead node with `spark.task.maxFailures = 0`;     SNAP-2381  Data inconsistency due to concurrent putInto/update operations  Concurrent putInto/update operations and inserts in column tables with overlapping keys may cause data inconsistency.    This problem is not seen when all the concurrent operations deal with different sets of rows. You can either ensure serialized mutable operations on column tables or these should be working on a distinct set of key columns.    SNAP-2457  Inconsistent results during further transformation when using snappySession.sql() from jobs, Zeppelin etc.   When using snappySession.sql() from jobs, Zeppelin etc, if a further transformation is applied on the DataFrame, it may give incorrect results due to plan caching.    If you are using SnappyJobs and using snappySession.sql(\"sql string\") you must ensure that further transformation is not done. For example:   \nval df1 = snappySession.sql(\"sql string\")\nval df2 = df1.repartition(12) // modifying df1\ndf2.collect() \n\nThe above operation will give inconsistent results, if you are using df2 further in your code.\nTo avoid this problem, you can use snappySession.sqlUncached(\"sql string\"). For example:  \nval df1 = snappySession.sqlUncached(\"sql string\")\nval df2 = df1.repartition(12) // modifying df1\ndf2.collect()", 
            "title": "Known Issues"
        }, 
        {
            "location": "/prev_doc_ver/", 
            "text": "Archived SnappyData Documentation\n\n\nClick a release to check the corresponding archived product documentation of SnappyData:\n\n\n\n\n\n\nSnappyData 1.0.2\n\n\n\n\n\n\nSnappyData 1.0.1\n\n\n\n\n\n\nSnappyData 1.0.0", 
            "title": "Doc Archives"
        }, 
        {
            "location": "/prev_doc_ver/#archived-snappydata-documentation", 
            "text": "Click a release to check the corresponding archived product documentation of SnappyData:    SnappyData 1.0.2    SnappyData 1.0.1    SnappyData 1.0.0", 
            "title": "Archived SnappyData Documentation"
        }, 
        {
            "location": "/techsupport/", 
            "text": "Mail Us\n\n\nYou can contact the SnappyData support team using:\n\n\n\n\nchomp@snappydata.io\n\n\nsupport@snappydata.io\n \n\n\n\n\nCommunity\n\n\nThe following channels are monitored for comments/questions:\n\n\n\n\n\n\nStackoverflow\n\n\n\n\n\n\nSlack\n\n\n\n\n\n\nGitter\n \n\n\n\n\n\n\nIRC\n \n\n\n\n\n\n\nReddit\n \n\n\n\n\n\n\nJIRA", 
            "title": "Contact and Support"
        }, 
        {
            "location": "/techsupport/#mail-us", 
            "text": "You can contact the SnappyData support team using:   chomp@snappydata.io  support@snappydata.io", 
            "title": "Mail Us"
        }, 
        {
            "location": "/techsupport/#community", 
            "text": "The following channels are monitored for comments/questions:    Stackoverflow    Slack    Gitter      IRC      Reddit      JIRA", 
            "title": "Community"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "LICENSE\n\n\n                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n\n\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n\n\n\n\nDefinitions.\n\n\n\"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n\n\"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n\n\"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n\n\"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n\n\"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n\n\"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n\n\n\n\n\nGrant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n\n\n\n\n\nGrant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n\n\n\n\n\nRedistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n\n(a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n\n(b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n\n(c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n\nYou may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n\n\n\n\n\nSubmission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n\n\n\n\n\nTrademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n\n\n\n\n\nDisclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n\n\n\n\n\nLimitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n\n\n\n\n\nAccepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n\n\n\n\n\nEND OF TERMS AND CONDITIONS\n\n\nAPPENDIX: How to apply the Apache License to your work.\n\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n\n\n\nCopyright 2018 SnappyData Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/LICENSE/#license", 
            "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION    Definitions.  \"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.  \"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.  \"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.  \"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.  \"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.  \"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.  \"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).  \"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.  \"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"  \"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.    Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.    Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.    Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:  (a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and  (b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and  (c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and  (d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.  You may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.    Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.    Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.    Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.    Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.    Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.    END OF TERMS AND CONDITIONS  APPENDIX: How to apply the Apache License to your work.    To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.  Copyright 2018 SnappyData Inc.  Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "LICENSE"
        }
    ]
}