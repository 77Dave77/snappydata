{
    "docs": [
        {
            "location": "/", 
            "text": "Table of Contents\n\n\n\n\nIntroduction\n\n\nDownload binary distribution\n\n\nCommunity Support\n\n\nLink with SnappyData distribution\n\n\nWorking with SnappyData Source Code\n\n\nBuilding SnappyData from source\n\n\n\n\n\n\nKey Features\n\n\nGetting started\n\n\nObjectives\n\n\nSnappyData Cluster\n\n\nStep 1 - Start the SnappyData cluster\n\n\n\n\n\n\nInteracting with SnappyData\n\n\nGetting Started with SQL\n\n\nColumn and Row tables\n\n\nStep 2 - Create column table, row table and load data\n\n\nOLAP and OLTP queries\n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nApproximate query processing (AQP)\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nStream analytics using SQL and Spark Streaming\n\n\nTop-K Elements in a Stream\n\n\nStep 5 - Create and Query Stream Table and Top-K Declaratively\n\n\n\n\n\n\nGetting Started with Spark API\n\n\nColumn and Row tables\n\n\nStep 2 - Create column table, row table and load data\n\n\nOLAP and OLTP Store\n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nApproximate query processing (AQP)\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nStream analytics using Spark Streaming\n\n\nTop-K Elements in a Stream\n\n\nStep 5 - Create and Query Stream Table and Top-K\n\n\nWorking with Spark shell and spark-submit\n\n\nStep 6 - Submit a Spark App that interacts with SnappyData\n\n\n\n\n\n\nFinal Step - Stop the SnappyData Cluster\n\n\n\n\nIntroduction\n\n\nSnappyData is a \ndistributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster\n. We realize this platform through a seamless integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics). \n\n\n\n\nDownload binary distribution\n\n\nYou can download the latest version of SnappyData from [here][2]. SnappyData has been tested on Linux (mention kernel version) and Mac (OS X 10.9 and 10.10?). If not already installed, you will need to download scala 2.10 and \nJava 8\n.  (this info should also be in the download page on our web site) \nSkip to Getting Started\n\n\nCommunity Support\n\n\nWe monitor channels listed below for comments/questions. We prefer using Stackoverflow. \n\n\nStackoverflow\n \n    \nSlack\n        Gitter \n          \nIRC\n \n             \nReddit\n \n          JIRA \n\n\nLink with SnappyData distribution\n\n\nSnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:\n\n\ngroupId: io.snappydata\nartifactId: snappydata_2.10\nversion: 0.1_preview\n\n\n\n\nWorking with SnappyData Source Code\n\n\n(Info for our download page?)\nIf you are interested in working with the latest code or contributing to SnappyData development, you can also check out the master branch from Git:\n\n\nMaster development branch\ngit clone https://github.com/SnappyDataInc/snappydata.git\n\n###### 0.1 preview release branch with stability fixes ######\ngit clone https://github.com/SnappyDataInc/snappydata.git -b 0.1_preview (??)\n\n\n\n\nBuilding SnappyData from source\n\n\nYou will find the instructions for building, layout of the code, integration with IDEs using Gradle, etc, \nhere\n\n\n\n\nNOTE:\nSnappyData is built using Spark 1.6 (build xx) which is packaged as part of SnappyData. While you can build your application using Apache Spark 1.5, you will need to link to Snappy-spark to make  use of the SnappyData extensions. Gradle build tasks are packaged.  \n\n\n\n\nKey Features\n\n\n\n\n100% compatible with Spark\n: Use SnappyData as a database as well as use any of the Spark APIs - ML, Graph, etc. on the same data\n\n\nIn-memory row and column store\n: Run the store collocated in Spark executors (i.e. a single compute and data cluster) or in its own process space (i.e. separate compute and data cluster)\n\n\nSQL standard compliance\n: Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.\n\n\nSQL based extensions for streaming processing\n: Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.\n\n\nInteractive analytics using Approximate query processing(AQP)\n: We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions. \n\n\nMutate, transact on data in Spark\n: Use SQL to insert, update, delete data in tables(something that you cannot do in Spark). We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store. \n\n\nOptimizations\n: Use indexes to improve query performance in the row store (the GemFire SQL optimizer automatically uses in-memory indexes when available) \n\n\nHigh availability not just Fault tolerance\n: Data is instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications with continuous HA.\n\n\nDurability and recovery:\n Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled. \n\n\n\n\nRead SnappyData \ndocs\n for a more detailed list of all features and semantics. \n\n\nGetting started\n\n\nObjectives\n\n\n\n\nIn-memory Column and Row tables\n: Illustrate both SQL syntax and Spark API to create and manage column tables for large data and how row tables can be used for reference data and can be replicated to each node in the cluster. \n\n\nOLAP, OLTP operations\n: We run analytic class SQL queries (full scan with aggregations) on column tables and fully distributed join queries and observe the space requirements as well as the performance of these queries. For OLTP, we run simple update queries - you can note the Spark API extensions to support mutations in Spark. \n\n\nAQP\n: We run the same analytic queries by creating adjunct stratified samples to note the performance difference - can we get close to interactive query performance speeds?\n\n\nStreaming with SQL\n: We ingest twitter streams into both a probabilistic data structure for TopK time series analytics and the entire stream (full data set) into a row table. We run both ad-hoc queries on these streams (modeled as tables) as well as showcase our first preview for continuous querying support. The SnappyData value add demonstrated here is simpler, SQL centric abstractions on top of Spark streaming. And, of course, ingestion into the built-in store.\n\n\n\n\nIn this document, we discuss the features mentioned above and ask you to take steps to run the scripts that demonstrate these features. \n\n\nSnappyData Cluster\n\n\nSnappyData, a database server cluster, has three main components - Locator, Server and Lead. \n\n\n\n\nLocator\n: Provides discovery service for the cluster. Informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n\n\nLead Node\n: Acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n\n\nData Servers\n: Hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.\n\n\n\n\n\n\nDetails of about the architecture can be found \nhere\n. SnappyData also has multiple deployment options which can be found \nhere\n.\n\n\nStep 1 - Start the SnappyData cluster\n\n\n\n\nNote\n\n\nThe U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. \nSummary information on the number of on-time, delayed, canceled and diverted flights is available for the last 20 years. We use this data set in the examples below. You can learn more on this schema \nhere\n.\nDefault airline data shipped with product is of 15 MB compressed size. If you are interested in studying Approximate query processing we recommend downloading the full data set run this command (from quickstart/scripts directory):\n\n\n\n\n$ ./download_full_airlinedata.sh ../data \n\n\n\n\n\n\nDo we need this? seems to work with default cluster ...\n\n\n\n\nIn case you are running Getting Started with full dataset, configure snappy to start two servers with max heap size as 4G each. \n\n\n\n\n$ cat conf/servers\n# Two servers with total of 8G.\nyourhostName -J-Xmx4g\nyourhostName -J-Xmx4g \n\n\n\n\n\n\nPasswordless ssh\n\n\nThe quick start scripts use ssh to start up various processes. By default, this requires a password. To be able to log on to the localhost and run the script without being prompted for the password, please enable passwordless ssh.\n\n\n\n\nThe following script starts up a minimal set of essential components to form the cluster - A locator, one data server and one lead node. All nodes are started locally. To spin up remote nodes simply rename/copy the files without the template suffix and add the hostnames. The \narticle\n discusses the custom configuration and startup options.\n\n\n$ sbin/snappy-start-all.sh \n  (Roughly can take upto a minute. Associated logs are in the \u2018work\u2019 sub-directory)\nThis would output something like this ...\nlocalhost: Starting SnappyData Locator using peer discovery on: 0.0.0.0[10334]\n...\nlocalhost: SnappyData Locator pid: 56703 status: running\n\nlocalhost: Starting SnappyData Server using locators for peer discovery: jramnara-mbpro[10334]   (port used for members to form a p2p cluster)\nlocalhost: SnappyData Server pid: 56819 status: running\nlocalhost:   Distributed system now has 2 members.\n\nlocalhost: Starting SnappyData Leader using locators for peer discovery: jramnara-mbpro[10334]\nlocalhost: SnappyData Leader pid: 56932 status: running\nlocalhost:   Distributed system now has 3 members.\n\nlocalhost:   Other members: jramnara-mbpro(56703:locator)\nv0\n:54414, jramnara-mbpro(56819:datastore)\nv1\n:39737\n\n\n\n\n\nAt this point, the SnappyData cluster is up and running and is ready to accept Snappy jobs and SQL requests via JDBC/ODBC. You can \nmonitor the Spark cluster at port 4040\n. Once you load data and run queries, you can analyze the Spark SQL query plan, the job execution stages and storage details of column tables.\n\n\n\n\n\n\nInteracting with SnappyData\n\n\n\n\nWe assume some familiarity with \ncore Spark, Spark SQL and Spark Streaming concepts\n. \nAnd, you can try out the Spark \nQuick Start\n. All the commands and programs\nlisted in the Spark guides will work in SnappyData also.\n\n\n\n\nTo interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to SnappyData cluster and interact using SQL. On the other hand, users comfortable with Spark programming paradigm can write Snappy jobs to interact with SnappyData. Snappy jobs can be like a self contained Spark application or can share state with other jobs using SnappyData store. \n\n\nUnlike Apache Spark, which is primarily a computational engine, SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.\n\n\n\n\nLong running executors\n: Executors are running within the Snappy store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors. \n\n\nDriver runs in HA configuration\n: Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the \nSpark JobServer\n to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). \nRead \ndocs\n for details of the architecture.\n\n\n\n\nIn this document, we showcase mostly the same set of features via Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to \nGetting Started with Spark API\n.\n\n\nGetting Started with SQL\n\n\nFor SQL, the SnappyData SQL Shell (\nsnappy-shell\n) provides a simple way to inspect the catalog,  run admin operations,  manage the schema and run interactive queries. You can also use your favorite SQL tool like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster).\n\n\n// Run from the SnappyData base directory\n$ ./bin/snappy-shell\nVersion 2.0-SNAPSHOT.1\nsnappy\n \n\n-- Connect to the cluster ..\nsnappy\n connect client 'localhost:1527';\nsnappy\n show connections; \n\n-- Check the cluster status\nthis will list each cluster member and its status\nsnappy\n show members;\n\n\n\n\nColumn and Row tables\n\n\nColumn tables\n organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\n-- DDL to create a column table\nCREATE TABLE AIRLINE (\ncolumn definitions\n) USING column OPTIONS(buckets '5') ;\n\n\n\n\nRow tables\n, unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.\n\n\ncreate table\n DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary \ndocs\n for the details.\n\n\n-- DDL to create a row table\nCREATE TABLE AIRLINEREF (\ncolumn definitions\n) USING row OPTIONS() ;\n\n\n\n\nStep 2 - Create column table, row table and load data\n\n\n\n\nTo run the scripts with full airline data set, change the 'create_and_load_column_table.sql' script to point at the data set that you had downloaded in Step 1.\n\n\n\n\nSQL scripts to create and load column and row tables.\n\n\n-- Loads parquet formatted data into a temporary spark table \n-- then saves it in  column table called Airline.\nsnappy\n run './quickstart/scripts/create_and_load_column_table.sql';\n\n-- Creates the airline code table. Row tables can be replicated to each node \n-- so join processing with other tables can completely avoid shuffling \nsnappy\n run './quickstart/scripts/create_and_load_row_table.sql';\n\n-- See the status of system\nsnappy\n run './quickstart/scripts/status_queries.sql'\n\n\n\n\nYou can see the memory consumed on \nSpark Console\n. \n\n\nOLAP and OLTP queries\n\n\nSQL client connections (via JDBC or ODBC) are routed to the appropriate data server via the locator (Physical connections are automatically created in the driver and are transparently swizzled in case of failures also). When queries are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP.  Such queries are routed to the \nlead\n node where a \nSpark SQLContext\n is managed for each connection. The Query is planned using Spark's Catalyst engine and scheduled to be executed on the data servers. The number of partitions determine the number of concurrent tasks used across the data servers to parallel run the query. In this case, our column table was created using \n5 partitions(buckets)\n and hence will use 5 concurrent tasks. \n\n\n---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nsnappy\n select AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;\n\n\n\n\nFor low latency OLTP queries, the engine won't route it to the lead and instead execute it immediately without any scheduling overhead. Quite often, this may mean simply fetching a row by hashing a key (in microseconds).\n\n\n--- Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'\n--- the airline code can be updated in the row table\nUPDATE AIRLINEREF SET DESCRIPTION='Delta America' WHERE CAST(CODE AS VARCHAR(25))='DL';\n\n\n\n\nSpark SQL can cache DataFrames as temporary tables and the data set is immutable. SnappyData SQL is compatible with the SQL standard with support for transactions and DML (insert, update, delete) on tables. \nLink to Snappy Store SQL reference\n.  As we show later, any table in Snappy is also visible as Spark DataFrame. \n\n\nStep 3 - Run OLAP and OLTP queries\n\n\n-- Simply run the script or copy/paste one query at a time if you want to explore the query execution on the Spark console. \nsnappy\n run './quickstart/scripts/olap_queries.sql';\n\n---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nselect AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;\n\n\n\n\nEach query is executed as one or more Jobs and each Job executed in one or more stages. You can explore the query execution plan and metrics \nhere\n\n\n-- Run a simple update SQL statement on the replicated row table.\nsnappy\n run './quickstart/scripts/oltp_queries.sql';\n\n\n\n\nYou can now re-run olap_queries.sql to see the updated join result set.\n\n\n\n\nNote\n\nIn the current implementation we only support appending to Column tables. Future releases will support all DML operations. \nYou can execute transactions using commands \nautocommit off\n and \ncommit\n.  \n\n\n\n\nApproximate query processing (AQP)\n\n\nOLAP queries are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory queries above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs and DDL to specify one or more curated \nstratified samples\n on large tables. \n\n\n\n\nNote\n\n\nWe recommend downloading the \nonTime airline\n data for 2009-2015 which is about 52 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.\n\n\n\n\nThe following DDL creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your \nGroup by\n and \nWhere\n make up the \nQuery Column Set\n (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection. \n\n\nCREATE SAMPLE TABLE AIRLINE_SAMPLE\n   OPTIONS(\n    buckets '5',                          -- Number of partitions \n    qcs 'UniqueCarrier, Year_, Month_',   -- QueryColumnSet(qcs): The strata - 3% of each combination of Carrier, \n                                          -- Year and Month are stored as sample\n    fraction '0.03',                      -- How big should the sample be\n    strataReservoirSize '50',             -- Reservoir sampling to support streaming inserts\n    basetable 'Airline')                  -- The parent base table\n\n\n\n\nYou can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the \nWith Error\n constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set. \n\n\n-- What is the average arrival delay for all airlines for each month?;\nsnappy\n select avg(ArrDelay), Month_ from Airline where ArrDelay \n0 \n    group by Month_\n    with error .05 ;\n-- The above query will consult the sample and return an answer if the estimated answer \n-- is at least 95% accurate (here, by default we use a 95% confidence interval). Read [docs](docs) for more details.\n\n-- You can also access the error using built-in functions. \nsnappy\n select avg(ArrDelay) avgDelay, absolute_error(avgDelay), Month_ \n    from Airline where ArrDelay \n0 \n    group by Month_\n    with error .05 ;\n\n\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\n--- Creates and then samples a table from the Airline table \nsnappy\n run 'create_and_load_sample_table.sql';\n\n\n\n\nYou can now re-run the previous OLAP queries with an error constraint and compare the results.  You should notice a 10X or larger difference in query execution latency while the results remain nearly accurate. As a reminder, we recommend downloading the larger data set for this exercise.\n\n\n-- re-run olap queries with error constraint to automatically use sampling\nsnappy\n run 'olap_approx_queries.sql';\n\n\n\n\nStream analytics using SQL and Spark Streaming\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. You can also dynamically run SQL queries on these streams. There is no need to learn Spark streaming APIs or statically define all the rules to be executed on these streams.\n\n\nThe commands below consume tweets, filters our just the hashtags and converts these into Row objects, models the stream as a table (so it can be queried) and we then run ad-hoc SQL from remote clients on the current state of the stream. \n\n\n--- Inits the Streaming Context with the batch interval of 2 seconds.\n--- i.e. the stream is processed once every 2 seconds.\nsnappy\n STREAMING INIT 2\n--- create a stream table just containing the hashtags\nsnappy\n CREATE STREAM TABLE HASHTAG_FILESTREAMTABLE\n              (hashtag string)\n            USING file_stream\n            OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',\n              rowConverter 'org.apache.spark.sql.streaming.TweetToHashtagRow',\n              directory '/tmp/copiedtwitterdata')\n-- A file_stream data source monitors the directory and as files arrives they are ingested \n--   into the streaming pipeline. First converted into Rows using 'TweetToHashtagRow' then visible as table\n--- Start streaming context \nsnappy\n STREAMING START\n--- Adhoc sql on the stream table to query the current batch\n--- Get top 10 popular hashtags ----\nsnappy\n SELECT hashtag, count(*) as tagcount\n        FROM HASHTAG_FILESTREAMTABLE\n        GROUP BY hashtag\n        ORDER BY tagcount DESC limit 10;\n\n\n\n\nLater, in the Spark code section we further enhance to showcase \"continuous queries\" (CQ). Dynamic registration of CQs (from remote clients) will be available in the next release.\n\n\nTop-K Elements in a Stream\n\n\nFinding the \nk\n most popular elements in a data stream is a common analytic query. For instance, top-100 pages on a popular website in the last 10 mins, top-10 sales regions in the last week, etc. As you can tell, if the query is on a arbitrary time interval in the past, this will most likely mandate storing the entire stream. And, this could easily be millions to billions of events is use cases like in IoT. SnappyData provides SQL extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures and enables transparent querying using Spark SQL. More details about SnappyData's implementation of top-k can be found \nhere.\n\n\nSnappyData provides DDL extensions to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using regular SQL queries. \n\n\n--- Create a topk table from a stream table\nCREATE TOPK TABLE filestream_topktable ON HASHTAG_FILESTREAMTABLE OPTIONS\n(key 'hashtag', timeInterval '2000ms', size '10' );\n--- Query a topk table \nSELECT hashtag, COUNT(hashtag) AS TopKCount\nFROM filestream_topktable\nGROUP BY hashtag ORDER BY TopKCount limit 10;\n\n\n\n\nNow, lets try analyzing some tweets using this above syntax in real time using the packaged scripts ..\n\n\nStep 5 - Create and Query Stream Table and Top-K Declaratively\n\n\nYou can use the scripts that simulates the twitter stream by copying pre-loaded tweets in a tmp folder. Or, you could use a script that access the live twitter stream.  \n\n\nSteps to work with simulated Twitter stream\n\n\nCreate a file stream table that listens on a folder and then start the streaming context. \n\n\nsnappy\n run './quickstart/scripts/create_and_start_file_streaming.sql';\n\n\n\n\nRun the following utility in another terminal that simulates a twitter stream by copying tweets in the folder on which file stream table is listening.\n\n\n$ quickstart/scripts/simulateTwitterStream \n\n\n\n\nNow query the current batch of the stream using the following script. This also creates Topk structures. simulateTwitterStream script runs for only for a minute or so. Since our script is querying the current window, it will return no results after the streaming is over. \n\n\nsnappy\n run './quickstart/scripts/file_streaming_query.sql';\n\n\n\n\nSteps to work with live Twitter stream\n\n\nYou would have to generate authorization keys and secrets on \ntwitter apps\n and update SNAPPY_HOME/quickstart/scripts/create_and_start_twitter_streaming.sql with the keys and secrets.\n\n\n--- Run the create and start script that has keys and secrets to fetch live twitter stream\n--- Note: Currently, we do not encrypt the keys. \n-- This also creates Topk structures\nsnappy\n run './quickstart/scripts/create_and_start_twitter_streaming.sql';\n\nsnappy\n run './quickstart/scripts/twitter_streaming_query.sql';\n\n\n\n\nGetting Started with Spark API\n\n\nSnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's \nSQLContext\n to work with Row and Column tables. Any DataFrame can be managed as SnappyData table and any table can be accessed as a DataFrame. This is similar to \nHiveContext\n and it integrates the SQLContext functionality with the Snappy store. Similarly, SnappyStreamingContext is an entry point for SnappyData extensions to Spark streaming and it extends Spark's \nStreaming Context\n. \n\n\nApplications typically submit Jobs to SnappyData and do not explicitly create a SnappyContext or SnappyStreamingContext. These jobs are the primary mechanism to interact with SnappyData using Spark API. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. \n\n\nclass SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(snc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}\n\n\n\n\nThe implementation of \nrunJob\n function of SnappySQLJob uses SnappyContext to interact with SnappyData store to process and store tables. The implementation of runJob of SnappyStreamingJob uses SnappyStreamingContext to create streams and manage the streaming context. The jobs are submitted to lead node of SnappyData over REST API using a \nspark-submit\n like utility. See more details about jobs \nhere\n\n\nColumn and Row tables\n\n\nColumn tables\n organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\n// creating a column table in Snappy job\nsnappyContext.createTable(\nAIRLINE\n, \ncolumn\n, schema, Map(\nbuckets\n -\n \n5\n))\n\n\n\n\nRow tables\n, unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.\n\n\ncreate table\n DDL allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary \ndocs\n for the details.\n\n\n// creating a row table in Snappy job\nval airlineCodeDF = snappyContext.createTable(\nAIRLINEREF\n, \nrow\n, schema, Map())\n\n\n\n\nStep 2 - Create column table, row table and load data\n\n\n\n\nTo run the scripts with full airline data set, set the following config parameter to point at the data set that you had downloaded in Step 1.\n\n\n\n\nexport APP_PROPS=\"airline_file=full_dataset_folder\"\n\n\n\n\n\n\nSubmit CreateAndLoadAirlineDataJob over REST API to create row and column tables. See more details about jobs and job submission \nhere.\n. \n\n\n$ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp --class  io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{\nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  } }\n\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead hostNameOfLead:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{ \nduration\n: \n17.53 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.CreateAndLoadAirlineDataJob\n,\n  \nstartTime\n: \n2016-01-12T16:59:14.746+05:30\n,\n  \ncontext\n: \nsnappyContext1452598154529305363\n,\n  \nresult\n: \nSee /snappy/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n}\n# Tables are created\n\n\n\n\nThe output of the job can be found in CreateAndLoadAirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can see the size of the column tables on Spark UI which by default can be seen at http://hostNameOfLead:4040. \n\n\nOLAP and OLTP Store\n\n\nSnappyContext extends SQLContext and adds functionality to work with row and column tables. When queries inside jobs are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP. Such queries are planned using Spark's Catalyst engine and scheduled to be executed on the data servers. \n\n\nval resultDF = airlineDF.join(airlineCodeDF,\n        airlineDF.col(\nUniqueCarrier\n).equalTo(airlineCodeDF(\nCODE\n))).\n        groupBy(airlineDF(\nUniqueCarrier\n), airlineCodeDF(\nDESCRIPTION\n)).\n        agg(\nArrDelay\n -\n \navg\n).orderBy(\navg(ArrDelay)\n)\n\n\n\n\nFor low latency OLTP queries in jobs, SnappyData won't schedule these queries instead execute them immediately on SnappyData servers without any scheduling overhead. Quite often, this may mean simply fetching or updating a row by hashing a key (in nanoseconds). \n\n\n// Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'. Update the row table.\nval filterExpr: String = \n CODE ='DL'\n\nval newColumnValues: Row = Row(\nDelta America\n)\nval updateColumns = \nDESCRIPTION\n\nsnappyContext.update(rowTableName, filterExpr, newColumnValues, updateColumns)\n\n\n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nAirlineDataJob.scala runs OLAP and OLTP queries on Snappy tables. Also, it caches the same airline data in Spark cache and runs the same OLAP query on the Spark cache. With airline data set, we have seen both Spark cache and snappy store table to have more and less the same performance.  \n\n\n# Submit AirlineDataJob to SnappyData\n$ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp  --class  io.snappydata.examples.AirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{ \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n1b0d2e50-42da-4fdd-9ea2-69e29ab92de2\n,\n    \ncontext\n: \nsnappyContext1453196176725064822\n\n } }\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090  --job-id 1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n{ \nduration\n: \n6.617 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.AirlineDataJob\n,\n  \nstartTime\n: \n2016-01-19T15:06:16.771+05:30\n,\n  \ncontext\n: \nsnappyContext1453196176725064822\n,\n  \nresult\n: \nSee /snappy/work/localhost-lead-1/AirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n1b0d2e50-42da-4fdd-9ea2-69e29ab92de2\n\n}\n\n\n\n\nThe output of the job can be found in AirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can explore the Spark SQL query plan on Spark UI which by default can be seen at http://hostNameOfLead:4040.\n\n\nApproximate query processing (AQP)\n\n\nOLAP jobs are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory jobs above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs to specify one or more curated \nstratified samples\n on large tables. \n\n\n\n\nNote\n\n\nWe recommend downloading the \nonTime airline\n data for 2009-2015 which is about 50 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.\n\n\n\n\nThe following scala code creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your \nGroup by\n and \nWhere\n make us the \nQuery Column Set\n (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection. \n\n\nval sampleDF = snappyContext.createTable(sampleTable, \n        \ncolumn_sample\n, // DataSource provider for sample tables\n        updatedSchema, Map(\nbuckets\n -\n \n5\n,\n          \nqcs\n -\n \nUniqueCarrier, Year_, Month_\n,\n          \nfraction\n -\n \n0.03\n,\n          \nstrataReservoirSize\n -\n \n50\n,\n          \nbasetable\n -\n \nAirline\n\n        ))\n\n\n\n\nYou can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the \nWith Error\n constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set. \n\n\n// Query Snappy Store's Sample table :Which Airlines arrive On schedule? JOIN with reference table\nsampleResult = sampleDF.join(airlineCodeDF,\n        sampleDF.col(\nUniqueCarrier\n).equalTo(airlineCodeDF(\nCODE\n))).\n          groupBy(sampleDF(\nUniqueCarrier\n), airlineCodeDF(\nDESCRIPTION\n)).\n          agg(\nArrDelay\n -\n \navg\n).orderBy(\navg(ArrDelay)\n)\n\n // Query Snappy Store's Airline table with error clause.\nairlineDF.groupBy(airlineDF(\nMonth_\n))\n  .agg(\nArrDelay\n -\n \navg\n)\n  .orderBy(\nMonth_\n).withError(0.05,0.95)\n\n\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nCreateAndLoadAirlineDataJob and AirlineDataJob executed in the previous sections created the sample tables and executed OLAP queries over them.\n\n\nStream analytics using Spark Streaming\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. Also, SnappyData introduces \"continuous queries\" (CQ) on the stream. One can define a continous query as a SQL query on the stream with window and slide extensions which is returned as SchemaDStream i.e. DStream with schema. SnappyData's extensions provide functionality to insert a SchemaDStream into snappy store. \n\n\nDynamic registration of CQs (from remote clients) will be available in the next release.\n\n\n// create a stream table declaratively \nsnsc.sql(\nCREATE STREAM TABLE RETWEETTABLE (retweetId long, \n +\n    \nretweetCnt int, retweetTxt string) USING file_stream \n +\n    \nOPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2', \n +\n    \nrowConverter 'org.apache.spark.sql.streaming.TweetToRetweetRow',\n +\n    \ndirectory '/tmp/copiedtwitterdata')\n);\n\n// Register a continous query on the stream table with window and slide parameters\nval retweetStream: SchemaDStream = snsc.registerCQ(\nSELECT retweetId, retweetCnt FROM RETWEETTABLE \n +\n    \nwindow (duration '2' seconds, slide '2' seconds)\n)\n\n// Create a row table to hold the retweets based on their id \nsnsc.snappyContext.sql(s\nCREATE TABLE $tableName (retweetId bigint PRIMARY KEY, \n +\n    s\nretweetCnt int, retweetTxt string) USING row OPTIONS ()\n)\n\n// Iterate over the stream and insert it into snappy store\nretweetStream.foreachDataFrame(df =\n {\n    df.write.mode(SaveMode.Append).saveAsTable(tableName)\n})\n\n\n\n\nTop-K Elements in a Stream\n\n\nContinuously finding the \nk\n most popular elements in a data stream is a common analytic query. SnappyData provides extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures. More details about SnappyData's implementation of top-k can be found \nhere.\n\n\nSnappyData provides API in SnappyContext to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using another API. \n\n\n--- Create a topk table from a stream table\nsnappyContext.createApproxTSTopK(\ntopktable\n, \nhashtag\n,\n    Some(schema), Map(\n      \nepoch\n -\n System.currentTimeMillis().toString,\n      \ntimeInterval\n -\n \n2000ms\n,\n      \nsize\n -\n \n10\n,\n      \nbasetable\n -\n \nHASHTAGTABLE\n\n    ))\n--- Query a topk table for the last two seconds\nval topKDF = snappyContext.queryApproxTSTopK(\ntopktable\n,\n                System.currentTimeMillis - 2000,\n                System.currentTimeMillis)\n\n\n\n\nStep 5 -  Create and Query Stream Table and Top-K\n\n\nIdeally, we would like you to try this example using live twitter stream. For that, you would have to generate authorization keys and secrets on twitter apps. Alternatively, you can use use file stream scripts that simulate twitter stream by copying pre-loaded tweets in a tmp folder.\n\n\nSteps to work with live Twitter stream\n\n\n# Set the keys and secrets to fetch live twitter stream\n# Note: Currently, we do not encrypt the keys. \n$ export APP_PROPS=\nconsumerKey=\nconsumerKey\n,consumerSecret=\nconsumerSecret\n,accessToken=\naccessToken\n,accessTokenSecret=\naccessTokenSecret\n\n\n# submit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a snappy store table \n# This job runs streaming for two minutes. \n$ /bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream\n\n\n\n\n\nSteps to work with simulated Twitter stream\n\n\nSubmit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a gemxd table. It starts the streaming and waits for two minutes. \n\n\n# Submit the TwitterPopularTagsJob \n$ ./bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream\n\n# Run the following utility in another terminal to simulate a twitter stream by copying tweets in the folder on which file stream table is listening.\n$ quickstart/scripts/simulateTwitterStream \n\n\n\n\n\nThe output of the job can be found in TwitterPopularTagsJob_timestamp.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. \n\n\nWorking with Spark shell and spark-submit\n\n\nSnappyData, out-of-the-box, collocates Spark executors and the data store for efficient data intensive computations. But, it may desirable to isolate the computational cluster for other reasons - for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate for a  cache data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion. \n\n\n# Start the spark shell in local mode. Pass Snappy locator\u2019s host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041\nscala\n\nTry few commands on the spark-shell \n\n# fetch the tables and using sqlContext which is going to be an instance of SnappyContext in this case\nscala\n val airlinerefDF = sqlContext.table(\nairlineref\n).show\nscala\n val airlineDF = sqlContext.table(\nairline\n).show\n\n# you can now work with the dataframes to fetch the data.\n\n\n\n\nStep 6 - Submit a Spark App that interacts with SnappyData\n\n\n# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041 $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n\n# The results can be seen on the command line. \n\n\n\n\nFinal Step - Stop the SnappyData Cluster\n\n\n$ sbin/snappy-stop-all.sh \nlocalhost: The SnappyData Leader has stopped.\nlocalhost: The SnappyData Server has stopped.\nlocalhost: The SnappyData Locator has stopped.", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/#table-of-contents", 
            "text": "Introduction  Download binary distribution  Community Support  Link with SnappyData distribution  Working with SnappyData Source Code  Building SnappyData from source    Key Features  Getting started  Objectives  SnappyData Cluster  Step 1 - Start the SnappyData cluster    Interacting with SnappyData  Getting Started with SQL  Column and Row tables  Step 2 - Create column table, row table and load data  OLAP and OLTP queries  Step 3 - Run OLAP and OLTP queries  Approximate query processing (AQP)  Step 4 - Create, Load and Query Sample Table  Stream analytics using SQL and Spark Streaming  Top-K Elements in a Stream  Step 5 - Create and Query Stream Table and Top-K Declaratively    Getting Started with Spark API  Column and Row tables  Step 2 - Create column table, row table and load data  OLAP and OLTP Store  Step 3 - Run OLAP and OLTP queries  Approximate query processing (AQP)  Step 4 - Create, Load and Query Sample Table  Stream analytics using Spark Streaming  Top-K Elements in a Stream  Step 5 - Create and Query Stream Table and Top-K  Working with Spark shell and spark-submit  Step 6 - Submit a Spark App that interacts with SnappyData    Final Step - Stop the SnappyData Cluster", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/#introduction", 
            "text": "SnappyData is a  distributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster . We realize this platform through a seamless integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics).", 
            "title": "Introduction"
        }, 
        {
            "location": "/#download-binary-distribution", 
            "text": "You can download the latest version of SnappyData from [here][2]. SnappyData has been tested on Linux (mention kernel version) and Mac (OS X 10.9 and 10.10?). If not already installed, you will need to download scala 2.10 and  Java 8 .  (this info should also be in the download page on our web site)  Skip to Getting Started", 
            "title": "Download binary distribution"
        }, 
        {
            "location": "/#community-support", 
            "text": "We monitor channels listed below for comments/questions. We prefer using Stackoverflow.   Stackoverflow        Slack         Gitter             IRC                 Reddit             JIRA", 
            "title": "Community Support"
        }, 
        {
            "location": "/#link-with-snappydata-distribution", 
            "text": "SnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:  groupId: io.snappydata\nartifactId: snappydata_2.10\nversion: 0.1_preview", 
            "title": "Link with SnappyData distribution"
        }, 
        {
            "location": "/#working-with-snappydata-source-code", 
            "text": "(Info for our download page?)\nIf you are interested in working with the latest code or contributing to SnappyData development, you can also check out the master branch from Git:  Master development branch\ngit clone https://github.com/SnappyDataInc/snappydata.git\n\n###### 0.1 preview release branch with stability fixes ######\ngit clone https://github.com/SnappyDataInc/snappydata.git -b 0.1_preview (??)", 
            "title": "Working with SnappyData Source Code"
        }, 
        {
            "location": "/#building-snappydata-from-source", 
            "text": "You will find the instructions for building, layout of the code, integration with IDEs using Gradle, etc,  here   NOTE:\nSnappyData is built using Spark 1.6 (build xx) which is packaged as part of SnappyData. While you can build your application using Apache Spark 1.5, you will need to link to Snappy-spark to make  use of the SnappyData extensions. Gradle build tasks are packaged.", 
            "title": "Building SnappyData from source"
        }, 
        {
            "location": "/#key-features", 
            "text": "100% compatible with Spark : Use SnappyData as a database as well as use any of the Spark APIs - ML, Graph, etc. on the same data  In-memory row and column store : Run the store collocated in Spark executors (i.e. a single compute and data cluster) or in its own process space (i.e. separate compute and data cluster)  SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.  SQL based extensions for streaming processing : Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.  Interactive analytics using Approximate query processing(AQP) : We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions.   Mutate, transact on data in Spark : Use SQL to insert, update, delete data in tables(something that you cannot do in Spark). We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store.   Optimizations : Use indexes to improve query performance in the row store (the GemFire SQL optimizer automatically uses in-memory indexes when available)   High availability not just Fault tolerance : Data is instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications with continuous HA.  Durability and recovery:  Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.    Read SnappyData  docs  for a more detailed list of all features and semantics.", 
            "title": "Key Features"
        }, 
        {
            "location": "/#getting-started", 
            "text": "", 
            "title": "Getting started"
        }, 
        {
            "location": "/#objectives", 
            "text": "In-memory Column and Row tables : Illustrate both SQL syntax and Spark API to create and manage column tables for large data and how row tables can be used for reference data and can be replicated to each node in the cluster.   OLAP, OLTP operations : We run analytic class SQL queries (full scan with aggregations) on column tables and fully distributed join queries and observe the space requirements as well as the performance of these queries. For OLTP, we run simple update queries - you can note the Spark API extensions to support mutations in Spark.   AQP : We run the same analytic queries by creating adjunct stratified samples to note the performance difference - can we get close to interactive query performance speeds?  Streaming with SQL : We ingest twitter streams into both a probabilistic data structure for TopK time series analytics and the entire stream (full data set) into a row table. We run both ad-hoc queries on these streams (modeled as tables) as well as showcase our first preview for continuous querying support. The SnappyData value add demonstrated here is simpler, SQL centric abstractions on top of Spark streaming. And, of course, ingestion into the built-in store.   In this document, we discuss the features mentioned above and ask you to take steps to run the scripts that demonstrate these features.", 
            "title": "Objectives"
        }, 
        {
            "location": "/#snappydata-cluster", 
            "text": "SnappyData, a database server cluster, has three main components - Locator, Server and Lead.    Locator : Provides discovery service for the cluster. Informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.  Lead Node : Acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.  Data Servers : Hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.    Details of about the architecture can be found  here . SnappyData also has multiple deployment options which can be found  here .", 
            "title": "SnappyData Cluster"
        }, 
        {
            "location": "/#step-1-start-the-snappydata-cluster", 
            "text": "", 
            "title": "Step 1 - Start the SnappyData cluster"
        }, 
        {
            "location": "/#note", 
            "text": "The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. \nSummary information on the number of on-time, delayed, canceled and diverted flights is available for the last 20 years. We use this data set in the examples below. You can learn more on this schema  here .\nDefault airline data shipped with product is of 15 MB compressed size. If you are interested in studying Approximate query processing we recommend downloading the full data set run this command (from quickstart/scripts directory):   $ ./download_full_airlinedata.sh ../data", 
            "title": "Note"
        }, 
        {
            "location": "/#do-we-need-this-seems-to-work-with-default-cluster", 
            "text": "In case you are running Getting Started with full dataset, configure snappy to start two servers with max heap size as 4G each.    $ cat conf/servers\n# Two servers with total of 8G.\nyourhostName -J-Xmx4g\nyourhostName -J-Xmx4g", 
            "title": "Do we need this? seems to work with default cluster ..."
        }, 
        {
            "location": "/#passwordless-ssh", 
            "text": "The quick start scripts use ssh to start up various processes. By default, this requires a password. To be able to log on to the localhost and run the script without being prompted for the password, please enable passwordless ssh.   The following script starts up a minimal set of essential components to form the cluster - A locator, one data server and one lead node. All nodes are started locally. To spin up remote nodes simply rename/copy the files without the template suffix and add the hostnames. The  article  discusses the custom configuration and startup options.  $ sbin/snappy-start-all.sh \n  (Roughly can take upto a minute. Associated logs are in the \u2018work\u2019 sub-directory)\nThis would output something like this ...\nlocalhost: Starting SnappyData Locator using peer discovery on: 0.0.0.0[10334]\n...\nlocalhost: SnappyData Locator pid: 56703 status: running\n\nlocalhost: Starting SnappyData Server using locators for peer discovery: jramnara-mbpro[10334]   (port used for members to form a p2p cluster)\nlocalhost: SnappyData Server pid: 56819 status: running\nlocalhost:   Distributed system now has 2 members.\n\nlocalhost: Starting SnappyData Leader using locators for peer discovery: jramnara-mbpro[10334]\nlocalhost: SnappyData Leader pid: 56932 status: running\nlocalhost:   Distributed system now has 3 members.\n\nlocalhost:   Other members: jramnara-mbpro(56703:locator) v0 :54414, jramnara-mbpro(56819:datastore) v1 :39737  At this point, the SnappyData cluster is up and running and is ready to accept Snappy jobs and SQL requests via JDBC/ODBC. You can  monitor the Spark cluster at port 4040 . Once you load data and run queries, you can analyze the Spark SQL query plan, the job execution stages and storage details of column tables.", 
            "title": "Passwordless ssh"
        }, 
        {
            "location": "/#interacting-with-snappydata", 
            "text": "We assume some familiarity with  core Spark, Spark SQL and Spark Streaming concepts . \nAnd, you can try out the Spark  Quick Start . All the commands and programs\nlisted in the Spark guides will work in SnappyData also.   To interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to SnappyData cluster and interact using SQL. On the other hand, users comfortable with Spark programming paradigm can write Snappy jobs to interact with SnappyData. Snappy jobs can be like a self contained Spark application or can share state with other jobs using SnappyData store.   Unlike Apache Spark, which is primarily a computational engine, SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.   Long running executors : Executors are running within the Snappy store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors.   Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the  Spark JobServer  to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). \nRead  docs  for details of the architecture.   In this document, we showcase mostly the same set of features via Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to  Getting Started with Spark API .", 
            "title": "Interacting with SnappyData"
        }, 
        {
            "location": "/#getting-started-with-sql", 
            "text": "For SQL, the SnappyData SQL Shell ( snappy-shell ) provides a simple way to inspect the catalog,  run admin operations,  manage the schema and run interactive queries. You can also use your favorite SQL tool like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster).  // Run from the SnappyData base directory\n$ ./bin/snappy-shell\nVersion 2.0-SNAPSHOT.1\nsnappy  \n\n-- Connect to the cluster ..\nsnappy  connect client 'localhost:1527';\nsnappy  show connections; \n\n-- Check the cluster status\nthis will list each cluster member and its status\nsnappy  show members;", 
            "title": "Getting Started with SQL"
        }, 
        {
            "location": "/#column-and-row-tables", 
            "text": "Column tables  organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  -- DDL to create a column table\nCREATE TABLE AIRLINE ( column definitions ) USING column OPTIONS(buckets '5') ;  Row tables , unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.  create table  DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary  docs  for the details.  -- DDL to create a row table\nCREATE TABLE AIRLINEREF ( column definitions ) USING row OPTIONS() ;", 
            "title": "Column and Row tables"
        }, 
        {
            "location": "/#step-2-create-column-table-row-table-and-load-data", 
            "text": "To run the scripts with full airline data set, change the 'create_and_load_column_table.sql' script to point at the data set that you had downloaded in Step 1.   SQL scripts to create and load column and row tables.  -- Loads parquet formatted data into a temporary spark table \n-- then saves it in  column table called Airline.\nsnappy  run './quickstart/scripts/create_and_load_column_table.sql';\n\n-- Creates the airline code table. Row tables can be replicated to each node \n-- so join processing with other tables can completely avoid shuffling \nsnappy  run './quickstart/scripts/create_and_load_row_table.sql';\n\n-- See the status of system\nsnappy  run './quickstart/scripts/status_queries.sql'  You can see the memory consumed on  Spark Console .", 
            "title": "Step 2 - Create column table, row table and load data"
        }, 
        {
            "location": "/#olap-and-oltp-queries", 
            "text": "SQL client connections (via JDBC or ODBC) are routed to the appropriate data server via the locator (Physical connections are automatically created in the driver and are transparently swizzled in case of failures also). When queries are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP.  Such queries are routed to the  lead  node where a  Spark SQLContext  is managed for each connection. The Query is planned using Spark's Catalyst engine and scheduled to be executed on the data servers. The number of partitions determine the number of concurrent tasks used across the data servers to parallel run the query. In this case, our column table was created using  5 partitions(buckets)  and hence will use 5 concurrent tasks.   ---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nsnappy  select AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;  For low latency OLTP queries, the engine won't route it to the lead and instead execute it immediately without any scheduling overhead. Quite often, this may mean simply fetching a row by hashing a key (in microseconds).  --- Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'\n--- the airline code can be updated in the row table\nUPDATE AIRLINEREF SET DESCRIPTION='Delta America' WHERE CAST(CODE AS VARCHAR(25))='DL';  Spark SQL can cache DataFrames as temporary tables and the data set is immutable. SnappyData SQL is compatible with the SQL standard with support for transactions and DML (insert, update, delete) on tables.  Link to Snappy Store SQL reference .  As we show later, any table in Snappy is also visible as Spark DataFrame.", 
            "title": "OLAP and OLTP queries"
        }, 
        {
            "location": "/#step-3-run-olap-and-oltp-queries", 
            "text": "-- Simply run the script or copy/paste one query at a time if you want to explore the query execution on the Spark console. \nsnappy  run './quickstart/scripts/olap_queries.sql';\n\n---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nselect AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;  Each query is executed as one or more Jobs and each Job executed in one or more stages. You can explore the query execution plan and metrics  here  -- Run a simple update SQL statement on the replicated row table.\nsnappy  run './quickstart/scripts/oltp_queries.sql';  You can now re-run olap_queries.sql to see the updated join result set.   Note \nIn the current implementation we only support appending to Column tables. Future releases will support all DML operations. \nYou can execute transactions using commands  autocommit off  and  commit .", 
            "title": "Step 3 - Run OLAP and OLTP queries"
        }, 
        {
            "location": "/#approximate-query-processing-aqp", 
            "text": "OLAP queries are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory queries above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs and DDL to specify one or more curated  stratified samples  on large tables.", 
            "title": "Approximate query processing (AQP)"
        }, 
        {
            "location": "/#note_1", 
            "text": "We recommend downloading the  onTime airline  data for 2009-2015 which is about 52 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.   The following DDL creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your  Group by  and  Where  make up the  Query Column Set  (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection.   CREATE SAMPLE TABLE AIRLINE_SAMPLE\n   OPTIONS(\n    buckets '5',                          -- Number of partitions \n    qcs 'UniqueCarrier, Year_, Month_',   -- QueryColumnSet(qcs): The strata - 3% of each combination of Carrier, \n                                          -- Year and Month are stored as sample\n    fraction '0.03',                      -- How big should the sample be\n    strataReservoirSize '50',             -- Reservoir sampling to support streaming inserts\n    basetable 'Airline')                  -- The parent base table  You can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the  With Error  constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set.   -- What is the average arrival delay for all airlines for each month?;\nsnappy  select avg(ArrDelay), Month_ from Airline where ArrDelay  0 \n    group by Month_\n    with error .05 ;\n-- The above query will consult the sample and return an answer if the estimated answer \n-- is at least 95% accurate (here, by default we use a 95% confidence interval). Read [docs](docs) for more details.\n\n-- You can also access the error using built-in functions. \nsnappy  select avg(ArrDelay) avgDelay, absolute_error(avgDelay), Month_ \n    from Airline where ArrDelay  0 \n    group by Month_\n    with error .05 ;", 
            "title": "Note"
        }, 
        {
            "location": "/#step-4-create-load-and-query-sample-table", 
            "text": "--- Creates and then samples a table from the Airline table \nsnappy  run 'create_and_load_sample_table.sql';  You can now re-run the previous OLAP queries with an error constraint and compare the results.  You should notice a 10X or larger difference in query execution latency while the results remain nearly accurate. As a reminder, we recommend downloading the larger data set for this exercise.  -- re-run olap queries with error constraint to automatically use sampling\nsnappy  run 'olap_approx_queries.sql';", 
            "title": "Step 4 - Create, Load and Query Sample Table"
        }, 
        {
            "location": "/#stream-analytics-using-sql-and-spark-streaming", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. You can also dynamically run SQL queries on these streams. There is no need to learn Spark streaming APIs or statically define all the rules to be executed on these streams.  The commands below consume tweets, filters our just the hashtags and converts these into Row objects, models the stream as a table (so it can be queried) and we then run ad-hoc SQL from remote clients on the current state of the stream.   --- Inits the Streaming Context with the batch interval of 2 seconds.\n--- i.e. the stream is processed once every 2 seconds.\nsnappy  STREAMING INIT 2\n--- create a stream table just containing the hashtags\nsnappy  CREATE STREAM TABLE HASHTAG_FILESTREAMTABLE\n              (hashtag string)\n            USING file_stream\n            OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',\n              rowConverter 'org.apache.spark.sql.streaming.TweetToHashtagRow',\n              directory '/tmp/copiedtwitterdata')\n-- A file_stream data source monitors the directory and as files arrives they are ingested \n--   into the streaming pipeline. First converted into Rows using 'TweetToHashtagRow' then visible as table\n--- Start streaming context \nsnappy  STREAMING START\n--- Adhoc sql on the stream table to query the current batch\n--- Get top 10 popular hashtags ----\nsnappy  SELECT hashtag, count(*) as tagcount\n        FROM HASHTAG_FILESTREAMTABLE\n        GROUP BY hashtag\n        ORDER BY tagcount DESC limit 10;  Later, in the Spark code section we further enhance to showcase \"continuous queries\" (CQ). Dynamic registration of CQs (from remote clients) will be available in the next release.", 
            "title": "Stream analytics using SQL and Spark Streaming"
        }, 
        {
            "location": "/#top-k-elements-in-a-stream", 
            "text": "Finding the  k  most popular elements in a data stream is a common analytic query. For instance, top-100 pages on a popular website in the last 10 mins, top-10 sales regions in the last week, etc. As you can tell, if the query is on a arbitrary time interval in the past, this will most likely mandate storing the entire stream. And, this could easily be millions to billions of events is use cases like in IoT. SnappyData provides SQL extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures and enables transparent querying using Spark SQL. More details about SnappyData's implementation of top-k can be found  here.  SnappyData provides DDL extensions to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using regular SQL queries.   --- Create a topk table from a stream table\nCREATE TOPK TABLE filestream_topktable ON HASHTAG_FILESTREAMTABLE OPTIONS\n(key 'hashtag', timeInterval '2000ms', size '10' );\n--- Query a topk table \nSELECT hashtag, COUNT(hashtag) AS TopKCount\nFROM filestream_topktable\nGROUP BY hashtag ORDER BY TopKCount limit 10;  Now, lets try analyzing some tweets using this above syntax in real time using the packaged scripts ..", 
            "title": "Top-K Elements in a Stream"
        }, 
        {
            "location": "/#step-5-create-and-query-stream-table-and-top-k-declaratively", 
            "text": "You can use the scripts that simulates the twitter stream by copying pre-loaded tweets in a tmp folder. Or, you could use a script that access the live twitter stream.", 
            "title": "Step 5 - Create and Query Stream Table and Top-K Declaratively"
        }, 
        {
            "location": "/#steps-to-work-with-simulated-twitter-stream", 
            "text": "Create a file stream table that listens on a folder and then start the streaming context.   snappy  run './quickstart/scripts/create_and_start_file_streaming.sql';  Run the following utility in another terminal that simulates a twitter stream by copying tweets in the folder on which file stream table is listening.  $ quickstart/scripts/simulateTwitterStream   Now query the current batch of the stream using the following script. This also creates Topk structures. simulateTwitterStream script runs for only for a minute or so. Since our script is querying the current window, it will return no results after the streaming is over.   snappy  run './quickstart/scripts/file_streaming_query.sql';", 
            "title": "Steps to work with simulated Twitter stream"
        }, 
        {
            "location": "/#steps-to-work-with-live-twitter-stream", 
            "text": "You would have to generate authorization keys and secrets on  twitter apps  and update SNAPPY_HOME/quickstart/scripts/create_and_start_twitter_streaming.sql with the keys and secrets.  --- Run the create and start script that has keys and secrets to fetch live twitter stream\n--- Note: Currently, we do not encrypt the keys. \n-- This also creates Topk structures\nsnappy  run './quickstart/scripts/create_and_start_twitter_streaming.sql';\n\nsnappy  run './quickstart/scripts/twitter_streaming_query.sql';", 
            "title": "Steps to work with live Twitter stream"
        }, 
        {
            "location": "/#getting-started-with-spark-api", 
            "text": "SnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's  SQLContext  to work with Row and Column tables. Any DataFrame can be managed as SnappyData table and any table can be accessed as a DataFrame. This is similar to  HiveContext  and it integrates the SQLContext functionality with the Snappy store. Similarly, SnappyStreamingContext is an entry point for SnappyData extensions to Spark streaming and it extends Spark's  Streaming Context .   Applications typically submit Jobs to SnappyData and do not explicitly create a SnappyContext or SnappyStreamingContext. These jobs are the primary mechanism to interact with SnappyData using Spark API. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.   class SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(snc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}  The implementation of  runJob  function of SnappySQLJob uses SnappyContext to interact with SnappyData store to process and store tables. The implementation of runJob of SnappyStreamingJob uses SnappyStreamingContext to create streams and manage the streaming context. The jobs are submitted to lead node of SnappyData over REST API using a  spark-submit  like utility. See more details about jobs  here", 
            "title": "Getting Started with Spark API"
        }, 
        {
            "location": "/#column-and-row-tables_1", 
            "text": "Column tables  organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  // creating a column table in Snappy job\nsnappyContext.createTable( AIRLINE ,  column , schema, Map( buckets  -   5 ))  Row tables , unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.  create table  DDL allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary  docs  for the details.  // creating a row table in Snappy job\nval airlineCodeDF = snappyContext.createTable( AIRLINEREF ,  row , schema, Map())", 
            "title": "Column and Row tables"
        }, 
        {
            "location": "/#step-2-create-column-table-row-table-and-load-data_1", 
            "text": "To run the scripts with full airline data set, set the following config parameter to point at the data set that you had downloaded in Step 1.   export APP_PROPS=\"airline_file=full_dataset_folder\"    Submit CreateAndLoadAirlineDataJob over REST API to create row and column tables. See more details about jobs and job submission  here. .   $ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp --class  io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{ status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  } }\n\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead hostNameOfLead:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n{  duration :  17.53 secs ,\n   classPath :  io.snappydata.examples.CreateAndLoadAirlineDataJob ,\n   startTime :  2016-01-12T16:59:14.746+05:30 ,\n   context :  snappyContext1452598154529305363 ,\n   result :  See /snappy/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n}\n# Tables are created  The output of the job can be found in CreateAndLoadAirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can see the size of the column tables on Spark UI which by default can be seen at http://hostNameOfLead:4040.", 
            "title": "Step 2 - Create column table, row table and load data"
        }, 
        {
            "location": "/#olap-and-oltp-store", 
            "text": "SnappyContext extends SQLContext and adds functionality to work with row and column tables. When queries inside jobs are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP. Such queries are planned using Spark's Catalyst engine and scheduled to be executed on the data servers.   val resultDF = airlineDF.join(airlineCodeDF,\n        airlineDF.col( UniqueCarrier ).equalTo(airlineCodeDF( CODE ))).\n        groupBy(airlineDF( UniqueCarrier ), airlineCodeDF( DESCRIPTION )).\n        agg( ArrDelay  -   avg ).orderBy( avg(ArrDelay) )  For low latency OLTP queries in jobs, SnappyData won't schedule these queries instead execute them immediately on SnappyData servers without any scheduling overhead. Quite often, this may mean simply fetching or updating a row by hashing a key (in nanoseconds).   // Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'. Update the row table.\nval filterExpr: String =   CODE ='DL' \nval newColumnValues: Row = Row( Delta America )\nval updateColumns =  DESCRIPTION \nsnappyContext.update(rowTableName, filterExpr, newColumnValues, updateColumns)", 
            "title": "OLAP and OLTP Store"
        }, 
        {
            "location": "/#step-3-run-olap-and-oltp-queries_1", 
            "text": "AirlineDataJob.scala runs OLAP and OLTP queries on Snappy tables. Also, it caches the same airline data in Spark cache and runs the same OLAP query on the Spark cache. With airline data set, we have seen both Spark cache and snappy store table to have more and less the same performance.    # Submit AirlineDataJob to SnappyData\n$ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp  --class  io.snappydata.examples.AirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{  status :  STARTED ,\n   result : {\n     jobId :  1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 ,\n     context :  snappyContext1453196176725064822 \n } }\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090  --job-id 1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n{  duration :  6.617 secs ,\n   classPath :  io.snappydata.examples.AirlineDataJob ,\n   startTime :  2016-01-19T15:06:16.771+05:30 ,\n   context :  snappyContext1453196176725064822 ,\n   result :  See /snappy/work/localhost-lead-1/AirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n}  The output of the job can be found in AirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can explore the Spark SQL query plan on Spark UI which by default can be seen at http://hostNameOfLead:4040.", 
            "title": "Step 3 - Run OLAP and OLTP queries"
        }, 
        {
            "location": "/#approximate-query-processing-aqp_1", 
            "text": "OLAP jobs are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory jobs above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs to specify one or more curated  stratified samples  on large tables.", 
            "title": "Approximate query processing (AQP)"
        }, 
        {
            "location": "/#note_2", 
            "text": "We recommend downloading the  onTime airline  data for 2009-2015 which is about 50 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.   The following scala code creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your  Group by  and  Where  make us the  Query Column Set  (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection.   val sampleDF = snappyContext.createTable(sampleTable, \n         column_sample , // DataSource provider for sample tables\n        updatedSchema, Map( buckets  -   5 ,\n           qcs  -   UniqueCarrier, Year_, Month_ ,\n           fraction  -   0.03 ,\n           strataReservoirSize  -   50 ,\n           basetable  -   Airline \n        ))  You can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the  With Error  constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set.   // Query Snappy Store's Sample table :Which Airlines arrive On schedule? JOIN with reference table\nsampleResult = sampleDF.join(airlineCodeDF,\n        sampleDF.col( UniqueCarrier ).equalTo(airlineCodeDF( CODE ))).\n          groupBy(sampleDF( UniqueCarrier ), airlineCodeDF( DESCRIPTION )).\n          agg( ArrDelay  -   avg ).orderBy( avg(ArrDelay) )\n\n // Query Snappy Store's Airline table with error clause.\nairlineDF.groupBy(airlineDF( Month_ ))\n  .agg( ArrDelay  -   avg )\n  .orderBy( Month_ ).withError(0.05,0.95)", 
            "title": "Note"
        }, 
        {
            "location": "/#step-4-create-load-and-query-sample-table_1", 
            "text": "CreateAndLoadAirlineDataJob and AirlineDataJob executed in the previous sections created the sample tables and executed OLAP queries over them.", 
            "title": "Step 4 - Create, Load and Query Sample Table"
        }, 
        {
            "location": "/#stream-analytics-using-spark-streaming", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. Also, SnappyData introduces \"continuous queries\" (CQ) on the stream. One can define a continous query as a SQL query on the stream with window and slide extensions which is returned as SchemaDStream i.e. DStream with schema. SnappyData's extensions provide functionality to insert a SchemaDStream into snappy store.   Dynamic registration of CQs (from remote clients) will be available in the next release.  // create a stream table declaratively \nsnsc.sql( CREATE STREAM TABLE RETWEETTABLE (retweetId long,   +\n     retweetCnt int, retweetTxt string) USING file_stream   +\n     OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',   +\n     rowConverter 'org.apache.spark.sql.streaming.TweetToRetweetRow',  +\n     directory '/tmp/copiedtwitterdata') );\n\n// Register a continous query on the stream table with window and slide parameters\nval retweetStream: SchemaDStream = snsc.registerCQ( SELECT retweetId, retweetCnt FROM RETWEETTABLE   +\n     window (duration '2' seconds, slide '2' seconds) )\n\n// Create a row table to hold the retweets based on their id \nsnsc.snappyContext.sql(s CREATE TABLE $tableName (retweetId bigint PRIMARY KEY,   +\n    s retweetCnt int, retweetTxt string) USING row OPTIONS () )\n\n// Iterate over the stream and insert it into snappy store\nretweetStream.foreachDataFrame(df =  {\n    df.write.mode(SaveMode.Append).saveAsTable(tableName)\n})", 
            "title": "Stream analytics using Spark Streaming"
        }, 
        {
            "location": "/#top-k-elements-in-a-stream_1", 
            "text": "Continuously finding the  k  most popular elements in a data stream is a common analytic query. SnappyData provides extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures. More details about SnappyData's implementation of top-k can be found  here.  SnappyData provides API in SnappyContext to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using another API.   --- Create a topk table from a stream table\nsnappyContext.createApproxTSTopK( topktable ,  hashtag ,\n    Some(schema), Map(\n       epoch  -  System.currentTimeMillis().toString,\n       timeInterval  -   2000ms ,\n       size  -   10 ,\n       basetable  -   HASHTAGTABLE \n    ))\n--- Query a topk table for the last two seconds\nval topKDF = snappyContext.queryApproxTSTopK( topktable ,\n                System.currentTimeMillis - 2000,\n                System.currentTimeMillis)", 
            "title": "Top-K Elements in a Stream"
        }, 
        {
            "location": "/#step-5-create-and-query-stream-table-and-top-k", 
            "text": "Ideally, we would like you to try this example using live twitter stream. For that, you would have to generate authorization keys and secrets on twitter apps. Alternatively, you can use use file stream scripts that simulate twitter stream by copying pre-loaded tweets in a tmp folder.", 
            "title": "Step 5 -  Create and Query Stream Table and Top-K"
        }, 
        {
            "location": "/#steps-to-work-with-live-twitter-stream_1", 
            "text": "# Set the keys and secrets to fetch live twitter stream\n# Note: Currently, we do not encrypt the keys. \n$ export APP_PROPS= consumerKey= consumerKey ,consumerSecret= consumerSecret ,accessToken= accessToken ,accessTokenSecret= accessTokenSecret \n\n# submit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a snappy store table \n# This job runs streaming for two minutes. \n$ /bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream", 
            "title": "Steps to work with live Twitter stream"
        }, 
        {
            "location": "/#steps-to-work-with-simulated-twitter-stream_1", 
            "text": "Submit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a gemxd table. It starts the streaming and waits for two minutes.   # Submit the TwitterPopularTagsJob \n$ ./bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream\n\n# Run the following utility in another terminal to simulate a twitter stream by copying tweets in the folder on which file stream table is listening.\n$ quickstart/scripts/simulateTwitterStream   The output of the job can be found in TwitterPopularTagsJob_timestamp.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/.", 
            "title": "Steps to work with simulated Twitter stream"
        }, 
        {
            "location": "/#working-with-spark-shell-and-spark-submit", 
            "text": "SnappyData, out-of-the-box, collocates Spark executors and the data store for efficient data intensive computations. But, it may desirable to isolate the computational cluster for other reasons - for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate for a  cache data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion.   # Start the spark shell in local mode. Pass Snappy locator\u2019s host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041\nscala \nTry few commands on the spark-shell \n\n# fetch the tables and using sqlContext which is going to be an instance of SnappyContext in this case\nscala  val airlinerefDF = sqlContext.table( airlineref ).show\nscala  val airlineDF = sqlContext.table( airline ).show\n\n# you can now work with the dataframes to fetch the data.", 
            "title": "Working with Spark shell and spark-submit"
        }, 
        {
            "location": "/#step-6-submit-a-spark-app-that-interacts-with-snappydata", 
            "text": "# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041 $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n\n# The results can be seen on the command line.", 
            "title": "Step 6 - Submit a Spark App that interacts with SnappyData"
        }, 
        {
            "location": "/#final-step-stop-the-snappydata-cluster", 
            "text": "$ sbin/snappy-stop-all.sh \nlocalhost: The SnappyData Leader has stopped.\nlocalhost: The SnappyData Server has stopped.\nlocalhost: The SnappyData Locator has stopped.", 
            "title": "Final Step - Stop the SnappyData Cluster"
        }, 
        {
            "location": "/README/", 
            "text": "Table of Contents\n\n\n\n\nIntroduction\n\n\nDownload binary distribution\n\n\nCommunity Support\n\n\nLink with SnappyData distribution\n\n\nWorking with SnappyData Source Code\n\n\nBuilding SnappyData from source\n\n\n\n\n\n\nKey Features\n\n\nGetting started\n\n\nObjectives\n\n\nSnappyData Cluster\n\n\nStep 1 - Start the SnappyData cluster\n\n\n\n\n\n\nInteracting with SnappyData\n\n\nGetting Started with SQL\n\n\nColumn and Row tables\n\n\nStep 2 - Create column table, row table and load data\n\n\nOLAP and OLTP queries\n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nApproximate query processing (AQP)\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nStream analytics using SQL and Spark Streaming\n\n\nTop-K Elements in a Stream\n\n\nStep 5 - Create and Query Stream Table and Top-K Declaratively\n\n\n\n\n\n\nGetting Started with Spark API\n\n\nColumn and Row tables\n\n\nStep 2 - Create column table, row table and load data\n\n\nOLAP and OLTP Store\n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nApproximate query processing (AQP)\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nStream analytics using Spark Streaming\n\n\nTop-K Elements in a Stream\n\n\nStep 5 - Create and Query Stream Table and Top-K\n\n\nWorking with Spark shell and spark-submit\n\n\nStep 6 - Submit a Spark App that interacts with SnappyData\n\n\n\n\n\n\nFinal Step - Stop the SnappyData Cluster\n\n\n\n\nIntroduction\n\n\nSnappyData is a \ndistributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster\n. We realize this platform through a seamless integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics). \n\n\n\n\nDownload binary distribution\n\n\nYou can download the latest version of SnappyData from [here][2]. SnappyData has been tested on Linux (mention kernel version) and Mac (OS X 10.9 and 10.10?). If not already installed, you will need to download scala 2.10 and \nJava 8\n.  (this info should also be in the download page on our web site) \nSkip to Getting Started\n\n\nCommunity Support\n\n\nWe monitor channels listed below for comments/questions. We prefer using Stackoverflow. \n\n\nStackoverflow\n \n    \nSlack\n        Gitter \n          \nIRC\n \n             \nReddit\n \n          JIRA \n\n\nLink with SnappyData distribution\n\n\nSnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:\n\n\ngroupId: io.snappydata\nartifactId: snappydata_2.10\nversion: 0.1_preview\n\n\n\n\nWorking with SnappyData Source Code\n\n\n(Info for our download page?)\nIf you are interested in working with the latest code or contributing to SnappyData development, you can also check out the master branch from Git:\n\n\nMaster development branch\ngit clone https://github.com/SnappyDataInc/snappydata.git\n\n###### 0.1 preview release branch with stability fixes ######\ngit clone https://github.com/SnappyDataInc/snappydata.git -b 0.1_preview (??)\n\n\n\n\nBuilding SnappyData from source\n\n\nYou will find the instructions for building, layout of the code, integration with IDEs using Gradle, etc, \nhere\n\n\n\n\nNOTE:\nSnappyData is built using Spark 1.6 (build xx) which is packaged as part of SnappyData. While you can build your application using Apache Spark 1.5, you will need to link to Snappy-spark to make  use of the SnappyData extensions. Gradle build tasks are packaged.  \n\n\n\n\nKey Features\n\n\n\n\n100% compatible with Spark\n: Use SnappyData as a database as well as use any of the Spark APIs - ML, Graph, etc. on the same data\n\n\nIn-memory row and column store\n: Run the store collocated in Spark executors (i.e. a single compute and data cluster) or in its own process space (i.e. separate compute and data cluster)\n\n\nSQL standard compliance\n: Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.\n\n\nSQL based extensions for streaming processing\n: Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.\n\n\nInteractive analytics using Approximate query processing(AQP)\n: We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions. \n\n\nMutate, transact on data in Spark\n: Use SQL to insert, update, delete data in tables(something that you cannot do in Spark). We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store. \n\n\nOptimizations\n: Use indexes to improve query performance in the row store (the GemFire SQL optimizer automatically uses in-memory indexes when available) \n\n\nHigh availability not just Fault tolerance\n: Data is instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications with continuous HA.\n\n\nDurability and recovery:\n Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled. \n\n\n\n\nRead SnappyData \ndocs\n for a more detailed list of all features and semantics. \n\n\nGetting started\n\n\nObjectives\n\n\n\n\nIn-memory Column and Row tables\n: Illustrate both SQL syntax and Spark API to create and manage column tables for large data and how row tables can be used for reference data and can be replicated to each node in the cluster. \n\n\nOLAP, OLTP operations\n: We run analytic class SQL queries (full scan with aggregations) on column tables and fully distributed join queries and observe the space requirements as well as the performance of these queries. For OLTP, we run simple update queries - you can note the Spark API extensions to support mutations in Spark. \n\n\nAQP\n: We run the same analytic queries by creating adjunct stratified samples to note the performance difference - can we get close to interactive query performance speeds?\n\n\nStreaming with SQL\n: We ingest twitter streams into both a probabilistic data structure for TopK time series analytics and the entire stream (full data set) into a row table. We run both ad-hoc queries on these streams (modeled as tables) as well as showcase our first preview for continuous querying support. The SnappyData value add demonstrated here is simpler, SQL centric abstractions on top of Spark streaming. And, of course, ingestion into the built-in store.\n\n\n\n\nIn this document, we discuss the features mentioned above and ask you to take steps to run the scripts that demonstrate these features. \n\n\nSnappyData Cluster\n\n\nSnappyData, a database server cluster, has three main components - Locator, Server and Lead. \n\n\n\n\nLocator\n: Provides discovery service for the cluster. Informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n\n\nLead Node\n: Acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n\n\nData Servers\n: Hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.\n\n\n\n\n\n\nDetails of about the architecture can be found \nhere\n. SnappyData also has multiple deployment options which can be found \nhere\n.\n\n\nStep 1 - Start the SnappyData cluster\n\n\n\n\nNote\n\n\nThe U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. \nSummary information on the number of on-time, delayed, canceled and diverted flights is available for the last 20 years. We use this data set in the examples below. You can learn more on this schema \nhere\n.\nDefault airline data shipped with product is of 15 MB compressed size. If you are interested in studying Approximate query processing we recommend downloading the full data set run this command (from quickstart/scripts directory):\n\n\n\n\n$ ./download_full_airlinedata.sh ../data \n\n\n\n\n\n\nDo we need this? seems to work with default cluster ...\n\n\n\n\nIn case you are running Getting Started with full dataset, configure snappy to start two servers with max heap size as 4G each. \n\n\n\n\n$ cat conf/servers\n# Two servers with total of 8G.\nyourhostName -J-Xmx4g\nyourhostName -J-Xmx4g \n\n\n\n\n\n\nPasswordless ssh\n\n\nThe quick start scripts use ssh to start up various processes. By default, this requires a password. To be able to log on to the localhost and run the script without being prompted for the password, please enable passwordless ssh.\n\n\n\n\nThe following script starts up a minimal set of essential components to form the cluster - A locator, one data server and one lead node. All nodes are started locally. To spin up remote nodes simply rename/copy the files without the template suffix and add the hostnames. The \narticle\n discusses the custom configuration and startup options.\n\n\n$ sbin/snappy-start-all.sh \n  (Roughly can take upto a minute. Associated logs are in the \u2018work\u2019 sub-directory)\nThis would output something like this ...\nlocalhost: Starting SnappyData Locator using peer discovery on: 0.0.0.0[10334]\n...\nlocalhost: SnappyData Locator pid: 56703 status: running\n\nlocalhost: Starting SnappyData Server using locators for peer discovery: jramnara-mbpro[10334]   (port used for members to form a p2p cluster)\nlocalhost: SnappyData Server pid: 56819 status: running\nlocalhost:   Distributed system now has 2 members.\n\nlocalhost: Starting SnappyData Leader using locators for peer discovery: jramnara-mbpro[10334]\nlocalhost: SnappyData Leader pid: 56932 status: running\nlocalhost:   Distributed system now has 3 members.\n\nlocalhost:   Other members: jramnara-mbpro(56703:locator)\nv0\n:54414, jramnara-mbpro(56819:datastore)\nv1\n:39737\n\n\n\n\n\nAt this point, the SnappyData cluster is up and running and is ready to accept Snappy jobs and SQL requests via JDBC/ODBC. You can \nmonitor the Spark cluster at port 4040\n. Once you load data and run queries, you can analyze the Spark SQL query plan, the job execution stages and storage details of column tables.\n\n\n\n\n\n\nInteracting with SnappyData\n\n\n\n\nWe assume some familiarity with \ncore Spark, Spark SQL and Spark Streaming concepts\n. \nAnd, you can try out the Spark \nQuick Start\n. All the commands and programs\nlisted in the Spark guides will work in SnappyData also.\n\n\n\n\nTo interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to SnappyData cluster and interact using SQL. On the other hand, users comfortable with Spark programming paradigm can write Snappy jobs to interact with SnappyData. Snappy jobs can be like a self contained Spark application or can share state with other jobs using SnappyData store. \n\n\nUnlike Apache Spark, which is primarily a computational engine, SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.\n\n\n\n\nLong running executors\n: Executors are running within the Snappy store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors. \n\n\nDriver runs in HA configuration\n: Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the \nSpark JobServer\n to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). \nRead \ndocs\n for details of the architecture.\n\n\n\n\nIn this document, we showcase mostly the same set of features via Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to \nGetting Started with Spark API\n.\n\n\nGetting Started with SQL\n\n\nFor SQL, the SnappyData SQL Shell (\nsnappy-shell\n) provides a simple way to inspect the catalog,  run admin operations,  manage the schema and run interactive queries. You can also use your favorite SQL tool like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster).\n\n\n// Run from the SnappyData base directory\n$ ./bin/snappy-shell\nVersion 2.0-SNAPSHOT.1\nsnappy\n \n\n-- Connect to the cluster ..\nsnappy\n connect client 'localhost:1527';\nsnappy\n show connections; \n\n-- Check the cluster status\nthis will list each cluster member and its status\nsnappy\n show members;\n\n\n\n\nColumn and Row tables\n\n\nColumn tables\n organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\n-- DDL to create a column table\nCREATE TABLE AIRLINE (\ncolumn definitions\n) USING column OPTIONS(buckets '5') ;\n\n\n\n\nRow tables\n, unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.\n\n\ncreate table\n DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary \ndocs\n for the details.\n\n\n-- DDL to create a row table\nCREATE TABLE AIRLINEREF (\ncolumn definitions\n) USING row OPTIONS() ;\n\n\n\n\nStep 2 - Create column table, row table and load data\n\n\n\n\nTo run the scripts with full airline data set, change the 'create_and_load_column_table.sql' script to point at the data set that you had downloaded in Step 1.\n\n\n\n\nSQL scripts to create and load column and row tables.\n\n\n-- Loads parquet formatted data into a temporary spark table \n-- then saves it in  column table called Airline.\nsnappy\n run './quickstart/scripts/create_and_load_column_table.sql';\n\n-- Creates the airline code table. Row tables can be replicated to each node \n-- so join processing with other tables can completely avoid shuffling \nsnappy\n run './quickstart/scripts/create_and_load_row_table.sql';\n\n-- See the status of system\nsnappy\n run './quickstart/scripts/status_queries.sql'\n\n\n\n\nYou can see the memory consumed on \nSpark Console\n. \n\n\nOLAP and OLTP queries\n\n\nSQL client connections (via JDBC or ODBC) are routed to the appropriate data server via the locator (Physical connections are automatically created in the driver and are transparently swizzled in case of failures also). When queries are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP.  Such queries are routed to the \nlead\n node where a \nSpark SQLContext\n is managed for each connection. The Query is planned using Spark's Catalyst engine and scheduled to be executed on the data servers. The number of partitions determine the number of concurrent tasks used across the data servers to parallel run the query. In this case, our column table was created using \n5 partitions(buckets)\n and hence will use 5 concurrent tasks. \n\n\n---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nsnappy\n select AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;\n\n\n\n\nFor low latency OLTP queries, the engine won't route it to the lead and instead execute it immediately without any scheduling overhead. Quite often, this may mean simply fetching a row by hashing a key (in microseconds).\n\n\n--- Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'\n--- the airline code can be updated in the row table\nUPDATE AIRLINEREF SET DESCRIPTION='Delta America' WHERE CAST(CODE AS VARCHAR(25))='DL';\n\n\n\n\nSpark SQL can cache DataFrames as temporary tables and the data set is immutable. SnappyData SQL is compatible with the SQL standard with support for transactions and DML (insert, update, delete) on tables. \nLink to Snappy Store SQL reference\n.  As we show later, any table in Snappy is also visible as Spark DataFrame. \n\n\nStep 3 - Run OLAP and OLTP queries\n\n\n-- Simply run the script or copy/paste one query at a time if you want to explore the query execution on the Spark console. \nsnappy\n run './quickstart/scripts/olap_queries.sql';\n\n---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nselect AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;\n\n\n\n\nEach query is executed as one or more Jobs and each Job executed in one or more stages. You can explore the query execution plan and metrics \nhere\n\n\n-- Run a simple update SQL statement on the replicated row table.\nsnappy\n run './quickstart/scripts/oltp_queries.sql';\n\n\n\n\nYou can now re-run olap_queries.sql to see the updated join result set.\n\n\n\n\nNote\n\nIn the current implementation we only support appending to Column tables. Future releases will support all DML operations. \nYou can execute transactions using commands \nautocommit off\n and \ncommit\n.  \n\n\n\n\nApproximate query processing (AQP)\n\n\nOLAP queries are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory queries above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs and DDL to specify one or more curated \nstratified samples\n on large tables. \n\n\n\n\nNote\n\n\nWe recommend downloading the \nonTime airline\n data for 2009-2015 which is about 52 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.\n\n\n\n\nThe following DDL creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your \nGroup by\n and \nWhere\n make up the \nQuery Column Set\n (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection. \n\n\nCREATE SAMPLE TABLE AIRLINE_SAMPLE\n   OPTIONS(\n    buckets '5',                          -- Number of partitions \n    qcs 'UniqueCarrier, Year_, Month_',   -- QueryColumnSet(qcs): The strata - 3% of each combination of Carrier, \n                                          -- Year and Month are stored as sample\n    fraction '0.03',                      -- How big should the sample be\n    strataReservoirSize '50',             -- Reservoir sampling to support streaming inserts\n    basetable 'Airline')                  -- The parent base table\n\n\n\n\nYou can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the \nWith Error\n constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set. \n\n\n-- What is the average arrival delay for all airlines for each month?;\nsnappy\n select avg(ArrDelay), Month_ from Airline where ArrDelay \n0 \n    group by Month_\n    with error .05 ;\n-- The above query will consult the sample and return an answer if the estimated answer \n-- is at least 95% accurate (here, by default we use a 95% confidence interval). Read [docs](docs) for more details.\n\n-- You can also access the error using built-in functions. \nsnappy\n select avg(ArrDelay) avgDelay, absolute_error(avgDelay), Month_ \n    from Airline where ArrDelay \n0 \n    group by Month_\n    with error .05 ;\n\n\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\n--- Creates and then samples a table from the Airline table \nsnappy\n run 'create_and_load_sample_table.sql';\n\n\n\n\nYou can now re-run the previous OLAP queries with an error constraint and compare the results.  You should notice a 10X or larger difference in query execution latency while the results remain nearly accurate. As a reminder, we recommend downloading the larger data set for this exercise.\n\n\n-- re-run olap queries with error constraint to automatically use sampling\nsnappy\n run 'olap_approx_queries.sql';\n\n\n\n\nStream analytics using SQL and Spark Streaming\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. You can also dynamically run SQL queries on these streams. There is no need to learn Spark streaming APIs or statically define all the rules to be executed on these streams.\n\n\nThe commands below consume tweets, filters our just the hashtags and converts these into Row objects, models the stream as a table (so it can be queried) and we then run ad-hoc SQL from remote clients on the current state of the stream. \n\n\n--- Inits the Streaming Context with the batch interval of 2 seconds.\n--- i.e. the stream is processed once every 2 seconds.\nsnappy\n STREAMING INIT 2\n--- create a stream table just containing the hashtags\nsnappy\n CREATE STREAM TABLE HASHTAG_FILESTREAMTABLE\n              (hashtag string)\n            USING file_stream\n            OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',\n              rowConverter 'org.apache.spark.sql.streaming.TweetToHashtagRow',\n              directory '/tmp/copiedtwitterdata')\n-- A file_stream data source monitors the directory and as files arrives they are ingested \n--   into the streaming pipeline. First converted into Rows using 'TweetToHashtagRow' then visible as table\n--- Start streaming context \nsnappy\n STREAMING START\n--- Adhoc sql on the stream table to query the current batch\n--- Get top 10 popular hashtags ----\nsnappy\n SELECT hashtag, count(*) as tagcount\n        FROM HASHTAG_FILESTREAMTABLE\n        GROUP BY hashtag\n        ORDER BY tagcount DESC limit 10;\n\n\n\n\nLater, in the Spark code section we further enhance to showcase \"continuous queries\" (CQ). Dynamic registration of CQs (from remote clients) will be available in the next release.\n\n\nTop-K Elements in a Stream\n\n\nFinding the \nk\n most popular elements in a data stream is a common analytic query. For instance, top-100 pages on a popular website in the last 10 mins, top-10 sales regions in the last week, etc. As you can tell, if the query is on a arbitrary time interval in the past, this will most likely mandate storing the entire stream. And, this could easily be millions to billions of events is use cases like in IoT. SnappyData provides SQL extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures and enables transparent querying using Spark SQL. More details about SnappyData's implementation of top-k can be found \nhere.\n\n\nSnappyData provides DDL extensions to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using regular SQL queries. \n\n\n--- Create a topk table from a stream table\nCREATE TOPK TABLE filestream_topktable ON HASHTAG_FILESTREAMTABLE OPTIONS\n(key 'hashtag', timeInterval '2000ms', size '10' );\n--- Query a topk table \nSELECT hashtag, COUNT(hashtag) AS TopKCount\nFROM filestream_topktable\nGROUP BY hashtag ORDER BY TopKCount limit 10;\n\n\n\n\nNow, lets try analyzing some tweets using this above syntax in real time using the packaged scripts ..\n\n\nStep 5 - Create and Query Stream Table and Top-K Declaratively\n\n\nYou can use the scripts that simulates the twitter stream by copying pre-loaded tweets in a tmp folder. Or, you could use a script that access the live twitter stream.  \n\n\nSteps to work with simulated Twitter stream\n\n\nCreate a file stream table that listens on a folder and then start the streaming context. \n\n\nsnappy\n run './quickstart/scripts/create_and_start_file_streaming.sql';\n\n\n\n\nRun the following utility in another terminal that simulates a twitter stream by copying tweets in the folder on which file stream table is listening.\n\n\n$ quickstart/scripts/simulateTwitterStream \n\n\n\n\nNow query the current batch of the stream using the following script. This also creates Topk structures. simulateTwitterStream script runs for only for a minute or so. Since our script is querying the current window, it will return no results after the streaming is over. \n\n\nsnappy\n run './quickstart/scripts/file_streaming_query.sql';\n\n\n\n\nSteps to work with live Twitter stream\n\n\nYou would have to generate authorization keys and secrets on \ntwitter apps\n and update SNAPPY_HOME/quickstart/scripts/create_and_start_twitter_streaming.sql with the keys and secrets.\n\n\n--- Run the create and start script that has keys and secrets to fetch live twitter stream\n--- Note: Currently, we do not encrypt the keys. \n-- This also creates Topk structures\nsnappy\n run './quickstart/scripts/create_and_start_twitter_streaming.sql';\n\nsnappy\n run './quickstart/scripts/twitter_streaming_query.sql';\n\n\n\n\nGetting Started with Spark API\n\n\nSnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's \nSQLContext\n to work with Row and Column tables. Any DataFrame can be managed as SnappyData table and any table can be accessed as a DataFrame. This is similar to \nHiveContext\n and it integrates the SQLContext functionality with the Snappy store. Similarly, SnappyStreamingContext is an entry point for SnappyData extensions to Spark streaming and it extends Spark's \nStreaming Context\n. \n\n\nApplications typically submit Jobs to SnappyData and do not explicitly create a SnappyContext or SnappyStreamingContext. These jobs are the primary mechanism to interact with SnappyData using Spark API. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. \n\n\nclass SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(snc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}\n\n\n\n\nThe implementation of \nrunJob\n function of SnappySQLJob uses SnappyContext to interact with SnappyData store to process and store tables. The implementation of runJob of SnappyStreamingJob uses SnappyStreamingContext to create streams and manage the streaming context. The jobs are submitted to lead node of SnappyData over REST API using a \nspark-submit\n like utility. See more details about jobs \nhere\n\n\nColumn and Row tables\n\n\nColumn tables\n organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\n// creating a column table in Snappy job\nsnappyContext.createTable(\nAIRLINE\n, \ncolumn\n, schema, Map(\nbuckets\n -\n \n5\n))\n\n\n\n\nRow tables\n, unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.\n\n\ncreate table\n DDL allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary \ndocs\n for the details.\n\n\n// creating a row table in Snappy job\nval airlineCodeDF = snappyContext.createTable(\nAIRLINEREF\n, \nrow\n, schema, Map())\n\n\n\n\nStep 2 - Create column table, row table and load data\n\n\n\n\nTo run the scripts with full airline data set, set the following config parameter to point at the data set that you had downloaded in Step 1.\n\n\n\n\nexport APP_PROPS=\"airline_file=full_dataset_folder\"\n\n\n\n\n\n\nSubmit CreateAndLoadAirlineDataJob over REST API to create row and column tables. See more details about jobs and job submission \nhere.\n. \n\n\n$ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp --class  io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{\nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  } }\n\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead hostNameOfLead:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{ \nduration\n: \n17.53 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.CreateAndLoadAirlineDataJob\n,\n  \nstartTime\n: \n2016-01-12T16:59:14.746+05:30\n,\n  \ncontext\n: \nsnappyContext1452598154529305363\n,\n  \nresult\n: \nSee /snappy/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n}\n# Tables are created\n\n\n\n\nThe output of the job can be found in CreateAndLoadAirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can see the size of the column tables on Spark UI which by default can be seen at http://hostNameOfLead:4040. \n\n\nOLAP and OLTP Store\n\n\nSnappyContext extends SQLContext and adds functionality to work with row and column tables. When queries inside jobs are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP. Such queries are planned using Spark's Catalyst engine and scheduled to be executed on the data servers. \n\n\nval resultDF = airlineDF.join(airlineCodeDF,\n        airlineDF.col(\nUniqueCarrier\n).equalTo(airlineCodeDF(\nCODE\n))).\n        groupBy(airlineDF(\nUniqueCarrier\n), airlineCodeDF(\nDESCRIPTION\n)).\n        agg(\nArrDelay\n -\n \navg\n).orderBy(\navg(ArrDelay)\n)\n\n\n\n\nFor low latency OLTP queries in jobs, SnappyData won't schedule these queries instead execute them immediately on SnappyData servers without any scheduling overhead. Quite often, this may mean simply fetching or updating a row by hashing a key (in nanoseconds). \n\n\n// Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'. Update the row table.\nval filterExpr: String = \n CODE ='DL'\n\nval newColumnValues: Row = Row(\nDelta America\n)\nval updateColumns = \nDESCRIPTION\n\nsnappyContext.update(rowTableName, filterExpr, newColumnValues, updateColumns)\n\n\n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nAirlineDataJob.scala runs OLAP and OLTP queries on Snappy tables. Also, it caches the same airline data in Spark cache and runs the same OLAP query on the Spark cache. With airline data set, we have seen both Spark cache and snappy store table to have more and less the same performance.  \n\n\n# Submit AirlineDataJob to SnappyData\n$ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp  --class  io.snappydata.examples.AirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{ \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n1b0d2e50-42da-4fdd-9ea2-69e29ab92de2\n,\n    \ncontext\n: \nsnappyContext1453196176725064822\n\n } }\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090  --job-id 1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n{ \nduration\n: \n6.617 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.AirlineDataJob\n,\n  \nstartTime\n: \n2016-01-19T15:06:16.771+05:30\n,\n  \ncontext\n: \nsnappyContext1453196176725064822\n,\n  \nresult\n: \nSee /snappy/work/localhost-lead-1/AirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n1b0d2e50-42da-4fdd-9ea2-69e29ab92de2\n\n}\n\n\n\n\nThe output of the job can be found in AirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can explore the Spark SQL query plan on Spark UI which by default can be seen at http://hostNameOfLead:4040.\n\n\nApproximate query processing (AQP)\n\n\nOLAP jobs are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory jobs above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs to specify one or more curated \nstratified samples\n on large tables. \n\n\n\n\nNote\n\n\nWe recommend downloading the \nonTime airline\n data for 2009-2015 which is about 50 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.\n\n\n\n\nThe following scala code creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your \nGroup by\n and \nWhere\n make us the \nQuery Column Set\n (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection. \n\n\nval sampleDF = snappyContext.createTable(sampleTable, \n        \ncolumn_sample\n, // DataSource provider for sample tables\n        updatedSchema, Map(\nbuckets\n -\n \n5\n,\n          \nqcs\n -\n \nUniqueCarrier, Year_, Month_\n,\n          \nfraction\n -\n \n0.03\n,\n          \nstrataReservoirSize\n -\n \n50\n,\n          \nbasetable\n -\n \nAirline\n\n        ))\n\n\n\n\nYou can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the \nWith Error\n constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set. \n\n\n// Query Snappy Store's Sample table :Which Airlines arrive On schedule? JOIN with reference table\nsampleResult = sampleDF.join(airlineCodeDF,\n        sampleDF.col(\nUniqueCarrier\n).equalTo(airlineCodeDF(\nCODE\n))).\n          groupBy(sampleDF(\nUniqueCarrier\n), airlineCodeDF(\nDESCRIPTION\n)).\n          agg(\nArrDelay\n -\n \navg\n).orderBy(\navg(ArrDelay)\n)\n\n // Query Snappy Store's Airline table with error clause.\nairlineDF.groupBy(airlineDF(\nMonth_\n))\n  .agg(\nArrDelay\n -\n \navg\n)\n  .orderBy(\nMonth_\n).withError(0.05,0.95)\n\n\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nCreateAndLoadAirlineDataJob and AirlineDataJob executed in the previous sections created the sample tables and executed OLAP queries over them.\n\n\nStream analytics using Spark Streaming\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. Also, SnappyData introduces \"continuous queries\" (CQ) on the stream. One can define a continous query as a SQL query on the stream with window and slide extensions which is returned as SchemaDStream i.e. DStream with schema. SnappyData's extensions provide functionality to insert a SchemaDStream into snappy store. \n\n\nDynamic registration of CQs (from remote clients) will be available in the next release.\n\n\n// create a stream table declaratively \nsnsc.sql(\nCREATE STREAM TABLE RETWEETTABLE (retweetId long, \n +\n    \nretweetCnt int, retweetTxt string) USING file_stream \n +\n    \nOPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2', \n +\n    \nrowConverter 'org.apache.spark.sql.streaming.TweetToRetweetRow',\n +\n    \ndirectory '/tmp/copiedtwitterdata')\n);\n\n// Register a continous query on the stream table with window and slide parameters\nval retweetStream: SchemaDStream = snsc.registerCQ(\nSELECT retweetId, retweetCnt FROM RETWEETTABLE \n +\n    \nwindow (duration '2' seconds, slide '2' seconds)\n)\n\n// Create a row table to hold the retweets based on their id \nsnsc.snappyContext.sql(s\nCREATE TABLE $tableName (retweetId bigint PRIMARY KEY, \n +\n    s\nretweetCnt int, retweetTxt string) USING row OPTIONS ()\n)\n\n// Iterate over the stream and insert it into snappy store\nretweetStream.foreachDataFrame(df =\n {\n    df.write.mode(SaveMode.Append).saveAsTable(tableName)\n})\n\n\n\n\nTop-K Elements in a Stream\n\n\nContinuously finding the \nk\n most popular elements in a data stream is a common analytic query. SnappyData provides extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures. More details about SnappyData's implementation of top-k can be found \nhere.\n\n\nSnappyData provides API in SnappyContext to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using another API. \n\n\n--- Create a topk table from a stream table\nsnappyContext.createApproxTSTopK(\ntopktable\n, \nhashtag\n,\n    Some(schema), Map(\n      \nepoch\n -\n System.currentTimeMillis().toString,\n      \ntimeInterval\n -\n \n2000ms\n,\n      \nsize\n -\n \n10\n,\n      \nbasetable\n -\n \nHASHTAGTABLE\n\n    ))\n--- Query a topk table for the last two seconds\nval topKDF = snappyContext.queryApproxTSTopK(\ntopktable\n,\n                System.currentTimeMillis - 2000,\n                System.currentTimeMillis)\n\n\n\n\nStep 5 -  Create and Query Stream Table and Top-K\n\n\nIdeally, we would like you to try this example using live twitter stream. For that, you would have to generate authorization keys and secrets on twitter apps. Alternatively, you can use use file stream scripts that simulate twitter stream by copying pre-loaded tweets in a tmp folder.\n\n\nSteps to work with live Twitter stream\n\n\n# Set the keys and secrets to fetch live twitter stream\n# Note: Currently, we do not encrypt the keys. \n$ export APP_PROPS=\nconsumerKey=\nconsumerKey\n,consumerSecret=\nconsumerSecret\n,accessToken=\naccessToken\n,accessTokenSecret=\naccessTokenSecret\n\n\n# submit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a snappy store table \n# This job runs streaming for two minutes. \n$ /bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream\n\n\n\n\n\nSteps to work with simulated Twitter stream\n\n\nSubmit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a gemxd table. It starts the streaming and waits for two minutes. \n\n\n# Submit the TwitterPopularTagsJob \n$ ./bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream\n\n# Run the following utility in another terminal to simulate a twitter stream by copying tweets in the folder on which file stream table is listening.\n$ quickstart/scripts/simulateTwitterStream \n\n\n\n\n\nThe output of the job can be found in TwitterPopularTagsJob_timestamp.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. \n\n\nWorking with Spark shell and spark-submit\n\n\nSnappyData, out-of-the-box, collocates Spark executors and the data store for efficient data intensive computations. But, it may desirable to isolate the computational cluster for other reasons - for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate for a  cache data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion. \n\n\n# Start the spark shell in local mode. Pass Snappy locator\u2019s host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041\nscala\n\nTry few commands on the spark-shell \n\n# fetch the tables and using sqlContext which is going to be an instance of SnappyContext in this case\nscala\n val airlinerefDF = sqlContext.table(\nairlineref\n).show\nscala\n val airlineDF = sqlContext.table(\nairline\n).show\n\n# you can now work with the dataframes to fetch the data.\n\n\n\n\nStep 6 - Submit a Spark App that interacts with SnappyData\n\n\n# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041 $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n\n# The results can be seen on the command line. \n\n\n\n\nFinal Step - Stop the SnappyData Cluster\n\n\n$ sbin/snappy-stop-all.sh \nlocalhost: The SnappyData Leader has stopped.\nlocalhost: The SnappyData Server has stopped.\nlocalhost: The SnappyData Locator has stopped.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/README/#table-of-contents", 
            "text": "Introduction  Download binary distribution  Community Support  Link with SnappyData distribution  Working with SnappyData Source Code  Building SnappyData from source    Key Features  Getting started  Objectives  SnappyData Cluster  Step 1 - Start the SnappyData cluster    Interacting with SnappyData  Getting Started with SQL  Column and Row tables  Step 2 - Create column table, row table and load data  OLAP and OLTP queries  Step 3 - Run OLAP and OLTP queries  Approximate query processing (AQP)  Step 4 - Create, Load and Query Sample Table  Stream analytics using SQL and Spark Streaming  Top-K Elements in a Stream  Step 5 - Create and Query Stream Table and Top-K Declaratively    Getting Started with Spark API  Column and Row tables  Step 2 - Create column table, row table and load data  OLAP and OLTP Store  Step 3 - Run OLAP and OLTP queries  Approximate query processing (AQP)  Step 4 - Create, Load and Query Sample Table  Stream analytics using Spark Streaming  Top-K Elements in a Stream  Step 5 - Create and Query Stream Table and Top-K  Working with Spark shell and spark-submit  Step 6 - Submit a Spark App that interacts with SnappyData    Final Step - Stop the SnappyData Cluster", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/README/#introduction", 
            "text": "SnappyData is a  distributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster . We realize this platform through a seamless integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics).", 
            "title": "Introduction"
        }, 
        {
            "location": "/README/#download-binary-distribution", 
            "text": "You can download the latest version of SnappyData from [here][2]. SnappyData has been tested on Linux (mention kernel version) and Mac (OS X 10.9 and 10.10?). If not already installed, you will need to download scala 2.10 and  Java 8 .  (this info should also be in the download page on our web site)  Skip to Getting Started", 
            "title": "Download binary distribution"
        }, 
        {
            "location": "/README/#community-support", 
            "text": "We monitor channels listed below for comments/questions. We prefer using Stackoverflow.   Stackoverflow        Slack         Gitter             IRC                 Reddit             JIRA", 
            "title": "Community Support"
        }, 
        {
            "location": "/README/#link-with-snappydata-distribution", 
            "text": "SnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:  groupId: io.snappydata\nartifactId: snappydata_2.10\nversion: 0.1_preview", 
            "title": "Link with SnappyData distribution"
        }, 
        {
            "location": "/README/#working-with-snappydata-source-code", 
            "text": "(Info for our download page?)\nIf you are interested in working with the latest code or contributing to SnappyData development, you can also check out the master branch from Git:  Master development branch\ngit clone https://github.com/SnappyDataInc/snappydata.git\n\n###### 0.1 preview release branch with stability fixes ######\ngit clone https://github.com/SnappyDataInc/snappydata.git -b 0.1_preview (??)", 
            "title": "Working with SnappyData Source Code"
        }, 
        {
            "location": "/README/#building-snappydata-from-source", 
            "text": "You will find the instructions for building, layout of the code, integration with IDEs using Gradle, etc,  here   NOTE:\nSnappyData is built using Spark 1.6 (build xx) which is packaged as part of SnappyData. While you can build your application using Apache Spark 1.5, you will need to link to Snappy-spark to make  use of the SnappyData extensions. Gradle build tasks are packaged.", 
            "title": "Building SnappyData from source"
        }, 
        {
            "location": "/README/#key-features", 
            "text": "100% compatible with Spark : Use SnappyData as a database as well as use any of the Spark APIs - ML, Graph, etc. on the same data  In-memory row and column store : Run the store collocated in Spark executors (i.e. a single compute and data cluster) or in its own process space (i.e. separate compute and data cluster)  SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.  SQL based extensions for streaming processing : Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.  Interactive analytics using Approximate query processing(AQP) : We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions.   Mutate, transact on data in Spark : Use SQL to insert, update, delete data in tables(something that you cannot do in Spark). We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store.   Optimizations : Use indexes to improve query performance in the row store (the GemFire SQL optimizer automatically uses in-memory indexes when available)   High availability not just Fault tolerance : Data is instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications with continuous HA.  Durability and recovery:  Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.    Read SnappyData  docs  for a more detailed list of all features and semantics.", 
            "title": "Key Features"
        }, 
        {
            "location": "/README/#getting-started", 
            "text": "", 
            "title": "Getting started"
        }, 
        {
            "location": "/README/#objectives", 
            "text": "In-memory Column and Row tables : Illustrate both SQL syntax and Spark API to create and manage column tables for large data and how row tables can be used for reference data and can be replicated to each node in the cluster.   OLAP, OLTP operations : We run analytic class SQL queries (full scan with aggregations) on column tables and fully distributed join queries and observe the space requirements as well as the performance of these queries. For OLTP, we run simple update queries - you can note the Spark API extensions to support mutations in Spark.   AQP : We run the same analytic queries by creating adjunct stratified samples to note the performance difference - can we get close to interactive query performance speeds?  Streaming with SQL : We ingest twitter streams into both a probabilistic data structure for TopK time series analytics and the entire stream (full data set) into a row table. We run both ad-hoc queries on these streams (modeled as tables) as well as showcase our first preview for continuous querying support. The SnappyData value add demonstrated here is simpler, SQL centric abstractions on top of Spark streaming. And, of course, ingestion into the built-in store.   In this document, we discuss the features mentioned above and ask you to take steps to run the scripts that demonstrate these features.", 
            "title": "Objectives"
        }, 
        {
            "location": "/README/#snappydata-cluster", 
            "text": "SnappyData, a database server cluster, has three main components - Locator, Server and Lead.    Locator : Provides discovery service for the cluster. Informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.  Lead Node : Acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.  Data Servers : Hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.    Details of about the architecture can be found  here . SnappyData also has multiple deployment options which can be found  here .", 
            "title": "SnappyData Cluster"
        }, 
        {
            "location": "/README/#step-1-start-the-snappydata-cluster", 
            "text": "", 
            "title": "Step 1 - Start the SnappyData cluster"
        }, 
        {
            "location": "/README/#note", 
            "text": "The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. \nSummary information on the number of on-time, delayed, canceled and diverted flights is available for the last 20 years. We use this data set in the examples below. You can learn more on this schema  here .\nDefault airline data shipped with product is of 15 MB compressed size. If you are interested in studying Approximate query processing we recommend downloading the full data set run this command (from quickstart/scripts directory):   $ ./download_full_airlinedata.sh ../data", 
            "title": "Note"
        }, 
        {
            "location": "/README/#do-we-need-this-seems-to-work-with-default-cluster", 
            "text": "In case you are running Getting Started with full dataset, configure snappy to start two servers with max heap size as 4G each.    $ cat conf/servers\n# Two servers with total of 8G.\nyourhostName -J-Xmx4g\nyourhostName -J-Xmx4g", 
            "title": "Do we need this? seems to work with default cluster ..."
        }, 
        {
            "location": "/README/#passwordless-ssh", 
            "text": "The quick start scripts use ssh to start up various processes. By default, this requires a password. To be able to log on to the localhost and run the script without being prompted for the password, please enable passwordless ssh.   The following script starts up a minimal set of essential components to form the cluster - A locator, one data server and one lead node. All nodes are started locally. To spin up remote nodes simply rename/copy the files without the template suffix and add the hostnames. The  article  discusses the custom configuration and startup options.  $ sbin/snappy-start-all.sh \n  (Roughly can take upto a minute. Associated logs are in the \u2018work\u2019 sub-directory)\nThis would output something like this ...\nlocalhost: Starting SnappyData Locator using peer discovery on: 0.0.0.0[10334]\n...\nlocalhost: SnappyData Locator pid: 56703 status: running\n\nlocalhost: Starting SnappyData Server using locators for peer discovery: jramnara-mbpro[10334]   (port used for members to form a p2p cluster)\nlocalhost: SnappyData Server pid: 56819 status: running\nlocalhost:   Distributed system now has 2 members.\n\nlocalhost: Starting SnappyData Leader using locators for peer discovery: jramnara-mbpro[10334]\nlocalhost: SnappyData Leader pid: 56932 status: running\nlocalhost:   Distributed system now has 3 members.\n\nlocalhost:   Other members: jramnara-mbpro(56703:locator) v0 :54414, jramnara-mbpro(56819:datastore) v1 :39737  At this point, the SnappyData cluster is up and running and is ready to accept Snappy jobs and SQL requests via JDBC/ODBC. You can  monitor the Spark cluster at port 4040 . Once you load data and run queries, you can analyze the Spark SQL query plan, the job execution stages and storage details of column tables.", 
            "title": "Passwordless ssh"
        }, 
        {
            "location": "/README/#interacting-with-snappydata", 
            "text": "We assume some familiarity with  core Spark, Spark SQL and Spark Streaming concepts . \nAnd, you can try out the Spark  Quick Start . All the commands and programs\nlisted in the Spark guides will work in SnappyData also.   To interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to SnappyData cluster and interact using SQL. On the other hand, users comfortable with Spark programming paradigm can write Snappy jobs to interact with SnappyData. Snappy jobs can be like a self contained Spark application or can share state with other jobs using SnappyData store.   Unlike Apache Spark, which is primarily a computational engine, SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.   Long running executors : Executors are running within the Snappy store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors.   Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the  Spark JobServer  to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). \nRead  docs  for details of the architecture.   In this document, we showcase mostly the same set of features via Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to  Getting Started with Spark API .", 
            "title": "Interacting with SnappyData"
        }, 
        {
            "location": "/README/#getting-started-with-sql", 
            "text": "For SQL, the SnappyData SQL Shell ( snappy-shell ) provides a simple way to inspect the catalog,  run admin operations,  manage the schema and run interactive queries. You can also use your favorite SQL tool like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster).  // Run from the SnappyData base directory\n$ ./bin/snappy-shell\nVersion 2.0-SNAPSHOT.1\nsnappy  \n\n-- Connect to the cluster ..\nsnappy  connect client 'localhost:1527';\nsnappy  show connections; \n\n-- Check the cluster status\nthis will list each cluster member and its status\nsnappy  show members;", 
            "title": "Getting Started with SQL"
        }, 
        {
            "location": "/README/#column-and-row-tables", 
            "text": "Column tables  organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  -- DDL to create a column table\nCREATE TABLE AIRLINE ( column definitions ) USING column OPTIONS(buckets '5') ;  Row tables , unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.  create table  DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary  docs  for the details.  -- DDL to create a row table\nCREATE TABLE AIRLINEREF ( column definitions ) USING row OPTIONS() ;", 
            "title": "Column and Row tables"
        }, 
        {
            "location": "/README/#step-2-create-column-table-row-table-and-load-data", 
            "text": "To run the scripts with full airline data set, change the 'create_and_load_column_table.sql' script to point at the data set that you had downloaded in Step 1.   SQL scripts to create and load column and row tables.  -- Loads parquet formatted data into a temporary spark table \n-- then saves it in  column table called Airline.\nsnappy  run './quickstart/scripts/create_and_load_column_table.sql';\n\n-- Creates the airline code table. Row tables can be replicated to each node \n-- so join processing with other tables can completely avoid shuffling \nsnappy  run './quickstart/scripts/create_and_load_row_table.sql';\n\n-- See the status of system\nsnappy  run './quickstart/scripts/status_queries.sql'  You can see the memory consumed on  Spark Console .", 
            "title": "Step 2 - Create column table, row table and load data"
        }, 
        {
            "location": "/README/#olap-and-oltp-queries", 
            "text": "SQL client connections (via JDBC or ODBC) are routed to the appropriate data server via the locator (Physical connections are automatically created in the driver and are transparently swizzled in case of failures also). When queries are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP.  Such queries are routed to the  lead  node where a  Spark SQLContext  is managed for each connection. The Query is planned using Spark's Catalyst engine and scheduled to be executed on the data servers. The number of partitions determine the number of concurrent tasks used across the data servers to parallel run the query. In this case, our column table was created using  5 partitions(buckets)  and hence will use 5 concurrent tasks.   ---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nsnappy  select AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;  For low latency OLTP queries, the engine won't route it to the lead and instead execute it immediately without any scheduling overhead. Quite often, this may mean simply fetching a row by hashing a key (in microseconds).  --- Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'\n--- the airline code can be updated in the row table\nUPDATE AIRLINEREF SET DESCRIPTION='Delta America' WHERE CAST(CODE AS VARCHAR(25))='DL';  Spark SQL can cache DataFrames as temporary tables and the data set is immutable. SnappyData SQL is compatible with the SQL standard with support for transactions and DML (insert, update, delete) on tables.  Link to Snappy Store SQL reference .  As we show later, any table in Snappy is also visible as Spark DataFrame.", 
            "title": "OLAP and OLTP queries"
        }, 
        {
            "location": "/README/#step-3-run-olap-and-oltp-queries", 
            "text": "-- Simply run the script or copy/paste one query at a time if you want to explore the query execution on the Spark console. \nsnappy  run './quickstart/scripts/olap_queries.sql';\n\n---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nselect AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline_sample, airlineref\n  where airline_sample.UniqueCarrier = airlineref.Code \n  group by UniqueCarrier, description \n  order by arrivalDelay;  Each query is executed as one or more Jobs and each Job executed in one or more stages. You can explore the query execution plan and metrics  here  -- Run a simple update SQL statement on the replicated row table.\nsnappy  run './quickstart/scripts/oltp_queries.sql';  You can now re-run olap_queries.sql to see the updated join result set.   Note \nIn the current implementation we only support appending to Column tables. Future releases will support all DML operations. \nYou can execute transactions using commands  autocommit off  and  commit .", 
            "title": "Step 3 - Run OLAP and OLTP queries"
        }, 
        {
            "location": "/README/#approximate-query-processing-aqp", 
            "text": "OLAP queries are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory queries above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs and DDL to specify one or more curated  stratified samples  on large tables.", 
            "title": "Approximate query processing (AQP)"
        }, 
        {
            "location": "/README/#note_1", 
            "text": "We recommend downloading the  onTime airline  data for 2009-2015 which is about 52 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.   The following DDL creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your  Group by  and  Where  make up the  Query Column Set  (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection.   CREATE SAMPLE TABLE AIRLINE_SAMPLE\n   OPTIONS(\n    buckets '5',                          -- Number of partitions \n    qcs 'UniqueCarrier, Year_, Month_',   -- QueryColumnSet(qcs): The strata - 3% of each combination of Carrier, \n                                          -- Year and Month are stored as sample\n    fraction '0.03',                      -- How big should the sample be\n    strataReservoirSize '50',             -- Reservoir sampling to support streaming inserts\n    basetable 'Airline')                  -- The parent base table  You can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the  With Error  constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set.   -- What is the average arrival delay for all airlines for each month?;\nsnappy  select avg(ArrDelay), Month_ from Airline where ArrDelay  0 \n    group by Month_\n    with error .05 ;\n-- The above query will consult the sample and return an answer if the estimated answer \n-- is at least 95% accurate (here, by default we use a 95% confidence interval). Read [docs](docs) for more details.\n\n-- You can also access the error using built-in functions. \nsnappy  select avg(ArrDelay) avgDelay, absolute_error(avgDelay), Month_ \n    from Airline where ArrDelay  0 \n    group by Month_\n    with error .05 ;", 
            "title": "Note"
        }, 
        {
            "location": "/README/#step-4-create-load-and-query-sample-table", 
            "text": "--- Creates and then samples a table from the Airline table \nsnappy  run 'create_and_load_sample_table.sql';  You can now re-run the previous OLAP queries with an error constraint and compare the results.  You should notice a 10X or larger difference in query execution latency while the results remain nearly accurate. As a reminder, we recommend downloading the larger data set for this exercise.  -- re-run olap queries with error constraint to automatically use sampling\nsnappy  run 'olap_approx_queries.sql';", 
            "title": "Step 4 - Create, Load and Query Sample Table"
        }, 
        {
            "location": "/README/#stream-analytics-using-sql-and-spark-streaming", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. You can also dynamically run SQL queries on these streams. There is no need to learn Spark streaming APIs or statically define all the rules to be executed on these streams.  The commands below consume tweets, filters our just the hashtags and converts these into Row objects, models the stream as a table (so it can be queried) and we then run ad-hoc SQL from remote clients on the current state of the stream.   --- Inits the Streaming Context with the batch interval of 2 seconds.\n--- i.e. the stream is processed once every 2 seconds.\nsnappy  STREAMING INIT 2\n--- create a stream table just containing the hashtags\nsnappy  CREATE STREAM TABLE HASHTAG_FILESTREAMTABLE\n              (hashtag string)\n            USING file_stream\n            OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',\n              rowConverter 'org.apache.spark.sql.streaming.TweetToHashtagRow',\n              directory '/tmp/copiedtwitterdata')\n-- A file_stream data source monitors the directory and as files arrives they are ingested \n--   into the streaming pipeline. First converted into Rows using 'TweetToHashtagRow' then visible as table\n--- Start streaming context \nsnappy  STREAMING START\n--- Adhoc sql on the stream table to query the current batch\n--- Get top 10 popular hashtags ----\nsnappy  SELECT hashtag, count(*) as tagcount\n        FROM HASHTAG_FILESTREAMTABLE\n        GROUP BY hashtag\n        ORDER BY tagcount DESC limit 10;  Later, in the Spark code section we further enhance to showcase \"continuous queries\" (CQ). Dynamic registration of CQs (from remote clients) will be available in the next release.", 
            "title": "Stream analytics using SQL and Spark Streaming"
        }, 
        {
            "location": "/README/#top-k-elements-in-a-stream", 
            "text": "Finding the  k  most popular elements in a data stream is a common analytic query. For instance, top-100 pages on a popular website in the last 10 mins, top-10 sales regions in the last week, etc. As you can tell, if the query is on a arbitrary time interval in the past, this will most likely mandate storing the entire stream. And, this could easily be millions to billions of events is use cases like in IoT. SnappyData provides SQL extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures and enables transparent querying using Spark SQL. More details about SnappyData's implementation of top-k can be found  here.  SnappyData provides DDL extensions to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using regular SQL queries.   --- Create a topk table from a stream table\nCREATE TOPK TABLE filestream_topktable ON HASHTAG_FILESTREAMTABLE OPTIONS\n(key 'hashtag', timeInterval '2000ms', size '10' );\n--- Query a topk table \nSELECT hashtag, COUNT(hashtag) AS TopKCount\nFROM filestream_topktable\nGROUP BY hashtag ORDER BY TopKCount limit 10;  Now, lets try analyzing some tweets using this above syntax in real time using the packaged scripts ..", 
            "title": "Top-K Elements in a Stream"
        }, 
        {
            "location": "/README/#step-5-create-and-query-stream-table-and-top-k-declaratively", 
            "text": "You can use the scripts that simulates the twitter stream by copying pre-loaded tweets in a tmp folder. Or, you could use a script that access the live twitter stream.", 
            "title": "Step 5 - Create and Query Stream Table and Top-K Declaratively"
        }, 
        {
            "location": "/README/#steps-to-work-with-simulated-twitter-stream", 
            "text": "Create a file stream table that listens on a folder and then start the streaming context.   snappy  run './quickstart/scripts/create_and_start_file_streaming.sql';  Run the following utility in another terminal that simulates a twitter stream by copying tweets in the folder on which file stream table is listening.  $ quickstart/scripts/simulateTwitterStream   Now query the current batch of the stream using the following script. This also creates Topk structures. simulateTwitterStream script runs for only for a minute or so. Since our script is querying the current window, it will return no results after the streaming is over.   snappy  run './quickstart/scripts/file_streaming_query.sql';", 
            "title": "Steps to work with simulated Twitter stream"
        }, 
        {
            "location": "/README/#steps-to-work-with-live-twitter-stream", 
            "text": "You would have to generate authorization keys and secrets on  twitter apps  and update SNAPPY_HOME/quickstart/scripts/create_and_start_twitter_streaming.sql with the keys and secrets.  --- Run the create and start script that has keys and secrets to fetch live twitter stream\n--- Note: Currently, we do not encrypt the keys. \n-- This also creates Topk structures\nsnappy  run './quickstart/scripts/create_and_start_twitter_streaming.sql';\n\nsnappy  run './quickstart/scripts/twitter_streaming_query.sql';", 
            "title": "Steps to work with live Twitter stream"
        }, 
        {
            "location": "/README/#getting-started-with-spark-api", 
            "text": "SnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's  SQLContext  to work with Row and Column tables. Any DataFrame can be managed as SnappyData table and any table can be accessed as a DataFrame. This is similar to  HiveContext  and it integrates the SQLContext functionality with the Snappy store. Similarly, SnappyStreamingContext is an entry point for SnappyData extensions to Spark streaming and it extends Spark's  Streaming Context .   Applications typically submit Jobs to SnappyData and do not explicitly create a SnappyContext or SnappyStreamingContext. These jobs are the primary mechanism to interact with SnappyData using Spark API. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.   class SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(snc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}  The implementation of  runJob  function of SnappySQLJob uses SnappyContext to interact with SnappyData store to process and store tables. The implementation of runJob of SnappyStreamingJob uses SnappyStreamingContext to create streams and manage the streaming context. The jobs are submitted to lead node of SnappyData over REST API using a  spark-submit  like utility. See more details about jobs  here", 
            "title": "Getting Started with Spark API"
        }, 
        {
            "location": "/README/#column-and-row-tables_1", 
            "text": "Column tables  organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  // creating a column table in Snappy job\nsnappyContext.createTable( AIRLINE ,  column , schema, Map( buckets  -   5 ))  Row tables , unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.  create table  DDL allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  Read our preliminary  docs  for the details.  // creating a row table in Snappy job\nval airlineCodeDF = snappyContext.createTable( AIRLINEREF ,  row , schema, Map())", 
            "title": "Column and Row tables"
        }, 
        {
            "location": "/README/#step-2-create-column-table-row-table-and-load-data_1", 
            "text": "To run the scripts with full airline data set, set the following config parameter to point at the data set that you had downloaded in Step 1.   export APP_PROPS=\"airline_file=full_dataset_folder\"    Submit CreateAndLoadAirlineDataJob over REST API to create row and column tables. See more details about jobs and job submission  here. .   $ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp --class  io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{ status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  } }\n\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead hostNameOfLead:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n{  duration :  17.53 secs ,\n   classPath :  io.snappydata.examples.CreateAndLoadAirlineDataJob ,\n   startTime :  2016-01-12T16:59:14.746+05:30 ,\n   context :  snappyContext1452598154529305363 ,\n   result :  See /snappy/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n}\n# Tables are created  The output of the job can be found in CreateAndLoadAirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can see the size of the column tables on Spark UI which by default can be seen at http://hostNameOfLead:4040.", 
            "title": "Step 2 - Create column table, row table and load data"
        }, 
        {
            "location": "/README/#olap-and-oltp-store", 
            "text": "SnappyContext extends SQLContext and adds functionality to work with row and column tables. When queries inside jobs are executed they are parsed initially by the SnappyData server to determine if it is a OLAP class or a OLTP class query.  Currently, all column table queries are considered OLAP. Such queries are planned using Spark's Catalyst engine and scheduled to be executed on the data servers.   val resultDF = airlineDF.join(airlineCodeDF,\n        airlineDF.col( UniqueCarrier ).equalTo(airlineCodeDF( CODE ))).\n        groupBy(airlineDF( UniqueCarrier ), airlineCodeDF( DESCRIPTION )).\n        agg( ArrDelay  -   avg ).orderBy( avg(ArrDelay) )  For low latency OLTP queries in jobs, SnappyData won't schedule these queries instead execute them immediately on SnappyData servers without any scheduling overhead. Quite often, this may mean simply fetching or updating a row by hashing a key (in nanoseconds).   // Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'. Update the row table.\nval filterExpr: String =   CODE ='DL' \nval newColumnValues: Row = Row( Delta America )\nval updateColumns =  DESCRIPTION \nsnappyContext.update(rowTableName, filterExpr, newColumnValues, updateColumns)", 
            "title": "OLAP and OLTP Store"
        }, 
        {
            "location": "/README/#step-3-run-olap-and-oltp-queries_1", 
            "text": "AirlineDataJob.scala runs OLAP and OLTP queries on Snappy tables. Also, it caches the same airline data in Spark cache and runs the same OLAP query on the Spark cache. With airline data set, we have seen both Spark cache and snappy store table to have more and less the same performance.    # Submit AirlineDataJob to SnappyData\n$ bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name airlineApp  --class  io.snappydata.examples.AirlineDataJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n{  status :  STARTED ,\n   result : {\n     jobId :  1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 ,\n     context :  snappyContext1453196176725064822 \n } }\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090  --job-id 1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n{  duration :  6.617 secs ,\n   classPath :  io.snappydata.examples.AirlineDataJob ,\n   startTime :  2016-01-19T15:06:16.771+05:30 ,\n   context :  snappyContext1453196176725064822 ,\n   result :  See /snappy/work/localhost-lead-1/AirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n}  The output of the job can be found in AirlineDataJob.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/. You can explore the Spark SQL query plan on Spark UI which by default can be seen at http://hostNameOfLead:4040.", 
            "title": "Step 3 - Run OLAP and OLTP queries"
        }, 
        {
            "location": "/README/#approximate-query-processing-aqp_1", 
            "text": "OLAP jobs are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory jobs above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like AQP.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs to specify one or more curated  stratified samples  on large tables.", 
            "title": "Approximate query processing (AQP)"
        }, 
        {
            "location": "/README/#note_2", 
            "text": "We recommend downloading the  onTime airline  data for 2009-2015 which is about 50 million records. With the above data set (1 million rows) only about third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.   The following scala code creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your  Group by  and  Where  make us the  Query Column Set  (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection.   val sampleDF = snappyContext.createTable(sampleTable, \n         column_sample , // DataSource provider for sample tables\n        updatedSchema, Map( buckets  -   5 ,\n           qcs  -   UniqueCarrier, Year_, Month_ ,\n           fraction  -   0.03 ,\n           strataReservoirSize  -   50 ,\n           basetable  -   Airline \n        ))  You can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the  With Error  constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set.   // Query Snappy Store's Sample table :Which Airlines arrive On schedule? JOIN with reference table\nsampleResult = sampleDF.join(airlineCodeDF,\n        sampleDF.col( UniqueCarrier ).equalTo(airlineCodeDF( CODE ))).\n          groupBy(sampleDF( UniqueCarrier ), airlineCodeDF( DESCRIPTION )).\n          agg( ArrDelay  -   avg ).orderBy( avg(ArrDelay) )\n\n // Query Snappy Store's Airline table with error clause.\nairlineDF.groupBy(airlineDF( Month_ ))\n  .agg( ArrDelay  -   avg )\n  .orderBy( Month_ ).withError(0.05,0.95)", 
            "title": "Note"
        }, 
        {
            "location": "/README/#step-4-create-load-and-query-sample-table_1", 
            "text": "CreateAndLoadAirlineDataJob and AirlineDataJob executed in the previous sections created the sample tables and executed OLAP queries over them.", 
            "title": "Step 4 - Create, Load and Query Sample Table"
        }, 
        {
            "location": "/README/#stream-analytics-using-spark-streaming", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively done using SQL and you can analyze these streams using SQL. Also, SnappyData introduces \"continuous queries\" (CQ) on the stream. One can define a continous query as a SQL query on the stream with window and slide extensions which is returned as SchemaDStream i.e. DStream with schema. SnappyData's extensions provide functionality to insert a SchemaDStream into snappy store.   Dynamic registration of CQs (from remote clients) will be available in the next release.  // create a stream table declaratively \nsnsc.sql( CREATE STREAM TABLE RETWEETTABLE (retweetId long,   +\n     retweetCnt int, retweetTxt string) USING file_stream   +\n     OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',   +\n     rowConverter 'org.apache.spark.sql.streaming.TweetToRetweetRow',  +\n     directory '/tmp/copiedtwitterdata') );\n\n// Register a continous query on the stream table with window and slide parameters\nval retweetStream: SchemaDStream = snsc.registerCQ( SELECT retweetId, retweetCnt FROM RETWEETTABLE   +\n     window (duration '2' seconds, slide '2' seconds) )\n\n// Create a row table to hold the retweets based on their id \nsnsc.snappyContext.sql(s CREATE TABLE $tableName (retweetId bigint PRIMARY KEY,   +\n    s retweetCnt int, retweetTxt string) USING row OPTIONS () )\n\n// Iterate over the stream and insert it into snappy store\nretweetStream.foreachDataFrame(df =  {\n    df.write.mode(SaveMode.Append).saveAsTable(tableName)\n})", 
            "title": "Stream analytics using Spark Streaming"
        }, 
        {
            "location": "/README/#top-k-elements-in-a-stream_1", 
            "text": "Continuously finding the  k  most popular elements in a data stream is a common analytic query. SnappyData provides extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds temporal component (i.e. data can be queried based on time interval) to these structures. More details about SnappyData's implementation of top-k can be found  here.  SnappyData provides API in SnappyContext to create Top-k structure. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using another API.   --- Create a topk table from a stream table\nsnappyContext.createApproxTSTopK( topktable ,  hashtag ,\n    Some(schema), Map(\n       epoch  -  System.currentTimeMillis().toString,\n       timeInterval  -   2000ms ,\n       size  -   10 ,\n       basetable  -   HASHTAGTABLE \n    ))\n--- Query a topk table for the last two seconds\nval topKDF = snappyContext.queryApproxTSTopK( topktable ,\n                System.currentTimeMillis - 2000,\n                System.currentTimeMillis)", 
            "title": "Top-K Elements in a Stream"
        }, 
        {
            "location": "/README/#step-5-create-and-query-stream-table-and-top-k", 
            "text": "Ideally, we would like you to try this example using live twitter stream. For that, you would have to generate authorization keys and secrets on twitter apps. Alternatively, you can use use file stream scripts that simulate twitter stream by copying pre-loaded tweets in a tmp folder.", 
            "title": "Step 5 -  Create and Query Stream Table and Top-K"
        }, 
        {
            "location": "/README/#steps-to-work-with-live-twitter-stream_1", 
            "text": "# Set the keys and secrets to fetch live twitter stream\n# Note: Currently, we do not encrypt the keys. \n$ export APP_PROPS= consumerKey= consumerKey ,consumerSecret= consumerSecret ,accessToken= accessToken ,accessTokenSecret= accessTokenSecret \n\n# submit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a snappy store table \n# This job runs streaming for two minutes. \n$ /bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream", 
            "title": "Steps to work with live Twitter stream"
        }, 
        {
            "location": "/README/#steps-to-work-with-simulated-twitter-stream_1", 
            "text": "Submit the TwitterPopularTagsJob that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a gemxd table. It starts the streaming and waits for two minutes.   # Submit the TwitterPopularTagsJob \n$ ./bin/snappy-job.sh submit --lead hostNameOfLead:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar --stream\n\n# Run the following utility in another terminal to simulate a twitter stream by copying tweets in the folder on which file stream table is listening.\n$ quickstart/scripts/simulateTwitterStream   The output of the job can be found in TwitterPopularTagsJob_timestamp.out in the lead directory which by default is SNAPPY_HOME/work/localhost-lead-*/.", 
            "title": "Steps to work with simulated Twitter stream"
        }, 
        {
            "location": "/README/#working-with-spark-shell-and-spark-submit", 
            "text": "SnappyData, out-of-the-box, collocates Spark executors and the data store for efficient data intensive computations. But, it may desirable to isolate the computational cluster for other reasons - for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate for a  cache data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion.   # Start the spark shell in local mode. Pass Snappy locator\u2019s host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041\nscala \nTry few commands on the spark-shell \n\n# fetch the tables and using sqlContext which is going to be an instance of SnappyContext in this case\nscala  val airlinerefDF = sqlContext.table( airlineref ).show\nscala  val airlineDF = sqlContext.table( airline ).show\n\n# you can now work with the dataframes to fetch the data.", 
            "title": "Working with Spark shell and spark-submit"
        }, 
        {
            "location": "/README/#step-6-submit-a-spark-app-that-interacts-with-snappydata", 
            "text": "# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041 $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n\n# The results can be seen on the command line.", 
            "title": "Step 6 - Submit a Spark App that interacts with SnappyData"
        }, 
        {
            "location": "/README/#final-step-stop-the-snappydata-cluster", 
            "text": "$ sbin/snappy-stop-all.sh \nlocalhost: The SnappyData Leader has stopped.\nlocalhost: The SnappyData Server has stopped.\nlocalhost: The SnappyData Locator has stopped.", 
            "title": "Final Step - Stop the SnappyData Cluster"
        }, 
        {
            "location": "/build-instructions/", 
            "text": "Build Quickstart\n\n\nAs of now, only the \"integrated\" build seems to work. Quickstart to compile\nproject:\n\n\n1. git clone git@github.com:SnappyDataInc/snappydata.git --recursive\n2. cd snappydata\n3. ./gradlew clean assemble\n\n\n\n\nRepository layout\n\n\nThere were few proposals about how to manage the various repositories mentioned in \nthis document\n. Based on few discussions, we shortlisted Proposal 4 in the document.\n\n\nAccording to \"Proposal 4\" gemxd and snappy-spark repositories will be independent of any other repository. There will be a third repository that will hold the code of Snappy - snappy-commons. Snappy-Commons will have two projects:\n\n\n(a) \nsnappy-core\n - Any code that is an extension to Spark code and is not dependent on gemxd, job server etc. should go in here. For e.g. SnappyContext, cluster manager etc.\n\n\n(b) \nsnappy-tools\n - This is the code that serves as the bridge between GemXD and snappy-spark.  For e.g. query routing, job server initialization etc.\n\n\nCode in snappy-tools can depend on snappy-core but it cannot happen other way round.\n\n\nThe snappy-spark repository has to be copied or moved inside snappy-commons for an integrated build.\n\n\n(c) \nsnappy-spark\n - This is the Spark code with Snappy modifcations.\n\n\nSimilarly the GemfireXD repository can be copied or moved inside snappy-commons by name \nsnappy-store\n for an integrated build with GemFireXD. The branch of GemFireXD to use is also \nsnappy-store\n that has been branched from rebrand_Dec13 recently for this purpose.\n\n\n(d) \nsnappy-store\n - This is the GemFireXD with Snappy additions.\n\n\n(e) \nsnappy-aqp\n - This is the Snappy Data proprietary code (AQP error estimation)\n\n\nNote that git operations have still to be done separately on snappy-commons, snappy-spark and snappy-store(GemFireXD) repositories.\n\n\nBuilding using gradle\n\n\nGradle builds have been arranged in a way so that all of snappy projects including snappy's spark variant can be built from the top-level. In addition snappy-spark and GemFireXD (inside snappy-store) can also be built separately. If the snappy-spark directory is not present inside snappy-commons, then it will try to use locally published snappy-spark artifacts instead. Likewise if there is no snappy-store directory, then it will use the local artifacts inside local-repo in snappy-commons:\n  * The full build and Intellij import has been tested with only JDK7. If you are using JDK8, then you are on your own (though it will likely work). On Ubuntu/Mint systems, best way to get Oracle JDK7 as default:\n\n\n- add webupd8 java repository: ````sudo add-apt-repository ppa:webupd8team/java````\n- install and set jdk7 as default: ````sudo aptitude install oracle-java7-set-default````\n- you can also install oracle-java7-unlimited-jce-policy package for enhanced JCE encryption\n- this will set java to point to JDK7 version and also set JAVA_HOME, so start a new shell for the changes to take effect; also run ````source /etc/profile.d/jdk.sh```` to update JAVA_HOME (or else you will need to logoff and login again for the JAVA_HOME setting to get applied)\n\n\n\n\n\nEnsure that snappy-spark repository has been moved/cloned inside snappy-commons by \"snappy-spark\" name. Similarly move the GemFireXD (snappy-store branch) repository inside snappy-commons by \"snappy-store\" name. The integrated build depends on its name and presence inside else it will use the local artifacts as mentioned before. \nDO NOT JUST SYMLINK THE DIRECTORIES\n -- that is known to cause trouble with IDE though command-line build may go through.\n\n\nUpdate both repos (snappy-commons and snappy-spark) to latest version and the GemFireXD repository in snappy-store to latest snappy-store branch. Then test the build with: \n./gradlew clean\n \n \n./gradlew assemble\n\n\nIf you see an error like \"Could not find hadoop-common-tests.jar\", then clear maven cache artifacts: \nrm -rf ~/.m2/repository/org/apache/hadoop\n, so that gradle can download all required depedencies, then run assemble target again.\n\n\nRun a snappy-core test application: \n./gradlew :snappy-core_2.10:run -PmainClass=io.snappydata.app.SparkSQLTest\n\n    AND/OR a GemFireXD junit test: \n./gradlew :snappy-store:gemfirexd:tools:test -Dtest.single=\\*\\*/BugsTest\n\n\n\n\nSetting up Intellij with gradle\n\n\nIf the build works fine, then import into Intellij:\n  * Update Intellij to the latest version, including the latest Scala plugin. Check using \nHelp-\nCheck for Update\n. The scala plugin version in \nFile-\nSettings-\nPlugins-\nScala\n should be at least 1.5.4 else update the plugin from that page.\n  * Double check that Scala plugin is enabled in \nFile-\nSettings-\nPlugins\n, as also the Gradle plugin. Note that update in previous step could have disabled either or both, so don't assume it would be enabled.\n  * Select import project, then point to the snappy-commons directory. Use external Gradle import. Add \n-XX:MaxPermSize=350m\n to VM options in global Gradle settings. Select defaults, next, next ... finish. Ignore \"Gradle location is unknown warning\". Ensure that a JDK7 installation has been selected.\n  * Disable the \"Unindexed remote maven repositories found\" warning message.\n  * Once import finishes, go to \nFile-\nSettings-\nEditor-\nCode Style-\nScala\n. Set the scheme as \nProject\n. Check that the same has been set in Java's Code Style too. Then OK to close it. Next copy \ncodeStyleSettings.xml\n in snappy-commons to .idea directory created by Intellij and then \nFile-\nSynchronize\n just to be sure. Check that settings are now applied in \nFile-\nSettings-\nEditor-\nCode Style-\nJava\n which should show TabSize, Indent as 2 and continuation indent as 4 (same for Scala).\n  * If the Gradle tab is not visible immediately, then select it from window list popup at the left-bottom corner of IDE. If you click on that window list icon, then the tabs will appear permanently.\n  * Generate avro and GemFireXD required sources by expanding: \nsnappy-commons_2.10-\nTasks-\nother\n. Right click on \ngenerateSources\n and run it. The Run item may not be available if indexing is still in progress, so wait for it to finish. The first run may take a while as it downloads jars etc. This step has to be done the first time, or if \n./gradlew clean\n has been run, or you have made changes to \njavacc/avro/messages.xml\n source files. \nIF YOU GET UNEXPECTED DATABASE NOT FOUND OR NPE ERRORS IN GemFireXD LAYER, THEN FIRST THING TO TRY IS TO RUN THE generateSources TARGET AGAIN.\n\n  * Increase the compiler heap sizes or else the build can take quite long especially with integrated GemFireXD. In \nFile-\nSettings-\nBuild, Execution, Deployment-\nCompiler increase\n, \nBuild process heap size\n to say 1536 or 2048. Similarly increase JVM maximum heap size in \nLanguages \n Frameworks-\nScala Compiler Server\n to 1536 or 2048.\n  * Test the full build.\n  * \nOpen Run-\nEdit Configurations\n. Expand Defaults, and select Application. Add \n-XX:MaxPermSize=350m\n in VM options. Similarly add it to VM parameters for ScalaTest and JUnit.\n  * For JUnit configuration also append \n/build-artifacts\n to the working directory i.e. the directory should be \n$MODULE_DIR$/build-artifacts\n. Likewise change working directory for ScalaTest to be inside build-artifacts otherwise all intermediate log and other files (especially created by GemFireXD) will pollute the source tree and may need to cleaned manually.\n  * Try \nRun-\nRun\n... on a test like SparkSQLTest.\n\n\nRunning a junit/scalatest\n\n\nRunning an application like SparkSQLTest should be straightforward -- just ensure that MaxPermSize has been increased as mentioned above especially for Spark/Snappy tests. For running junit/scalatest:\n\n\n\n\nWhen selecting a run configuration for junit/scalatest, avoid selecting the gradle one (green round icon) otherwise that will launch an external gradle process that will start building the project all over again. Use the normal junit (red+green arrows icon) or scalatest (junit like with red overlay).\n\n\nFor JUnit tests, ensure that working directory is \n$MODULE_DIR$/build-artifacts\n as mentioned before. Otherwise many GemFireXD tests will fail to find the resource files required in many tests. They will also pollute the checkouts with large number of log files etc, so this will allow those to go into build-artifacts that can also be cleaned up easily.\n\n\n\n\nManual sources and docs imports\n\n\nIf sources and docs were selected during initial import, then it can take a long time to get sources+docs for all dependencies. Instead one way could be to get the sources+docs for only scala-lang jars. The project setup after import already links sources and javadocs to appropriate locations in .m2 local cache, but since sources+docs were not selected during import so Maven may not have downloaded them yet. Check if you already have sources in m2 cache by opening a scala-lang class like Seq (hit \nShift-\nCtrl-\nT\n when using eclipse bindings and type scala.collection.Seq) and check if sources+docs are correctly shown. If not, then to easily download for selected jars do this:\n  * Open the \nFile-\nProject Structure-\nLibraries\n\n  * Click on the \n+\n sign at the top to add new library, and choose Maven.\n  * In the box, provide \nscala-library-2.10.4\n and click on the search tool.\n  * Select the \norg.scala-lang:scala-library:2.10.4\n in the drop down. Then check \"Sources\", \"JavaDocs\" options and go ahead.\n  * Do the same for others like \nscala-reflect-2.10.4\n and \nscala-compiler-2.10.4\n as required.\n  * Once this is done, don't select OK on the main Project Structure box. Instead hit \"Cancel\" and it should be all good since we only wanted to get Maven to download the sources and docs for these jars.\n\n\nGit configuration to use keyring/keychain\n\n\nSnappy is currently hosting private repositories and will continue to do\nso for foreseable future. One way to avoid passing credentials everytime could\nhave been to upload the public SSH key and use git:// URL. However, that doesn't\nwork at least in Pune network due to firewall issue (and proxy server not\nsupporting proxying ssh). However, it is possible to configure git to enable\nusing gnome-keyring on Linux platforms, and KeyChain on OSX to avoid it.\n(sumedh: latter not verified by me yet, so someone who uses OSX should do it)\n\n\nOn Linux Ubuntu/Mint:\n\n\nInstall gnome-keyring dev files: sudo aptitude install libgnome-keyring-dev\n\n\nBuild git-credential-gnome-keyring:\n\n\ncd build/git-gnome-keyring\nmake\n\n\n\nCopy to PATH (optional):\n\n\nsudo cp git-credential-gnome-keyring /usr/local/bin\nmake clean\n\n\n\nNote that if you skip this step then need to give full path in the next\nstep i.e. /path-to-snappy-commons/build/git-gnome-keyring/git-credential-gnome-keyring\n\n\nConfigure git: git config --global credential.helper gnome-keyring\n\n\nSimilarly on OSX locate git-credential-osxkeychain, build it if not present\n(it is named \"osxkeychain\" instead of gnome-keyring), then set in git config.\n\n\nNow your git password will be stored in keyring/keychain which is normally\nunlocked automatically on login (or you will be asked to unlock on first use).\n\n\nOn Linux, you can install \"seahorse\", if not already, to see/modify all\nthe passwords in keyring (GUI menu \"Passwords and Keys\" under Preferences\nor Accessories or System Tools)", 
            "title": "Building from source, project layout"
        }, 
        {
            "location": "/build-instructions/#build-quickstart", 
            "text": "As of now, only the \"integrated\" build seems to work. Quickstart to compile\nproject:  1. git clone git@github.com:SnappyDataInc/snappydata.git --recursive\n2. cd snappydata\n3. ./gradlew clean assemble", 
            "title": "Build Quickstart"
        }, 
        {
            "location": "/build-instructions/#repository-layout", 
            "text": "There were few proposals about how to manage the various repositories mentioned in  this document . Based on few discussions, we shortlisted Proposal 4 in the document.  According to \"Proposal 4\" gemxd and snappy-spark repositories will be independent of any other repository. There will be a third repository that will hold the code of Snappy - snappy-commons. Snappy-Commons will have two projects:  (a)  snappy-core  - Any code that is an extension to Spark code and is not dependent on gemxd, job server etc. should go in here. For e.g. SnappyContext, cluster manager etc.  (b)  snappy-tools  - This is the code that serves as the bridge between GemXD and snappy-spark.  For e.g. query routing, job server initialization etc.  Code in snappy-tools can depend on snappy-core but it cannot happen other way round.  The snappy-spark repository has to be copied or moved inside snappy-commons for an integrated build.  (c)  snappy-spark  - This is the Spark code with Snappy modifcations.  Similarly the GemfireXD repository can be copied or moved inside snappy-commons by name  snappy-store  for an integrated build with GemFireXD. The branch of GemFireXD to use is also  snappy-store  that has been branched from rebrand_Dec13 recently for this purpose.  (d)  snappy-store  - This is the GemFireXD with Snappy additions.  (e)  snappy-aqp  - This is the Snappy Data proprietary code (AQP error estimation)  Note that git operations have still to be done separately on snappy-commons, snappy-spark and snappy-store(GemFireXD) repositories.", 
            "title": "Repository layout"
        }, 
        {
            "location": "/build-instructions/#building-using-gradle", 
            "text": "Gradle builds have been arranged in a way so that all of snappy projects including snappy's spark variant can be built from the top-level. In addition snappy-spark and GemFireXD (inside snappy-store) can also be built separately. If the snappy-spark directory is not present inside snappy-commons, then it will try to use locally published snappy-spark artifacts instead. Likewise if there is no snappy-store directory, then it will use the local artifacts inside local-repo in snappy-commons:\n  * The full build and Intellij import has been tested with only JDK7. If you are using JDK8, then you are on your own (though it will likely work). On Ubuntu/Mint systems, best way to get Oracle JDK7 as default:  - add webupd8 java repository: ````sudo add-apt-repository ppa:webupd8team/java````\n- install and set jdk7 as default: ````sudo aptitude install oracle-java7-set-default````\n- you can also install oracle-java7-unlimited-jce-policy package for enhanced JCE encryption\n- this will set java to point to JDK7 version and also set JAVA_HOME, so start a new shell for the changes to take effect; also run ````source /etc/profile.d/jdk.sh```` to update JAVA_HOME (or else you will need to logoff and login again for the JAVA_HOME setting to get applied)   Ensure that snappy-spark repository has been moved/cloned inside snappy-commons by \"snappy-spark\" name. Similarly move the GemFireXD (snappy-store branch) repository inside snappy-commons by \"snappy-store\" name. The integrated build depends on its name and presence inside else it will use the local artifacts as mentioned before.  DO NOT JUST SYMLINK THE DIRECTORIES  -- that is known to cause trouble with IDE though command-line build may go through.  Update both repos (snappy-commons and snappy-spark) to latest version and the GemFireXD repository in snappy-store to latest snappy-store branch. Then test the build with:  ./gradlew clean     ./gradlew assemble  If you see an error like \"Could not find hadoop-common-tests.jar\", then clear maven cache artifacts:  rm -rf ~/.m2/repository/org/apache/hadoop , so that gradle can download all required depedencies, then run assemble target again.  Run a snappy-core test application:  ./gradlew :snappy-core_2.10:run -PmainClass=io.snappydata.app.SparkSQLTest \n    AND/OR a GemFireXD junit test:  ./gradlew :snappy-store:gemfirexd:tools:test -Dtest.single=\\*\\*/BugsTest", 
            "title": "Building using gradle"
        }, 
        {
            "location": "/build-instructions/#setting-up-intellij-with-gradle", 
            "text": "If the build works fine, then import into Intellij:\n  * Update Intellij to the latest version, including the latest Scala plugin. Check using  Help- Check for Update . The scala plugin version in  File- Settings- Plugins- Scala  should be at least 1.5.4 else update the plugin from that page.\n  * Double check that Scala plugin is enabled in  File- Settings- Plugins , as also the Gradle plugin. Note that update in previous step could have disabled either or both, so don't assume it would be enabled.\n  * Select import project, then point to the snappy-commons directory. Use external Gradle import. Add  -XX:MaxPermSize=350m  to VM options in global Gradle settings. Select defaults, next, next ... finish. Ignore \"Gradle location is unknown warning\". Ensure that a JDK7 installation has been selected.\n  * Disable the \"Unindexed remote maven repositories found\" warning message.\n  * Once import finishes, go to  File- Settings- Editor- Code Style- Scala . Set the scheme as  Project . Check that the same has been set in Java's Code Style too. Then OK to close it. Next copy  codeStyleSettings.xml  in snappy-commons to .idea directory created by Intellij and then  File- Synchronize  just to be sure. Check that settings are now applied in  File- Settings- Editor- Code Style- Java  which should show TabSize, Indent as 2 and continuation indent as 4 (same for Scala).\n  * If the Gradle tab is not visible immediately, then select it from window list popup at the left-bottom corner of IDE. If you click on that window list icon, then the tabs will appear permanently.\n  * Generate avro and GemFireXD required sources by expanding:  snappy-commons_2.10- Tasks- other . Right click on  generateSources  and run it. The Run item may not be available if indexing is still in progress, so wait for it to finish. The first run may take a while as it downloads jars etc. This step has to be done the first time, or if  ./gradlew clean  has been run, or you have made changes to  javacc/avro/messages.xml  source files.  IF YOU GET UNEXPECTED DATABASE NOT FOUND OR NPE ERRORS IN GemFireXD LAYER, THEN FIRST THING TO TRY IS TO RUN THE generateSources TARGET AGAIN. \n  * Increase the compiler heap sizes or else the build can take quite long especially with integrated GemFireXD. In  File- Settings- Build, Execution, Deployment- Compiler increase ,  Build process heap size  to say 1536 or 2048. Similarly increase JVM maximum heap size in  Languages   Frameworks- Scala Compiler Server  to 1536 or 2048.\n  * Test the full build.\n  *  Open Run- Edit Configurations . Expand Defaults, and select Application. Add  -XX:MaxPermSize=350m  in VM options. Similarly add it to VM parameters for ScalaTest and JUnit.\n  * For JUnit configuration also append  /build-artifacts  to the working directory i.e. the directory should be  $MODULE_DIR$/build-artifacts . Likewise change working directory for ScalaTest to be inside build-artifacts otherwise all intermediate log and other files (especially created by GemFireXD) will pollute the source tree and may need to cleaned manually.\n  * Try  Run- Run ... on a test like SparkSQLTest.", 
            "title": "Setting up Intellij with gradle"
        }, 
        {
            "location": "/build-instructions/#running-a-junitscalatest", 
            "text": "Running an application like SparkSQLTest should be straightforward -- just ensure that MaxPermSize has been increased as mentioned above especially for Spark/Snappy tests. For running junit/scalatest:   When selecting a run configuration for junit/scalatest, avoid selecting the gradle one (green round icon) otherwise that will launch an external gradle process that will start building the project all over again. Use the normal junit (red+green arrows icon) or scalatest (junit like with red overlay).  For JUnit tests, ensure that working directory is  $MODULE_DIR$/build-artifacts  as mentioned before. Otherwise many GemFireXD tests will fail to find the resource files required in many tests. They will also pollute the checkouts with large number of log files etc, so this will allow those to go into build-artifacts that can also be cleaned up easily.", 
            "title": "Running a junit/scalatest"
        }, 
        {
            "location": "/build-instructions/#manual-sources-and-docs-imports", 
            "text": "If sources and docs were selected during initial import, then it can take a long time to get sources+docs for all dependencies. Instead one way could be to get the sources+docs for only scala-lang jars. The project setup after import already links sources and javadocs to appropriate locations in .m2 local cache, but since sources+docs were not selected during import so Maven may not have downloaded them yet. Check if you already have sources in m2 cache by opening a scala-lang class like Seq (hit  Shift- Ctrl- T  when using eclipse bindings and type scala.collection.Seq) and check if sources+docs are correctly shown. If not, then to easily download for selected jars do this:\n  * Open the  File- Project Structure- Libraries \n  * Click on the  +  sign at the top to add new library, and choose Maven.\n  * In the box, provide  scala-library-2.10.4  and click on the search tool.\n  * Select the  org.scala-lang:scala-library:2.10.4  in the drop down. Then check \"Sources\", \"JavaDocs\" options and go ahead.\n  * Do the same for others like  scala-reflect-2.10.4  and  scala-compiler-2.10.4  as required.\n  * Once this is done, don't select OK on the main Project Structure box. Instead hit \"Cancel\" and it should be all good since we only wanted to get Maven to download the sources and docs for these jars.", 
            "title": "Manual sources and docs imports"
        }, 
        {
            "location": "/build-instructions/#git-configuration-to-use-keyringkeychain", 
            "text": "Snappy is currently hosting private repositories and will continue to do\nso for foreseable future. One way to avoid passing credentials everytime could\nhave been to upload the public SSH key and use git:// URL. However, that doesn't\nwork at least in Pune network due to firewall issue (and proxy server not\nsupporting proxying ssh). However, it is possible to configure git to enable\nusing gnome-keyring on Linux platforms, and KeyChain on OSX to avoid it.\n(sumedh: latter not verified by me yet, so someone who uses OSX should do it)  On Linux Ubuntu/Mint:  Install gnome-keyring dev files: sudo aptitude install libgnome-keyring-dev  Build git-credential-gnome-keyring:  cd build/git-gnome-keyring\nmake  Copy to PATH (optional):  sudo cp git-credential-gnome-keyring /usr/local/bin\nmake clean  Note that if you skip this step then need to give full path in the next\nstep i.e. /path-to-snappy-commons/build/git-gnome-keyring/git-credential-gnome-keyring  Configure git: git config --global credential.helper gnome-keyring  Similarly on OSX locate git-credential-osxkeychain, build it if not present\n(it is named \"osxkeychain\" instead of gnome-keyring), then set in git config.  Now your git password will be stored in keyring/keychain which is normally\nunlocked automatically on login (or you will be asked to unlock on first use).  On Linux, you can install \"seahorse\", if not already, to see/modify all\nthe passwords in keyring (GUI menu \"Passwords and Keys\" under Preferences\nor Accessories or System Tools)", 
            "title": "Git configuration to use keyring/keychain"
        }, 
        {
            "location": "/snappyIntroduction/", 
            "text": "Introduction\n\n\nSnappyData is a \ndistributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster\n. We realize this platform through a seamless deep integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics) \n\n\n\n\nConceptually, you could \nthink of SnappyData as a in-memory database that embeds Spark as its computational engine\n - to process streams, work with myriad data sources like HDFS, and process data through a rich set of higher level abstractions. While the SnappyData engine is primarily designed for SQL processing, applications can work with Objects through Spark RDDs and the newly introduced Spark DataSets. \n\n\nAny Spark DataFrame can be easily managed as a SnappyData Table or conversely any table can be accessed as a DataFrame. \n\n\nBy default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted Spark executors are automatically launched within the Snappy process space (JVMs). There is no need to connect and manage external data store clusters. The Snappy store can synchronously replicate for HA with strong consistency and store/recover from disk for additional reliability.\n\n\n\n\nExtensions to the Spark Runtime\n\n\nSnappyData makes the following contributions to deliver a unified and optimized runtime.\n\n1. \nIntegrating an operational in-memory data store with Spark\u2019s computational model:\n We introduce a number of extensions to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy. We extend this design by allowing low latency and fine grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, we extend the runtime with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby state is shared across many clients and applications. \n\n\n\n\nUnified API for OLAP, OLTP, and Streaming:\n Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL. \nWhile Spark deserves much of the credit for being the first of its kind to offer a unified API, we further extend its API to  \n\n\nallow for OLTP operations, e.g., transactions and mutations (inserts/updates/deletions) on tables  \n\n\nbe conformant with SQL standards, e.g., allowing tables alterations, constraints, indexes, and   \n\n\n\n\nsupport declarative stream processing in SQL\n\n\n\n\n\n\nOptimizing Spark application execution times:\n Our goal is to eliminate the need for yet another external store (e.g., a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting colocated schema designs (tables and streams) where related data is colocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios. \n\n\n\n\n\n\nApproximate Query Processing support built into Spark:\n To deliver analytics at truly interactive speeds, we have equipped SnappyData with state-of-the-art AQP techniques, as well as a number of novel features. \nSnappyData is the first AQP engine to  \n\n\n\n\nProvide automatic bias correction for arbitrarily complex SQL queries  \n\n\nProvide an intuitive means for end users to express their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts  \n\n\nProvide error estimates for arbitrarily complex queries on streams (Unlike traditional load shedding techniques that are restricted to simple queries)", 
            "title": "Overview"
        }, 
        {
            "location": "/snappyIntroduction/#introduction", 
            "text": "SnappyData is a  distributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster . We realize this platform through a seamless deep integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics)    Conceptually, you could  think of SnappyData as a in-memory database that embeds Spark as its computational engine  - to process streams, work with myriad data sources like HDFS, and process data through a rich set of higher level abstractions. While the SnappyData engine is primarily designed for SQL processing, applications can work with Objects through Spark RDDs and the newly introduced Spark DataSets.   Any Spark DataFrame can be easily managed as a SnappyData Table or conversely any table can be accessed as a DataFrame.   By default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted Spark executors are automatically launched within the Snappy process space (JVMs). There is no need to connect and manage external data store clusters. The Snappy store can synchronously replicate for HA with strong consistency and store/recover from disk for additional reliability.", 
            "title": "Introduction"
        }, 
        {
            "location": "/snappyIntroduction/#extensions-to-the-spark-runtime", 
            "text": "SnappyData makes the following contributions to deliver a unified and optimized runtime. \n1.  Integrating an operational in-memory data store with Spark\u2019s computational model:  We introduce a number of extensions to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy. We extend this design by allowing low latency and fine grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, we extend the runtime with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby state is shared across many clients and applications.    Unified API for OLAP, OLTP, and Streaming:  Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL. \nWhile Spark deserves much of the credit for being the first of its kind to offer a unified API, we further extend its API to    allow for OLTP operations, e.g., transactions and mutations (inserts/updates/deletions) on tables    be conformant with SQL standards, e.g., allowing tables alterations, constraints, indexes, and      support declarative stream processing in SQL    Optimizing Spark application execution times:  Our goal is to eliminate the need for yet another external store (e.g., a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting colocated schema designs (tables and streams) where related data is colocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios.     Approximate Query Processing support built into Spark:  To deliver analytics at truly interactive speeds, we have equipped SnappyData with state-of-the-art AQP techniques, as well as a number of novel features. \nSnappyData is the first AQP engine to     Provide automatic bias correction for arbitrarily complex SQL queries    Provide an intuitive means for end users to express their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts    Provide error estimates for arbitrarily complex queries on streams (Unlike traditional load shedding techniques that are restricted to simple queries)", 
            "title": "Extensions to the Spark Runtime"
        }, 
        {
            "location": "/features/", 
            "text": "Key Features\n\n\n\n\n100% compatible with Spark\n - Use SnappyData as a database but also use any of the Spark APIs - ML, Graph, etc\n\n\nin-memory row and column stores\n: run the store collocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster)\n\n\nSQL standard compliance\n: Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.\n\n\nSQL based extensions for streaming processing\n: Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.\n\n\nInteractive analytics using Approximate query processing(AQP)\n: We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions. \n\n\nMutate, transact on data in Spark\n: You can use SQL to insert, update, delete data in tables as one would expect. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store. \n\n\nOptimizations - Indexing\n: You can index your row store and the GemFire SQL optimizer will automatically use in-memory indexes when available. \n\n\nOptimizations - colocation\n: SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be collocated using declarative custom partitioning strategies(e.g. common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent. \n\n\nHigh availability not just Fault tolerance\n: Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications continuous HA.\n\n\nDurability and recovery:\n Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled. \n\n\n\n\nSpark challenges for mixed workloads (OLTP, OLAP)\n\n\nSpark is designed as a computational engine for processing batch jobs. Each Spark application (e.g., a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an ex- ternal storage tier, such as HDFS. We, on the other hand, target a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database on the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and de-coupled from individual applications.\n\n\nA second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces\n\n\n\n\na single point of contention for all requests, and \n\n\na barrier for achieving high availability (HA). Executors are shutdown if the driver fails, requiring a full refresh of any cached state.\n\n\n\n\nSpark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, we need to manage more complex data structures (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mu- tability characteristics and conformance to SQL.\n\n\nFinally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Key features"
        }, 
        {
            "location": "/features/#key-features", 
            "text": "100% compatible with Spark  - Use SnappyData as a database but also use any of the Spark APIs - ML, Graph, etc  in-memory row and column stores : run the store collocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster)  SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.  SQL based extensions for streaming processing : Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.  Interactive analytics using Approximate query processing(AQP) : We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions.   Mutate, transact on data in Spark : You can use SQL to insert, update, delete data in tables as one would expect. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store.   Optimizations - Indexing : You can index your row store and the GemFire SQL optimizer will automatically use in-memory indexes when available.   Optimizations - colocation : SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be collocated using declarative custom partitioning strategies(e.g. common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent.   High availability not just Fault tolerance : Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications continuous HA.  Durability and recovery:  Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.", 
            "title": "Key Features"
        }, 
        {
            "location": "/features/#spark-challenges-for-mixed-workloads-oltp-olap", 
            "text": "Spark is designed as a computational engine for processing batch jobs. Each Spark application (e.g., a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an ex- ternal storage tier, such as HDFS. We, on the other hand, target a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database on the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and de-coupled from individual applications.  A second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces   a single point of contention for all requests, and   a barrier for achieving high availability (HA). Executors are shutdown if the driver fails, requiring a full refresh of any cached state.   Spark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, we need to manage more complex data structures (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mu- tability characteristics and conformance to SQL.  Finally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Spark challenges for mixed workloads (OLTP, OLAP)"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture overview\n\n\nThis section presents a high level overview of SnappyData\u2019s core components, as well as our data pipeline as streams are ingested into our in-memory store and subsequently interacted with and analyzed.\n\n\nCore components\n\n\nFigure 1 depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, we have omitted standard components, such as security and monitoring.\n\n\n\n\n\nThe storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. See \nRow/Column table\n section for details on the syntax and available features. \n\n\nWe support two primary programming models \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and is supports the Spark SQL dialect with several extensions to make the language compatible to the SQL standard. One could perceive SnappyData as a SQL database that uses Spark API as its language for stored procedures.Our \nstream processing\n is primarily through Spark Streaming, but it is integrated and runs in-situ with our store .\n\n\nThe OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos(not yet supported). We route all OLTP operations immediately to appropriate data partitions without incurring any scheduling overhead.\n\n\nTo support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, we use a P2P (peer-to-peer) cluster membership service that en- sures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.\n\n\nIn addition to the \u201cexact\u201d dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for approximate query processing (AQP) and will exploit appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.\n\n\nTo understand the data flow architecture, we first walk through a real time use case that involves stream processing, ingesting into a in-memory store and interactive analytics. \n\n\nUse case\n\n\nLocation based services from telco network providers\n\n\nThe global proliferation of mobile devices has created a growing market for location based services. In addition to locality-aware search and navigation, network providers are increasingly relying on location-based advertising, emergency call positioning, road traffic optimization, efficient call routing, triggering preemptive maintenance of cell towers, roaming analytics, and tracking vulnerable people in real time. Telemetry events are delivered as Call Detail Records (CDR), containing hundreds of attributes about each call. Ingested CDRs are cleansed and transformed for consumption by various applications. Not being able to correlate customer support calls with location specific network congestion information is a problem that frustrates customers and network technicians alike. The ability to do this in real time may involve expensive joins to history, tower traffic data and subscriber profiles. Incoming streams generate hundreds of aggregate metrics and KPIs (key performance indicators) grouped by subscriber, cell phone type, cell tower, and location. This requires continuous updates to counters accessed through primary keys (such as the subscriberID). While the generated data is massive, it still needs to be interactively queried by a data analyst for network performance analysis. Location-based services represent another common problem among our customers that involves high concurrency, continuous data updates, complex queries, time series data, and a source that cannot be throttled.\n\n\nData ingestion pipeline\n\n\nThe data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive archicture for real-time applications. The steps to support these tasks are depicted in Figure 2, and explained below.\n\n\n\n\n\n\n\n\n\nOnce the SnappyData cluster is started and before any live streams can be processed, we can ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (e.g., HDFS) can be loaded in parallel into a colum- nar format table with or without compression. Reference data that is often mutating can be managed as row tables.\n\n\n\n\n\n\nWe rely on Spark Streaming\u2019s parallel receivers to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emit- ted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.\n\n\n\n\n\n\nNext, we use SQL to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), we can seamlessly combine these data structures in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high velocity streams, SnappyData can still provide answers in real time by resorting to approximation.\n\n\n\n\n\n\nThe stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (e.g., maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.\n\n\n\n\n\n\nTo prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.\n\n\n\n\n\n\nOnce ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use approximate query processing to ensure interactive analytics on massive historical data in accordance to users\u2019 requested accuracy.\n\n\n\n\n\n\nHybrid Cluster Manager\n\n\nAs shown in Figure above, spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (e.g., YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a sin- gleton (SparkContext) object which it uses to communicate with its set of executors.\n\n\nWhile Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet additional requirements (1\u20134) as an operational database.\n\n\n\n\n\n\nHigh concurrency\n : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.\n\n\n\n\n\n\nState sharing\n : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real time data sharing.\n\n\n\n\n\n\nHigh availability (HA) \u2014 As a highly concurrent distributed system that offers low latency access to data, we must protect applications from node failures (caused by soft- ware bugs and hardware/network failures). High availability of data and transparent handling of failed operations there- fore become an important requirement for SnappyData.\n\n\n\n\n\n\nConsistency \u2014 As a highly available system that of- fers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture in section 5.1, we explain how SnappyData meets each of these require- ments in the subsequent sections.\n\n\n\n\n\n\nSnappyData Cluster Architecture\n\n\nA SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members (see figure 4).\n1. Locator. Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n2. Lead Node. The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n3. Data Servers. A data server member hosts data, em- beds a Spark executor, and also contains a SQL engine ca- pable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.\n\n\nHigh Concurrency in SnappyData\n\n\nThousands of concurrent ODBC and JDBC clients can si- multaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incom- ing requests from these clients into (i) low latency requests and (ii) high latency ones. For low latency operations, we completely bypass Spark\u2019s scheduling mechanism and di- rectly operate on the data. We route high latency opera- tions (e.g., compute intensive queries) through Spark\u2019s fair scheduling mechanism. This makes SnappyData a respon- sive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.\n\n\nState Sharing in SnappyData\n\n\nA SnappyData cluster is designed to be a long running clustered database. State is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. (See section 4 for more details on table types and redundancy.) Nodes in the cluster stay up for a long time and their life-cycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#architecture-overview", 
            "text": "This section presents a high level overview of SnappyData\u2019s core components, as well as our data pipeline as streams are ingested into our in-memory store and subsequently interacted with and analyzed.", 
            "title": "Architecture overview"
        }, 
        {
            "location": "/architecture/#core-components", 
            "text": "Figure 1 depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, we have omitted standard components, such as security and monitoring.   The storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. See  Row/Column table  section for details on the syntax and available features.   We support two primary programming models \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and is supports the Spark SQL dialect with several extensions to make the language compatible to the SQL standard. One could perceive SnappyData as a SQL database that uses Spark API as its language for stored procedures.Our  stream processing  is primarily through Spark Streaming, but it is integrated and runs in-situ with our store .  The OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos(not yet supported). We route all OLTP operations immediately to appropriate data partitions without incurring any scheduling overhead.  To support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, we use a P2P (peer-to-peer) cluster membership service that en- sures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.  In addition to the \u201cexact\u201d dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for approximate query processing (AQP) and will exploit appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.  To understand the data flow architecture, we first walk through a real time use case that involves stream processing, ingesting into a in-memory store and interactive analytics.", 
            "title": "Core components"
        }, 
        {
            "location": "/architecture/#use-case", 
            "text": "", 
            "title": "Use case"
        }, 
        {
            "location": "/architecture/#location-based-services-from-telco-network-providers", 
            "text": "The global proliferation of mobile devices has created a growing market for location based services. In addition to locality-aware search and navigation, network providers are increasingly relying on location-based advertising, emergency call positioning, road traffic optimization, efficient call routing, triggering preemptive maintenance of cell towers, roaming analytics, and tracking vulnerable people in real time. Telemetry events are delivered as Call Detail Records (CDR), containing hundreds of attributes about each call. Ingested CDRs are cleansed and transformed for consumption by various applications. Not being able to correlate customer support calls with location specific network congestion information is a problem that frustrates customers and network technicians alike. The ability to do this in real time may involve expensive joins to history, tower traffic data and subscriber profiles. Incoming streams generate hundreds of aggregate metrics and KPIs (key performance indicators) grouped by subscriber, cell phone type, cell tower, and location. This requires continuous updates to counters accessed through primary keys (such as the subscriberID). While the generated data is massive, it still needs to be interactively queried by a data analyst for network performance analysis. Location-based services represent another common problem among our customers that involves high concurrency, continuous data updates, complex queries, time series data, and a source that cannot be throttled.", 
            "title": "Location based services from telco network providers"
        }, 
        {
            "location": "/architecture/#data-ingestion-pipeline", 
            "text": "The data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive archicture for real-time applications. The steps to support these tasks are depicted in Figure 2, and explained below.     Once the SnappyData cluster is started and before any live streams can be processed, we can ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (e.g., HDFS) can be loaded in parallel into a colum- nar format table with or without compression. Reference data that is often mutating can be managed as row tables.    We rely on Spark Streaming\u2019s parallel receivers to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emit- ted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.    Next, we use SQL to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), we can seamlessly combine these data structures in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high velocity streams, SnappyData can still provide answers in real time by resorting to approximation.    The stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (e.g., maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.    To prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.    Once ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use approximate query processing to ensure interactive analytics on massive historical data in accordance to users\u2019 requested accuracy.", 
            "title": "Data ingestion pipeline"
        }, 
        {
            "location": "/architecture/#hybrid-cluster-manager", 
            "text": "As shown in Figure above, spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (e.g., YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a sin- gleton (SparkContext) object which it uses to communicate with its set of executors.  While Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet additional requirements (1\u20134) as an operational database.    High concurrency  : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.    State sharing  : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real time data sharing.    High availability (HA) \u2014 As a highly concurrent distributed system that offers low latency access to data, we must protect applications from node failures (caused by soft- ware bugs and hardware/network failures). High availability of data and transparent handling of failed operations there- fore become an important requirement for SnappyData.    Consistency \u2014 As a highly available system that of- fers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture in section 5.1, we explain how SnappyData meets each of these require- ments in the subsequent sections.", 
            "title": "Hybrid Cluster Manager"
        }, 
        {
            "location": "/architecture/#snappydata-cluster-architecture", 
            "text": "A SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members (see figure 4).\n1. Locator. Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n2. Lead Node. The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n3. Data Servers. A data server member hosts data, em- beds a Spark executor, and also contains a SQL engine ca- pable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.", 
            "title": "SnappyData Cluster Architecture"
        }, 
        {
            "location": "/architecture/#high-concurrency-in-snappydata", 
            "text": "Thousands of concurrent ODBC and JDBC clients can si- multaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incom- ing requests from these clients into (i) low latency requests and (ii) high latency ones. For low latency operations, we completely bypass Spark\u2019s scheduling mechanism and di- rectly operate on the data. We route high latency opera- tions (e.g., compute intensive queries) through Spark\u2019s fair scheduling mechanism. This makes SnappyData a respon- sive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.", 
            "title": "High Concurrency in SnappyData"
        }, 
        {
            "location": "/architecture/#state-sharing-in-snappydata", 
            "text": "A SnappyData cluster is designed to be a long running clustered database. State is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. (See section 4 for more details on table types and redundancy.) Nodes in the cluster stay up for a long time and their life-cycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "State Sharing in SnappyData"
        }, 
        {
            "location": "/configuration/", 
            "text": "Table of Contents\n\n\n\n\nConfiguration\n\n\nConfiguration files\n\n\nSnappyData specific properties\n\n\nSpark specific properties\n\n\nExample Configuration\n\n\nEnvironment settings\n\n\nPer Component Configuration\n\n\nsnappy-shell Command Line Utility\n\n\nLogging\n\n\n\n\nConfiguration\n\n\nSnappyData, a database server cluster, has three main components - Locator, Server and Lead. Lead node embeds a Spark driver and Server node embeds a Spark Executor. Server node also embeds a snappy store. As discussed in \nGetting Started\n, SnappyData cluster can be started with default configurations using script sbin/snappy-start-all.sh. This script starts up a locator, one data server and one lead node. However, SnappyData can be configured to start multiple components on different nodes. Also, each component can be configured individually using configuration files. In this document, we discuss how the components can be individually configured. We also discuss various other configurations of SnappyData. \n\n\nConfiguration files\n\n\nConfiguration files for locator, lead and server should be created in SNAPPY_HOME with names as conf/locators, conf/leads and conf/servers respectively. These files contain the hostnames of the nodes (one per line) where you intend to start the member. You can specify the properties to configure individual members. \n\n\nSnappyData specific properties\n\n\nThe following are the few important SnappyData properties that you would like to configure. \n\n\n\n\n-peer-discovery-port\n: This is a locator specific property. This is the port on which locator listens for member discovery. It defaults to 10334. \n\n\n-client-port\n: Port that a member listens on for client connections. \n\n\n-locators\n: List of other locators as comma-separated host:port values. For locators, the list must include all other locators in use. For Servers and Leads, the list must include all the locators of the distributed system.\n\n\n-dir\n: SnappyData members need to have working directory. The member working directory provides a default location for log, persistence, and status files for each member. If not specified, SnappyData creates the member directory in \nSNAPPY_HOME\\work\n. \n\n\n-classpath\n: This can be used to provide any application specific code to the lead and servers. We envisage having setJar like functionality going forward but for now, the application jars have to be provided during startup. \n\n\n-heap-size\n: Set a fixed heap size and for the Java VM. \n\n\n-J\n: Use this to configure any JVM specific property. For e.g. -J-XX:MaxPermSize=512m. \n\n\n\n\nFor a detail list of SnappyData configurations for Leads and Servers, you can consult \nthis\n. For a detail list of SnappyData configurations for Locators, you can consult \nthis\n.\n\n\nSpark specific properties\n\n\nSince SnappyData embeds Spark components, \nSpark Runtime environment properties\n (like  spark.driver.memory, spark.executor.memory, spark.driver.extraJavaOptions, spark.executorEnv) do not take effect. They have to specified using SnappyData configuration properties. Apart from these properties, other Spark properties can be specified in the configuration file of the Lead nodes. You have to prepend them with a \nhyphen(-)\n. The Spark properties that are specified on the Lead node are sent to the Server nodes. Any Spark property that is specified in the Servers or Locators file do not take effect. \n\n\n\n\nNote:\n\n\nCurrently we do not honour properties specified using spark-config.sh. \n\n\n\n\nExample Configuration\n\n\nLet's say you want to start two Locators (on node-a:9999 and node-b:8888), two servers (node-c and node-c) and a lead (node-l). Also, you would like to change the Spark UI port from 4040 to 9090. Also, you would like to set spark.executor.cores as 10 on all servers. The following can be your conf files. \n\n\n$ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999\n\n$ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -locators=node-b:8888,node-a:9999\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -J-XX:MaxPermSize=512m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10\n\n\n\n\n\n\nNote:\n\n\nconf files are consulted when servers are started and also when they are stopped. So, we do not recommend changing the conf files while the cluster is running. \n\n\n\n\nEnvironment settings\n\n\nAny Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in \nSNAPPY_HOME/conf\n. \n\n\nPer Component Configuration\n\n\nMost of the time, components would be sharing the same properties. For e.g. you would want all servers to start with 4096m while leads to start with 2048m. You can configure these by specifying LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, LEAD_STARTUP_OPTIONS environment variables in conf/snappy-env.sh. \n\n\n$ cat conf/snappy-env.sh\nSERVER_STARTUP_OPTIONS=\n-heap-size=4096m\n\nLEAD_STARTUP_OPTIONS=\n-heap-size=2048m\n\n\n\n\n\nsnappy-shell Command Line Utility\n\n\nInstead of starting SnappyData members using ssh scripts, they can be individually configured and started using command line. \n\n\n$ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334\n\n$ bin/snappy-shell locator stop\n$ bin/snappy-shell server stop\n\n\n\n\nLogging\n\n\nCurrently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property \n-log-file\n as the path of the directory. There is a log4j.properties file that is shipped with the jar. We recommend not to change it at this moment. However, to change the logging levels, you can create a conf/log4j.properties with the following details: \n\n\n$ cat conf/log4j.properties \nlog4j.rootCategory=DEBUG, file", 
            "title": "Configuring the cluster"
        }, 
        {
            "location": "/configuration/#table-of-contents", 
            "text": "Configuration  Configuration files  SnappyData specific properties  Spark specific properties  Example Configuration  Environment settings  Per Component Configuration  snappy-shell Command Line Utility  Logging", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/configuration/#configuration", 
            "text": "SnappyData, a database server cluster, has three main components - Locator, Server and Lead. Lead node embeds a Spark driver and Server node embeds a Spark Executor. Server node also embeds a snappy store. As discussed in  Getting Started , SnappyData cluster can be started with default configurations using script sbin/snappy-start-all.sh. This script starts up a locator, one data server and one lead node. However, SnappyData can be configured to start multiple components on different nodes. Also, each component can be configured individually using configuration files. In this document, we discuss how the components can be individually configured. We also discuss various other configurations of SnappyData.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#configuration-files", 
            "text": "Configuration files for locator, lead and server should be created in SNAPPY_HOME with names as conf/locators, conf/leads and conf/servers respectively. These files contain the hostnames of the nodes (one per line) where you intend to start the member. You can specify the properties to configure individual members.", 
            "title": "Configuration files"
        }, 
        {
            "location": "/configuration/#snappydata-specific-properties", 
            "text": "The following are the few important SnappyData properties that you would like to configure.    -peer-discovery-port : This is a locator specific property. This is the port on which locator listens for member discovery. It defaults to 10334.   -client-port : Port that a member listens on for client connections.   -locators : List of other locators as comma-separated host:port values. For locators, the list must include all other locators in use. For Servers and Leads, the list must include all the locators of the distributed system.  -dir : SnappyData members need to have working directory. The member working directory provides a default location for log, persistence, and status files for each member. If not specified, SnappyData creates the member directory in  SNAPPY_HOME\\work .   -classpath : This can be used to provide any application specific code to the lead and servers. We envisage having setJar like functionality going forward but for now, the application jars have to be provided during startup.   -heap-size : Set a fixed heap size and for the Java VM.   -J : Use this to configure any JVM specific property. For e.g. -J-XX:MaxPermSize=512m.    For a detail list of SnappyData configurations for Leads and Servers, you can consult  this . For a detail list of SnappyData configurations for Locators, you can consult  this .", 
            "title": "SnappyData specific properties"
        }, 
        {
            "location": "/configuration/#spark-specific-properties", 
            "text": "Since SnappyData embeds Spark components,  Spark Runtime environment properties  (like  spark.driver.memory, spark.executor.memory, spark.driver.extraJavaOptions, spark.executorEnv) do not take effect. They have to specified using SnappyData configuration properties. Apart from these properties, other Spark properties can be specified in the configuration file of the Lead nodes. You have to prepend them with a  hyphen(-) . The Spark properties that are specified on the Lead node are sent to the Server nodes. Any Spark property that is specified in the Servers or Locators file do not take effect.", 
            "title": "Spark specific properties"
        }, 
        {
            "location": "/configuration/#note", 
            "text": "Currently we do not honour properties specified using spark-config.sh.", 
            "title": "Note:"
        }, 
        {
            "location": "/configuration/#example-configuration", 
            "text": "Let's say you want to start two Locators (on node-a:9999 and node-b:8888), two servers (node-c and node-c) and a lead (node-l). Also, you would like to change the Spark UI port from 4040 to 9090. Also, you would like to set spark.executor.cores as 10 on all servers. The following can be your conf files.   $ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999\n\n$ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -locators=node-b:8888,node-a:9999\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -J-XX:MaxPermSize=512m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10", 
            "title": "Example Configuration"
        }, 
        {
            "location": "/configuration/#note_1", 
            "text": "conf files are consulted when servers are started and also when they are stopped. So, we do not recommend changing the conf files while the cluster is running.", 
            "title": "Note:"
        }, 
        {
            "location": "/configuration/#environment-settings", 
            "text": "Any Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in  SNAPPY_HOME/conf .", 
            "title": "Environment settings"
        }, 
        {
            "location": "/configuration/#per-component-configuration", 
            "text": "Most of the time, components would be sharing the same properties. For e.g. you would want all servers to start with 4096m while leads to start with 2048m. You can configure these by specifying LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, LEAD_STARTUP_OPTIONS environment variables in conf/snappy-env.sh.   $ cat conf/snappy-env.sh\nSERVER_STARTUP_OPTIONS= -heap-size=4096m \nLEAD_STARTUP_OPTIONS= -heap-size=2048m", 
            "title": "Per Component Configuration"
        }, 
        {
            "location": "/configuration/#snappy-shell-command-line-utility", 
            "text": "Instead of starting SnappyData members using ssh scripts, they can be individually configured and started using command line.   $ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334\n\n$ bin/snappy-shell locator stop\n$ bin/snappy-shell server stop", 
            "title": "snappy-shell Command Line Utility"
        }, 
        {
            "location": "/configuration/#logging", 
            "text": "Currently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property  -log-file  as the path of the directory. There is a log4j.properties file that is shipped with the jar. We recommend not to change it at this moment. However, to change the logging levels, you can create a conf/log4j.properties with the following details:   $ cat conf/log4j.properties \nlog4j.rootCategory=DEBUG, file", 
            "title": "Logging"
        }, 
        {
            "location": "/connectingToCluster/", 
            "text": "Using the SnappyData SQL Shell\n\n\nThe SnappyData SQL Shell (\nsnappy-shell\n) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. Internally it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.\n\n\n\n\n\n\n// from the SnappyData base directory  \n$ cd quickstart/scripts  \n$ ../../bin/snappy-shell  \nVersion 2.0-SNAPSHOT.1  \nsnappy\n \n\n//Connect to the cluster as a client  \nsnappy\n connect client 'localhost:1527';\n\n//Show active connections  \nsnappy\n show connections;\n\n//Display cluster members by querying a system table  \nsnappy\n select id, kind, status, host, port from sys.members;\n//or\nsnappy\n show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy\n run 'create_and_load_column_table.sql';\n\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy\n run 'create_and_load_row_table.sql';\n\n//Start pulse to monitor the SnappyData cluster  \nsnappy\n start pulse;\n\n\n\n\nThe complete list of commands available through \nsnappy_shell\n can be found \nhere\n\n\nUsing JDBC with SnappyData\n\n\nSnappyData ships with a few JDBC drivers. \nThe connection URL typically points to one of the Locators. Underneath the covers the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically swizzling underlying physical connections in case servers were to fail. \n\n\n\n// 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection (\njdbc:snappydata://locatorHostName:1527/\n);\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint\n\n\n\n\nAccessing SnappyData Tables from Spark code\n\n\nSpark applications access the SnappyStore using the new \nSpark Data Source API\n. \n\n\nBy default, SnappyData servers runs the Spark Executors collocated with the data store. And, the default store provider is SnappyData. \nWhen the spark program connects to the cluster using a SnappyContext (extends SQLContext), there is no need to configure the database URL and other options.  \n\n\n// Here is an Scala example \n  val sc = new org.apache.spark.SparkContext(conf)\n  val snContext = org.apache.spark.sql.SnappyContext(sc)\n\n  val props = Map[String, String]()\n  // Save some application dataframe into a SnappyData row table\n  myAppDataFrame.write.format(\nrow\n).options(props).saveAsTable(\nMutableTable\n)\n\n\n\n\n\nWhen running a native spark program, you can access SnappyData purely as a DataSource ...\n\n\n// Access SnappyData as a storage cluster .. \n  val sc = new org.apache.spark.SparkContext(conf)\n  val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n  val props = Map(\n    \nurl\n -\n \njdbc:snappydata://locatorHostName:1527/\n,\n    \npoolImpl\n -\n \ntomcat\n, \n    \nuser\n -\n \napp\n,\n    \npassword\n -\n \napp\n\n    )\n\n  // Save some application dataframe into a SnappyData row table\n  myAppDataFrame.write.format(\njdbc\n).options(props).saveAsTable(\nMutableTable\n)", 
            "title": "Connecting using JDBC, Spark"
        }, 
        {
            "location": "/connectingToCluster/#using-the-snappydata-sql-shell", 
            "text": "The SnappyData SQL Shell ( snappy-shell ) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. Internally it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.   \n// from the SnappyData base directory  \n$ cd quickstart/scripts  \n$ ../../bin/snappy-shell  \nVersion 2.0-SNAPSHOT.1  \nsnappy  \n\n//Connect to the cluster as a client  \nsnappy  connect client 'localhost:1527';\n\n//Show active connections  \nsnappy  show connections;\n\n//Display cluster members by querying a system table  \nsnappy  select id, kind, status, host, port from sys.members;\n//or\nsnappy  show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy  run 'create_and_load_column_table.sql';\n\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy  run 'create_and_load_row_table.sql';\n\n//Start pulse to monitor the SnappyData cluster  \nsnappy  start pulse;  The complete list of commands available through  snappy_shell  can be found  here", 
            "title": "Using the SnappyData SQL Shell"
        }, 
        {
            "location": "/connectingToCluster/#using-jdbc-with-snappydata", 
            "text": "SnappyData ships with a few JDBC drivers. \nThe connection URL typically points to one of the Locators. Underneath the covers the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically swizzling underlying physical connections in case servers were to fail.   \n// 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection ( jdbc:snappydata://locatorHostName:1527/ );\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint", 
            "title": "Using JDBC with SnappyData"
        }, 
        {
            "location": "/connectingToCluster/#accessing-snappydata-tables-from-spark-code", 
            "text": "Spark applications access the SnappyStore using the new  Spark Data Source API .   By default, SnappyData servers runs the Spark Executors collocated with the data store. And, the default store provider is SnappyData. \nWhen the spark program connects to the cluster using a SnappyContext (extends SQLContext), there is no need to configure the database URL and other options.    // Here is an Scala example \n  val sc = new org.apache.spark.SparkContext(conf)\n  val snContext = org.apache.spark.sql.SnappyContext(sc)\n\n  val props = Map[String, String]()\n  // Save some application dataframe into a SnappyData row table\n  myAppDataFrame.write.format( row ).options(props).saveAsTable( MutableTable )  When running a native spark program, you can access SnappyData purely as a DataSource ...  // Access SnappyData as a storage cluster .. \n  val sc = new org.apache.spark.SparkContext(conf)\n  val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n  val props = Map(\n     url  -   jdbc:snappydata://locatorHostName:1527/ ,\n     poolImpl  -   tomcat , \n     user  -   app ,\n     password  -   app \n    )\n\n  // Save some application dataframe into a SnappyData row table\n  myAppDataFrame.write.format( jdbc ).options(props).saveAsTable( MutableTable )", 
            "title": "Accessing SnappyData Tables from Spark code"
        }, 
        {
            "location": "/jobs/", 
            "text": "Building Snappy applications using Spark API\n\n\n\n\n..Explain the SQLJob and programming example\nSubmitting jobs\nRunning Spark native programs \u2026 submitting and running jobs\n\n\n\n\n\n\nSnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the rich higher level APIs. Working with the SnappyData Tables themselves builds on top of Spark SQL. So, we recommend getting to know the \nconcepts in SparkSQL\n (and hence some core Spark concepts). \n\n\nYou primarily interact with SnappyData tables using SQL (a richer, more compliant SQL) or the \nDataFrame API\n. And, you can store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. \n\n\nIn Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in catalog and persisted using the SnappyStore to disk (i.e. the tables will be there when the cluster recovers). This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. \n\n\nSnappyContext\n\n\nA SnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's [[org.apache.spark.sql.SQLContext]] to work with Row and Column tables. Any DataFrame can be managed as SnappyData tables and any table can be accessed as a DataFrame. This is similar to [[org.apache.spark.sql.hive.HiveContext HiveContext]] - integrates the SQLContext functionality with the Snappy store.\n\n\nWhen running in the \nembedded\n mode (i.e. Spark executor collocated with Snappy data store), Applications typically submit Jobs to the Snappy-JobServer (provide link) and do not explicitly create a SnappyContext. A single shared context managed by SnappyData makes it possible to re-use Executors across client connections or applications.\n\n\n\n\nTodo: Provide a simple local[*] example \u2026 create sparkContext; snContext = SnappyContext(sc); store into a table\nSee Spark SQL guide  \u2026 scribe along similar lines ..\ndescribe few important methods in SnappyContext \n\n\n\n\nDeployment topologies with SnappyData \u2026 hemant can own this?\n\n\n\n\nrunning in local mode \u2026 provide an example with SnappyContext and master URL\n\n\nrunning in embedded mode \u2026 DB and Spark programs are collocated \u2026 who is playing the role of the master, URL? \nThis is our primary execution model where Spark is running collocated within the database and the recommended deployment topology for real time data intensive applications to achieve highest possible performance. \n\n\n\n\nsnappy locator start -peer-discovery-address=localhost -dir=locator1\nsnappy server start -locators=localhost[10334] -dir=server1\nsnappy lead start -locators=localhost[10334] -dir=lead1\n\n\n\n\nAs soon as the lead node joins the system, it starts a Spark driver and then asks server nodes to start Spark executors within them. \n\n\n\n\nshow architecture and explain\n\n\n\n\n\n\nrunning Spark in standalone cluster\n\n\n\n\n\n\nWhat else?\n\n\n\n\nRunning Spark programs inside the database\n\n\nTo create a job that can be submitted through the job server, the job must implement the \nSnappySQLJob or SnappyStreamingJob\n trait. Your job will look like:\n\n\nclass SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(sc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}\n\n\n\n\n\n\nThe \nJob\n traits are simply extensions of the \nSparkJob\n implemented by \nSpark JobServer\n. \n\n\n\n\n\u2022 runJob contains the implementation of the Job. The SparkContext is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.\n\u2022 validate allows for an initial validation of the context and any provided configuration. If the context and configuration are OK to run the job, returning spark.jobserver.SparkJobValid will let the job execute, otherwise returning spark.jobserver.SparkJobInvalid(reason) prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an HTTP/1.1 400 Bad Request status code.\u2028validate helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources.\n\n\nSee examples(link?) for Spark and spark streaming jobs in the \n/examples directory.\n\n\n(Todo: In what way have we extended the SparkJob trait? )\nSnappyData manages a SparkContext as a singleton but creates one SQLContext per incoming SQL Connection. \n\n\nSubmitting jobs\n\n\nRunning Spark native programs \u2026 submitting and running jobs\n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n\n\n\n\nThis utility submits the job and returns a JSON that has a jobId of this job. \n\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  }\n}\n\n\n\n\nThis job ID can be used to query the status of the running job. \n\n\n$ bin/snappy-job.sh status  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n\n{\n  \nduration\n: \n17.53 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.CreateAndLoadAirlineDataJob\n,\n  \nstartTime\n: \n2016-01-12T16:59:14.746+05:30\n,\n  \ncontext\n: \nsnappyContext1452598154529305363\n,\n  \nresult\n: \nSee /home/hemant/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n}\n\n\n\n\nOnce the tables are created, they can be queried by firing another job. \n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar\n\n\n\n\nThe status of this job can be queried in the same manner as shown above. The result of the this job will return a file path that has the query results. \n\n\nStreaming jobs\n\n\nAn implementation of SnappyStreamingJob can be submitted to lead of SnappyData by specifying --stream as a parameter to the snappy-job.sh. \n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar \\ \n    --stream\n\n\n\n\nRunning a standalone Spark compute cluster\n\n\n\n\nNOTE: SnappyData, out-of-the-box, collocates Spark executors and the data store for efficient data intensive computations. \nBut, it may desirable to isolate the computational cluster for other reasons - for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate for a  cache data set repeatedly. \nTo support such scenarios it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion.", 
            "title": "Developing Apps using the Spark API"
        }, 
        {
            "location": "/jobs/#building-snappy-applications-using-spark-api", 
            "text": "..Explain the SQLJob and programming example\nSubmitting jobs\nRunning Spark native programs \u2026 submitting and running jobs    SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the rich higher level APIs. Working with the SnappyData Tables themselves builds on top of Spark SQL. So, we recommend getting to know the  concepts in SparkSQL  (and hence some core Spark concepts).   You primarily interact with SnappyData tables using SQL (a richer, more compliant SQL) or the  DataFrame API . And, you can store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame.   In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in catalog and persisted using the SnappyStore to disk (i.e. the tables will be there when the cluster recovers). This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters.", 
            "title": "Building Snappy applications using Spark API"
        }, 
        {
            "location": "/jobs/#snappycontext", 
            "text": "A SnappyContext is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's [[org.apache.spark.sql.SQLContext]] to work with Row and Column tables. Any DataFrame can be managed as SnappyData tables and any table can be accessed as a DataFrame. This is similar to [[org.apache.spark.sql.hive.HiveContext HiveContext]] - integrates the SQLContext functionality with the Snappy store.  When running in the  embedded  mode (i.e. Spark executor collocated with Snappy data store), Applications typically submit Jobs to the Snappy-JobServer (provide link) and do not explicitly create a SnappyContext. A single shared context managed by SnappyData makes it possible to re-use Executors across client connections or applications.   Todo: Provide a simple local[*] example \u2026 create sparkContext; snContext = SnappyContext(sc); store into a table\nSee Spark SQL guide  \u2026 scribe along similar lines ..\ndescribe few important methods in SnappyContext", 
            "title": "SnappyContext"
        }, 
        {
            "location": "/jobs/#deployment-topologies-with-snappydata-hemant-can-own-this", 
            "text": "running in local mode \u2026 provide an example with SnappyContext and master URL  running in embedded mode \u2026 DB and Spark programs are collocated \u2026 who is playing the role of the master, URL? \nThis is our primary execution model where Spark is running collocated within the database and the recommended deployment topology for real time data intensive applications to achieve highest possible performance.    snappy locator start -peer-discovery-address=localhost -dir=locator1\nsnappy server start -locators=localhost[10334] -dir=server1\nsnappy lead start -locators=localhost[10334] -dir=lead1  As soon as the lead node joins the system, it starts a Spark driver and then asks server nodes to start Spark executors within them.    show architecture and explain    running Spark in standalone cluster    What else?", 
            "title": "Deployment topologies with SnappyData \u2026 hemant can own this?"
        }, 
        {
            "location": "/jobs/#running-spark-programs-inside-the-database", 
            "text": "To create a job that can be submitted through the job server, the job must implement the  SnappySQLJob or SnappyStreamingJob  trait. Your job will look like:  class SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(sc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}   The  Job  traits are simply extensions of the  SparkJob  implemented by  Spark JobServer .    \u2022 runJob contains the implementation of the Job. The SparkContext is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.\n\u2022 validate allows for an initial validation of the context and any provided configuration. If the context and configuration are OK to run the job, returning spark.jobserver.SparkJobValid will let the job execute, otherwise returning spark.jobserver.SparkJobInvalid(reason) prevents the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an HTTP/1.1 400 Bad Request status code.\u2028validate helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources.  See examples(link?) for Spark and spark streaming jobs in the  /examples directory.  (Todo: In what way have we extended the SparkJob trait? )\nSnappyData manages a SparkContext as a singleton but creates one SQLContext per incoming SQL Connection.", 
            "title": "Running Spark programs inside the database"
        }, 
        {
            "location": "/jobs/#submitting-jobs", 
            "text": "Running Spark native programs \u2026 submitting and running jobs  $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar  This utility submits the job and returns a JSON that has a jobId of this job.   {\n   status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  }\n}  This job ID can be used to query the status of the running job.   $ bin/snappy-job.sh status  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n\n{\n   duration :  17.53 secs ,\n   classPath :  io.snappydata.examples.CreateAndLoadAirlineDataJob ,\n   startTime :  2016-01-12T16:59:14.746+05:30 ,\n   context :  snappyContext1452598154529305363 ,\n   result :  See /home/hemant/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n}  Once the tables are created, they can be queried by firing another job.   $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar  The status of this job can be queried in the same manner as shown above. The result of the this job will return a file path that has the query results.", 
            "title": "Submitting jobs"
        }, 
        {
            "location": "/jobs/#streaming-jobs", 
            "text": "An implementation of SnappyStreamingJob can be submitted to lead of SnappyData by specifying --stream as a parameter to the snappy-job.sh.   $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --app-jar $SNAPPY_HOME/lib/quickstart-0.1.0-SNAPSHOT.jar \\ \n    --stream", 
            "title": "Streaming jobs"
        }, 
        {
            "location": "/jobs/#running-a-standalone-spark-compute-cluster", 
            "text": "NOTE: SnappyData, out-of-the-box, collocates Spark executors and the data store for efficient data intensive computations. \nBut, it may desirable to isolate the computational cluster for other reasons - for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate for a  cache data set repeatedly. \nTo support such scenarios it is also possible to run native Spark jobs that accesses a SnappyData cluster as a storage layer in a parallel fashion.", 
            "title": "Running a standalone Spark compute cluster"
        }, 
        {
            "location": "/rowAndColumnTables/", 
            "text": "Row and column tables\n\n\nColumn tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\nRow tables, unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.\n\n\nCreate table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.\n\n\nDDL and DML Syntax for tables\n\n\nCREATE TABLE [IF NOT EXISTS] table_name\n   (\n  COLUMN_DEFININTION\n   )\nUSING 'row | column'\nOPTIONS (\nCOLOCATE_WITH 'table_name',  // Default none\nPARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table.\nBUCKETS  'NumPartitions', // Default 113\nREDUNDANCY        '1' ,\nRECOVER_DELAY     '-1',\nMAX_PART_SIZE      '50',\nEVICTION_BY  \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT,\nPERSISTENT   \u2018DISKSTORE_NAME ASYNCHRONOUS |  SYNCHRONOUS\u2019, // Empty string will map to default disk store.\nOFFHEAP \u2018true | false\u2019 ,\nEXPIRE \u2018TIMETOLIVE in seconds',\n)\n[AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name\n\n\n\nFor row format tables column definition can take underlying GemFire XD syntax to create a table.e.g.note the PRIMARY KEY clause below.\n\n\nsnc.sql(\"CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT) USING row options(BUCKETS '5')\" )\n\n\n\nBut for column table its restricted to Spark syntax for column definition e.g.\n\n\nsnc.sql(\"CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '5')\" )\n\n\n\nClauses like PRIMARY KEY , NOT NUL etc. are not supported for column definition. \n\n\nSpark API for managing tables\n\n\nGet a reference to SnappyContext\n\n\nval snc: SnappyContext = SnappyContext.getOrCreate(sparkContext)\n\n\n\nCreate a SnappyStore table using Spark APIs\n\n\nval props = Map('BUCKETS','5')  // This map should contain required DDL extensions, described in next section\ncase class Data(col1: Int, col2: Int, col3: Int)\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sc.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\nval dataDF = snc.createDataFrame(rdd)\nsnc.createTable(\"column_table\", \"column\", dataDF.schema, props)\n//or create a row format table\nsnc.createTable(\"row_table\", \"row\", dataDF.schema, props)\n\n\n\nDrop a SnappyStore table using Spark APIs\n\n\nsnc.dropTable(tableName, ifExists = true)\n\n\n\nDDL extensions to SnappyStore tables\n\n\nThe below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified default values are attached. See next section for various restrictions. \n\n\n\n\nCOLOCATE_WITH  : The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist\n\n\nPARTITION_BY  : Use the PARTITION_BY {COLUMN} clause to provide a set of column names that will determine the partitioning. As a shortcut you can use PARTITION BY PRIMARY KEY to refer to the primary key columns defined for the table . If not specified it will be a replicated table.\n\n\nBUCKETS  : The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 113.\n\n\nREDUNDANCY : Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail.\n\n\nRECOVER_DELAY : Use the RECOVERY_DELAY clause to specify the default time in milliseconds that existing members will wait before satisfying redundancy after a member crashes. The default is -1, which indicates that redundancy is not recovered after a member fails.\n\n\nMAX_PART_SIZE : The MAXPARTSIZE attribute specifies the maximum memory for any partition on a member in megabytes. Use it to load-balance partitions among available members. If you omit MAXPARTSIZE, then GemFire XD calculates a default value for the table based on available heap memory. You can view the MAXPARTSIZE setting by querying the EVICTIONATTRS column in SYSTABLES.\n\n\nEVICTION_BY : Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store\n\n\nPERSISTENT :  When you specify the PERSISTENT keyword, GemFire XD persists the in-memory table data to a local GemFire XD disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.\n\n\nOFFHEAP : SnappyStore enables you to store the data for selected tables outside of the JVM heap. Storing a table in off-heap memory can improve performance for the table by reducing the CPU resources required to manage the table's data in the heap (garbage collection)\n\n\nEXPIRE: You can use the EXPIRE clause with tables to control SnappyStore memory usage. It will expire the rows after configured TTL.\n\n\n\n\nRestrictions on column tables in the preview release\n\n\n\n\nColumn tables can not specify any primary key, unique key constarints.\n\n\nIndex on column table is not supported.\n\n\nOption EXPIRE is not applicable for column tables.\n\n\nOption EVICTION_BY with value LRUCOUNT is not applicable for column tables. \n\n\n\n\nDML operations on tables\n\n\nINSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\nUPDATE tablename SET column = value [, column = value ...] [WHERE expression]\nDELETE FROM tablename1 [WHERE expression]\nTRUNCATE TABLE tablename1;\n\n\n\nAPI extensions provided in SnappyContext\n\n\nWe have added several APIs in SnappyContext to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables.\n\n\n//  Applicable for both row \n column tables\ndef insert(tableName: String, rows: Row*): Int .\n\n// Only for row tables\ndef put(tableName: String, rows: Row*): Int\ndef update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*): Int\ndef delete(tableName: String, filterExpr: String): Int\n\n\n\nUsage SnappyConytext.insert(): Insert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r =\n\n  snappyContext.insert(\"tableName\", Row.fromSeq(r))\n}\n\n\n\nUsage SnappyConytext.put(): Upsert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r =\n\n  snc.put(tableName, Row.fromSeq(r))\n}\n\n\n\nUsage SnappyConytext.update(): Update all rows in table that match passed filter expression\n\n\nsnc.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" )\n\n\n\nUsage SnappyConytext.delete(): Delete all rows in table that match passed filter expression\n\n\nsnc.delete(tableName, \"ITEMREF = 3\")\n\n\n\nExplain the delta row buffer and how queries are executed\n\n\nCatalog in SnappyStore\n\n\nWe use a persistent Hive catalog for all our metadata storage. All table, schema definition are stored here in a reliable manner. As we intend be able to quickly recover from driver failover, we chose GemFireXd itself to store meta information. This gives us ability to query underlying GemFireXD to reconstruct the metastore incase of a driver failover. \n\n\nThere are pending work towards unifying DRDA \n Spark layer catalog, which will part of future releases. \n\n\nSQL Reference to the Syntax\n\n\nFor detailed syntax for GemFire XD check\nhttp://gemfirexd.docs.pivotal.io/docs-gemfirexd/reference/sql-language-reference.html", 
            "title": "Row and Column tables"
        }, 
        {
            "location": "/rowAndColumnTables/#row-and-column-tables", 
            "text": "Column tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or a average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  Row tables, unlike column tables are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.  Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.", 
            "title": "Row and column tables"
        }, 
        {
            "location": "/rowAndColumnTables/#ddl-and-dml-syntax-for-tables", 
            "text": "CREATE TABLE [IF NOT EXISTS] table_name\n   (\n  COLUMN_DEFININTION\n   )\nUSING 'row | column'\nOPTIONS (\nCOLOCATE_WITH 'table_name',  // Default none\nPARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table.\nBUCKETS  'NumPartitions', // Default 113\nREDUNDANCY        '1' ,\nRECOVER_DELAY     '-1',\nMAX_PART_SIZE      '50',\nEVICTION_BY  \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT,\nPERSISTENT   \u2018DISKSTORE_NAME ASYNCHRONOUS |  SYNCHRONOUS\u2019, // Empty string will map to default disk store.\nOFFHEAP \u2018true | false\u2019 ,\nEXPIRE \u2018TIMETOLIVE in seconds',\n)\n[AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name  For row format tables column definition can take underlying GemFire XD syntax to create a table.e.g.note the PRIMARY KEY clause below.  snc.sql(\"CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT) USING row options(BUCKETS '5')\" )  But for column table its restricted to Spark syntax for column definition e.g.  snc.sql(\"CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '5')\" )  Clauses like PRIMARY KEY , NOT NUL etc. are not supported for column definition.", 
            "title": "DDL and DML Syntax for tables"
        }, 
        {
            "location": "/rowAndColumnTables/#spark-api-for-managing-tables", 
            "text": "Get a reference to SnappyContext  val snc: SnappyContext = SnappyContext.getOrCreate(sparkContext)  Create a SnappyStore table using Spark APIs  val props = Map('BUCKETS','5')  // This map should contain required DDL extensions, described in next section\ncase class Data(col1: Int, col2: Int, col3: Int)\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sc.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\nval dataDF = snc.createDataFrame(rdd)\nsnc.createTable(\"column_table\", \"column\", dataDF.schema, props)\n//or create a row format table\nsnc.createTable(\"row_table\", \"row\", dataDF.schema, props)  Drop a SnappyStore table using Spark APIs  snc.dropTable(tableName, ifExists = true)", 
            "title": "Spark API for managing tables"
        }, 
        {
            "location": "/rowAndColumnTables/#ddl-extensions-to-snappystore-tables", 
            "text": "The below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified default values are attached. See next section for various restrictions.    COLOCATE_WITH  : The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist  PARTITION_BY  : Use the PARTITION_BY {COLUMN} clause to provide a set of column names that will determine the partitioning. As a shortcut you can use PARTITION BY PRIMARY KEY to refer to the primary key columns defined for the table . If not specified it will be a replicated table.  BUCKETS  : The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 113.  REDUNDANCY : Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail.  RECOVER_DELAY : Use the RECOVERY_DELAY clause to specify the default time in milliseconds that existing members will wait before satisfying redundancy after a member crashes. The default is -1, which indicates that redundancy is not recovered after a member fails.  MAX_PART_SIZE : The MAXPARTSIZE attribute specifies the maximum memory for any partition on a member in megabytes. Use it to load-balance partitions among available members. If you omit MAXPARTSIZE, then GemFire XD calculates a default value for the table based on available heap memory. You can view the MAXPARTSIZE setting by querying the EVICTIONATTRS column in SYSTABLES.  EVICTION_BY : Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store  PERSISTENT :  When you specify the PERSISTENT keyword, GemFire XD persists the in-memory table data to a local GemFire XD disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.  OFFHEAP : SnappyStore enables you to store the data for selected tables outside of the JVM heap. Storing a table in off-heap memory can improve performance for the table by reducing the CPU resources required to manage the table's data in the heap (garbage collection)  EXPIRE: You can use the EXPIRE clause with tables to control SnappyStore memory usage. It will expire the rows after configured TTL.", 
            "title": "DDL extensions to SnappyStore tables"
        }, 
        {
            "location": "/rowAndColumnTables/#restrictions-on-column-tables-in-the-preview-release", 
            "text": "Column tables can not specify any primary key, unique key constarints.  Index on column table is not supported.  Option EXPIRE is not applicable for column tables.  Option EVICTION_BY with value LRUCOUNT is not applicable for column tables.", 
            "title": "Restrictions on column tables in the preview release"
        }, 
        {
            "location": "/rowAndColumnTables/#dml-operations-on-tables", 
            "text": "INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\nUPDATE tablename SET column = value [, column = value ...] [WHERE expression]\nDELETE FROM tablename1 [WHERE expression]\nTRUNCATE TABLE tablename1;", 
            "title": "DML operations on tables"
        }, 
        {
            "location": "/rowAndColumnTables/#api-extensions-provided-in-snappycontext", 
            "text": "We have added several APIs in SnappyContext to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables.  //  Applicable for both row   column tables\ndef insert(tableName: String, rows: Row*): Int .\n\n// Only for row tables\ndef put(tableName: String, rows: Row*): Int\ndef update(tableName: String, filterExpr: String, newColumnValues: Row, updateColumns: String*): Int\ndef delete(tableName: String, filterExpr: String): Int  Usage SnappyConytext.insert(): Insert one or more [[org.apache.spark.sql.Row]] into an existing table  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r = \n  snappyContext.insert(\"tableName\", Row.fromSeq(r))\n}  Usage SnappyConytext.put(): Upsert one or more [[org.apache.spark.sql.Row]] into an existing table  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r = \n  snc.put(tableName, Row.fromSeq(r))\n}  Usage SnappyConytext.update(): Update all rows in table that match passed filter expression  snc.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" )  Usage SnappyConytext.delete(): Delete all rows in table that match passed filter expression  snc.delete(tableName, \"ITEMREF = 3\")  Explain the delta row buffer and how queries are executed", 
            "title": "API extensions provided in SnappyContext"
        }, 
        {
            "location": "/rowAndColumnTables/#catalog-in-snappystore", 
            "text": "We use a persistent Hive catalog for all our metadata storage. All table, schema definition are stored here in a reliable manner. As we intend be able to quickly recover from driver failover, we chose GemFireXd itself to store meta information. This gives us ability to query underlying GemFireXD to reconstruct the metastore incase of a driver failover.   There are pending work towards unifying DRDA   Spark layer catalog, which will part of future releases.", 
            "title": "Catalog in SnappyStore"
        }, 
        {
            "location": "/rowAndColumnTables/#sql-reference-to-the-syntax", 
            "text": "For detailed syntax for GemFire XD check\nhttp://gemfirexd.docs.pivotal.io/docs-gemfirexd/reference/sql-language-reference.html", 
            "title": "SQL Reference to the Syntax"
        }, 
        {
            "location": "/aqp/", 
            "text": "Approximate Query processing (AQP)\n\n\nOverview (from paper) \u2026 basic ideas in stratified sampling \u2026 when is it useful\n\n\nCreating and managing stratified samples\n\n\n\u2026 DDL and examples \u2026 \n\u2026 how do we manage samples with streaming data \u2026 continuous appends.. reservoir sampling brief\n\n\nAnalytic querying on samples\n\n\nsupported aggregations \u2026 \nexamples \nconstraining errors \n\n\nApproximate TopK analytics for time series data\n\n\nOverview: Refer to the rough paper that hemant created (jags has several edits)\nAPI and examples\nWhen should it be used?", 
            "title": "Approximate query processing"
        }, 
        {
            "location": "/aqp/#approximate-query-processing-aqp", 
            "text": "", 
            "title": "Approximate Query processing (AQP)"
        }, 
        {
            "location": "/aqp/#overview-from-paper-basic-ideas-in-stratified-sampling-when-is-it-useful", 
            "text": "", 
            "title": "Overview (from paper) \u2026 basic ideas in stratified sampling \u2026 when is it useful"
        }, 
        {
            "location": "/aqp/#creating-and-managing-stratified-samples", 
            "text": "\u2026 DDL and examples \u2026 \n\u2026 how do we manage samples with streaming data \u2026 continuous appends.. reservoir sampling brief", 
            "title": "Creating and managing stratified samples"
        }, 
        {
            "location": "/aqp/#analytic-querying-on-samples", 
            "text": "supported aggregations \u2026 \nexamples \nconstraining errors", 
            "title": "Analytic querying on samples"
        }, 
        {
            "location": "/aqp/#approximate-topk-analytics-for-time-series-data", 
            "text": "Overview: Refer to the rough paper that hemant created (jags has several edits)\nAPI and examples\nWhen should it be used?", 
            "title": "Approximate TopK analytics for time series data"
        }, 
        {
            "location": "/streamingWithSQL/", 
            "text": "SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. \nHere is a brief overview of \nSpark streaming\n from the Spark Streaming guide. \n\n\nSpark Streaming Overview\n\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex\u2028algorithms expressed with high-level functions like \nmap\n, \nreduce\n, \njoin\n and \nwindow\n.\n\n\nFinally, processed data can be pushed out to filesystems, databases,\u2028and live dashboards. In fact, you can apply Spark's \nmachine learning\n and \ngraph processing\n algorithms on data streams.\u2028\u2028\n\n\n\n  \n\n\n\n\n\nInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n\n\n \n\n \n\n\n\nSpark Streaming provides a high-level abstraction called \ndiscretized stream\n or \nDStream\n,\u2028which represents a continuous stream of data. DStreams can be created either from input data\u2028streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level\u2028operations on other DStreams. Internally, a DStream is represented as a sequence of\u2028\nRDDs\n.\u2028\n\n\nAdditional details on the Spark Streaming concepts and programming is covered \nhere\n.\n\n\nSnappyData Streaming extensions over Spark\n\n\nWe offer the following enhancements over Spark Streaming : \n\n\n\n\n\n\nManage Streams declaratively\n: Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to the any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. \n\n\n\n\n\n\nSQL based stream processing\n: With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. \n\n\n\n\n\n\nContinuous queries and time windows\n: Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query. \n\n\n\n\n\n\nOLAP optimizations\n: By integrating and collocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store.\n\n\n\n\n\n\nReduced shuffling through co-partitioning\n: With SnappyData, the partitioning key used by the input queue (e.g., for Kafka sources), the stream processor and the underlying store can all be the same. This dramatically reduces the need to shuffle records.\n\n\n\n\n\n\nApproximate stream analytics\n: When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.\n\n\n\n\n\n\nWorking with stream tables\n\n\nSnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.\n\n\n// DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFININTION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'\nOPTIONS (\n // multiple stream source specific options\nstoragelevel '', \nrowConverter '', \nzkQuorum '',\ngroupId '',\ntopics '', \nkafkaParams '',\nconsumerKey '',\nconsumerSecret '',\naccessToken '',\naccessTokenSecret '',\nhostname '',\nport '',\ndirectory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT \nbatchIntervalSeconds\n\n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP\n\n\n\nFor example to create a stream table using kafka source : \n\n\nval sc = new SparkContext(new SparkConf().setAppName(\"example\").setMaster(\"local[*]\"))\nval snc = SnappyContext.getOrCreate(sc)\nvar snsc = SnappyStreamingContext(snc, Seconds(1))\n\nsnsc.sql(\"create stream table streamTable (userId string, clickStreamLog string) \" +\n    \"using kafka_stream options (\" +\n    \"storagelevel 'MEMORY_AND_DISK_SER_2', \" +\n    \"rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \" +\n    \"zkQuorum 'localhost:2181', \" +\n    \"groupId 'streamConsumer', \" +\n    \"topics 'streamTopic:01')\")\n\n// You can get a handle of underlying DStream of the table\nval dStream = snsc.getSchemaDStream(\"streamTable\")\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto(tableName))\n\n\n\nThe streamTable created in above example can be accessed from snappy-shell and can be queried using ad-hoc SQL queries.\n\n\nStream SQL through Snappy-Shell\n\n\nStart a SnappyData cluster and connect through snappy-shell : \n\n\n//create a connection\nsnappy\n connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy\n streaming init 2;\n\n// Create a stream table\nsnappy\n create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter') ;\n\n// Start the streaming \nsnappy\n streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy\n select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy\n drop table streamTable;\n\n// Stop the streaming\nsnappy\n streaming stop;\n\n\n\nSchemaDStream\n\n\nSchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL query on DStreams. It is similar to SchemaRDD, which offers the similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provide foreachDataFrame API.SchemaDStream can be registered as table.\n\n\nRegistering Continuous queries\n\n\n//You can join two stream tables and produce a result stream. \nval resultStream = snsc.registerCQ(\"SELECT s1.id, s1.text FROM stream1 window (duration '2' seconds, \nslide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\")\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto(tableName))\n\n\n\nHow do you register on SchemaDStreams? \nHow can you control the time windows for such queries?\n\n\nDynamic(ad-hoc) Conitnous queries\n\n\nUnlike Spark streaming, you do not need to register all your stream output transformations (which is continous query in this case) before the start of StreamingContext. The CQs can be registered even after the SnappyStreamingContext has started.\n\n\nWhat is currently out-of-scope?\n\n\nContinous Queries through command line(Snappy-Shell)", 
            "title": "Stream processing using SQL"
        }, 
        {
            "location": "/streamingWithSQL/#spark-streaming-overview", 
            "text": "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex\u2028algorithms expressed with high-level functions like  map ,  reduce ,  join  and  window .  Finally, processed data can be pushed out to filesystems, databases,\u2028and live dashboards. In fact, you can apply Spark's  machine learning  and  graph processing  algorithms on data streams.\u2028\u2028  \n     Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.  \n  \n   Spark Streaming provides a high-level abstraction called  discretized stream  or  DStream ,\u2028which represents a continuous stream of data. DStreams can be created either from input data\u2028streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level\u2028operations on other DStreams. Internally, a DStream is represented as a sequence of\u2028 RDDs .\u2028  Additional details on the Spark Streaming concepts and programming is covered  here .", 
            "title": "Spark Streaming Overview"
        }, 
        {
            "location": "/streamingWithSQL/#snappydata-streaming-extensions-over-spark", 
            "text": "We offer the following enhancements over Spark Streaming :     Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to the any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions.     SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams.     Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query.     OLAP optimizations : By integrating and collocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store.    Reduced shuffling through co-partitioning : With SnappyData, the partitioning key used by the input queue (e.g., for Kafka sources), the stream processor and the underlying store can all be the same. This dramatically reduces the need to shuffle records.    Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.", 
            "title": "SnappyData Streaming extensions over Spark"
        }, 
        {
            "location": "/streamingWithSQL/#working-with-stream-tables", 
            "text": "SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.  // DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFININTION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'\nOPTIONS (\n // multiple stream source specific options\nstoragelevel '', \nrowConverter '', \nzkQuorum '',\ngroupId '',\ntopics '', \nkafkaParams '',\nconsumerKey '',\nconsumerSecret '',\naccessToken '',\naccessTokenSecret '',\nhostname '',\nport '',\ndirectory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT  batchIntervalSeconds \n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP  For example to create a stream table using kafka source :   val sc = new SparkContext(new SparkConf().setAppName(\"example\").setMaster(\"local[*]\"))\nval snc = SnappyContext.getOrCreate(sc)\nvar snsc = SnappyStreamingContext(snc, Seconds(1))\n\nsnsc.sql(\"create stream table streamTable (userId string, clickStreamLog string) \" +\n    \"using kafka_stream options (\" +\n    \"storagelevel 'MEMORY_AND_DISK_SER_2', \" +\n    \"rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \" +\n    \"zkQuorum 'localhost:2181', \" +\n    \"groupId 'streamConsumer', \" +\n    \"topics 'streamTopic:01')\")\n\n// You can get a handle of underlying DStream of the table\nval dStream = snsc.getSchemaDStream(\"streamTable\")\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto(tableName))  The streamTable created in above example can be accessed from snappy-shell and can be queried using ad-hoc SQL queries.", 
            "title": "Working with stream tables"
        }, 
        {
            "location": "/streamingWithSQL/#stream-sql-through-snappy-shell", 
            "text": "Start a SnappyData cluster and connect through snappy-shell :   //create a connection\nsnappy  connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy  streaming init 2;\n\n// Create a stream table\nsnappy  create stream table streamTable (id long, text string, fullName string, country string, retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '', accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter') ;\n\n// Start the streaming \nsnappy  streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy  select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy  drop table streamTable;\n\n// Stop the streaming\nsnappy  streaming stop;", 
            "title": "Stream SQL through Snappy-Shell"
        }, 
        {
            "location": "/streamingWithSQL/#schemadstream", 
            "text": "SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL query on DStreams. It is similar to SchemaRDD, which offers the similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provide foreachDataFrame API.SchemaDStream can be registered as table.", 
            "title": "SchemaDStream"
        }, 
        {
            "location": "/streamingWithSQL/#registering-continuous-queries", 
            "text": "//You can join two stream tables and produce a result stream. \nval resultStream = snsc.registerCQ(\"SELECT s1.id, s1.text FROM stream1 window (duration '2' seconds, \nslide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\")\n\n// You can also save the DataFrames to an external table\ndStream.foreachDataFrame(_.write.insertInto(tableName))  How do you register on SchemaDStreams? \nHow can you control the time windows for such queries?", 
            "title": "Registering Continuous queries"
        }, 
        {
            "location": "/streamingWithSQL/#dynamicad-hoc-conitnous-queries", 
            "text": "Unlike Spark streaming, you do not need to register all your stream output transformations (which is continous query in this case) before the start of StreamingContext. The CQs can be registered even after the SnappyStreamingContext has started.", 
            "title": "Dynamic(ad-hoc) Conitnous queries"
        }, 
        {
            "location": "/streamingWithSQL/#what-is-currently-out-of-scope", 
            "text": "Continous Queries through command line(Snappy-Shell)", 
            "title": "What is currently out-of-scope?"
        }, 
        {
            "location": "/deployment/", 
            "text": "Deployment toplogies\n\n\nThis section provides a short overview of the different runtime deployment architectures available and recommendations on when to choose one over the other. \nThere are three deployment modes available in snappydata. \n\n\n\n\n\n\n\n\nDeployment Mode\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUnified Cluster\n\n\nReal time production application. Here the Spark Executor(compute) and Snappy DataStore are collocated\n\n\n\n\n\n\nSplit Cluster\n\n\nSpark executors and SnappyStore form independent clusters. Use for computationally heavy computing and Batch processing\n\n\n\n\n\n\nLocal\n\n\nThis is for development where client application, the executors and data store are all running in the same JVM\n\n\n\n\n\n\n\n\nUnified cluster mode (aka 'Embedded store' mode)\n\n\nThis is the default cluster model where Spark computations and in-memory data store run collocated in the same JVM. This is our ootb configuration and suitable for most SnappyData real time production environments. You launch Snappy Data servers to bootstrap any data from disk, replicas or from external data sources and Spark executors are dynamically launched when the first Spark Job arrives. \n\n\nYou either start SnappyData members using the \nsnappy_start_all\n script or you start them individually. \n\n\n# start members using the ssh scripts \n$ sbin\\snappy-start-all.sh\n\n# start members individually\n$ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334\n\n\n\n\nSpark applications are coordinated by a SparkContext instance that runs in the Application's main program called the 'Driver'. The driver coordinates the execution by running parallel tasks on executors and is responsible for delivering results to the application when 'Jobs'(i.e. actions like print() ) are executed. \nWhen executing in this unified cluster mode there can only be a single Spark Context (a single coordinator if you may) for the cluster. To support multiple concurrent Jobs or applications Snappydata manages a singleton SparkContext created and running in the 'Lead' node. i.e. the Spark context is fully managed by Snappydata. Applications simply submit \nJobs\n and don't have to be concerned about HA for the context or the driver program. \nThe rationale for our design is further explored \nhere\n. \n\n\nFully managed Spark driver and context\n\n\nPrograms can connect to the lead node and submit Jobs. The Driver is managed by the Snappy cluster in the lead node and the application doesn\u2019t create or managed the Spark context. Applications implement the \nSnappySQLJob\n or the \nSnappyStreamingJob\n trait as describing in the \n'Building Spark Apps'\n section.\n\n\nApplication managed Spark driver and context\n\n\nWhile Snappy recommends the use of these above mentioned scala traits to implement your application, you could also run your native Spark program on the unified cluster with a slight change to the cluster URL. \n\n\nval conf = new SparkConf().\n              // here the locator url is passed as part of the master url\n              setMasterURL(\nsnappydata://localhost:10334\n).\n              set(\njobserver.enabled\n, \ntrue\n)\nval sc = new SparkContext(conf) \n\n\n\n\n\n\nNote\n\n\nWe currently don't support external cluster managers like YARN when operating in this mode. While, it is easy to expand and redistribute the data by starting new data servers dynamically we expect such dynamic resource allocations to be a planned and seldom exercised option. Re-distributing large quantities of data can be very expensive and can slow down running applications. \nFor computational intensive workloads or batch processing workloads where extensive data shuffling is involved consider using the Split cluster mode(describe next). \n\n\n\n\nSplit cluster mode\n\n\nIn this mode, Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Apache Spark runs in this mode. \n\n\nSpecifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.\n\n\nThe driver program managing the SparkContext also participate as a peer member in the SnappyData distributed system and gets access to the store catalog information. To enable this, you must set the \nlocator\n host/port in the configuration (see example below). When executors running the spark cluster access these tables the catalog meta data is used to locate the store servers managing data partitions and would be accessed in parallel. \nRead the \nSpark cluster overview\n for more details on the native Spark architecture. \n\n\nval conf = new SparkConf().\n              // Here the spark context connects with Spark's master running on 7077. \n              setMasterURL(\nspark://localhost:7077\n).\n              set(\nsnappydata.store.locators\n, \nlocalhost:10334\n) \nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// The following code connects with the snappy locator to fetch hive metastore. \nval snappyContext = SnappyContext(sc) \n\n\n\n\n\nThe catalog is initialized lazily when SnappyData functionality is accessed. \nThe big benefit even while the clusters for compute and data is split is that the catalog is immediately visible to the Spark executors nodes and applications don\u2019t have to explicitly manage connections and schema related information. This design is quite similar to the Spark\u2019s native support for Hive. \n\n\nWhen accessing partitioned data, the partitions are fetched as compressed blobs that is fully compatible with the columnar compression built into Spark. All access is automatically parallelized. \n\n\nLocal mode\n\n\nAs the name implies, use this mode to execute everything locally in the application JVM. The local vs cluster modes are described in the \nSpark Programming guide\n.\n\n\nval conf = new SparkConf().\n               setMasterURL(\nsnappydata:local[*]\n). \n               // Starting jobserver helps when you would want to test your jobs in a local mode. \n               set(\njobserver.enabled\n, \ntrue\n)\nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// JobServer is started too.", 
            "title": "Deployment topologies"
        }, 
        {
            "location": "/deployment/#deployment-toplogies", 
            "text": "This section provides a short overview of the different runtime deployment architectures available and recommendations on when to choose one over the other. \nThere are three deployment modes available in snappydata.      Deployment Mode  Description      Unified Cluster  Real time production application. Here the Spark Executor(compute) and Snappy DataStore are collocated    Split Cluster  Spark executors and SnappyStore form independent clusters. Use for computationally heavy computing and Batch processing    Local  This is for development where client application, the executors and data store are all running in the same JVM", 
            "title": "Deployment toplogies"
        }, 
        {
            "location": "/deployment/#unified-cluster-mode-aka-embedded-store-mode", 
            "text": "This is the default cluster model where Spark computations and in-memory data store run collocated in the same JVM. This is our ootb configuration and suitable for most SnappyData real time production environments. You launch Snappy Data servers to bootstrap any data from disk, replicas or from external data sources and Spark executors are dynamically launched when the first Spark Job arrives.   You either start SnappyData members using the  snappy_start_all  script or you start them individually.   # start members using the ssh scripts \n$ sbin\\snappy-start-all.sh\n\n# start members individually\n$ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334  Spark applications are coordinated by a SparkContext instance that runs in the Application's main program called the 'Driver'. The driver coordinates the execution by running parallel tasks on executors and is responsible for delivering results to the application when 'Jobs'(i.e. actions like print() ) are executed. \nWhen executing in this unified cluster mode there can only be a single Spark Context (a single coordinator if you may) for the cluster. To support multiple concurrent Jobs or applications Snappydata manages a singleton SparkContext created and running in the 'Lead' node. i.e. the Spark context is fully managed by Snappydata. Applications simply submit  Jobs  and don't have to be concerned about HA for the context or the driver program. \nThe rationale for our design is further explored  here .", 
            "title": "Unified cluster mode (aka 'Embedded store' mode)"
        }, 
        {
            "location": "/deployment/#fully-managed-spark-driver-and-context", 
            "text": "Programs can connect to the lead node and submit Jobs. The Driver is managed by the Snappy cluster in the lead node and the application doesn\u2019t create or managed the Spark context. Applications implement the  SnappySQLJob  or the  SnappyStreamingJob  trait as describing in the  'Building Spark Apps'  section.", 
            "title": "Fully managed Spark driver and context"
        }, 
        {
            "location": "/deployment/#application-managed-spark-driver-and-context", 
            "text": "While Snappy recommends the use of these above mentioned scala traits to implement your application, you could also run your native Spark program on the unified cluster with a slight change to the cluster URL.   val conf = new SparkConf().\n              // here the locator url is passed as part of the master url\n              setMasterURL( snappydata://localhost:10334 ).\n              set( jobserver.enabled ,  true )\nval sc = new SparkContext(conf)", 
            "title": "Application managed Spark driver and context"
        }, 
        {
            "location": "/deployment/#note", 
            "text": "We currently don't support external cluster managers like YARN when operating in this mode. While, it is easy to expand and redistribute the data by starting new data servers dynamically we expect such dynamic resource allocations to be a planned and seldom exercised option. Re-distributing large quantities of data can be very expensive and can slow down running applications. \nFor computational intensive workloads or batch processing workloads where extensive data shuffling is involved consider using the Split cluster mode(describe next).", 
            "title": "Note"
        }, 
        {
            "location": "/deployment/#split-cluster-mode", 
            "text": "In this mode, Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Apache Spark runs in this mode.   Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.  The driver program managing the SparkContext also participate as a peer member in the SnappyData distributed system and gets access to the store catalog information. To enable this, you must set the  locator  host/port in the configuration (see example below). When executors running the spark cluster access these tables the catalog meta data is used to locate the store servers managing data partitions and would be accessed in parallel. \nRead the  Spark cluster overview  for more details on the native Spark architecture.   val conf = new SparkConf().\n              // Here the spark context connects with Spark's master running on 7077. \n              setMasterURL( spark://localhost:7077 ).\n              set( snappydata.store.locators ,  localhost:10334 ) \nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// The following code connects with the snappy locator to fetch hive metastore. \nval snappyContext = SnappyContext(sc)   The catalog is initialized lazily when SnappyData functionality is accessed. \nThe big benefit even while the clusters for compute and data is split is that the catalog is immediately visible to the Spark executors nodes and applications don\u2019t have to explicitly manage connections and schema related information. This design is quite similar to the Spark\u2019s native support for Hive.   When accessing partitioned data, the partitions are fetched as compressed blobs that is fully compatible with the columnar compression built into Spark. All access is automatically parallelized.", 
            "title": "Split cluster mode"
        }, 
        {
            "location": "/deployment/#local-mode", 
            "text": "As the name implies, use this mode to execute everything locally in the application JVM. The local vs cluster modes are described in the  Spark Programming guide .  val conf = new SparkConf().\n               setMasterURL( snappydata:local[*] ). \n               // Starting jobserver helps when you would want to test your jobs in a local mode. \n               set( jobserver.enabled ,  true )\nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// JobServer is started too.", 
            "title": "Local mode"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "LICENSE\n\n\nCopyright (c) 2016 SnappyData, Inc. All rights reserved.", 
            "title": "License"
        }, 
        {
            "location": "/LICENSE/#license", 
            "text": "Copyright (c) 2016 SnappyData, Inc. All rights reserved.", 
            "title": "LICENSE"
        }
    ]
}