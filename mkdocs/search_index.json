{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nSnappyData is a \ndistributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP (online transaction processing) and OLAP (online analytical processing) in a single integrated cluster\n. We realize this platform through a seamless integration of Apache Spark (as a big data computational engine) with GemFire XD (as an in-memory transactional store with scale-out SQL semantics). \n\n\n\n\nDownload binary distribution\n\n\nYou can download the latest versions of SnappyData here:\n\n\n\n\nSnappyData 0.6 download link \n(tar.gz)\n \n(zip)\n\n\nSnappyData 0.6(hadoop provided) download link \n(tar.gz)\n \n(zip)\n\n\n\n\nSnappyData has been tested on Linux and Mac OSX. If not already installed, you will need to download \nJava 8\n. \n\n\nCommunity Support\n\n\nWe monitor channels listed below for comments/questions.\n\n\nStackoverflow\n \n    \nSlack\n        Gitter \n          \nIRC\n \n             \nReddit\n \n          JIRA \n\n\nLink with SnappyData distribution\n\n\nSnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:\n\n\ngroupId: io.snappydata\nartifactId: snappydata-core_2.11\nversion: 0.6\n\ngroupId: io.snappydata\nartifactId: snappydata-cluster_2.11\nversion: 0.6\n\n\n\n\nWorking with SnappyData Source Code\n\n\nIf you are interested in working with the latest code or contributing to SnappyData development, you can also check out the master branch from Git:\n\n\nMaster development branch\ngit clone https://github.com/SnappyDataInc/snappydata.git --recursive\n\n###### 0.6 release branch with stability and other fixes ######\ngit clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.6 --recursive\n\n\n\n\nBuilding SnappyData from source\n\n\nYou will find the instructions for building, layout of the code, integration with IDEs using Gradle, etc here:\n\n\n\n\nSnappyData Build Instructions\n\n\n\n\n\n\nNOTE:\nSnappyData is built using Spark 1.6 (build xx) which is packaged as part of SnappyData. While you can build your application using Apache Spark 1.5, you will need to link to Snappy-spark to make  use of the SnappyData extensions. Gradle build tasks are packaged.  \n\n\n\n\nSetting up passwordless SSH\n\n\nThe quick start scripts use ssh to start up various processes. You can install ssh on ubuntu with \nsudo apt-get install ssh\n. ssh comes packaged with Mac OSX, however, make sure ssh is enabled by going to \nSystem Preferences -\n Sharing\n and enabling \nRemote Login\n. Enabling passwordless ssh is the easiest way to work with SnappyData and prevents you from having to put in your ssh password multiple times. \n\n\nGenerate an RSA key with \n\n\nssh-keygen -t rsa\n\n\nThis will spawn some prompts. If you want truly passwordless ssh, just press enter instead of entering a password.\n\n\nFinally, copy your key to authorized keys:\n\n\ncat ~/.ssh/id_rsa.pub \n ~/.ssh/authorized_keys\n\n\nMore detail on passwordless ssh can be found \nhere\n and \nhere\n.\n\n\nKey Features\n\n\n\n\n100% compatible with Spark\n: Use SnappyData as a database as well as use any of the Spark APIs - ML, Graph, etc. on the same data\n\n\nIn-memory row and column store\n: Run the store collocated in Spark executors (i.e. a single compute and data cluster) or in its own process space (i.e. separate compute and data cluster)\n\n\nSQL standard compliance\n: Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.\n\n\nSQL based extensions for streaming processing\n: Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.\n\n\nInteractive analytics using Synopsis Data Engine (SDE)\n: We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and are completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions. \n\n\nMutate, transact on data in Spark\n: Use SQL to insert, update, delete data in tables. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData are visible as DataFrames without having to maintain multiples copies of your data. \n\n\nOptimizations\n: Use indexes to improve query performance in the row store (the GemFire SQL optimizer automatically uses in-memory indexes when available) \n\n\nHigh availability not just Fault tolerance\n: Data is instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications with continuous HA.\n\n\nDurability and recovery:\n Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled. \n\n\n\n\nRead SnappyData \ndocs\n for a more detailed list of all features and semantics. \n\n\nGetting started\n\n\nEach header under \"Getting Started\" that contains a conceptual explanation meant to orient you will append an \"(explanation)\". Each header that contains actual steps for executing the \"Getting Started\" will prepend \"Step\".\n\n\nObjectives\n\n\n\n\nIn-memory Column and Row tables\n: Illustrate both SQL syntax and the Spark API to create and manage column tables for large data and illustrate how row tables can be used for reference data and can be replicated to each node in the cluster. \n\n\nOLAP, OLTP operations\n: We run analytic class SQL queries (full scan with aggregations) on column tables and fully distributed join queries and observe the space requirements as well as the performance of these queries. For OLTP, we run simple update queries - you can note the Spark API extensions to support mutations in Spark. \n\n\nSDE\n: We run the same analytic queries by creating adjunct stratified samples to note the performance difference - can we get close to interactive query performance speeds?\n\n\nStreaming with SQL\n: We ingest twitter streams into both a probabilistic data structure for TopK time series analytics and the entire stream (full data set) into a row table. We run both ad-hoc queries on these streams (modeled as tables) as well as showcase our first preview for continuous querying support. What SnappyData demonstrates here is simpler, SQL centric abstractions on top of Spark streaming. And, of course, ingestion into the built-in store.\n\n\n\n\nIn this document, we discuss the features mentioned above and ask you to take steps to run the scripts that demonstrate these features. \n\n\nSnappyData Cluster (explanation)\n\n\nSnappyData, a database server cluster, has three main components - Locator, Server and Lead. \n\n\n\n\nLocator\n: Provides discovery service for the cluster. Informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n\n\nLead Node\n: Acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n\n\nData Servers\n: Hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.\n\n\n\n\n\n\nDetails of about the architecture can be found here:\n\n\nArchitecture\n \n\n\nSnappyData also has multiple deployment options which can be found here:\n\n\nDeployment Options\n.\n\n\nStep 1 - Start the SnappyData cluster\n\n\nClick the screenshot below to view the screencast that goes along with \"Getting Started with SQL\"\n\n\n\n\nThe first step is to configure SNAPPY_HOME in your environment:\n\n\nexport SNAPPY_HOME=/path/to/snappy/root\n\n\nThe remainder of \"Getting Started\" is based on a set of \nairline data\n we run different queries on. That data is packaged with SnappyData, however, it is only a portion of the full dataset. To download the full dataset, from \n/snappy/\n run \n./quickstart/scripts/download_full_airlinedata.sh ./quickstart/data\n. This is recommended for the Synopsis Data Engine portion of \"Getting Started,\" but not necessary. Note that to run the above script, you need curl installed. \nHere\n is one way to install it on ubuntu.\n\n\nPasswordless ssh\n\n\nThe quick start scripts use ssh to start up various processes. You can install ssh on ubuntu with \nsudo apt-get install ssh\n. By default, ssh requires a password. To be able to log on to the localhost and run the script without being prompted for the password, please enable \npasswordless ssh\n. Otherwise, set up ssh for localhost with \nssh localhost\n\n\nNavigate to the /snappy/ root directory. The start script starts up a minimal set of essential components to form the cluster - A locator, one data server and one lead node. All nodes are started locally. \nRun the start script with:\n \n\n\n./sbin/snappy-start-all.sh\n\n\n\n\nThis may take 30 seconds or more to bootstrap the entire cluster on your local machine (logs are in the 'work' sub-directory). The output should look something like this \u2026\n\n\n$ sbin/snappy-start-all.sh \nlocalhost: Starting SnappyData Locator using peer discovery on: 0.0.0.0[10334]\n...\nlocalhost: SnappyData Locator pid: 56703 status: running\n\nlocalhost: Starting SnappyData Server using locators for peer discovery: jramnara-mbpro[10334]   (port used for members to form a p2p cluster)\nlocalhost: SnappyData Server pid: 56819 status: running\nlocalhost:   Distributed system now has 2 members.\n\nlocalhost: Starting SnappyData Leader using locators for peer discovery: jramnara-mbpro[10334]\nlocalhost: SnappyData Leader pid: 56932 status: running\nlocalhost:   Distributed system now has 3 members.\n\nlocalhost:   Other members: jramnara-mbpro(56703:locator)\nv0\n:54414, jramnara-mbpro(56819:datastore)\nv1\n:39737\n\n\n\n\n\nTo spin up remote nodes simply rename/copy the files without the template suffix and add the hostnames. View custom configuration and startup options here:\n\n\nCustom Configuration\n\n\nAt this point, the SnappyData cluster is up and running and is ready to accept jobs and SQL requests via JDBC/ODBC. You can \nmonitor the Spark cluster at port 4040\n. Once you load data and run queries, you can analyze the Spark SQL query plan, the job execution stages and storage details of column tables.\n\n\n\n\n\n\nInteracting with SnappyData (explanation)\n\n\n\n\nFor the section on the Spark API, we assume some familiarity with \ncore Spark, Spark SQL and Spark Streaming concepts\n. \nAnd, you can try out the Spark \nQuick Start\n. All the commands and programs\nlisted in the Spark guides will work in SnappyData also. For the section on SQL, no Spark knowledge is necessary.\n\n\n\n\nTo interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self contained Spark application or can share state with other jobs using the SnappyData store. \n\n\nUnlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.\n\n\n\n\nLong running executors\n: Executors are running within the SnappyData store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors. \n\n\nDriver runs in HA configuration\n: Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the \nSpark JobServer\n to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). \nRead our \ndocs\n for details on the architecture.\n\n\n\n\nIn this document, we showcase mostly the same set of features via the Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to the Spark API section:\n\n\nGetting Started with Spark API\n.\n\n\nGetting Started with SQL\n\n\nFor SQL, the SnappyData SQL Shell (\nsnappy-shell\n) provides a simple way to inspect the catalog,  run admin operations,  manage the schema and run interactive queries. You can also use your favorite SQL tool like SquirrelSQL or DBVisualizer (a JDBC connection to the cluster).\n\n\nFrom the SnappyData base directory, /snappy/, run: \n\n\n./bin/snappy-shell\n\n\n\n\nConnect to the cluster with\n\n\nsnappy\n connect client 'localhost:1527';\n\n\nYou can view connections with\n\n\nsnappy\n show connections;\n\n\nAnd check member status with:\n\n\nsnappy\n show members;\n\n\nColumn and Row tables (explanation)\n\n\nColumn tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\n-- DDL to create a column table\nCREATE TABLE AIRLINE (\ncolumn definitions\n) USING column OPTIONS(buckets '5') ;\n\n\n\n\nRow tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys. A row's location is determined by a hash function and hence very fast for point lookups or updates.\n\n\ncreate table\n DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  \n\n\n-- DDL to create a row table\nCREATE TABLE AIRLINEREF (\ncolumn definitions\n) USING row OPTIONS() ;\n\n\n\n\nRead our preliminary \nrow \n column table docs\n for the details.\n\n\nStep 2 - Create column table, row table and load data\n\n\nThe below SQL scripts create and populate the tables we need to continue. The displayed command assumes you started the snappy-shell the base directory, /snappy/. If you started the snappy-shell from /bin/, for example, you need to prepend the filepath with /..\n\n\nTo create and load a column table:\n\n\nsnappy\n run './quickstart/scripts/create_and_load_column_table.sql';\n\n\n\n\nTo create and load a row table:\n\n\nsnappy\n run './quickstart/scripts/create_and_load_row_table.sql';\n\n\n\n\nTo see the status of the system:\n\n\nsnappy\n run './quickstart/scripts/status_queries.sql'\n\n\n\n\n\nYou can see the memory consumed in the Spark Console, http://\nthe-lead-host-name\n`:4040/Snappy Store. \n\n\nOLAP and OLTP queries (explanation)\n\n\nSQL client connections (via JDBC or ODBC) are routed to the appropriate data server via the locator (Physical connections are automatically created in the driver and are transparently swizzled in case of failures also). When queries are executed they are parsed initially by the SnappyData server to determine if the query is an OLAP class or an OLTP class query.  Currently, all column table queries are considered OLAP.  Such queries are routed to the \nlead\n node where a \nSpark SQLContext\n is managed for each connection. The query is planned using Spark's Catalyst engine and scheduled to be executed on the data servers. The number of partitions determine the number of concurrent tasks used across the data servers to run the query in parallel. In this case, our column table was created using \n5 partitions (buckets)\n and hence will use 5 concurrent tasks. \n\n\n---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nsnappy\n select AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline, airlineref\n  where airline.UniqueCarrier = airlineref.Code\n  group by UniqueCarrier, description \n  order by arrivalDelay;\n\n\n\n\nFor low latency OLTP queries, the engine won't route it to the lead and instead execute it immediately without any scheduling overhead. Quite often, this may mean simply fetching a row by hashing a key (in microseconds).\n\n\n--- Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'\n--- the airline code can be updated in the row table\nUPDATE AIRLINEREF SET DESCRIPTION='Delta America' WHERE CAST(CODE AS VARCHAR(25))='DL';\n\n\n\n\nSpark SQL can cache DataFrames as temporary tables and the data set is immutable. SnappyData SQL is compatible with the SQL standard with support for transactions and DML (insert, update, delete) on tables. \nLink to Snappy Store SQL reference\n.  As we show later, any table in SnappyData is also visible as a Spark DataFrame. \n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nThe OLAP query we're executing is asking \"which airlines arrive on schedule?\" which requires a join to a reference table. You have the option to run the packaged query script or paste one query at a time. To run the script, in the snappy-shell paste:\n\n\nsnappy\n run './quickstart/scripts/olap_queries.sql';\n\n\n\n\nTo paste the actual query, paste:\n\n\nselect AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline, airlineref\n  where airline.UniqueCarrier = airlineref.Code\n  group by UniqueCarrier, description \n  order by arrivalDelay;\n\n\n\n\nEach query is executed as one or more Jobs and each Job executed in one or more stages. You can explore the query execution plan and metrics \nthe Spark Console\n under \"SQL.\"\n\n\nThe OLTP query we're executing is updating \"Delta Airlines\" to \"Delta America.\" Paste the following command:\n\n\nsnappy\n run './quickstart/scripts/oltp_queries.sql';\n\n\n\n\nYou can now re-run olap_queries.sql to see the updated join result set.\n\n\n\n\nNote\n\nIn the current implementation we only support appending to Column tables. Future releases will support all DML operations. \nYou can execute transactions using commands \nautocommit off\n and \ncommit\n.  \n\n\n\n\nSynopsis Data Engine (SDE) (explanation)\n\n\n\n\nIf you downloaded the full airline data (52 million records) set in \nStep 1\n, edit the \n'create_and_load_column_table.sql'\n script which is in \nquickstart/scripts\n to point to \nairlineParquetData_2007-15\n directory. Make sure you copy + paste the starting quote mark to the end after you change \nairlineParquetData\n to enclose the statement. If you enter a quote mark directly from your keyboard it may break the script.  This script loads parquet formatted data into a temporary spark table then saves it in column table called Airline. You need to load the table again using \nrun './quickstart/scripts/create_and_load_column_table.sql';\n Ideally, you'd re-run the olap queries script as well to see the speedup between non-SDE and SDE. \n\n\n\n\nOLAP queries are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory queries above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like SDE.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs and DDL to specify one or more curated \nstratified samples\n on large tables. \n\n\nThe following DDL creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your \nGroup by\n and \nWhere\n clauses make up the \nQuery Column Set\n (strata columns). Multiple samples can be created and queries that are executed on the base table are analyzed for appropriate sample selection. \n\n\nCREATE SAMPLE TABLE AIRLINE_SAMPLE\n   ON AIRLINE                             -- The parent base table\n   OPTIONS(\n    buckets '5',                          -- Number of partitions \n    qcs 'UniqueCarrier, Year_, Month_',   -- QueryColumnSet(qcs): The strata - 3% of each combination of Carrier, \n                                          -- Year and Month are stored as sample\n    fraction '0.03',                      -- How big should the sample be\n    strataReservoirSize '50'              -- Reservoir sampling to support streaming inserts\n\n\n\n\nYou can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the \nWith Error\n constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set. \n\n\n-- What is the average arrival delay for all airlines for each month?;\nsnappy\n select avg(ArrDelay), Month_ from Airline where ArrDelay \n0 \n    group by Month_\n    with error;\n-- The above query will consult the sample and return an answer if the estimated answer \n-- is at least 90% accurate (here, by default we use a 95% confidence interval).\n\n-- You can also access the error using built-in functions. \nsnappy\n select avg(ArrDelay) avgDelay, absolute_error(avgDelay), Month_ \n    from Airline where ArrDelay \n0 \n    group by Month_\n    with error;\n\n\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nLike Step 3, we need to first create and load the sample table. The following script will sample the Airline table:\n\n\nsnappy\n run './quickstart/scripts/create_and_load_sample_table.sql';\n\n\n\n\nYou can now re-run the previous OLAP queries with an error constraint and compare the results. The following script will run the previous queries with an error constraint:\n\n\nsnappy\n run './quickstart/scripts/olap_approx_queries.sql';\n\n\n\n\nYou should notice a 10X or larger difference in query execution latency while the results remain nearly as accurate. As a reminder, we recommend downloading the larger data set for this exercise.\n\n\nStream analytics using SQL and Spark Streaming (explanation)\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and you can analyze these streams using SQL. You can also dynamically run SQL queries on these streams. There is no need to learn Spark streaming APIs or statically define all the rules to be executed on these streams.\n\n\nThe commands below consume tweets, then they filter out just the hashtags and then they convert hashtags into Row objects. The commands then model the stream as a table (so it can be queried) and we then run ad-hoc SQL from remote clients on the current state of the stream. \n\n\n--- Inits the Streaming Context with the batch interval of 2 seconds.\n--- i.e. the stream is processed once every 2 seconds.\nsnappy\n STREAMING INIT 2 SECS;\n--- Create a stream table just containing the hashtags.\n--- /tmp/copiedtwitterdata is the directory that Streaming will use to find and read new text files.\n--- We use quickstart/scripts/simulateTwitterStream script in below example to simulate a twitter stream by\n--- copying tweets in /tmp/copiedtwitterdata folder.\nsnappy\n CREATE STREAM TABLE HASHTAG_FILESTREAMTABLE\n              (hashtag string)\n            USING file_stream\n            OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',\n              rowConverter 'org.apache.spark.sql.streaming.TweetToHashtagRow',\n              directory '/tmp/copiedtwitterdata')\n--- A file_stream data source monitors the directory and as files arrives they are ingested\n--- into the streaming pipeline. First converted into Rows using 'TweetToHashtagRow' then visible as table\n--- Start streaming context \nsnappy\n STREAMING START\n--- Adhoc sql on the stream table to query the current batch\n--- Get top 10 popular hashtags ----\nsnappy\n SELECT hashtag, count(*) as tagcount\n        FROM HASHTAG_FILESTREAMTABLE\n        GROUP BY hashtag\n        ORDER BY tagcount DESC limit 10;\n\n\n\n\nLater, in the Spark API section we further enhance this concept to showcase \n\"continuous queries\" (CQ)\n. Dynamic registration of CQs (from remote clients) will be available in the next release.\n\n\nTop-K Elements in a Stream (explanation)\n\n\nFinding the \nk\n most popular elements in a data stream is a common analytic query. For instance, the top-100 pages on a popular website in the last 10 mins, the top-10 sales regions in the last week, etc. As you can see, if the query is on an arbitrary time interval in the past, this will most likely mandate storing the entire stream. And, this could easily be millions to billions of events in use cases in the Internet of Things, for example. SnappyData provides SQL extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds a temporal component (i.e. data can be queried based on a time interval) to these structures and enables transparent querying using Spark SQL. More details about SnappyData's implementation of top-k can be found here:\n\n\nDetailed SDE Documentation\n\n\nSnappyData provides DDL extensions to create Top-k structures. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as data arrives. The Top-k structures can be queried using regular SQL queries. \n\n\n--- Create a topk table from a stream table\nCREATE TOPK TABLE filestream_topktable ON HASHTAG_FILESTREAMTABLE OPTIONS\n(key 'hashtag', timeInterval '2000ms', size '10' );\n--- Query a topk table \nSELECT hashtag, COUNT(hashtag) AS TopKCount\nFROM filestream_topktable\nGROUP BY hashtag ORDER BY TopKCount limit 10;\n\n\n\n\nNow, lets try analyzing some tweets using this above syntax in real time using SnappyData's packaged scripts.\n\n\nStep 5 - Create and Query Stream Table and Top-K Declaratively\n\n\nYou can use the scripts that simulates the twitter stream by copying pre-loaded tweets in a tmp folder. Or, you could use a script that accesses the live twitter stream.  \n\n\nSteps to work with simulated Twitter stream\n\n\nCreate a file stream table that listens on a folder and then start the streaming context. In the snappy-shell, paste:\n\n\nsnappy\n run './quickstart/scripts/create_and_start_file_streaming.sql';\n\n\n\n\nRun the following utility in another terminal that simulates a twitter stream by copying tweets in the folder on which file stream table is listening.\n\n\n$ quickstart/scripts/simulateTwitterStream \n\n\n\n\nNow query the current batch of the stream using the following script. \n\n\n--- Run this command multiple times to query current batch at different times \nrun './quickstart/scripts/file_streaming_query.sql';\n--- Stop the streaming \nsnappy\n STREAMING STOP;\n\n\n\n\nThis also creates Topk structures. simulateTwitterStream script runs for only for a minute or so.\n\n\nSteps to work with live Twitter stream\n\n\nTo work with the live twitter stream, you will have to generate authorization keys and secrets on \ntwitter apps\n and update \nSNAPPY_HOME/quickstart/scripts/create_and_start_twitter_streaming.sql\n with the keys and secrets.\n\n\n--- Run the create and start script that has keys and secrets to fetch live twitter stream\n--- Note: Currently, we do not encrypt the keys. \n-- This also creates Topk structures\nsnappy\n run './quickstart/scripts/create_and_start_twitter_streaming.sql';\n\n\n\n\nNow query the current batch of the stream using the following script. \n\n\n--- Run this command multiple times to query current batch at different times \nsnappy\n run './quickstart/scripts/twitter_streaming_query.sql';\n--- Stop the streaming \nsnappy\n STREAMING STOP;\n\n\n\n\nWe also have an \nAd Analytics code example\n and associated \nscreencast\n that showcases many useful features of SnappyData.\n\n\nGetting Started with Spark API\n\n\nSnappyContext\n is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's \nSQLContext\n to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame. This is similar to \nHiveContext\n and it integrates the SQLContext functionality with the SnappyData store. Similarly, \nSnappyStreamingContext\n is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's \nStreaming Context\n. \n\n\nApplications typically submit Jobs to SnappyData and do not explicitly create a SnappyContext or SnappyStreamingContext. These jobs are the primary mechanism to interact with SnappyData using the Spark API. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait. \n\n\nclass SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(snc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}\n\n\n\n\nThe implementation of the \nrunJob\n function from SnappySQLJob uses a SnappyContext to interact with the SnappyData store to process and store tables. The implementation of runJob from SnappyStreamingJob uses a SnappyStreamingContext to create streams and manage the streaming context. The jobs are submitted to the lead node of SnappyData over REST API using a \nspark-submit\n like utility. See more details about jobs here:\n\n\nSnappyData Jobs\n\n\nColumn and Row tables (explanation)\n\n\nColumn tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\n// creating a column table in Snappy job\nsnappyContext.createTable(\nAIRLINE\n, \ncolumn\n, schema, Map(\nbuckets\n -\n \n5\n))\n\n\n\n\nRow tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys. A row's location is determined by a hash function and hence very fast for point lookups or updates.\n\n\ncreate table\n DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.  \n\n\n// creating a row table in Snappy job\nval airlineCodeDF = snappyContext.createTable(\nAIRLINEREF\n, \nrow\n, schema, Map())\n\n\n\n\nRead our preliminary \nrow \n column table docs\n for the details\n\n\nStep 2 - Create column table, row table and load data\n\n\n\n\nIf you downloaded the full airline data set in \nStep 1\n, set the following config parameter to point at the data set.\n\nexport APP_PROPS=\"airline_file=/path/to/full/airline/dataset\"\n\n\n\n\nSubmit \nCreateAndLoadAirlineDataJob\n over the REST API to create row and column tables. See more details about jobs and job submission here:\n\n\nSnappyData jobs \n job submission\n. \n\n\n# Submit a job to Lead node on port 8090 \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name airlineApp --class  io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar ./examples/jars/quickstart-0.6.jar\n{\nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  } }\n\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{ \nduration\n: \n17.53 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.CreateAndLoadAirlineDataJob\n,\n  \nstartTime\n: \n2016-01-12T16:59:14.746+05:30\n,\n  \ncontext\n: \nsnappyContext1452598154529305363\n,\n  \nresult\n: \nSee /snappy/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n}\n# Tables are created\n\n\n\n\nThe output of the job can be found in \nCreateAndLoadAirlineDataJob.out\n in the lead directory which by default is \nSNAPPY_HOME/work/localhost-lead-*/\n. You can see the size of the column tables on Spark UI which by default can be seen at http://hostNameOfLead:4040. \n\n\nOLAP and OLTP Store (explanation)\n\n\nSnappyContext\n extends SQLContext and adds functionality to work with row and column tables. When queries inside jobs are executed they are parsed initially by the SnappyData server to determine if it is an OLAP class or an OLTP class query.  Currently, all column table queries are considered OLAP. Such queries are planned using Spark's Catalyst engine and scheduled to be executed on the data servers. \n\n\nval resultDF = airlineDF.join(airlineCodeDF,\n        airlineDF.col(\nUniqueCarrier\n).equalTo(airlineCodeDF(\nCODE\n))).\n        groupBy(airlineDF(\nUniqueCarrier\n), airlineCodeDF(\nDESCRIPTION\n)).\n        agg(\nArrDelay\n -\n \navg\n).orderBy(\navg(ArrDelay)\n)\n\n\n\n\nFor low latency OLTP queries in jobs, SnappyData won't schedule these queries. Instead SnappyData executes them immediately on SnappyData servers without any scheduling overhead. Quite often, this may mean simply fetching or updating a row by hashing a key (in nanoseconds). \n\n\n// Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'. Update the row table.\nval filterExpr: String = \n CODE ='DL'\n\nval newColumnValues: Row = Row(\nDelta America\n)\nval updateColumns = \nDESCRIPTION\n\nsnappyContext.update(rowTableName, filterExpr, newColumnValues, updateColumns)\n\n\n\n\nStep 3 - Run OLAP and OLTP queries\n\n\nAirlineDataJob.scala\n runs OLAP and OLTP queries on SnappyData tables. Also, it caches the same airline data in the Spark cache and runs the same OLAP query on the Spark cache. With the airline data set, we have seen both the Spark cache and the SnappyData store to have more and less the same performance.  \n\n\n# Submit AirlineDataJob to SnappyData's Lead node on port 8090 \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name airlineApp  --class  io.snappydata.examples.AirlineDataJob --app-jar ./examples/jars/quickstart-0.6.jar\n{ \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n1b0d2e50-42da-4fdd-9ea2-69e29ab92de2\n,\n    \ncontext\n: \nsnappyContext1453196176725064822\n\n } }\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090  --job-id 1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n{ \nduration\n: \n6.617 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.AirlineDataJob\n,\n  \nstartTime\n: \n2016-01-19T15:06:16.771+05:30\n,\n  \ncontext\n: \nsnappyContext1453196176725064822\n,\n  \nresult\n: \nSee /snappy/work/localhost-lead-1/AirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n1b0d2e50-42da-4fdd-9ea2-69e29ab92de2\n\n}\n\n\n\n\nThe output of the job can be found in \nAirlineDataJob.out\n in the lead directory which by default is \nSNAPPY_HOME/work/localhost-lead-*/\n. You can explore the Spark SQL query plan on Spark UI which by default can be seen at http://hostNameOfLead:4040.\n\n\nSynopsis Data Engine (SDE) (explanation)\n\n\nOLAP jobs are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory jobs above executed in less than a second, the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like SDE.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs to specify one or more curated \nstratified samples\n on large tables. \n\n\n\n\nNote\n\n\nWe recommend downloading the full dataset mentioned in \nStep 1\n which is about 52 million records. With the above data set (1 million rows) only about a third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.\n\n\n\n\nThe following scala code creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your \nGroup by\n and \nWhere\n clauses make up the \nQuery Column Set\n (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection. \n\n\nval sampleDF = snappyContext.createTable(sampleTable, Some(\nAirline\n),\n        \ncolumn_sample\n, // DataSource provider for sample tables\n        updatedSchema, Map(\nbuckets\n -\n \n5\n,\n          \nqcs\n -\n \nUniqueCarrier, Year_, Month_\n,\n          \nfraction\n -\n \n0.03\n,\n          \nstrataReservoirSize\n -\n \n50\n\n        ))\n\n\n\n\nYou can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the \nWith Error\n constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set. \n\n\n// Query Snappy Store's Sample table :Which Airlines arrive On schedule? JOIN with reference table\nsampleResult = sampleDF.join(airlineCodeDF,\n        sampleDF.col(\nUniqueCarrier\n).equalTo(airlineCodeDF(\nCODE\n))).\n          groupBy(sampleDF(\nUniqueCarrier\n), airlineCodeDF(\nDESCRIPTION\n)).\n          agg(\nArrDelay\n -\n \navg\n).orderBy(\navg(ArrDelay)\n)\n\n // Query Snappy Store's Airline table with error clause.\nairlineDF.groupBy(airlineDF(\nMonth_\n))\n  .agg(\nArrDelay\n -\n \navg\n)\n  .orderBy(\nMonth_\n).withError(0.05,0.95)\n\n\n\n\nStep 4 - Create, Load and Query Sample Table\n\n\nCreateAndLoadAirlineDataJob\n and \nAirlineDataJob\n executed in the previous sections created the sample tables and executed OLAP queries over them.\n\n\nStream analytics using Spark Streaming (explanation)\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and streams can be analyzed using SQL. Also, SnappyData introduces \"continuous queries\" (CQ) on the stream. One can define a continous query as a SQL query on the stream with window and slide extensions which is returned as SchemaDStream i.e. DStream with schema. SnappyData's extensions provide functionality to insert a SchemaDStream into the SnappyData store. \n\n\nDynamic registration of CQs (from remote clients) will be available in the next release.\n\n\n// create a stream table declaratively \nsnsc.sql(\nCREATE STREAM TABLE RETWEETTABLE (retweetId long, \n +\n    \nretweetCnt int, retweetTxt string) USING file_stream \n +\n    \nOPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2', \n +\n    \nrowConverter 'org.apache.spark.sql.streaming.TweetToRetweetRow',\n +\n    \ndirectory '/tmp/copiedtwitterdata')\n);\n\n// Register a continous query on the stream table with window and slide parameters\nval retweetStream: SchemaDStream = snsc.registerCQ(\nSELECT retweetId, retweetCnt FROM RETWEETTABLE \n +\n    \nwindow (duration '2' seconds, slide '2' seconds)\n)\n\n// Create a row table to hold the retweets based on their id \nsnsc.snappyContext.sql(s\nCREATE TABLE $tableName (retweetId bigint PRIMARY KEY, \n +\n    s\nretweetCnt int, retweetTxt string) USING row OPTIONS ()\n)\n\n// Iterate over the stream and insert it into snappy store\nretweetStream.foreachDataFrame(df =\n {\n    df.write.insertInto(tableName)\n})\n\n\n\n\nTop-K Elements in a Stream (explanation)\n\n\nContinuously finding the \nk\n most popular elements in a data stream is a common analytic query. SnappyData provides extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds a temporal component (i.e. data can be queried based on a time interval) to these structures. More details about SnappyData's implementation of top-k can be found here:\n\n\nSnappyData's SDE Docs\n\n\nSnappyData provides an API in the \nSnappyContext\n to create a Top-k structure. And, if a stream table is specified as the base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using another API. \n\n\n--- Create a topk table from a stream table\nsnappyContext.createApproxTSTopK(\ntopktable\n, Some(\nhashtagTable\n), \nhashtag\n,\n    Some(schema), Map(\n      \nepoch\n -\n System.currentTimeMillis().toString,\n      \ntimeInterval\n -\n \n2000ms\n,\n      \nsize\n -\n \n10\n\n    ))\n--- Query a topk table for the last two seconds\nval topKDF = snappyContext.queryApproxTSTopK(\ntopktable\n,\n                System.currentTimeMillis - 2000,\n                System.currentTimeMillis)\n\n\n\n\nStep 5 -  Create and Query Stream Table and Top-K\n\n\nIdeally, we would like you to try this example using live twitter stream. For that, you would have to generate authorization keys and secrets on twitter apps. Alternatively, you can use use file stream scripts that simulate the twitter stream by copying pre-loaded tweets in a tmp folder.\n\n\nSteps to work with simulated Twitter stream\n\n\nSubmit the \nTwitterPopularTagsJob\n that declares a stream table, creates and populates a topk-structure, registers a CQ on it and stores the result in the Snappy-store. It starts streaming and waits for two minutes. \n\n\n# Submit the TwitterPopularTagsJob to SnappyData's Lead node on port 8090 \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar ./examples/jars/quickstart-0.6.jar --stream\n\n# Run the following utility in another terminal to simulate a twitter stream by copying tweets in the folder on which file stream table is listening.\n$ quickstart/scripts/simulateTwitterStream \n\n\n\n\n\nSteps to work with live Twitter stream\n\n\n# Set the keys and secrets to fetch the live twitter stream\n# Note: Currently, we do not encrypt the keys. \n$ export APP_PROPS=\nconsumerKey=\nconsumerKey\n,consumerSecret=\nconsumerSecret\n,accessToken=\naccessToken\n,accessTokenSecret=\naccessTokenSecret\n\n\n# submit the TwitterPopularTagsJob Lead node on port 8090 that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a snappy store table \n# This job runs streaming for two minutes. \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar ./examples/jars/quickstart-0.6.jar --stream\n\n\n\n\n\nThe output of the job can be found in \nTwitterPopularTagsJob_timestamp.out\n in the lead directory which by default is \nSNAPPY_HOME/work/localhost-lead-*/\n. \n\n\nWorking with Spark shell and spark-submit\n\n\nSnappyData, out-of-the-box, collocates Spark executors and the SnappyData store for efficient data intensive computations. But it may be desirable to isolate the computational cluster for other reasons, for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. \n\n\n# Start the spark shell in local mode. Pass SnappyData's locators host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=localhost:10334 --conf spark.ui.port=4041\nscala\n\nTry few commands on the spark-shell \n\n# fetch the tables and using sqlContext which is going to be an instance of SnappyContext in this case\nscala\n val airlinerefDF = sqlContext.table(\nairlineref\n).show\nscala\n val airlineDF = sqlContext.table(\nairline\n).show\n\n# you can now work with the dataframes to fetch the data.\n\n\n\n\nStep 6 - Submit a Scala or Java Spark App that interacts with SnappyData\n\n\n# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=localhost:10334 --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart-0.6.jar\n\n# The results can be seen on the command line. \n\n\n\n\nStep 7 - Submit a Python Spark App that interacts with SnappyData\n\n\n# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataPythonApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --master spark://masterhost:7077 --conf snappydata.store.locators=localhost:10334 --conf spark.ui.port=4041 $SNAPPY_HOME/quickstart/python/AirlineDataPythonApp.py\n\n\n\n\n\nFinal Step - Stop the SnappyData Cluster\n\n\n$ sbin/snappy-stop-all.sh \nlocalhost: The SnappyData Leader has stopped.\nlocalhost: The SnappyData Server has stopped.\nlocalhost: The SnappyData Locator has stopped.\n\n\n\n\nWe also have an \nAd Analytics code example\n and associated \nscreencast\n that showcases many useful features of SnappyData.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#introduction", 
            "text": "SnappyData is a  distributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP (online transaction processing) and OLAP (online analytical processing) in a single integrated cluster . We realize this platform through a seamless integration of Apache Spark (as a big data computational engine) with GemFire XD (as an in-memory transactional store with scale-out SQL semantics).", 
            "title": "Introduction"
        }, 
        {
            "location": "/#download-binary-distribution", 
            "text": "You can download the latest versions of SnappyData here:   SnappyData 0.6 download link  (tar.gz)   (zip)  SnappyData 0.6(hadoop provided) download link  (tar.gz)   (zip)   SnappyData has been tested on Linux and Mac OSX. If not already installed, you will need to download  Java 8 .", 
            "title": "Download binary distribution"
        }, 
        {
            "location": "/#community-support", 
            "text": "We monitor channels listed below for comments/questions.  Stackoverflow        Slack         Gitter             IRC                 Reddit             JIRA", 
            "title": "Community Support"
        }, 
        {
            "location": "/#link-with-snappydata-distribution", 
            "text": "SnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:  groupId: io.snappydata\nartifactId: snappydata-core_2.11\nversion: 0.6\n\ngroupId: io.snappydata\nartifactId: snappydata-cluster_2.11\nversion: 0.6", 
            "title": "Link with SnappyData distribution"
        }, 
        {
            "location": "/#working-with-snappydata-source-code", 
            "text": "If you are interested in working with the latest code or contributing to SnappyData development, you can also check out the master branch from Git:  Master development branch\ngit clone https://github.com/SnappyDataInc/snappydata.git --recursive\n\n###### 0.6 release branch with stability and other fixes ######\ngit clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.6 --recursive", 
            "title": "Working with SnappyData Source Code"
        }, 
        {
            "location": "/#building-snappydata-from-source", 
            "text": "You will find the instructions for building, layout of the code, integration with IDEs using Gradle, etc here:   SnappyData Build Instructions    NOTE:\nSnappyData is built using Spark 1.6 (build xx) which is packaged as part of SnappyData. While you can build your application using Apache Spark 1.5, you will need to link to Snappy-spark to make  use of the SnappyData extensions. Gradle build tasks are packaged.", 
            "title": "Building SnappyData from source"
        }, 
        {
            "location": "/#setting-up-passwordless-ssh", 
            "text": "The quick start scripts use ssh to start up various processes. You can install ssh on ubuntu with  sudo apt-get install ssh . ssh comes packaged with Mac OSX, however, make sure ssh is enabled by going to  System Preferences -  Sharing  and enabling  Remote Login . Enabling passwordless ssh is the easiest way to work with SnappyData and prevents you from having to put in your ssh password multiple times.   Generate an RSA key with   ssh-keygen -t rsa  This will spawn some prompts. If you want truly passwordless ssh, just press enter instead of entering a password.  Finally, copy your key to authorized keys:  cat ~/.ssh/id_rsa.pub   ~/.ssh/authorized_keys  More detail on passwordless ssh can be found  here  and  here .", 
            "title": "Setting up passwordless SSH"
        }, 
        {
            "location": "/#key-features", 
            "text": "100% compatible with Spark : Use SnappyData as a database as well as use any of the Spark APIs - ML, Graph, etc. on the same data  In-memory row and column store : Run the store collocated in Spark executors (i.e. a single compute and data cluster) or in its own process space (i.e. separate compute and data cluster)  SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.  SQL based extensions for streaming processing : Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.  Interactive analytics using Synopsis Data Engine (SDE) : We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and are completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions.   Mutate, transact on data in Spark : Use SQL to insert, update, delete data in tables. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData are visible as DataFrames without having to maintain multiples copies of your data.   Optimizations : Use indexes to improve query performance in the row store (the GemFire SQL optimizer automatically uses in-memory indexes when available)   High availability not just Fault tolerance : Data is instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications with continuous HA.  Durability and recovery:  Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.    Read SnappyData  docs  for a more detailed list of all features and semantics.", 
            "title": "Key Features"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Each header under \"Getting Started\" that contains a conceptual explanation meant to orient you will append an \"(explanation)\". Each header that contains actual steps for executing the \"Getting Started\" will prepend \"Step\".", 
            "title": "Getting started"
        }, 
        {
            "location": "/#objectives", 
            "text": "In-memory Column and Row tables : Illustrate both SQL syntax and the Spark API to create and manage column tables for large data and illustrate how row tables can be used for reference data and can be replicated to each node in the cluster.   OLAP, OLTP operations : We run analytic class SQL queries (full scan with aggregations) on column tables and fully distributed join queries and observe the space requirements as well as the performance of these queries. For OLTP, we run simple update queries - you can note the Spark API extensions to support mutations in Spark.   SDE : We run the same analytic queries by creating adjunct stratified samples to note the performance difference - can we get close to interactive query performance speeds?  Streaming with SQL : We ingest twitter streams into both a probabilistic data structure for TopK time series analytics and the entire stream (full data set) into a row table. We run both ad-hoc queries on these streams (modeled as tables) as well as showcase our first preview for continuous querying support. What SnappyData demonstrates here is simpler, SQL centric abstractions on top of Spark streaming. And, of course, ingestion into the built-in store.   In this document, we discuss the features mentioned above and ask you to take steps to run the scripts that demonstrate these features.", 
            "title": "Objectives"
        }, 
        {
            "location": "/#snappydata-cluster-explanation", 
            "text": "SnappyData, a database server cluster, has three main components - Locator, Server and Lead.    Locator : Provides discovery service for the cluster. Informs a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.  Lead Node : Acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.  Data Servers : Hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.    Details of about the architecture can be found here:  Architecture    SnappyData also has multiple deployment options which can be found here:  Deployment Options .", 
            "title": "SnappyData Cluster (explanation)"
        }, 
        {
            "location": "/#step-1-start-the-snappydata-cluster", 
            "text": "Click the screenshot below to view the screencast that goes along with \"Getting Started with SQL\"   The first step is to configure SNAPPY_HOME in your environment:  export SNAPPY_HOME=/path/to/snappy/root  The remainder of \"Getting Started\" is based on a set of  airline data  we run different queries on. That data is packaged with SnappyData, however, it is only a portion of the full dataset. To download the full dataset, from  /snappy/  run  ./quickstart/scripts/download_full_airlinedata.sh ./quickstart/data . This is recommended for the Synopsis Data Engine portion of \"Getting Started,\" but not necessary. Note that to run the above script, you need curl installed.  Here  is one way to install it on ubuntu.", 
            "title": "Step 1 - Start the SnappyData cluster"
        }, 
        {
            "location": "/#passwordless-ssh", 
            "text": "The quick start scripts use ssh to start up various processes. You can install ssh on ubuntu with  sudo apt-get install ssh . By default, ssh requires a password. To be able to log on to the localhost and run the script without being prompted for the password, please enable  passwordless ssh . Otherwise, set up ssh for localhost with  ssh localhost  Navigate to the /snappy/ root directory. The start script starts up a minimal set of essential components to form the cluster - A locator, one data server and one lead node. All nodes are started locally.  Run the start script with:    ./sbin/snappy-start-all.sh  This may take 30 seconds or more to bootstrap the entire cluster on your local machine (logs are in the 'work' sub-directory). The output should look something like this \u2026  $ sbin/snappy-start-all.sh \nlocalhost: Starting SnappyData Locator using peer discovery on: 0.0.0.0[10334]\n...\nlocalhost: SnappyData Locator pid: 56703 status: running\n\nlocalhost: Starting SnappyData Server using locators for peer discovery: jramnara-mbpro[10334]   (port used for members to form a p2p cluster)\nlocalhost: SnappyData Server pid: 56819 status: running\nlocalhost:   Distributed system now has 2 members.\n\nlocalhost: Starting SnappyData Leader using locators for peer discovery: jramnara-mbpro[10334]\nlocalhost: SnappyData Leader pid: 56932 status: running\nlocalhost:   Distributed system now has 3 members.\n\nlocalhost:   Other members: jramnara-mbpro(56703:locator) v0 :54414, jramnara-mbpro(56819:datastore) v1 :39737  To spin up remote nodes simply rename/copy the files without the template suffix and add the hostnames. View custom configuration and startup options here:  Custom Configuration  At this point, the SnappyData cluster is up and running and is ready to accept jobs and SQL requests via JDBC/ODBC. You can  monitor the Spark cluster at port 4040 . Once you load data and run queries, you can analyze the Spark SQL query plan, the job execution stages and storage details of column tables.", 
            "title": "Passwordless ssh"
        }, 
        {
            "location": "/#interacting-with-snappydata-explanation", 
            "text": "For the section on the Spark API, we assume some familiarity with  core Spark, Spark SQL and Spark Streaming concepts . \nAnd, you can try out the Spark  Quick Start . All the commands and programs\nlisted in the Spark guides will work in SnappyData also. For the section on SQL, no Spark knowledge is necessary.   To interact with SnappyData, we provide interfaces for developers familiar with Spark programming as well as SQL. JDBC can be used to connect to the SnappyData cluster and interact using SQL. On the other hand, users comfortable with the Spark programming paradigm can write jobs to interact with SnappyData. Jobs can be like a self contained Spark application or can share state with other jobs using the SnappyData store.   Unlike Apache Spark, which is primarily a computational engine, the SnappyData cluster holds mutable database state in its JVMs and requires all submitted Spark jobs/queries to share the same state (of course, with schema isolation and security as expected in a database). This required extending Spark in two fundamental ways.   Long running executors : Executors are running within the SnappyData store JVMs and form a p2p cluster.  Unlike Spark, the application Job is decoupled from the executors - submission of a job does not trigger launching of new executors.   Driver runs in HA configuration : Assignment of tasks to these executors are managed by the Spark Driver.  When a driver fails, this can result in the executors getting shutdown, taking down all cached state with it. Instead, we leverage the  Spark JobServer  to manage Jobs and queries within a \"lead\" node.  Multiple such leads can be started and provide HA (they automatically participate in the SnappyData cluster enabling HA). \nRead our  docs  for details on the architecture.   In this document, we showcase mostly the same set of features via the Spark API or using SQL. If you are familiar with Scala and understand Spark concepts you may choose to skip the SQL part go directly to the Spark API section:  Getting Started with Spark API .", 
            "title": "Interacting with SnappyData (explanation)"
        }, 
        {
            "location": "/#getting-started-with-sql", 
            "text": "For SQL, the SnappyData SQL Shell ( snappy-shell ) provides a simple way to inspect the catalog,  run admin operations,  manage the schema and run interactive queries. You can also use your favorite SQL tool like SquirrelSQL or DBVisualizer (a JDBC connection to the cluster).  From the SnappyData base directory, /snappy/, run:   ./bin/snappy-shell  Connect to the cluster with  snappy  connect client 'localhost:1527';  You can view connections with  snappy  show connections;  And check member status with:  snappy  show members;", 
            "title": "Getting Started with SQL"
        }, 
        {
            "location": "/#column-and-row-tables-explanation", 
            "text": "Column tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  -- DDL to create a column table\nCREATE TABLE AIRLINE ( column definitions ) USING column OPTIONS(buckets '5') ;  Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys. A row's location is determined by a hash function and hence very fast for point lookups or updates.  create table  DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.    -- DDL to create a row table\nCREATE TABLE AIRLINEREF ( column definitions ) USING row OPTIONS() ;  Read our preliminary  row   column table docs  for the details.", 
            "title": "Column and Row tables (explanation)"
        }, 
        {
            "location": "/#step-2-create-column-table-row-table-and-load-data", 
            "text": "The below SQL scripts create and populate the tables we need to continue. The displayed command assumes you started the snappy-shell the base directory, /snappy/. If you started the snappy-shell from /bin/, for example, you need to prepend the filepath with /..  To create and load a column table:  snappy  run './quickstart/scripts/create_and_load_column_table.sql';  To create and load a row table:  snappy  run './quickstart/scripts/create_and_load_row_table.sql';  To see the status of the system:  snappy  run './quickstart/scripts/status_queries.sql'  \nYou can see the memory consumed in the Spark Console, http:// the-lead-host-name `:4040/Snappy Store.", 
            "title": "Step 2 - Create column table, row table and load data"
        }, 
        {
            "location": "/#olap-and-oltp-queries-explanation", 
            "text": "SQL client connections (via JDBC or ODBC) are routed to the appropriate data server via the locator (Physical connections are automatically created in the driver and are transparently swizzled in case of failures also). When queries are executed they are parsed initially by the SnappyData server to determine if the query is an OLAP class or an OLTP class query.  Currently, all column table queries are considered OLAP.  Such queries are routed to the  lead  node where a  Spark SQLContext  is managed for each connection. The query is planned using Spark's Catalyst engine and scheduled to be executed on the data servers. The number of partitions determine the number of concurrent tasks used across the data servers to run the query in parallel. In this case, our column table was created using  5 partitions (buckets)  and hence will use 5 concurrent tasks.   ---- Which Airlines Arrive On Schedule? JOIN with reference table ----\nsnappy  select AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline, airlineref\n  where airline.UniqueCarrier = airlineref.Code\n  group by UniqueCarrier, description \n  order by arrivalDelay;  For low latency OLTP queries, the engine won't route it to the lead and instead execute it immediately without any scheduling overhead. Quite often, this may mean simply fetching a row by hashing a key (in microseconds).  --- Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'\n--- the airline code can be updated in the row table\nUPDATE AIRLINEREF SET DESCRIPTION='Delta America' WHERE CAST(CODE AS VARCHAR(25))='DL';  Spark SQL can cache DataFrames as temporary tables and the data set is immutable. SnappyData SQL is compatible with the SQL standard with support for transactions and DML (insert, update, delete) on tables.  Link to Snappy Store SQL reference .  As we show later, any table in SnappyData is also visible as a Spark DataFrame.", 
            "title": "OLAP and OLTP queries (explanation)"
        }, 
        {
            "location": "/#step-3-run-olap-and-oltp-queries", 
            "text": "The OLAP query we're executing is asking \"which airlines arrive on schedule?\" which requires a join to a reference table. You have the option to run the packaged query script or paste one query at a time. To run the script, in the snappy-shell paste:  snappy  run './quickstart/scripts/olap_queries.sql';  To paste the actual query, paste:  select AVG(ArrDelay) arrivalDelay, description AirlineName, UniqueCarrier carrier \n  from airline, airlineref\n  where airline.UniqueCarrier = airlineref.Code\n  group by UniqueCarrier, description \n  order by arrivalDelay;  Each query is executed as one or more Jobs and each Job executed in one or more stages. You can explore the query execution plan and metrics  the Spark Console  under \"SQL.\"  The OLTP query we're executing is updating \"Delta Airlines\" to \"Delta America.\" Paste the following command:  snappy  run './quickstart/scripts/oltp_queries.sql';  You can now re-run olap_queries.sql to see the updated join result set.   Note \nIn the current implementation we only support appending to Column tables. Future releases will support all DML operations. \nYou can execute transactions using commands  autocommit off  and  commit .", 
            "title": "Step 3 - Run OLAP and OLTP queries"
        }, 
        {
            "location": "/#synopsis-data-engine-sde-explanation", 
            "text": "If you downloaded the full airline data (52 million records) set in  Step 1 , edit the  'create_and_load_column_table.sql'  script which is in  quickstart/scripts  to point to  airlineParquetData_2007-15  directory. Make sure you copy + paste the starting quote mark to the end after you change  airlineParquetData  to enclose the statement. If you enter a quote mark directly from your keyboard it may break the script.  This script loads parquet formatted data into a temporary spark table then saves it in column table called Airline. You need to load the table again using  run './quickstart/scripts/create_and_load_column_table.sql';  Ideally, you'd re-run the olap queries script as well to see the speedup between non-SDE and SDE.    OLAP queries are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory queries above executed in less than a second the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like SDE.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs and DDL to specify one or more curated  stratified samples  on large tables.   The following DDL creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your  Group by  and  Where  clauses make up the  Query Column Set  (strata columns). Multiple samples can be created and queries that are executed on the base table are analyzed for appropriate sample selection.   CREATE SAMPLE TABLE AIRLINE_SAMPLE\n   ON AIRLINE                             -- The parent base table\n   OPTIONS(\n    buckets '5',                          -- Number of partitions \n    qcs 'UniqueCarrier, Year_, Month_',   -- QueryColumnSet(qcs): The strata - 3% of each combination of Carrier, \n                                          -- Year and Month are stored as sample\n    fraction '0.03',                      -- How big should the sample be\n    strataReservoirSize '50'              -- Reservoir sampling to support streaming inserts  You can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the  With Error  constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set.   -- What is the average arrival delay for all airlines for each month?;\nsnappy  select avg(ArrDelay), Month_ from Airline where ArrDelay  0 \n    group by Month_\n    with error;\n-- The above query will consult the sample and return an answer if the estimated answer \n-- is at least 90% accurate (here, by default we use a 95% confidence interval).\n\n-- You can also access the error using built-in functions. \nsnappy  select avg(ArrDelay) avgDelay, absolute_error(avgDelay), Month_ \n    from Airline where ArrDelay  0 \n    group by Month_\n    with error;", 
            "title": "Synopsis Data Engine (SDE) (explanation)"
        }, 
        {
            "location": "/#step-4-create-load-and-query-sample-table", 
            "text": "Like Step 3, we need to first create and load the sample table. The following script will sample the Airline table:  snappy  run './quickstart/scripts/create_and_load_sample_table.sql';  You can now re-run the previous OLAP queries with an error constraint and compare the results. The following script will run the previous queries with an error constraint:  snappy  run './quickstart/scripts/olap_approx_queries.sql';  You should notice a 10X or larger difference in query execution latency while the results remain nearly as accurate. As a reminder, we recommend downloading the larger data set for this exercise.", 
            "title": "Step 4 - Create, Load and Query Sample Table"
        }, 
        {
            "location": "/#stream-analytics-using-sql-and-spark-streaming-explanation", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and you can analyze these streams using SQL. You can also dynamically run SQL queries on these streams. There is no need to learn Spark streaming APIs or statically define all the rules to be executed on these streams.  The commands below consume tweets, then they filter out just the hashtags and then they convert hashtags into Row objects. The commands then model the stream as a table (so it can be queried) and we then run ad-hoc SQL from remote clients on the current state of the stream.   --- Inits the Streaming Context with the batch interval of 2 seconds.\n--- i.e. the stream is processed once every 2 seconds.\nsnappy  STREAMING INIT 2 SECS;\n--- Create a stream table just containing the hashtags.\n--- /tmp/copiedtwitterdata is the directory that Streaming will use to find and read new text files.\n--- We use quickstart/scripts/simulateTwitterStream script in below example to simulate a twitter stream by\n--- copying tweets in /tmp/copiedtwitterdata folder.\nsnappy  CREATE STREAM TABLE HASHTAG_FILESTREAMTABLE\n              (hashtag string)\n            USING file_stream\n            OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',\n              rowConverter 'org.apache.spark.sql.streaming.TweetToHashtagRow',\n              directory '/tmp/copiedtwitterdata')\n--- A file_stream data source monitors the directory and as files arrives they are ingested\n--- into the streaming pipeline. First converted into Rows using 'TweetToHashtagRow' then visible as table\n--- Start streaming context \nsnappy  STREAMING START\n--- Adhoc sql on the stream table to query the current batch\n--- Get top 10 popular hashtags ----\nsnappy  SELECT hashtag, count(*) as tagcount\n        FROM HASHTAG_FILESTREAMTABLE\n        GROUP BY hashtag\n        ORDER BY tagcount DESC limit 10;  Later, in the Spark API section we further enhance this concept to showcase  \"continuous queries\" (CQ) . Dynamic registration of CQs (from remote clients) will be available in the next release.", 
            "title": "Stream analytics using SQL and Spark Streaming (explanation)"
        }, 
        {
            "location": "/#top-k-elements-in-a-stream-explanation", 
            "text": "Finding the  k  most popular elements in a data stream is a common analytic query. For instance, the top-100 pages on a popular website in the last 10 mins, the top-10 sales regions in the last week, etc. As you can see, if the query is on an arbitrary time interval in the past, this will most likely mandate storing the entire stream. And, this could easily be millions to billions of events in use cases in the Internet of Things, for example. SnappyData provides SQL extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds a temporal component (i.e. data can be queried based on a time interval) to these structures and enables transparent querying using Spark SQL. More details about SnappyData's implementation of top-k can be found here:  Detailed SDE Documentation  SnappyData provides DDL extensions to create Top-k structures. And, if a stream table is specified as base table, the Top-k structure is automatically populated from it as data arrives. The Top-k structures can be queried using regular SQL queries.   --- Create a topk table from a stream table\nCREATE TOPK TABLE filestream_topktable ON HASHTAG_FILESTREAMTABLE OPTIONS\n(key 'hashtag', timeInterval '2000ms', size '10' );\n--- Query a topk table \nSELECT hashtag, COUNT(hashtag) AS TopKCount\nFROM filestream_topktable\nGROUP BY hashtag ORDER BY TopKCount limit 10;  Now, lets try analyzing some tweets using this above syntax in real time using SnappyData's packaged scripts.", 
            "title": "Top-K Elements in a Stream (explanation)"
        }, 
        {
            "location": "/#step-5-create-and-query-stream-table-and-top-k-declaratively", 
            "text": "You can use the scripts that simulates the twitter stream by copying pre-loaded tweets in a tmp folder. Or, you could use a script that accesses the live twitter stream.", 
            "title": "Step 5 - Create and Query Stream Table and Top-K Declaratively"
        }, 
        {
            "location": "/#steps-to-work-with-simulated-twitter-stream", 
            "text": "Create a file stream table that listens on a folder and then start the streaming context. In the snappy-shell, paste:  snappy  run './quickstart/scripts/create_and_start_file_streaming.sql';  Run the following utility in another terminal that simulates a twitter stream by copying tweets in the folder on which file stream table is listening.  $ quickstart/scripts/simulateTwitterStream   Now query the current batch of the stream using the following script.   --- Run this command multiple times to query current batch at different times \nrun './quickstart/scripts/file_streaming_query.sql';\n--- Stop the streaming \nsnappy  STREAMING STOP;  This also creates Topk structures. simulateTwitterStream script runs for only for a minute or so.", 
            "title": "Steps to work with simulated Twitter stream"
        }, 
        {
            "location": "/#steps-to-work-with-live-twitter-stream", 
            "text": "To work with the live twitter stream, you will have to generate authorization keys and secrets on  twitter apps  and update  SNAPPY_HOME/quickstart/scripts/create_and_start_twitter_streaming.sql  with the keys and secrets.  --- Run the create and start script that has keys and secrets to fetch live twitter stream\n--- Note: Currently, we do not encrypt the keys. \n-- This also creates Topk structures\nsnappy  run './quickstart/scripts/create_and_start_twitter_streaming.sql';  Now query the current batch of the stream using the following script.   --- Run this command multiple times to query current batch at different times \nsnappy  run './quickstart/scripts/twitter_streaming_query.sql';\n--- Stop the streaming \nsnappy  STREAMING STOP;  We also have an  Ad Analytics code example  and associated  screencast  that showcases many useful features of SnappyData.", 
            "title": "Steps to work with live Twitter stream"
        }, 
        {
            "location": "/#getting-started-with-spark-api", 
            "text": "SnappyContext  is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's  SQLContext  to work with Row and Column tables. Any DataFrame can be managed as a SnappyData table and any table can be accessed as a DataFrame. This is similar to  HiveContext  and it integrates the SQLContext functionality with the SnappyData store. Similarly,  SnappyStreamingContext  is an entry point for SnappyData extensions to Spark Streaming and it extends Spark's  Streaming Context .   Applications typically submit Jobs to SnappyData and do not explicitly create a SnappyContext or SnappyStreamingContext. These jobs are the primary mechanism to interact with SnappyData using the Spark API. A job implements either SnappySQLJob or SnappyStreamingJob (for streaming applications) trait.   class SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runJob(snc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def validate(sc: SnappyContext, config: Config): SparkJobValidation\n}  The implementation of the  runJob  function from SnappySQLJob uses a SnappyContext to interact with the SnappyData store to process and store tables. The implementation of runJob from SnappyStreamingJob uses a SnappyStreamingContext to create streams and manage the streaming context. The jobs are submitted to the lead node of SnappyData over REST API using a  spark-submit  like utility. See more details about jobs here:  SnappyData Jobs", 
            "title": "Getting Started with Spark API"
        }, 
        {
            "location": "/#column-and-row-tables-explanation_1", 
            "text": "Column tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  // creating a column table in Snappy job\nsnappyContext.createTable( AIRLINE ,  column , schema, Map( buckets  -   5 ))  Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys. A row's location is determined by a hash function and hence very fast for point lookups or updates.  create table  DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk , overflow to disk, be replicated for HA, etc.    // creating a row table in Snappy job\nval airlineCodeDF = snappyContext.createTable( AIRLINEREF ,  row , schema, Map())  Read our preliminary  row   column table docs  for the details", 
            "title": "Column and Row tables (explanation)"
        }, 
        {
            "location": "/#step-2-create-column-table-row-table-and-load-data_1", 
            "text": "If you downloaded the full airline data set in  Step 1 , set the following config parameter to point at the data set. export APP_PROPS=\"airline_file=/path/to/full/airline/dataset\"   Submit  CreateAndLoadAirlineDataJob  over the REST API to create row and column tables. See more details about jobs and job submission here:  SnappyData jobs   job submission .   # Submit a job to Lead node on port 8090 \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name airlineApp --class  io.snappydata.examples.CreateAndLoadAirlineDataJob --app-jar ./examples/jars/quickstart-0.6.jar\n{ status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  } }\n\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090 --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n{  duration :  17.53 secs ,\n   classPath :  io.snappydata.examples.CreateAndLoadAirlineDataJob ,\n   startTime :  2016-01-12T16:59:14.746+05:30 ,\n   context :  snappyContext1452598154529305363 ,\n   result :  See /snappy/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n}\n# Tables are created  The output of the job can be found in  CreateAndLoadAirlineDataJob.out  in the lead directory which by default is  SNAPPY_HOME/work/localhost-lead-*/ . You can see the size of the column tables on Spark UI which by default can be seen at http://hostNameOfLead:4040.", 
            "title": "Step 2 - Create column table, row table and load data"
        }, 
        {
            "location": "/#olap-and-oltp-store-explanation", 
            "text": "SnappyContext  extends SQLContext and adds functionality to work with row and column tables. When queries inside jobs are executed they are parsed initially by the SnappyData server to determine if it is an OLAP class or an OLTP class query.  Currently, all column table queries are considered OLAP. Such queries are planned using Spark's Catalyst engine and scheduled to be executed on the data servers.   val resultDF = airlineDF.join(airlineCodeDF,\n        airlineDF.col( UniqueCarrier ).equalTo(airlineCodeDF( CODE ))).\n        groupBy(airlineDF( UniqueCarrier ), airlineCodeDF( DESCRIPTION )).\n        agg( ArrDelay  -   avg ).orderBy( avg(ArrDelay) )  For low latency OLTP queries in jobs, SnappyData won't schedule these queries. Instead SnappyData executes them immediately on SnappyData servers without any scheduling overhead. Quite often, this may mean simply fetching or updating a row by hashing a key (in nanoseconds).   // Suppose a particular Airline company say 'Delta Air Lines Inc.' re-brands itself as 'Delta America'. Update the row table.\nval filterExpr: String =   CODE ='DL' \nval newColumnValues: Row = Row( Delta America )\nval updateColumns =  DESCRIPTION \nsnappyContext.update(rowTableName, filterExpr, newColumnValues, updateColumns)", 
            "title": "OLAP and OLTP Store (explanation)"
        }, 
        {
            "location": "/#step-3-run-olap-and-oltp-queries_1", 
            "text": "AirlineDataJob.scala  runs OLAP and OLTP queries on SnappyData tables. Also, it caches the same airline data in the Spark cache and runs the same OLAP query on the Spark cache. With the airline data set, we have seen both the Spark cache and the SnappyData store to have more and less the same performance.    # Submit AirlineDataJob to SnappyData's Lead node on port 8090 \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name airlineApp  --class  io.snappydata.examples.AirlineDataJob --app-jar ./examples/jars/quickstart-0.6.jar\n{  status :  STARTED ,\n   result : {\n     jobId :  1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 ,\n     context :  snappyContext1453196176725064822 \n } }\n# A JSON with jobId of the submitted job is returned. Use job ID can be used to query the status of the running job. \n$ bin/snappy-job.sh status --lead localhost:8090  --job-id 1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n{  duration :  6.617 secs ,\n   classPath :  io.snappydata.examples.AirlineDataJob ,\n   startTime :  2016-01-19T15:06:16.771+05:30 ,\n   context :  snappyContext1453196176725064822 ,\n   result :  See /snappy/work/localhost-lead-1/AirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  1b0d2e50-42da-4fdd-9ea2-69e29ab92de2 \n}  The output of the job can be found in  AirlineDataJob.out  in the lead directory which by default is  SNAPPY_HOME/work/localhost-lead-*/ . You can explore the Spark SQL query plan on Spark UI which by default can be seen at http://hostNameOfLead:4040.", 
            "title": "Step 3 - Run OLAP and OLTP queries"
        }, 
        {
            "location": "/#synopsis-data-engine-sde-explanation_1", 
            "text": "OLAP jobs are expensive as they require traversing through large data sets and shuffling data across nodes. While the in-memory jobs above executed in less than a second, the response times typically would be much higher with very large data sets. On top of this, concurrent execution for multiple users would also slow things down. Achieving interactive query speed in most analytic environments requires drastic new approaches like SDE.\nSimilar to how indexes provide performance benefits in traditional databases, SnappyData provides APIs to specify one or more curated  stratified samples  on large tables.", 
            "title": "Synopsis Data Engine (SDE) (explanation)"
        }, 
        {
            "location": "/#note", 
            "text": "We recommend downloading the full dataset mentioned in  Step 1  which is about 52 million records. With the above data set (1 million rows) only about a third of the time is spent in query execution engine and  sampling is unlikely to show much of any difference in speed.   The following scala code creates a sample that is 3% of the full data set and stratified on 3 columns. The commonly used dimensions in your  Group by  and  Where  clauses make up the  Query Column Set  (strata columns). Multiple samples can be created and queries executed on the base table are analyzed for appropriate sample selection.   val sampleDF = snappyContext.createTable(sampleTable, Some( Airline ),\n         column_sample , // DataSource provider for sample tables\n        updatedSchema, Map( buckets  -   5 ,\n           qcs  -   UniqueCarrier, Year_, Month_ ,\n           fraction  -   0.03 ,\n           strataReservoirSize  -   50 \n        ))  You can run queries directly on the sample table (stored in columnar format) or on the base table. For base table queries you have to specify the  With Error  constraint indicating to the SnappyData Query processor that a sample can be substituted for the full data set.   // Query Snappy Store's Sample table :Which Airlines arrive On schedule? JOIN with reference table\nsampleResult = sampleDF.join(airlineCodeDF,\n        sampleDF.col( UniqueCarrier ).equalTo(airlineCodeDF( CODE ))).\n          groupBy(sampleDF( UniqueCarrier ), airlineCodeDF( DESCRIPTION )).\n          agg( ArrDelay  -   avg ).orderBy( avg(ArrDelay) )\n\n // Query Snappy Store's Airline table with error clause.\nairlineDF.groupBy(airlineDF( Month_ ))\n  .agg( ArrDelay  -   avg )\n  .orderBy( Month_ ).withError(0.05,0.95)", 
            "title": "Note"
        }, 
        {
            "location": "/#step-4-create-load-and-query-sample-table_1", 
            "text": "CreateAndLoadAirlineDataJob  and  AirlineDataJob  executed in the previous sections created the sample tables and executed OLAP queries over them.", 
            "title": "Step 4 - Create, Load and Query Sample Table"
        }, 
        {
            "location": "/#stream-analytics-using-spark-streaming-explanation", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and streams can be analyzed using SQL. Also, SnappyData introduces \"continuous queries\" (CQ) on the stream. One can define a continous query as a SQL query on the stream with window and slide extensions which is returned as SchemaDStream i.e. DStream with schema. SnappyData's extensions provide functionality to insert a SchemaDStream into the SnappyData store.   Dynamic registration of CQs (from remote clients) will be available in the next release.  // create a stream table declaratively \nsnsc.sql( CREATE STREAM TABLE RETWEETTABLE (retweetId long,   +\n     retweetCnt int, retweetTxt string) USING file_stream   +\n     OPTIONS (storagelevel 'MEMORY_AND_DISK_SER_2',   +\n     rowConverter 'org.apache.spark.sql.streaming.TweetToRetweetRow',  +\n     directory '/tmp/copiedtwitterdata') );\n\n// Register a continous query on the stream table with window and slide parameters\nval retweetStream: SchemaDStream = snsc.registerCQ( SELECT retweetId, retweetCnt FROM RETWEETTABLE   +\n     window (duration '2' seconds, slide '2' seconds) )\n\n// Create a row table to hold the retweets based on their id \nsnsc.snappyContext.sql(s CREATE TABLE $tableName (retweetId bigint PRIMARY KEY,   +\n    s retweetCnt int, retweetTxt string) USING row OPTIONS () )\n\n// Iterate over the stream and insert it into snappy store\nretweetStream.foreachDataFrame(df =  {\n    df.write.insertInto(tableName)\n})", 
            "title": "Stream analytics using Spark Streaming (explanation)"
        }, 
        {
            "location": "/#top-k-elements-in-a-stream-explanation_1", 
            "text": "Continuously finding the  k  most popular elements in a data stream is a common analytic query. SnappyData provides extensions to Spark to maintain top-k approximate structures on streams. Also, SnappyData adds a temporal component (i.e. data can be queried based on a time interval) to these structures. More details about SnappyData's implementation of top-k can be found here:  SnappyData's SDE Docs  SnappyData provides an API in the  SnappyContext  to create a Top-k structure. And, if a stream table is specified as the base table, the Top-k structure is automatically populated from it as the data arrives. The Top-k structures can be queried using another API.   --- Create a topk table from a stream table\nsnappyContext.createApproxTSTopK( topktable , Some( hashtagTable ),  hashtag ,\n    Some(schema), Map(\n       epoch  -  System.currentTimeMillis().toString,\n       timeInterval  -   2000ms ,\n       size  -   10 \n    ))\n--- Query a topk table for the last two seconds\nval topKDF = snappyContext.queryApproxTSTopK( topktable ,\n                System.currentTimeMillis - 2000,\n                System.currentTimeMillis)", 
            "title": "Top-K Elements in a Stream (explanation)"
        }, 
        {
            "location": "/#step-5-create-and-query-stream-table-and-top-k", 
            "text": "Ideally, we would like you to try this example using live twitter stream. For that, you would have to generate authorization keys and secrets on twitter apps. Alternatively, you can use use file stream scripts that simulate the twitter stream by copying pre-loaded tweets in a tmp folder.", 
            "title": "Step 5 -  Create and Query Stream Table and Top-K"
        }, 
        {
            "location": "/#steps-to-work-with-simulated-twitter-stream_1", 
            "text": "Submit the  TwitterPopularTagsJob  that declares a stream table, creates and populates a topk-structure, registers a CQ on it and stores the result in the Snappy-store. It starts streaming and waits for two minutes.   # Submit the TwitterPopularTagsJob to SnappyData's Lead node on port 8090 \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar ./examples/jars/quickstart-0.6.jar --stream\n\n# Run the following utility in another terminal to simulate a twitter stream by copying tweets in the folder on which file stream table is listening.\n$ quickstart/scripts/simulateTwitterStream", 
            "title": "Steps to work with simulated Twitter stream"
        }, 
        {
            "location": "/#steps-to-work-with-live-twitter-stream_1", 
            "text": "# Set the keys and secrets to fetch the live twitter stream\n# Note: Currently, we do not encrypt the keys. \n$ export APP_PROPS= consumerKey= consumerKey ,consumerSecret= consumerSecret ,accessToken= accessToken ,accessTokenSecret= accessTokenSecret \n\n# submit the TwitterPopularTagsJob Lead node on port 8090 that declares a stream table, creates and populates a topk -structure, registers CQ on it and stores the result in a snappy store table \n# This job runs streaming for two minutes. \n$ ./bin/snappy-job.sh submit --lead localhost:8090 --app-name TwitterPopularTagsJob --class io.snappydata.examples.TwitterPopularTagsJob --app-jar ./examples/jars/quickstart-0.6.jar --stream  The output of the job can be found in  TwitterPopularTagsJob_timestamp.out  in the lead directory which by default is  SNAPPY_HOME/work/localhost-lead-*/ .", 
            "title": "Steps to work with live Twitter stream"
        }, 
        {
            "location": "/#working-with-spark-shell-and-spark-submit", 
            "text": "SnappyData, out-of-the-box, collocates Spark executors and the SnappyData store for efficient data intensive computations. But it may be desirable to isolate the computational cluster for other reasons, for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion.   # Start the spark shell in local mode. Pass SnappyData's locators host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=localhost:10334 --conf spark.ui.port=4041\nscala \nTry few commands on the spark-shell \n\n# fetch the tables and using sqlContext which is going to be an instance of SnappyContext in this case\nscala  val airlinerefDF = sqlContext.table( airlineref ).show\nscala  val airlineDF = sqlContext.table( airline ).show\n\n# you can now work with the dataframes to fetch the data.", 
            "title": "Working with Spark shell and spark-submit"
        }, 
        {
            "location": "/#step-6-submit-a-scala-or-java-spark-app-that-interacts-with-snappydata", 
            "text": "# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=localhost:10334 --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart-0.6.jar\n\n# The results can be seen on the command line.", 
            "title": "Step 6 - Submit a Scala or Java Spark App that interacts with SnappyData"
        }, 
        {
            "location": "/#step-7-submit-a-python-spark-app-that-interacts-with-snappydata", 
            "text": "# Start the Spark standalone cluster.\n$ sbin/start-all.sh \n# Submit AirlineDataPythonApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --master spark://masterhost:7077 --conf snappydata.store.locators=localhost:10334 --conf spark.ui.port=4041 $SNAPPY_HOME/quickstart/python/AirlineDataPythonApp.py", 
            "title": "Step 7 - Submit a Python Spark App that interacts with SnappyData"
        }, 
        {
            "location": "/#final-step-stop-the-snappydata-cluster", 
            "text": "$ sbin/snappy-stop-all.sh \nlocalhost: The SnappyData Leader has stopped.\nlocalhost: The SnappyData Server has stopped.\nlocalhost: The SnappyData Locator has stopped.  We also have an  Ad Analytics code example  and associated  screencast  that showcases many useful features of SnappyData.", 
            "title": "Final Step - Stop the SnappyData Cluster"
        }, 
        {
            "location": "/build-instructions/", 
            "text": "Build Quickstart\n\n\nBuilding SnappyData requires JDK 7+ installation (\nOracle Java SE\n). Quickstart to build all components of snappydata:\n\n\nLatest release branch\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.6 --recursive\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nMaster\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git --recursive\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nThe product will be in \nbuild-artifacts/scala-2.11/snappy\n\n\nIf you want to build only the top-level snappydata project but pull in jars for other projects (\nspark\n, \nstore\n, \nspark-jobserver\n):\n\n\nLatest release branch\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.6\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nMaster\n\n\n git clone https://github.com/SnappyDataInc/snappydata.git\n\n cd snappydata\n\n ./gradlew product\n\n\n\n\nRepository layout\n\n\n\n\n\n\ncore\n - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between \nspark\n and \nstore\n (GemFireXD). For example: SnappyContext, row and column store, streaming additions etc.\n\n\n\n\n\n\ncluster\n - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc.\n\n\n\n\n\n\nThis component depends on \ncore\n and \nstore\n. Code in \ncluster\n depends on \ncore\n but not the other way round.\n\n\n\n\n\n\nspark\n - \nApache Spark\n code with SnappyData enhancements.\n\n\n\n\n\n\nstore\n - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch.\n\n\n\n\n\n\nspark-jobserver\n - Fork of \nspark-jobserver\n project with some additions to integrate with SnappyData.\n\n\n\n\n\n\nThe \nspark\n, \nstore\n and \nspark-jobserver\n directories are required to be clones of the respective SnappyData repositories, and are integrated in the top-level snappydata project as git submodules. When working with submodules, updating the repositories follows the normal \ngit submodules\n. One can add some aliases in gitconfig to aid pull/push like:\n\n\n[alias]\n  spull = !git pull \n git submodule sync --recursive \n git submodule update --init --recursive\n  spush = push --recurse-submodules=on-demand\n\n\n\n\nThe above aliases can serve as useful shortcuts to pull and push all projects from top-level \nsnappydata\n repository.\n\n\nBuilding\n\n\nGradle is the build tool used for all the SnappyData projects. Changes to \nApache Spark\n and \nspark-jobserver\n forks include addition of gradle build scripts to allow building them independently as well as a subproject of snappydata. The only requirement for the build is a JDK 7+ installation. Currently most of the testing has been with JDK 7. The gradlew wrapper script will download all the other build dependencies as required.\n\n\nIf a user does not want to deal with submodules and only work on snappydata project, then can clone only the snappydata repository (without the --recursive option) and the build will pull those SnappyData project jar dependencies from maven central.\n\n\nIf working on all the separate projects integrated inside the top-level snappydata clone, the gradle build will recognize the same and build those projects too and include the same in the top-level product distribution jar. The \nspark\n and \nstore\n submodules can also be built and published independently.\n\n\nUseful build and test targets:\n\n\n./gradlew assemble      -  build all the sources\n./gradlew testClasses   -  build all the tests\n./gradlew product       -  build and place the product distribution\n                           (in build-artifacts/scala_2.11/snappy)\n./gradlew distTar       -  create a tar.gz archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew distZip       -  create a zip archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew buildAll      -  build all sources, tests, product, packages (all targets above)\n./gradlew checkAll      -  run testsuites of snappydata components\n./gradlew cleanAll      -  clean all build and test output\n./gradlew runQuickstart -  run the quickstart suite (the \nGetting Started\n section of docs)\n./gradlew precheckin    -  cleanAll, buildAll, scalaStyle, build docs,\n                           and run full snappydata testsuite including quickstart\n./gradlew precheckin -Pstore  -  cleanAll, buildAll, scalaStyle, build docs,\n                           run full snappydata testsuite including quickstart\n                           and also full SnappyData store testsuite\n\n\n\n\nThe default build directory is \nbuild-artifacts/scala-2.11\n for projects. Exception is \nstore\n project, where the default build directory is \nbuild-artifacts/\nos\n where \nos\n is \nlinux\n on Linux systems, \nosx\n on Mac, \nwindows\n on Windows.\n\n\nThe usual gradle test run targets (\ntest\n, \ncheck\n) work as expected for junit tests. Separate targets have been provided for running scala tests (\nscalaTest\n) while the \ncheck\n target will run both the junit and scalatests. One can run a single scala test suite class with \nsingleSuite\n option while running a single test within some suite works with the \n--tests\n option:\n\n\n ./gradlew core:scalaTest -PsingleSuite=**.ColumnTableTest  # run all tests in the class\n\n ./gradlew core:scalaTest \\\n\n    --tests \nTest the creation/dropping of table using SQL\n  # run a single test (use full name)\n\n\n\n\nRunning individual tests within some suite works using the \n--tests\n argument.\n\n\nSetting up Intellij with gradle\n\n\nIntellij is the IDE commonly used by the snappydata developers. Those who really prefer Eclipse can try the scala-IDE and gradle support, but has been seen to not work as well (e.g. gradle support is not integrated with scala plugin etc).  To import into Intellij:\n\n\n\n\nUpdate Intellij to the latest 14.x (or 15.x) version, including the latest Scala plugin. Older versions have trouble dealing with scala code particularly some of the code in \nspark\n.\n\n\nSelect import project, then point to the snappydata directory. Use external Gradle import. When using JDK 7, add \n-XX:MaxPermSize=350m\n to VM options in global Gradle settings. Select defaults, next, next ... finish. Ignore \n\"Gradle location is unknown warning\"\n. Ensure that a JDK 7/8 installation has been selected. Ignore and dismiss the \n\"Unindexed remote maven repositories found\"\n warning message, if seen.\n\n\nOnce import finishes, go to \nFile-\nSettings-\nEditor-\nCode Style-\nScala\n. Set the scheme as \nProject\n. Check that the same has been set in Java Code Style too. Then OK to close it. Next copy \ncodeStyleSettings.xml\n in snappydata top-level directory to .idea directory created by Intellij. Check that settings are now applied in \nFile-\nSettings-\nEditor-\nCode Style-\nJava\n which should show Indent as 2 and continuation indent as 4 (same for Scala).\n\n\nIf the Gradle tab is not visible immediately, then select it from window list popup at the left-bottom corner of IDE. If you click on that window list icon, then the tabs will appear permanently.\n\n\nGenerate avro and GemFireXD required sources by expanding: \nsnappydata_2.11-\nTasks-\nother\n. Right click on \ngenerateSources\n and run it. The Run item may not be available if indexing is still in progress, so wait for it to finish. The first run may take a while as it downloads jars etc. This step has to be done the first time, or if \n./gradlew clean\n has been run, or you have made changes to \njavacc/avro/messages.xml\n source files. \nIf you get unexpected \n\"Database not found\"\n or \nNullPointerException\n errors in GemFireXD layer, then first thing to try is to run the \ngenerateSources\n target again.\n\n\nIncrease the compiler heap sizes or else the build can take quite long especially with integrated \nspark\n and \nstore\n. In \nFile-\nSettings-\nBuild, Execution, Deployment-\nCompiler increase \n, \nBuild process heap size\n to say 1536 or 2048. Similarly increase JVM maximum heap size in \nLanguages \n Frameworks-\nScala Compiler Server\n to 1536 or 2048.\n\n\nTest the full build.\n\n\nFor JDK 7: \nOpen Run-\nEdit Configurations\n. Expand Defaults, and select Application. Add \n-XX:MaxPermSize=350m\n in VM options. Similarly add it to VM parameters for ScalaTest and JUnit. Most of unit tests will have trouble without this option.\n\n\nFor JUnit configuration also append \n/build-artifacts\n to the working directory i.e. the directory should be \n\\$MODULE_DIR\\$/build-artifacts\n. Likewise change working directory for ScalaTest to be inside \nbuild-artifacts\n otherwise all intermediate log and other files (especially created by GemFireXD) will pollute the source tree and may need to cleaned manually.\n\n\n\n\nRunning a scalatest/junit\n\n\nRunning scala/junit tests from Intellij should be straightforward -- just ensure that MaxPermSize has been increased when using JDK 7 as mentioned above especially for Spark/Snappy tests.\n- When selecting a run configuration for junit/scalatest, avoid selecting the gradle one (green round icon) otherwise that will launch an external gradle process that can start building the project again and won't be cleanly integrated with Intellij. Use the normal junit (red+green arrows icon) or scalatest (junit like with red overlay).\n- For JUnit tests, ensure that working directory is \n\\$MODULE_DIR\\$/build-artifacts\n as mentioned before. Otherwise many GemFireXD tests will fail to find the resource files required in tests. They will also pollute the checkouts with log files etc, so this will allow those to go into build-artifacts that is easier to clean. For that reason is may be preferable to do the same for scalatests.\n- Some of the tests use data files from the \ntests-common\n directory. For such tests, run the gradle task \nsnappydata_2.11-\nTasks-\nother-\ncopyResourcesAll\n to copy the resources in build area where Intellij runs can find it.", 
            "title": "Building from source, project layout"
        }, 
        {
            "location": "/build-instructions/#build-quickstart", 
            "text": "Building SnappyData requires JDK 7+ installation ( Oracle Java SE ). Quickstart to build all components of snappydata:  Latest release branch   git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.6 --recursive  cd snappydata  ./gradlew product  Master   git clone https://github.com/SnappyDataInc/snappydata.git --recursive  cd snappydata  ./gradlew product  The product will be in  build-artifacts/scala-2.11/snappy  If you want to build only the top-level snappydata project but pull in jars for other projects ( spark ,  store ,  spark-jobserver ):  Latest release branch   git clone https://github.com/SnappyDataInc/snappydata.git -b branch-0.6  cd snappydata  ./gradlew product  Master   git clone https://github.com/SnappyDataInc/snappydata.git  cd snappydata  ./gradlew product", 
            "title": "Build Quickstart"
        }, 
        {
            "location": "/build-instructions/#repository-layout", 
            "text": "core  - Extensions to Apache Spark that should not be dependent on SnappyData Spark additions, job server etc. It is also the bridge between  spark  and  store  (GemFireXD). For example: SnappyContext, row and column store, streaming additions etc.    cluster  - Provides the SnappyData implementation of cluster manager embedding GemFireXD, query routing, job server initialization etc.    This component depends on  core  and  store . Code in  cluster  depends on  core  but not the other way round.    spark  -  Apache Spark  code with SnappyData enhancements.    store  - Fork of gemfirexd-oss with SnappyData additions on the snappy/master branch.    spark-jobserver  - Fork of  spark-jobserver  project with some additions to integrate with SnappyData.    The  spark ,  store  and  spark-jobserver  directories are required to be clones of the respective SnappyData repositories, and are integrated in the top-level snappydata project as git submodules. When working with submodules, updating the repositories follows the normal  git submodules . One can add some aliases in gitconfig to aid pull/push like:  [alias]\n  spull = !git pull   git submodule sync --recursive   git submodule update --init --recursive\n  spush = push --recurse-submodules=on-demand  The above aliases can serve as useful shortcuts to pull and push all projects from top-level  snappydata  repository.", 
            "title": "Repository layout"
        }, 
        {
            "location": "/build-instructions/#building", 
            "text": "Gradle is the build tool used for all the SnappyData projects. Changes to  Apache Spark  and  spark-jobserver  forks include addition of gradle build scripts to allow building them independently as well as a subproject of snappydata. The only requirement for the build is a JDK 7+ installation. Currently most of the testing has been with JDK 7. The gradlew wrapper script will download all the other build dependencies as required.  If a user does not want to deal with submodules and only work on snappydata project, then can clone only the snappydata repository (without the --recursive option) and the build will pull those SnappyData project jar dependencies from maven central.  If working on all the separate projects integrated inside the top-level snappydata clone, the gradle build will recognize the same and build those projects too and include the same in the top-level product distribution jar. The  spark  and  store  submodules can also be built and published independently.  Useful build and test targets:  ./gradlew assemble      -  build all the sources\n./gradlew testClasses   -  build all the tests\n./gradlew product       -  build and place the product distribution\n                           (in build-artifacts/scala_2.11/snappy)\n./gradlew distTar       -  create a tar.gz archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew distZip       -  create a zip archive of product distribution\n                           (in build-artifacts/scala_2.11/distributions)\n./gradlew buildAll      -  build all sources, tests, product, packages (all targets above)\n./gradlew checkAll      -  run testsuites of snappydata components\n./gradlew cleanAll      -  clean all build and test output\n./gradlew runQuickstart -  run the quickstart suite (the  Getting Started  section of docs)\n./gradlew precheckin    -  cleanAll, buildAll, scalaStyle, build docs,\n                           and run full snappydata testsuite including quickstart\n./gradlew precheckin -Pstore  -  cleanAll, buildAll, scalaStyle, build docs,\n                           run full snappydata testsuite including quickstart\n                           and also full SnappyData store testsuite  The default build directory is  build-artifacts/scala-2.11  for projects. Exception is  store  project, where the default build directory is  build-artifacts/ os  where  os  is  linux  on Linux systems,  osx  on Mac,  windows  on Windows.  The usual gradle test run targets ( test ,  check ) work as expected for junit tests. Separate targets have been provided for running scala tests ( scalaTest ) while the  check  target will run both the junit and scalatests. One can run a single scala test suite class with  singleSuite  option while running a single test within some suite works with the  --tests  option:   ./gradlew core:scalaTest -PsingleSuite=**.ColumnTableTest  # run all tests in the class  ./gradlew core:scalaTest \\     --tests  Test the creation/dropping of table using SQL   # run a single test (use full name)  Running individual tests within some suite works using the  --tests  argument.", 
            "title": "Building"
        }, 
        {
            "location": "/build-instructions/#setting-up-intellij-with-gradle", 
            "text": "Intellij is the IDE commonly used by the snappydata developers. Those who really prefer Eclipse can try the scala-IDE and gradle support, but has been seen to not work as well (e.g. gradle support is not integrated with scala plugin etc).  To import into Intellij:   Update Intellij to the latest 14.x (or 15.x) version, including the latest Scala plugin. Older versions have trouble dealing with scala code particularly some of the code in  spark .  Select import project, then point to the snappydata directory. Use external Gradle import. When using JDK 7, add  -XX:MaxPermSize=350m  to VM options in global Gradle settings. Select defaults, next, next ... finish. Ignore  \"Gradle location is unknown warning\" . Ensure that a JDK 7/8 installation has been selected. Ignore and dismiss the  \"Unindexed remote maven repositories found\"  warning message, if seen.  Once import finishes, go to  File- Settings- Editor- Code Style- Scala . Set the scheme as  Project . Check that the same has been set in Java Code Style too. Then OK to close it. Next copy  codeStyleSettings.xml  in snappydata top-level directory to .idea directory created by Intellij. Check that settings are now applied in  File- Settings- Editor- Code Style- Java  which should show Indent as 2 and continuation indent as 4 (same for Scala).  If the Gradle tab is not visible immediately, then select it from window list popup at the left-bottom corner of IDE. If you click on that window list icon, then the tabs will appear permanently.  Generate avro and GemFireXD required sources by expanding:  snappydata_2.11- Tasks- other . Right click on  generateSources  and run it. The Run item may not be available if indexing is still in progress, so wait for it to finish. The first run may take a while as it downloads jars etc. This step has to be done the first time, or if  ./gradlew clean  has been run, or you have made changes to  javacc/avro/messages.xml  source files.  If you get unexpected  \"Database not found\"  or  NullPointerException  errors in GemFireXD layer, then first thing to try is to run the  generateSources  target again.  Increase the compiler heap sizes or else the build can take quite long especially with integrated  spark  and  store . In  File- Settings- Build, Execution, Deployment- Compiler increase  ,  Build process heap size  to say 1536 or 2048. Similarly increase JVM maximum heap size in  Languages   Frameworks- Scala Compiler Server  to 1536 or 2048.  Test the full build.  For JDK 7:  Open Run- Edit Configurations . Expand Defaults, and select Application. Add  -XX:MaxPermSize=350m  in VM options. Similarly add it to VM parameters for ScalaTest and JUnit. Most of unit tests will have trouble without this option.  For JUnit configuration also append  /build-artifacts  to the working directory i.e. the directory should be  \\$MODULE_DIR\\$/build-artifacts . Likewise change working directory for ScalaTest to be inside  build-artifacts  otherwise all intermediate log and other files (especially created by GemFireXD) will pollute the source tree and may need to cleaned manually.", 
            "title": "Setting up Intellij with gradle"
        }, 
        {
            "location": "/build-instructions/#running-a-scalatestjunit", 
            "text": "Running scala/junit tests from Intellij should be straightforward -- just ensure that MaxPermSize has been increased when using JDK 7 as mentioned above especially for Spark/Snappy tests.\n- When selecting a run configuration for junit/scalatest, avoid selecting the gradle one (green round icon) otherwise that will launch an external gradle process that can start building the project again and won't be cleanly integrated with Intellij. Use the normal junit (red+green arrows icon) or scalatest (junit like with red overlay).\n- For JUnit tests, ensure that working directory is  \\$MODULE_DIR\\$/build-artifacts  as mentioned before. Otherwise many GemFireXD tests will fail to find the resource files required in tests. They will also pollute the checkouts with log files etc, so this will allow those to go into build-artifacts that is easier to clean. For that reason is may be preferable to do the same for scalatests.\n- Some of the tests use data files from the  tests-common  directory. For such tests, run the gradle task  snappydata_2.11- Tasks- other- copyResourcesAll  to copy the resources in build area where Intellij runs can find it.", 
            "title": "Running a scalatest/junit"
        }, 
        {
            "location": "/snappyIntroduction/", 
            "text": "Introduction\n\n\nSnappyData is a \ndistributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster\n. We realize this platform through a seamless deep integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics) \n\n\n\n\nConceptually, you could \nthink of SnappyData as a in-memory database that embeds Spark as its computational engine\n - to process streams, work with myriad data sources like HDFS, and process data through a rich set of higher level abstractions. While the SnappyData engine is primarily designed for SQL processing, applications can work with Objects through Spark RDDs and the newly introduced Spark DataSets. \n\n\nAny Spark DataFrame can be easily managed as a SnappyData Table or conversely any table can be accessed as a DataFrame. \n\n\nBy default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted Spark executors are automatically launched within the Snappy process space (JVMs). There is no need to connect and manage external data store clusters. The Snappy store can synchronously replicate for HA with strong consistency and store/recover from disk for additional reliability.\n\n\n\n\nExtensions to the Spark Runtime\n\n\nSnappyData makes the following contributions to deliver a unified and optimized runtime.\n\n1. \nIntegrating an operational in-memory data store with Spark\u2019s computational model:\n We introduce a number of extensions to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy. We extend this design by allowing low latency and fine grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, we extend the runtime with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby state is shared across many clients and applications. \n\n\n\n\nUnified API for OLAP, OLTP, and Streaming:\n Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL. \nWhile Spark deserves much of the credit for being the first of its kind to offer a unified API, we further extend its API to  \n\n\nallow for OLTP operations, e.g., transactions and mutations (inserts/updates/deletions) on tables  \n\n\nbe conformant with SQL standards, e.g., allowing tables alterations, constraints, indexes, and   \n\n\n\n\nsupport declarative stream processing in SQL\n\n\n\n\n\n\nOptimizing Spark application execution times:\n Our goal is to eliminate the need for yet another external store (e.g., a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting colocated schema designs (tables and streams) where related data is colocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios. \n\n\n\n\n\n\nSynopsis Data Engine support built into Spark:\n To deliver analytics at truly interactive speeds, we have equipped SnappyData with state-of-the-art SDE techniques, as well as a number of novel features. \nSnappyData is the first SDE engine to  \n\n\n\n\nProvide automatic bias correction for arbitrarily complex SQL queries  \n\n\nProvide an intuitive means for end users to express their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts  \n\n\nProvide error estimates for arbitrarily complex queries on streams (Unlike traditional load shedding techniques that are restricted to simple queries)", 
            "title": "Overview"
        }, 
        {
            "location": "/snappyIntroduction/#introduction", 
            "text": "SnappyData is a  distributed in-memory data store for real-time operational analytics, delivering stream analytics, OLTP(online transaction processing) and OLAP(online analytical processing) in a single integrated cluster . We realize this platform through a seamless deep integration of Apache Spark (as a big data computational engine) with GemFire XD(as an in- memory transactional store with scale-out SQL semantics)    Conceptually, you could  think of SnappyData as a in-memory database that embeds Spark as its computational engine  - to process streams, work with myriad data sources like HDFS, and process data through a rich set of higher level abstractions. While the SnappyData engine is primarily designed for SQL processing, applications can work with Objects through Spark RDDs and the newly introduced Spark DataSets.   Any Spark DataFrame can be easily managed as a SnappyData Table or conversely any table can be accessed as a DataFrame.   By default, when the cluster is started, the data store is bootstrapped and when any Spark Jobs/OLAP queries are submitted Spark executors are automatically launched within the Snappy process space (JVMs). There is no need to connect and manage external data store clusters. The Snappy store can synchronously replicate for HA with strong consistency and store/recover from disk for additional reliability.", 
            "title": "Introduction"
        }, 
        {
            "location": "/snappyIntroduction/#extensions-to-the-spark-runtime", 
            "text": "SnappyData makes the following contributions to deliver a unified and optimized runtime. \n1.  Integrating an operational in-memory data store with Spark\u2019s computational model:  We introduce a number of extensions to fuse our runtime with that of Spark. Spark executors run in the same process space as our store\u2019s execution threads, sharing the same pool of memory. When Spark executes tasks in a partitioned manner, it is designed to keep all the available CPU cores busy. We extend this design by allowing low latency and fine grained operations to interleave and get higher priority, without involving the scheduler. Furthermore, to support high concurrency, we extend the runtime with a \u201cJob Server\u201d that decouples applications from data servers, operating much in the same way as a traditional database, whereby state is shared across many clients and applications.    Unified API for OLAP, OLTP, and Streaming:  Spark builds on a common set of abstractions to provide a rich API for a diverse range of applications, such as MapReduce, Machine learning, stream processing, and SQL. \nWhile Spark deserves much of the credit for being the first of its kind to offer a unified API, we further extend its API to    allow for OLTP operations, e.g., transactions and mutations (inserts/updates/deletions) on tables    be conformant with SQL standards, e.g., allowing tables alterations, constraints, indexes, and      support declarative stream processing in SQL    Optimizing Spark application execution times:  Our goal is to eliminate the need for yet another external store (e.g., a KV store) for Spark applications. With a deeply integrated store, SnappyData improves overall performance by minimizing network traffic and serialization costs. In addition, by promoting colocated schema designs (tables and streams) where related data is colocated in the same process space, SnappyData eliminates the need for shuffling altogether in several scenarios.     Synopsis Data Engine support built into Spark:  To deliver analytics at truly interactive speeds, we have equipped SnappyData with state-of-the-art SDE techniques, as well as a number of novel features. \nSnappyData is the first SDE engine to     Provide automatic bias correction for arbitrarily complex SQL queries    Provide an intuitive means for end users to express their accuracy requirements as high-level accuracy contracts (HAC), without overwhelming them with numerous statistical concepts    Provide error estimates for arbitrarily complex queries on streams (Unlike traditional load shedding techniques that are restricted to simple queries)", 
            "title": "Extensions to the Spark Runtime"
        }, 
        {
            "location": "/features/", 
            "text": "Key Features\n\n\n\n\n100% compatible with Spark\n - Use SnappyData as a database but also use any of the Spark APIs - ML, Graph, etc\n\n\nin-memory row and column stores\n: run the store collocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster)\n\n\nSQL standard compliance\n: Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.\n\n\nSQL based extensions for streaming processing\n: Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.\n\n\nInteractive analytics using Synopsis Data Engine (SDE)\n: We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions. \n\n\nMutate, transact on data in Spark\n: You can use SQL to insert, update, delete data in tables as one would expect. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store. \n\n\nOptimizations - Indexing\n: You can index your row store and the GemFire SQL optimizer will automatically use in-memory indexes when available. \n\n\nOptimizations - colocation\n: SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be collocated using declarative custom partitioning strategies(e.g. common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent. \n\n\nHigh availability not just Fault tolerance\n: Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications continuous HA.\n\n\nDurability and recovery:\n Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled. \n\n\n\n\nSpark challenges for mixed workloads (OLTP, OLAP)\n\n\nSpark is designed as a computational engine for processing batch jobs. Each Spark application (e.g., a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an ex- ternal storage tier, such as HDFS. We, on the other hand, target a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database on the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and de-coupled from individual applications.\n\n\nA second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces\n\n\n\n\na single point of contention for all requests, and \n\n\na barrier for achieving high availability (HA). Executors are shutdown if the driver fails, requiring a full refresh of any cached state.\n\n\n\n\nSpark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, we need to manage more complex data structures (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mu- tability characteristics and conformance to SQL.\n\n\nFinally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Key features"
        }, 
        {
            "location": "/features/#key-features", 
            "text": "100% compatible with Spark  - Use SnappyData as a database but also use any of the Spark APIs - ML, Graph, etc  in-memory row and column stores : run the store collocated in Spark executors or in its own process space (i.e. a computational cluster and a data cluster)  SQL standard compliance : Spark SQL + several SQL extensions: DML, DDL, indexing, constraints.  SQL based extensions for streaming processing : Use native Spark streaming, Dataframe APIs or declaratively specify your streams and how you want it processed. No need to learn Spark APIs to get going with stream processing or its subtleties when processing in parallel.  Interactive analytics using Synopsis Data Engine (SDE) : We introduce multiple synopses techniques through data structures like count-min-sketch and stratified sampling to dramatically reduce the in-memory space requirements and provide true interactive speeds for analytic queries. These structures can be created and managed by developers with little to no statistical background and can be completely transparent to the SQL developer running queries. Error estimators are also integrated with simple mechanisms to get to the errors through built-in SQL functions.   Mutate, transact on data in Spark : You can use SQL to insert, update, delete data in tables as one would expect. We also provide extensions to Spark\u2019s context so you can mutate data in your spark programs. Any tables in SnappyData is visible as DataFrames without having to maintain multiples copies of your data: cached RDDs in Spark and then separately in your data store.   Optimizations - Indexing : You can index your row store and the GemFire SQL optimizer will automatically use in-memory indexes when available.   Optimizations - colocation : SnappyData implements several optimizations to improve data locality and avoid shuffling data for queries on partitioned data sets. All related data can be collocated using declarative custom partitioning strategies(e.g. common shared business key). Reference data tables can be modeled as replicated tables when tables cannot share a common key. Replicas are always consistent.   High availability not just Fault tolerance : Data can be instantly replicated (one at a time or batch at a time) to other nodes in the cluster and is deeply integrated with a membership based distributed system to detect and handle failures instantaneously providing applications continuous HA.  Durability and recovery:  Data can also be managed on disk and automatically recovered. Utilities for backup and restore are bundled.", 
            "title": "Key Features"
        }, 
        {
            "location": "/features/#spark-challenges-for-mixed-workloads-oltp-olap", 
            "text": "Spark is designed as a computational engine for processing batch jobs. Each Spark application (e.g., a Map-reduce job) runs as an independent set of processes (i.e., executor JVMs) on the cluster. These JVMs are re- used for the lifetime of the application. While, data can be cached and reused in these JVMs for a single application, sharing data across applications or clients requires an ex- ternal storage tier, such as HDFS. We, on the other hand, target a real-time, \u201calways-on\u201d, operational design center\u2014 clients can connect at will, and share data across any number of concurrent connections. This is similar to any operational database on the market today. Thus, to manage data in the same JVM, our first challenge is to alter the life cycle of these executors so that they are long-lived and de-coupled from individual applications.  A second but related challenge is Spark\u2019s design for how user requests (i.e., jobs) are handled. A single driver orchestrates all the work done on the executors. Given our need for high concurrency and a hybrid OLTP-OLAP workload, this driver introduces   a single point of contention for all requests, and   a barrier for achieving high availability (HA). Executors are shutdown if the driver fails, requiring a full refresh of any cached state.   Spark\u2019s primary usage of memory is for caching RDDs and for shuffling blocks to other nodes. Data is managed in blocks and is immutable. On the other hand, we need to manage more complex data structures (along with indexes) for point access and updates. Therefore, another challenge is merging these two disparate storage systems with little impedance to the application. This challenge is exacerbated by current limitations of Spark SQL\u2014mostly related to mu- tability characteristics and conformance to SQL.  Finally, Spark\u2019s strong and growing community has zero tolerance for incompatible forks. This means that no changes can be made to Spark\u2019s execution model or its semantics for existing APIs. In other words, our changes have to be an extension.", 
            "title": "Spark challenges for mixed workloads (OLTP, OLAP)"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture overview\n\n\nThis section presents a high level overview of SnappyData\u2019s core components, as well as our data pipeline as streams are ingested into our in-memory store and subsequently interacted with and analyzed.\n\n\nCore components\n\n\nFigure 1 depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, we have omitted standard components, such as security and monitoring.\n\n \n\n\nThe storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. See \nRow/Column table\n section for details on the syntax and available features. \n\n\nWe support two primary programming models \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions to make the language compatible to the SQL standard. One could perceive SnappyData as a SQL database that uses Spark API as its language for stored procedures. Our \nstream processing\n is primarily through Spark Streaming, but it is integrated and runs in-situ with our store.\n\n\nThe OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). We route all OLTP operations immediately to appropriate data partitions without incurring any scheduling overhead.\n\n\nTo support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, we use a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.\n\n\nIn addition to the \u201cexact\u201d dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Synopsis Data Engine (SDE)) and will exploit appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.\n\n\nTo understand the data flow architecture, we first walk through a real time use case that involves stream processing, ingesting into an in-memory store and interactive analytics. \n\n\nUse case\n\n\nLocation based services from telco network providers\n\n\nThe global proliferation of mobile devices has created a growing market for location based services. In addition to locality-aware search and navigation, network providers are increasingly relying on location-based advertising, emergency call positioning, road traffic optimization, efficient call routing, triggering preemptive maintenance of cell towers, roaming analytics, and tracking vulnerable people in real time. Telemetry events are delivered as Call Detail Records (CDR), containing hundreds of attributes about each call. Ingested CDRs are cleansed and transformed for consumption by various applications. Not being able to correlate customer support calls with location specific network congestion information is a problem that frustrates customers and network technicians alike. The ability to do this in real time may involve expensive joins to history, tower traffic data and subscriber profiles. Incoming streams generate hundreds of aggregate metrics and KPIs (key performance indicators) grouped by subscriber, cell phone type, cell tower, and location. This requires continuous updates to counters accessed through primary keys (such as the subscriberID). While the generated data is massive, it still needs to be interactively queried by a data analyst for network performance analysis. Location-based services represent another common problem among our customers that involves high concurrency, continuous data updates, complex queries, time series data, and a source that cannot be throttled.\n\n\nData ingestion pipeline\n\n\nThe data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive architecture for real-time applications. The steps to support these tasks are depicted in Figure 2, and explained below.\n\n  \n\n\nOnce the SnappyData cluster is started and before any live streams can be processed, we can ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (e.g., HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables.\n\n\n\n\n\n\nWe rely on Spark Streaming\u2019s parallel receivers to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.\n\n\n\n\n\n\nNext, we use SQL to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), we can seamlessly combine these data structures in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high velocity streams, SnappyData can still provide answers in real time by resorting to approximation.\n\n\n\n\n\n\nThe stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (e.g., maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.\n\n\n\n\n\n\nTo prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.\n\n\n\n\n\n\nOnce ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance to users\u2019 requested accuracy.\n\n\n\n\n\n\nHybrid Cluster Manager\n\n\nAs shown in Figure above, spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (e.g., YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors.\n\n\nWhile Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet additional requirements (1\u20134) as an operational database.\n\n\n\n\n\n\nHigh concurrency\n : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.\n\n\n\n\n\n\nState sharing\n : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real time data sharing.\n\n\n\n\n\n\nHigh availability (HA) \u2014 As a highly concurrent distributed system that offers low latency access to data, we must protect applications from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations therefore become an important requirement for SnappyData.\n\n\n\n\n\n\nConsistency \u2014 As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture in section 5.1, we explain how SnappyData meets each of these requirements in the subsequent sections.\n\n\n\n\n\n\nSnappyData Cluster Architecture\n\n\nA SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members (see figure 4).\n1. Locator. Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n2. Lead Node. The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n3. Data Servers. A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.\n\n\nHigh Concurrency in SnappyData\n\n\nThousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into (i) low latency requests and (ii) high latency ones. For low latency operations, we completely bypass Spark\u2019s scheduling mechanism and directly operate on the data. We route high latency operations (e.g., compute intensive queries) through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.\n\n\nState Sharing in SnappyData\n\n\nA SnappyData cluster is designed to be a long running clustered database. State is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. (See section 4 for more details on table types and redundancy.) Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#architecture-overview", 
            "text": "This section presents a high level overview of SnappyData\u2019s core components, as well as our data pipeline as streams are ingested into our in-memory store and subsequently interacted with and analyzed.", 
            "title": "Architecture overview"
        }, 
        {
            "location": "/architecture/#core-components", 
            "text": "Figure 1 depicts the core components of SnappyData, where Spark\u2019s original components are highlighted in gray. To simplify, we have omitted standard components, such as security and monitoring.    The storage layer is primarily in-memory and manages data in either row or column formats. The column format is derived from Spark\u2019s RDD caching implementation and allows for compression. Row oriented tables can be indexed on keys or secondary columns, supporting fast reads and writes on index keys. See  Row/Column table  section for details on the syntax and available features.   We support two primary programming models \u2014 SQL and Spark\u2019s API. SQL access is through JDBC/ODBC and it supports the Spark SQL dialect with several extensions to make the language compatible to the SQL standard. One could perceive SnappyData as a SQL database that uses Spark API as its language for stored procedures. Our  stream processing  is primarily through Spark Streaming, but it is integrated and runs in-situ with our store.  The OLAP scheduler and job server coordinate all OLAP and Spark jobs and are capable of working with external cluster managers, such as YARN or Mesos (not yet supported). We route all OLTP operations immediately to appropriate data partitions without incurring any scheduling overhead.  To support replica consistency, fast point updates, and instantaneous detection of failure conditions in the cluster, we use a P2P (peer-to-peer) cluster membership service that ensures view consistency and virtual synchrony in the cluster. Any of the in-memory tables can be synchronously replicated using this P2P cluster.  In addition to the \u201cexact\u201d dataset, data can also be summarized using probabilistic data structures, such as stratified samples and other forms of synopses. Using our API, applications can choose to trade accuracy for performance. SnappyData\u2019s query engine has built-in support for Synopsis Data Engine (SDE)) and will exploit appropriate probabilistic data structures to meet the user\u2019s requested level of accuracy or performance.  To understand the data flow architecture, we first walk through a real time use case that involves stream processing, ingesting into an in-memory store and interactive analytics.", 
            "title": "Core components"
        }, 
        {
            "location": "/architecture/#use-case", 
            "text": "", 
            "title": "Use case"
        }, 
        {
            "location": "/architecture/#location-based-services-from-telco-network-providers", 
            "text": "The global proliferation of mobile devices has created a growing market for location based services. In addition to locality-aware search and navigation, network providers are increasingly relying on location-based advertising, emergency call positioning, road traffic optimization, efficient call routing, triggering preemptive maintenance of cell towers, roaming analytics, and tracking vulnerable people in real time. Telemetry events are delivered as Call Detail Records (CDR), containing hundreds of attributes about each call. Ingested CDRs are cleansed and transformed for consumption by various applications. Not being able to correlate customer support calls with location specific network congestion information is a problem that frustrates customers and network technicians alike. The ability to do this in real time may involve expensive joins to history, tower traffic data and subscriber profiles. Incoming streams generate hundreds of aggregate metrics and KPIs (key performance indicators) grouped by subscriber, cell phone type, cell tower, and location. This requires continuous updates to counters accessed through primary keys (such as the subscriberID). While the generated data is massive, it still needs to be interactively queried by a data analyst for network performance analysis. Location-based services represent another common problem among our customers that involves high concurrency, continuous data updates, complex queries, time series data, and a source that cannot be throttled.", 
            "title": "Location based services from telco network providers"
        }, 
        {
            "location": "/architecture/#data-ingestion-pipeline", 
            "text": "The data pipeline involving analytics while streams are being ingested and subsequent interactive analytics will be the pervasive architecture for real-time applications. The steps to support these tasks are depicted in Figure 2, and explained below.     Once the SnappyData cluster is started and before any live streams can be processed, we can ensure that the historical and reference datasets are readily accessible. The data sets may come from HDFS, enterprise relational databases (RDB), or disks managed by SnappyData. Immutable batch sources (e.g., HDFS) can be loaded in parallel into a columnar format table with or without compression. Reference data that is often mutating can be managed as row tables.    We rely on Spark Streaming\u2019s parallel receivers to consume data from multiple sources. These receivers produce a DStream, whereby the input is batched over small time intervals and emitted as a stream of RDDs. This batched data is typically transformed, enriched and emitted as one or more additional streams. The raw incoming stream may be persisted into HDFS for batch analytics.    Next, we use SQL to analyze these streams. As DStreams (RDDs) use the same processing and data model as data stored in tables (DataFrames), we can seamlessly combine these data structures in arbitrary SQL queries (referred to as continuous queries as they execute each time the stream emits a batch). When faced with complex analytics or high velocity streams, SnappyData can still provide answers in real time by resorting to approximation.    The stream processing layer can interact with the storage layer in a variety of ways. The enriched stream can be efficiently stored in a column table. The results of continuous queries may result in several point updates in the store (e.g., maintaining counters). The continuous queries may join, correlate, and aggregate with other streams, history or reference data tables. When records are written into column tables one (or a small batch) at a time, data goes through stages, arriving first into a delta row buffer that is capable of high write rates, and then aging into a columnar form. Our query sub-system (which extends Spark\u2019s Catalyst optimizer) merges the delta row buffer during query execution.    To prevent running out of memory, tables can be configured to evict or overflow to disk using an LRU strategy. For instance, an application may ingest all data into HDFS while preserving the last day\u2019s worth of data in memory.    Once ingested, the data is readily available for interactive analytics using SQL. Similar to stream analytics, SnappyData can again use Synopsis Data Engine to ensure interactive analytics on massive historical data in accordance to users\u2019 requested accuracy.", 
            "title": "Data ingestion pipeline"
        }, 
        {
            "location": "/architecture/#hybrid-cluster-manager", 
            "text": "As shown in Figure above, spark applications run as independent processes in the cluster, coordinated by the application\u2019s main program, called the driver program. Spark applications connect to cluster managers (e.g., YARN and Mesos) to acquire executors on nodes in the cluster. Executors are processes that run computations and store data for the running application. The driver program owns a singleton (SparkContext) object which it uses to communicate with its set of executors.  While Spark\u2019s approach is appropriate and geared towards compute-heavy tasks that scan large datasets, SnappyData must meet additional requirements (1\u20134) as an operational database.    High concurrency  : SnappyData use cases involve a mixture of compute-intensive workloads and low latency (sub-millisecond) OLTP operations such as point lookups (index-based search), and insert/update of a single record. The fair scheduler of Spark is not designed to meet the low latency requirements of such operations.    State sharing  : Each application submitted to Spark works in isolation. State sharing across applications requires an external store, which increases latency and is not viable for near real time data sharing.    High availability (HA) \u2014 As a highly concurrent distributed system that offers low latency access to data, we must protect applications from node failures (caused by software bugs and hardware/network failures). High availability of data and transparent handling of failed operations therefore become an important requirement for SnappyData.    Consistency \u2014 As a highly available system that offers concurrent data access, it becomes important to ensure that all applications have a consistent view of data.\nAfter an overview of our cluster architecture in section 5.1, we explain how SnappyData meets each of these requirements in the subsequent sections.", 
            "title": "Hybrid Cluster Manager"
        }, 
        {
            "location": "/architecture/#snappydata-cluster-architecture", 
            "text": "A SnappyData cluster is a peer-to-peer (P2P) network comprised of three distinct types of members (see figure 4).\n1. Locator. Locator members provide discovery service for the cluster. They inform a new member joining the group about other existing members. A cluster usually has more than one locator for high availability reasons.\n2. Lead Node. The lead node member acts as a Spark driver by maintaining a singleton SparkContext. There is one primary lead node at any given instance but there can be multiple secondary lead node instances on standby for fault tolerance. The lead node hosts a REST server to accept and run applications. The lead node also executes SQL queries routed to it by \u201cdata server\u201d members.\n3. Data Servers. A data server member hosts data, embeds a Spark executor, and also contains a SQL engine capable of executing certain queries independently and more efficiently than Spark. Data servers use intelligent query routing to either execute the query directly on the node, or pass it to the lead node for execution by Spark SQL.", 
            "title": "SnappyData Cluster Architecture"
        }, 
        {
            "location": "/architecture/#high-concurrency-in-snappydata", 
            "text": "Thousands of concurrent ODBC and JDBC clients can simultaneously connect to a SnappyData cluster. To support this degree of concurrency, SnappyData categorizes incoming requests from these clients into (i) low latency requests and (ii) high latency ones. For low latency operations, we completely bypass Spark\u2019s scheduling mechanism and directly operate on the data. We route high latency operations (e.g., compute intensive queries) through Spark\u2019s fair scheduling mechanism. This makes SnappyData a responsive system, capable of handling multiple low latency short operations as well as complex queries that iterate over large datasets simultaneously.", 
            "title": "High Concurrency in SnappyData"
        }, 
        {
            "location": "/architecture/#state-sharing-in-snappydata", 
            "text": "A SnappyData cluster is designed to be a long running clustered database. State is managed in tables that can be shared across any number of connecting applications. Data is stored in memory and replicated to at least one other node in the system. Data can be persisted to disk in shared nothing disk files for quick recovery. (See section 4 for more details on table types and redundancy.) Nodes in the cluster stay up for a long time and their lifecycle is independent of application lifetimes. SnappyData achieves this goal by decoupling its process startup and shutdown mechanisms from those used by Spark.", 
            "title": "State Sharing in SnappyData"
        }, 
        {
            "location": "/configuration/", 
            "text": "Configuration\n\n\nSnappyData, a database server cluster, has three main components - Locator, Server and Lead. Lead node embeds a Spark driver and Server node embeds a Spark Executor. Server node also embeds a snappy store. As discussed in Getting Started, SnappyData cluster can be started with default configurations using script sbin/snappy-start-all.sh. This script starts up a locator, one data server and one lead node. However, SnappyData can be configured to start multiple components on different nodes. Also, each component can be configured individually using configuration files. In this document, we discuss how the components can be individually configured. We also discuss various other configurations of SnappyData. \n\n\nConfiguration files\n\n\nConfiguration files for locator, lead and server should be created in SNAPPY_HOME with names as conf/locators, conf/leads and conf/servers respectively. These files contain the hostnames of the nodes (one per line) where you intend to start the member. You can specify the properties to configure individual members. \n\n\nSnappyData specific properties\n\n\nThe following are the few important SnappyData properties that you would like to configure. \n\n\n\n\n-peer-discovery-port\n: This is a locator specific property. This is the port on which locator listens for member discovery. It defaults to 10334. \n\n\n-client-port\n: Port that a member listens on for client connections. \n\n\n-locators\n: List of other locators as comma-separated host:port values. For locators, the list must include all other locators in use. For Servers and Leads, the list must include all the locators of the distributed system.\n\n\n-dir\n: SnappyData members need to have working directory. The member working directory provides a default location for log, persistence, and status files for each member. If not specified, SnappyData creates the member directory in \nSNAPPY_HOME\\work\n. \n\n\n-classpath\n: This can be used to provide any application specific code to the lead and servers. We envisage having setJar like functionality going forward but for now, the application jars have to be provided during startup. \n\n\n-heap-size\n: Set a fixed heap size and for the Java VM. \n\n\n-J\n: Use this to configure any JVM specific property. For e.g. -J-XX:MaxPermSize=512m. \n\n\n\n\nFor a detail list of SnappyData configurations for Leads and Servers, you can consult \nthis\n. For a detail list of SnappyData configurations for Locators, you can consult \nthis\n.\n\n\nHDFS with SnappyData store\n\n\nIf using SnappyData store persistence to Hadoop as documented \nhere\n, then you will need to add the \nhbase jar\n explicitly to CLASSPATH. The jar is now packed in the product tree, so that can be used or download from maven. Then add to conf/spark-env.sh:\n\n\nexport SPARK_DIST_CLASSPATH=/path/to/hbase-0.94.27.jar\n\n\n(subsitute the actual path for /path/to/ above)\n\n\nSpark specific properties\n\n\nSince SnappyData embeds Spark components, \nSpark Runtime environment properties\n (like  spark.driver.memory, spark.executor.memory, spark.driver.extraJavaOptions, spark.executorEnv) do not take effect. They have to be specified using SnappyData configuration properties. Apart from these properties, other Spark properties can be specified in the configuration file of the Lead nodes. You have to prepend them with a \nhyphen(-)\n. The Spark properties that are specified on the Lead node are sent to the Server nodes. Any Spark property that is specified in the conf/servers or conf/locators file is ignored. \n\n\n\n\nNote:\n\n\nCurrently we do not honour properties specified using spark-config.sh. \n\n\n\n\nExample Configuration\n\n\nLet's say you want to start two Locators (on node-a:9999 and node-b:8888), two servers (node-c and node-c) and a lead (node-l). Also, you would like to change the Spark UI port from 4040 to 9090. Also, you would like to set spark.executor.cores as 10 on all servers. The following can be your conf files. \n\n\n$ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999\n\n$ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -locators=node-b:8888,node-a:9999\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -J-XX:MaxPermSize=512m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10\n\n\n\n\n\n\nNote:\n\n\nconf files are consulted when servers are started and also when they are stopped. So, we do not recommend changing the conf files while the cluster is running. \n\n\n\n\nEnvironment settings\n\n\nAny Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in \nSNAPPY_HOME/conf\n. \n\n\nHadoop Provided settings\n\n\nIf you want run SnappyData with an already existing custom Hadoop cluster like MapR or Cloudera you should download Snappy without hadoop from the download link.\nThis will allow you to provide hadoop at runtime.\nTo do this you need to put an entry in $SNAPPY-HOME/conf/spark-env.sh an entry as below.\n\n\nexport SPARK_DIST_CLASSPATH=$($OTHER_HADOOP_HOME/bin/hadoop classpath)\n\n\nPer Component Configuration\n\n\nMost of the time, components would be sharing the same properties. For e.g. you would want all servers to start with 4096m while leads to start with 2048m. You can configure these by specifying LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, LEAD_STARTUP_OPTIONS environment variables in conf/snappy-env.sh. \n\n\n$ cat conf/snappy-env.sh\nSERVER_STARTUP_OPTIONS=\n-heap-size=4096m\n\nLEAD_STARTUP_OPTIONS=\n-heap-size=2048m\n\n\n\n\n\nsnappy-shell Command Line Utility\n\n\nInstead of starting SnappyData members using ssh scripts, they can be individually configured and started using command line. \n\n\n$ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334\n\n$ bin/snappy-shell locator stop\n$ bin/snappy-shell server stop\n\n\n\n\nLogging\n\n\nCurrently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property \n-log-file\n as the path of the directory. There is a log4j.properties file that is shipped with the jar. We recommend not to change it at this moment. However, to change the logging levels, you can create a conf/log4j.properties with the following details: \n\n\n$ cat conf/log4j.properties \nlog4j.rootCategory=DEBUG, file", 
            "title": "Configuring the cluster"
        }, 
        {
            "location": "/configuration/#configuration", 
            "text": "SnappyData, a database server cluster, has three main components - Locator, Server and Lead. Lead node embeds a Spark driver and Server node embeds a Spark Executor. Server node also embeds a snappy store. As discussed in Getting Started, SnappyData cluster can be started with default configurations using script sbin/snappy-start-all.sh. This script starts up a locator, one data server and one lead node. However, SnappyData can be configured to start multiple components on different nodes. Also, each component can be configured individually using configuration files. In this document, we discuss how the components can be individually configured. We also discuss various other configurations of SnappyData.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#configuration-files", 
            "text": "Configuration files for locator, lead and server should be created in SNAPPY_HOME with names as conf/locators, conf/leads and conf/servers respectively. These files contain the hostnames of the nodes (one per line) where you intend to start the member. You can specify the properties to configure individual members.", 
            "title": "Configuration files"
        }, 
        {
            "location": "/configuration/#snappydata-specific-properties", 
            "text": "The following are the few important SnappyData properties that you would like to configure.    -peer-discovery-port : This is a locator specific property. This is the port on which locator listens for member discovery. It defaults to 10334.   -client-port : Port that a member listens on for client connections.   -locators : List of other locators as comma-separated host:port values. For locators, the list must include all other locators in use. For Servers and Leads, the list must include all the locators of the distributed system.  -dir : SnappyData members need to have working directory. The member working directory provides a default location for log, persistence, and status files for each member. If not specified, SnappyData creates the member directory in  SNAPPY_HOME\\work .   -classpath : This can be used to provide any application specific code to the lead and servers. We envisage having setJar like functionality going forward but for now, the application jars have to be provided during startup.   -heap-size : Set a fixed heap size and for the Java VM.   -J : Use this to configure any JVM specific property. For e.g. -J-XX:MaxPermSize=512m.    For a detail list of SnappyData configurations for Leads and Servers, you can consult  this . For a detail list of SnappyData configurations for Locators, you can consult  this .", 
            "title": "SnappyData specific properties"
        }, 
        {
            "location": "/configuration/#hdfs-with-snappydata-store", 
            "text": "If using SnappyData store persistence to Hadoop as documented  here , then you will need to add the  hbase jar  explicitly to CLASSPATH. The jar is now packed in the product tree, so that can be used or download from maven. Then add to conf/spark-env.sh:  export SPARK_DIST_CLASSPATH=/path/to/hbase-0.94.27.jar  (subsitute the actual path for /path/to/ above)", 
            "title": "HDFS with SnappyData store"
        }, 
        {
            "location": "/configuration/#spark-specific-properties", 
            "text": "Since SnappyData embeds Spark components,  Spark Runtime environment properties  (like  spark.driver.memory, spark.executor.memory, spark.driver.extraJavaOptions, spark.executorEnv) do not take effect. They have to be specified using SnappyData configuration properties. Apart from these properties, other Spark properties can be specified in the configuration file of the Lead nodes. You have to prepend them with a  hyphen(-) . The Spark properties that are specified on the Lead node are sent to the Server nodes. Any Spark property that is specified in the conf/servers or conf/locators file is ignored.", 
            "title": "Spark specific properties"
        }, 
        {
            "location": "/configuration/#note", 
            "text": "Currently we do not honour properties specified using spark-config.sh.", 
            "title": "Note:"
        }, 
        {
            "location": "/configuration/#example-configuration", 
            "text": "Let's say you want to start two Locators (on node-a:9999 and node-b:8888), two servers (node-c and node-c) and a lead (node-l). Also, you would like to change the Spark UI port from 4040 to 9090. Also, you would like to set spark.executor.cores as 10 on all servers. The following can be your conf files.   $ cat conf/locators\nnode-a -peer-discovery-port=9999 -dir=/node-a/locator1 -heap-size=1024m -locators=node-b:8888\nnode-b -peer-discovery-port=8888 -dir=/node-b/locator2 -heap-size=1024m -locators=node-a:9999\n\n$ cat conf/servers\nnode-c -dir=/node-c/server1 -heap-size=4096m -locators=node-b:8888,node-a:9999\nnode-c -dir=/node-c/server2 -heap-size=4096m -locators=node-b:8888,node-a:9999\n\n$ cat conf/leads\n# This goes to the default directory \nnode-l -heap-size=4096m -J-XX:MaxPermSize=512m -spark.ui.port=9090 -locators=node-b:8888,node-a:9999 -spark.executor.cores=10", 
            "title": "Example Configuration"
        }, 
        {
            "location": "/configuration/#note_1", 
            "text": "conf files are consulted when servers are started and also when they are stopped. So, we do not recommend changing the conf files while the cluster is running.", 
            "title": "Note:"
        }, 
        {
            "location": "/configuration/#environment-settings", 
            "text": "Any Spark or SnappyData specific environment settings can be done by creating a snappy-env.sh or spark-env.sh in  SNAPPY_HOME/conf .", 
            "title": "Environment settings"
        }, 
        {
            "location": "/configuration/#hadoop-provided-settings", 
            "text": "If you want run SnappyData with an already existing custom Hadoop cluster like MapR or Cloudera you should download Snappy without hadoop from the download link.\nThis will allow you to provide hadoop at runtime.\nTo do this you need to put an entry in $SNAPPY-HOME/conf/spark-env.sh an entry as below.  export SPARK_DIST_CLASSPATH=$($OTHER_HADOOP_HOME/bin/hadoop classpath)", 
            "title": "Hadoop Provided settings"
        }, 
        {
            "location": "/configuration/#per-component-configuration", 
            "text": "Most of the time, components would be sharing the same properties. For e.g. you would want all servers to start with 4096m while leads to start with 2048m. You can configure these by specifying LOCATOR_STARTUP_OPTIONS, SERVER_STARTUP_OPTIONS, LEAD_STARTUP_OPTIONS environment variables in conf/snappy-env.sh.   $ cat conf/snappy-env.sh\nSERVER_STARTUP_OPTIONS= -heap-size=4096m \nLEAD_STARTUP_OPTIONS= -heap-size=2048m", 
            "title": "Per Component Configuration"
        }, 
        {
            "location": "/configuration/#snappy-shell-command-line-utility", 
            "text": "Instead of starting SnappyData members using ssh scripts, they can be individually configured and started using command line.   $ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334\n\n$ bin/snappy-shell locator stop\n$ bin/snappy-shell server stop", 
            "title": "snappy-shell Command Line Utility"
        }, 
        {
            "location": "/configuration/#logging", 
            "text": "Currently, log files for SnappyData components go inside the working directory. To change the log file directory, you can specify a property  -log-file  as the path of the directory. There is a log4j.properties file that is shipped with the jar. We recommend not to change it at this moment. However, to change the logging levels, you can create a conf/log4j.properties with the following details:   $ cat conf/log4j.properties \nlog4j.rootCategory=DEBUG, file", 
            "title": "Logging"
        }, 
        {
            "location": "/connectingToCluster/", 
            "text": "Using the SnappyData SQL Shell\n\n\nThe SnappyData SQL Shell (\nsnappy-shell\n) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. Internally it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.\n\n\n\n\n\n\n// from the SnappyData base directory  \n$ cd quickstart/scripts  \n$ ../../bin/snappy-shell  \nVersion 2.0-BETA\nsnappy\n \n\n//Connect to the cluster as a client  \nsnappy\n connect client 'localhost:1527';\n\n//Show active connections  \nsnappy\n show connections;\n\n//Display cluster members by querying a system table  \nsnappy\n select id, kind, status, host, port from sys.members;\n//or\nsnappy\n show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy\n run 'create_and_load_column_table.sql';\n\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy\n run 'create_and_load_row_table.sql';\n\n\n\n\nThe complete list of commands available through \nsnappy_shell\n can be found \nhere\n\n\nUsing the Spark Shell and spark-submit\n\n\nSnappyData, out-of-the-box, collocates Spark executors and the SnappyData store for efficient data intensive computations. But it may be desirable to isolate the computational cluster for other reasons, for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store snappydata.store.locators property needs to be provided while starting the spark-shell. When spark-shell is started with this property it provides \nSnappyContext\n as SQLContext and thus enabling the user to run all SnappyData functionalities.\n\n\n// from the SnappyData base directory  \n# Start the spark shell in local mode. Pass SnappyData's locators host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041\nscala\n\n#Try few commands on the spark-shell. Following command shows the tables created using the snappy-shell\nscala\n val airlineDF = sqlContext.table(\nairline\n).show\nscala\n val resultset = sqlContext.sql(\nselect * from airline\n)\n\n\n\n\nAny spark application can also use the SnappyData as store and spark as computational engine by providing an extra snappydata.store.locators property in the conf.\n\n\n# Start the Spark standalone cluster from SnappyData base directory \n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart-0.6.jar\n\n# The results can be seen on the command line.\n\n\n\n\nUsing JDBC with SnappyData\n\n\nSnappyData ships with a few JDBC drivers. \nThe connection URL typically points to one of the Locators. Underneath the covers the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically swizzling underlying physical connections in case servers were to fail. \n\n\n\n// 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection (\njdbc:snappydata://locatorHostName:1527/\n);\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint\n\n\n\n\nAccessing SnappyData Tables from Spark code\n\n\nSpark applications access the SnappyStore using the new \nSpark Data Source API\n. \n\n\nBy default, SnappyData servers runs the Spark Executors collocated with the data store. And, the default store provider is SnappyData. \nWhen the spark program connects to the cluster using a \nSnappyContext\n (extends SQLContext), there is no need to configure the database URL and other options.  \n\n\n// Here is an Scala example \n  val sc = new org.apache.spark.SparkContext(conf)\n  val snContext = org.apache.spark.sql.SnappyContext(sc)\n\n  val props = Map[String, String]()\n  // Save some application dataframe into a SnappyData row table\n  myAppDataFrame.write.format(\nrow\n).options(props).saveAsTable(\nMutableTable\n)\n\n\n\n\n\nWhen running a native spark program, you can access SnappyData purely as a DataSource ...\n\n\n// Access SnappyData as a storage cluster .. \n  val sc = new org.apache.spark.SparkContext(conf)\n  val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n  val props = Map(\n    \nurl\n -\n \njdbc:snappydata://locatorHostName:1527/\n,\n    \npoolImpl\n -\n \ntomcat\n, \n    \nuser\n -\n \napp\n,\n    \npassword\n -\n \napp\n\n    )\n\n  // Save some application dataframe into a JDBC DataSource\n  myAppDataFrame.write.format(\njdbc\n).options(props).saveAsTable(\nMutableTable\n)", 
            "title": "Connecting using JDBC, Spark"
        }, 
        {
            "location": "/connectingToCluster/#using-the-snappydata-sql-shell", 
            "text": "The SnappyData SQL Shell ( snappy-shell ) provides a simple command line interface to the SnappyData cluster. It allows you to run interactive queries on row and column stores, run administrative operations and run status commands on the cluster. Internally it uses JDBC to interact with the cluster. You can also use tools like SquirrelSQL or DBVisualizer( JDBC to connect to the cluster) to interact with SnappyData.   \n// from the SnappyData base directory  \n$ cd quickstart/scripts  \n$ ../../bin/snappy-shell  \nVersion 2.0-BETA\nsnappy  \n\n//Connect to the cluster as a client  \nsnappy  connect client 'localhost:1527';\n\n//Show active connections  \nsnappy  show connections;\n\n//Display cluster members by querying a system table  \nsnappy  select id, kind, status, host, port from sys.members;\n//or\nsnappy  show members;\n\n//Run a sql script. This particular script creates and loads a column table in the default schema  \nsnappy  run 'create_and_load_column_table.sql';\n\n\n//Run a sql script. This particular script creates and loads a row table in the default schema  \nsnappy  run 'create_and_load_row_table.sql';  The complete list of commands available through  snappy_shell  can be found  here", 
            "title": "Using the SnappyData SQL Shell"
        }, 
        {
            "location": "/connectingToCluster/#using-the-spark-shell-and-spark-submit", 
            "text": "SnappyData, out-of-the-box, collocates Spark executors and the SnappyData store for efficient data intensive computations. But it may be desirable to isolate the computational cluster for other reasons, for instance, a  computationally intensive Map-reduce machine learning algorithm that needs to iterate over a cached data set repeatedly. To support such scenarios it is also possible to run native Spark jobs that access a SnappyData cluster as a storage layer in a parallel fashion. To connect to the SnappyData store snappydata.store.locators property needs to be provided while starting the spark-shell. When spark-shell is started with this property it provides  SnappyContext  as SQLContext and thus enabling the user to run all SnappyData functionalities.  // from the SnappyData base directory  \n# Start the spark shell in local mode. Pass SnappyData's locators host:port as a conf parameter.\n# Change the UI port because the default port 4040 is being used by Snappy\u2019s lead. \n$ bin/spark-shell  --master local[*] --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041\nscala \n#Try few commands on the spark-shell. Following command shows the tables created using the snappy-shell\nscala  val airlineDF = sqlContext.table( airline ).show\nscala  val resultset = sqlContext.sql( select * from airline )  Any spark application can also use the SnappyData as store and spark as computational engine by providing an extra snappydata.store.locators property in the conf.  # Start the Spark standalone cluster from SnappyData base directory \n$ sbin/start-all.sh \n# Submit AirlineDataSparkApp to Spark Cluster with snappydata's locator host port.\n$ bin/spark-submit --class io.snappydata.examples.AirlineDataSparkApp --master spark://masterhost:7077 --conf snappydata.store.locators=locatorhost:port --conf spark.ui.port=4041 $SNAPPY_HOME/examples/jars/quickstart-0.6.jar\n\n# The results can be seen on the command line.", 
            "title": "Using the Spark Shell and spark-submit"
        }, 
        {
            "location": "/connectingToCluster/#using-jdbc-with-snappydata", 
            "text": "SnappyData ships with a few JDBC drivers. \nThe connection URL typically points to one of the Locators. Underneath the covers the driver acquires the endpoints for all the servers in the cluster along with load information and automatically connects clients to one of the data servers directly. The driver provides HA by automatically swizzling underlying physical connections in case servers were to fail.   \n// 1527 is the default port a Locator or Server uses to listen for thin client connections\nConnection c = DriverManager.getConnection ( jdbc:snappydata://locatorHostName:1527/ );\n// While, clients typically just point to a locator, you could also directly point the \n//   connection at a server endpoint", 
            "title": "Using JDBC with SnappyData"
        }, 
        {
            "location": "/connectingToCluster/#accessing-snappydata-tables-from-spark-code", 
            "text": "Spark applications access the SnappyStore using the new  Spark Data Source API .   By default, SnappyData servers runs the Spark Executors collocated with the data store. And, the default store provider is SnappyData. \nWhen the spark program connects to the cluster using a  SnappyContext  (extends SQLContext), there is no need to configure the database URL and other options.    // Here is an Scala example \n  val sc = new org.apache.spark.SparkContext(conf)\n  val snContext = org.apache.spark.sql.SnappyContext(sc)\n\n  val props = Map[String, String]()\n  // Save some application dataframe into a SnappyData row table\n  myAppDataFrame.write.format( row ).options(props).saveAsTable( MutableTable )  When running a native spark program, you can access SnappyData purely as a DataSource ...  // Access SnappyData as a storage cluster .. \n  val sc = new org.apache.spark.SparkContext(conf)\n  val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n  val props = Map(\n     url  -   jdbc:snappydata://locatorHostName:1527/ ,\n     poolImpl  -   tomcat , \n     user  -   app ,\n     password  -   app \n    )\n\n  // Save some application dataframe into a JDBC DataSource\n  myAppDataFrame.write.format( jdbc ).options(props).saveAsTable( MutableTable )", 
            "title": "Accessing SnappyData Tables from Spark code"
        }, 
        {
            "location": "/jobs/", 
            "text": "Building Snappy applications using Spark API\n\n\nSnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames.  So, we recommend getting to know the \nconcepts in SparkSQL\n and the \nDataFrame API\n. And, you can store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced \nhere\n.\n\n\nIn Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but, can also be reliably managed on disk. \n\n\nSnappyContext\n\n\nA \nSnappyContext\n is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's \nSQLContext\n to work with Row and Column tables. Any DataFrame can be managed as SnappyData tables and any table can be accessed as a DataFrame. This is similar to \nHiveContext\n - integrates the SQLContext functionality with the Snappy store.\n\n\nUsing SnappyContext to create table and query data\n\n\nBelow are examples to create a SnappyContext from SparkContext.\n\n\nScala\n\n\n  val conf = new org.apache.spark.SparkConf()\n               .setAppName(\nExampleTest\n)\n               .setMaster(\nlocal[*]\n)\n\n  val sc = new org.apache.spark.SparkContext(conf)\n  // get the SnappyContext\n  val snc = org.apache.spark.sql.SnappyContext(sc)\n\n\n\n\nJava\n\n\n  SparkConf conf = new org.apache.spark.SparkConf()\n               .setAppName(\nExampleTest\n)\n               .setMaster(\nlocal[*]\n);\n\n  JavaSparkContext sc = new JavaSparkContext(conf);\n  // get the SnappyContext\n  SnappyContext snc = SnappyContext.getOrCreate(sc);\n\n\n\n\nPython\n\n\nfrom pyspark.sql.snappy import SnappyContext\nfrom pyspark import SparkContext, SparkConf\n\nconf = SparkConf().setAppName(\nExampleTest\n).setMaster(\nlocal[*]\n)\nsc = SparkContext(conf=conf)\n# get the SnappyContext\nsnc = SnappyContext(sc)\n\n\n\n\nCreate columnar tables using API. Other than \ncreate\n and \ndrop\n table, rest are all based on the Spark SQL Data Source APIs. \n\n\nScala\n\n\n  val props1 = Map(\nBUCKETS\n -\n \n2\n)  // Number of partitions to use in the SnappyStore\n  case class Data(COL1: Int, COL2: Int, COL3: Int)\n  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n  val rdd = sc.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\n\n  val dataDF = snc.createDataFrame(rdd)\n\n  // create a column table\n  snc.dropTable(\nCOLUMN_TABLE\n, ifExists = true)\n\n  // \ncolumn\n is the table format (that is row or column)\n  // dataDF.schema provides the schema for table\n  snc.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, dataDF.schema, props1)\n  // append dataDF into the table\n  dataDF.write.insertInto(\nCOLUMN_TABLE\n)\n\n  val results1 = snc.sql(\nSELECT * FROM COLUMN_TABLE\n)\n  println(\ncontents of column table are:\n)\n  results1.foreach(println)\n\n\n\n\n\nJava\n\n\n\n    Map\nString, String\n props1 = new HashMap\n();\n    props1.put(\nbuckets\n, \n11\n);\n\n    JavaRDD\nRow\n jrdd = jsc.parallelize(Arrays.asList(\n        RowFactory.create(1,2,3),\n        RowFactory.create(7,8,9),\n        RowFactory.create(9,2,3),\n        RowFactory.create(4,2,3),\n        RowFactory.create(5,6,7)\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n        new StructField(\ncol1\n, DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField(\ncol2\n, DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField(\ncol3\n, DataTypes.IntegerType, false, Metadata.empty()),\n    });\n\n    DataFrame dataDF = snc.createDataFrame(jrdd, schema);\n\n    // create a column table\n    snc.dropTable(\nCOLUMN_TABLE\n, true);\n\n    // \ncolumn\n is the table format (that is row or column)\n    // dataDF.schema provides the schema for table\n    snc.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, dataDF.schema(), props1, false);\n    // append dataDF into the table\n    dataDF.write().insertInto(\nCOLUMN_TABLE\n);\n\n    DataFrame results1 = snc.sql(\nSELECT * FROM COLUMN_TABLE\n);\n    System.out.println(\ncontents of column table are:\n);\n    for (Row r : results1.select(\ncol1\n, \ncol2\n, \ncol3\n). collect()) {\n        System.out.println(r);\n    }\n\n\n\n\n\nPython\n\n\nfrom pyspark.sql.types import *\n\ndata = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]\nrdd = sc.parallelize(data)\nschema=StructType([StructField(\ncol1\n, IntegerType()), \n                   StructField(\ncol2\n, IntegerType()), \n                   StructField(\ncol3\n, IntegerType())])\n\ndataDF = snc.createDataFrame(rdd, schema)\n\n# create a column table\nsnc.dropTable(\nCOLUMN_TABLE\n, True)\n#\ncolumn\n is the table format (that is row or column)\n#dataDF.schema provides the schema for table\nsnc.createTable(\nCOLUMN_TABLE\n, \ncolumn\n, dataDF.schema, True, buckets=\n11\n)\n\n#append dataDF into the table\ndataDF.write.insertInto(\nCOLUMN_TABLE\n)\nresults1 = snc.sql(\nSELECT * FROM COLUMN_TABLE\n)\n\nprint(\ncontents of column table are:\n)\nresults1.select(\ncol1\n, \ncol2\n, \ncol3\n). show()\n\n\n\n\nThe optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster was expanded) a bucket is the smallest unit that can be moved around. For more details about the properties ('props1' map in above example) and createTable API refer to documentation for \nrow and column tables\n\n\nCreate row tables using API, update the contents of row table\n\n\n  // create a row format table called ROW_TABLE\n  snc.dropTable(\nROW_TABLE\n, ifExists = true)\n  // \nrow\n is the table format \n  // dataDF.schema provides the schema for table\n  val props2 = Map.empty[String, String]\n  snc.createTable(\nROW_TABLE\n, \nrow\n, dataDF.schema, props2)\n\n  // append dataDF into the data\n  dataDF.write.insertInto(\nROW_TABLE\n)\n\n  val results2 = snc.sql(\nselect * from ROW_TABLE\n)\n  println(\ncontents of row table are:\n)\n  results2.foreach(println)\n\n  // row tables can be mutated\n  // for example update \nROW_TABLE\n and set col3 to 99 where\n  // criteria \ncol3 = 3\n is true using update API\n  snc.update(\nROW_TABLE\n, \nCOL3 = 3\n, org.apache.spark.sql.Row(99), \nCOL3\n )\n\n  val results3 = snc.sql(\nSELECT * FROM ROW_TABLE\n)\n  println(\ncontents of row table are after setting col3 = 99 are:\n)\n  results3.foreach(println)\n\n  // update rows using sql update statement\n  snc.sql(\nUPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99\n)\n  val results4 = snc.sql(\nSELECT * FROM ROW_TABLE\n)\n  println(\ncontents of row table are after setting col1 = 100 are:\n)\n  results4.foreach(println)\n\n\n\n\nSnappyStreamingContext\n\n\nSnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.\n\n\nBelow example shows how to use the SnappyStreamingContext to apply a schema to existing DStream and then query the SchemaDStream with simple SQL. It also shows the SnappyStreamingContext ability to deal with sql queries.\n\n\nScala\n\n\nimport org.apache.spark.sql._\nimport org.apache.spark.streaming._\nimport scala.collection.mutable\nimport org.apache.spark.rdd._\nimport org.apache.spark.sql.types._\nimport scala.collection.immutable.Map\n\n  val snsc = new SnappyStreamingContext(sc, Duration(1))\n  val schema = StructType(List(StructField(\nid\n, IntegerType) ,StructField(\ntext\n, StringType)))\n\n  case class ShowCaseSchemaStream (loc:Int, text:String)\n\n  snsc.snappyContext.dropTable(\nstreamingExample\n, ifExists = true)\n  snsc.snappyContext.createTable(\nstreamingExample\n, \ncolumn\n,  schema, Map.empty[String, String] , false)\n\n  def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =\n ShowCaseSchemaStream( i, s\nText$i\n))\n\n  val dstream = snsc.queueStream[ShowCaseSchemaStream](\n                mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))\n\n  val schemaDStream = snsc.createSchemaDStream(dstream )\n\n  schemaDStream.foreachDataFrame(df =\n { \n      df.write.format(\ncolumn\n).\n      mode(SaveMode.Append).\n      options(Map.empty[String, String]).\n      saveAsTable(\nstreamingExample\n)    })\n\n  snsc.start()   \n  snsc.sql(\nselect count(*) from streamingExample\n).show\n\n\n\n\nPython\n\n\nfrom pyspark.streaming.snappy.context import SnappyStreamingContext\nfrom pyspark.sql.types import *\n\ndef  rddList(start, end): \n  return sc.parallelize(range(start,  end)).map(lambda i : ( i, \nText\n + str(i)))\n\ndef saveFunction(df):\n   df.write.format(\ncolumn\n).mode(\nappend\n).saveAsTable(\nstreamingExample\n)\n\nschema=StructType([StructField(\nloc\n, IntegerType()), \n                   StructField(\ntext\n, StringType())])\n\nsnsc = SnappyStreamingContext(sc, 1)\n\nsnsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])\n\nsnsc._snappycontext.dropTable(\nstreamingExample\n , True)\nsnsc._snappycontext.createTable(\nstreamingExample\n, \ncolumn\n, schema)\n\ndstream = snsc.queueStream(getQueueOfRDDs())\nschemadstream = snsc.createSchemaDStream(dstream, schema)\nschemadstream.foreachDataFrame(lambda df: saveFunction(df))\nsnsc.start()\n\nsnsc.sql(\nselect count(*) from streamingExample\n).show()\n\n\n\n\n\n\nNote - Currently Snappy dont have Python API's added for continuous queries and SDE/Sampling.\n\n\n\n\nRunning Spark programs inside the database\n\n\n\n\nNote: Above simple example uses local mode (i.e. development mode) to create tables and update data. In the production environment, users will want to deploy the SnappyData system as a unified cluster (default cluster model that consists of servers that embed colocated Spark executors and Snappy stores, locators, and a job server enabled lead node) or as a split cluster (where Spark executors and Snappy stores form independent clusters). Refer to the  \ndeployment\n chapter for all the supported deployment modes and the \nconfiguration\n chapter for configuring the cluster. This mode is supported in both Java and Scala. Support for Python is yet not added.\n\n\n\n\nTo create a job that can be submitted through the job server, the job must implement the \nSnappySQLJob or SnappyStreamingJob\n trait. Your job will look like:\n\n\nScala\n\n\nclass SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation\n}\n\n\n\n\nJava\n\n\nclass SnappySampleJob extends SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  public Object runSnappyJob(SnappyContext snc, Config jobConfig) {//Implementation}\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(SnappyContext snc, Config config) {//validate}\n}\n\n\n\n\n\nScala\n\n\nclass SnappyStreamingSampleJob implements SnappyStreamingJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation\n}\n\n\n\n\nJava\n\n\nclass SnappyStreamingSampleJob extends JavaSnappyStreamingJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)\n  {//validate}\n}\n\n\n\n\n\n\nThe \nJob\n traits are simply extensions of the \nSparkJob\n implemented by \nSpark JobServer\n. \n\n\n\n\n\u2022 \nrunSnappyJob\n contains the implementation of the Job.\nThe \nSnappyContext\n/\nSnappyStreamingContext\n is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.\n\n\n\u2022 \nisValidJob\n allows for an initial validation of the context and any provided configuration.\n If the context and configuration are OK to run the job, returning spark.jobserver.SnappyJobValid\n  will let the job execute, otherwise returning spark.jobserver.SnappyJobInvalid(reason) prevents\n   the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an HTTP/1.1 400 Bad Request status code.\u2028validate helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources.\n\n\nSee \nexamples\n for Spark and spark streaming jobs. \n\n\nSnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs\n\n\nSubmitting jobs\n\n\nFollowing command submits \nCreateAndLoadAirlineDataJob\n from the \nexamples\n directory.   This job creates dataframes from parquet files, loads the data from dataframe into column tables and row tables and creates sample table on column table in its runJob method. The program is compiled into a jar file (quickstart-0.6.jar) and submitted to jobs server as shown below.\n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart-0.6.jar\n\n\n\n\nThe utility snappy-job.sh submits the job and returns a JSON that has a jobId of this job.\n\n\n\n\n--lead option specifies the host name of the lead node along with the port on which it accepts jobs (8090)\n\n\n--app-name option specifies the name given to the submitted app\n\n\n--class specifies the name of the class that contains implementation of the Spark job to be run\n\n\n--app-jar specifies the jar file that packages the code for Spark job\n\n\n\n\nThe status returned by the utility is shown below:\n\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n,\n    \ncontext\n: \nsnappyContext1452598154529305363\n\n  }\n}\n\n\n\n\nThis job ID can be used to query the status of the running job. \n\n\n$ bin/snappy-job.sh status  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{\n  \nduration\n: \n17.53 secs\n,\n  \nclassPath\n: \nio.snappydata.examples.CreateAndLoadAirlineDataJob\n,\n  \nstartTime\n: \n2016-01-12T16:59:14.746+05:30\n,\n  \ncontext\n: \nsnappyContext1452598154529305363\n,\n  \nresult\n: \nSee /home/hemant/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out\n,\n  \nstatus\n: \nFINISHED\n,\n  \njobId\n: \n321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n}\n\n\n\n\nOnce the tables are created, they can be queried by firing another job. Please refer to \nAirlineDataJob\n from \nexamples\n for the implementation of the job. \n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart-0.6.jar\n\n\n\n\nThe status of this job can be queried in the same manner as shown above. The result of the this job will return a file path that has the query results. \n\n\nPython users can also submit the python script using spark-submit in split cluster mode. For example below script can be used to read the data loaded by the CreateAndLoadAirlineDataJob. \"snappydata.store.locators\" property denotes the locator url of the snappy cluster and it is used to connect to the snappy cluster.\n\n\n$ bin/spark-submit \\\n  --master spark://pnq-nthanvi02:7077 \\\n  --conf snappydata.store.locators=localhost:10334 \\\n  --conf spark.ui.port=4042  \n  python/examples/AirlineDataPythonApp.py\n\n\n\n\nStreaming jobs\n\n\nAn implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying \n--stream\n as an option to the submit command. This option will cause creation of a new SnappyStreamingContext before the job is submitted. Alternatively, user may specify the name of an existing/pre-created streaming context as \n--context \ncontext-name\n with the submit command.\n\n\nFor example, \nTwitterPopularTagsJob\n from the \nexamples\n directory can be submitted as follows. This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, top 10 popular tweets.\n\n\n$ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart-0.6.jar \\\n    --stream\n\n{\n  \nstatus\n: \nSTARTED\n,\n  \nresult\n: {\n    \njobId\n: \n982ac142-3550-41e1-aace-6987cb39fec8\n,\n    \ncontext\n: \nsnappyStreamingContext1463987084945028747\n\n  }\n}\n\n\n\n\nUser needs to stop the currently running streaming job followed by its streaming context if the user intends to submit another streaming job with a new streaming context.\n\n\n$ bin/snappy-job.sh stop  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 982ac142-3550-41e1-aace-6987cb39fec8\n\n$ bin/snappy-job.sh listcontexts  \\\n    --lead hostNameOfLead:8090\n[\nsnappyContext1452598154529305363\n, \nsnappyStreamingContext1463987084945028747\n, \nsnappyStreamingContext\n]\n\n$ bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \\\n    --lead hostNameOfLead:8090", 
            "title": "Developing Apps using the Spark API"
        }, 
        {
            "location": "/jobs/#building-snappy-applications-using-spark-api", 
            "text": "SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames.  So, we recommend getting to know the  concepts in SparkSQL  and the  DataFrame API . And, you can store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced  here .  In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but, can also be reliably managed on disk.", 
            "title": "Building Snappy applications using Spark API"
        }, 
        {
            "location": "/jobs/#snappycontext", 
            "text": "A  SnappyContext  is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's  SQLContext  to work with Row and Column tables. Any DataFrame can be managed as SnappyData tables and any table can be accessed as a DataFrame. This is similar to  HiveContext  - integrates the SQLContext functionality with the Snappy store.", 
            "title": "SnappyContext"
        }, 
        {
            "location": "/jobs/#using-snappycontext-to-create-table-and-query-data", 
            "text": "Below are examples to create a SnappyContext from SparkContext.", 
            "title": "Using SnappyContext to create table and query data"
        }, 
        {
            "location": "/jobs/#scala", 
            "text": "val conf = new org.apache.spark.SparkConf()\n               .setAppName( ExampleTest )\n               .setMaster( local[*] )\n\n  val sc = new org.apache.spark.SparkContext(conf)\n  // get the SnappyContext\n  val snc = org.apache.spark.sql.SnappyContext(sc)", 
            "title": "Scala"
        }, 
        {
            "location": "/jobs/#java", 
            "text": "SparkConf conf = new org.apache.spark.SparkConf()\n               .setAppName( ExampleTest )\n               .setMaster( local[*] );\n\n  JavaSparkContext sc = new JavaSparkContext(conf);\n  // get the SnappyContext\n  SnappyContext snc = SnappyContext.getOrCreate(sc);", 
            "title": "Java"
        }, 
        {
            "location": "/jobs/#python", 
            "text": "from pyspark.sql.snappy import SnappyContext\nfrom pyspark import SparkContext, SparkConf\n\nconf = SparkConf().setAppName( ExampleTest ).setMaster( local[*] )\nsc = SparkContext(conf=conf)\n# get the SnappyContext\nsnc = SnappyContext(sc)  Create columnar tables using API. Other than  create  and  drop  table, rest are all based on the Spark SQL Data Source APIs.", 
            "title": "Python"
        }, 
        {
            "location": "/jobs/#scala_1", 
            "text": "val props1 = Map( BUCKETS  -   2 )  // Number of partitions to use in the SnappyStore\n  case class Data(COL1: Int, COL2: Int, COL3: Int)\n  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\n  val rdd = sc.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\n\n  val dataDF = snc.createDataFrame(rdd)\n\n  // create a column table\n  snc.dropTable( COLUMN_TABLE , ifExists = true)\n\n  //  column  is the table format (that is row or column)\n  // dataDF.schema provides the schema for table\n  snc.createTable( COLUMN_TABLE ,  column , dataDF.schema, props1)\n  // append dataDF into the table\n  dataDF.write.insertInto( COLUMN_TABLE )\n\n  val results1 = snc.sql( SELECT * FROM COLUMN_TABLE )\n  println( contents of column table are: )\n  results1.foreach(println)", 
            "title": "Scala"
        }, 
        {
            "location": "/jobs/#java_1", 
            "text": "Map String, String  props1 = new HashMap ();\n    props1.put( buckets ,  11 );\n\n    JavaRDD Row  jrdd = jsc.parallelize(Arrays.asList(\n        RowFactory.create(1,2,3),\n        RowFactory.create(7,8,9),\n        RowFactory.create(9,2,3),\n        RowFactory.create(4,2,3),\n        RowFactory.create(5,6,7)\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n        new StructField( col1 , DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField( col2 , DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField( col3 , DataTypes.IntegerType, false, Metadata.empty()),\n    });\n\n    DataFrame dataDF = snc.createDataFrame(jrdd, schema);\n\n    // create a column table\n    snc.dropTable( COLUMN_TABLE , true);\n\n    //  column  is the table format (that is row or column)\n    // dataDF.schema provides the schema for table\n    snc.createTable( COLUMN_TABLE ,  column , dataDF.schema(), props1, false);\n    // append dataDF into the table\n    dataDF.write().insertInto( COLUMN_TABLE );\n\n    DataFrame results1 = snc.sql( SELECT * FROM COLUMN_TABLE );\n    System.out.println( contents of column table are: );\n    for (Row r : results1.select( col1 ,  col2 ,  col3 ). collect()) {\n        System.out.println(r);\n    }", 
            "title": "Java"
        }, 
        {
            "location": "/jobs/#python_1", 
            "text": "from pyspark.sql.types import *\n\ndata = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]\nrdd = sc.parallelize(data)\nschema=StructType([StructField( col1 , IntegerType()), \n                   StructField( col2 , IntegerType()), \n                   StructField( col3 , IntegerType())])\n\ndataDF = snc.createDataFrame(rdd, schema)\n\n# create a column table\nsnc.dropTable( COLUMN_TABLE , True)\n# column  is the table format (that is row or column)\n#dataDF.schema provides the schema for table\nsnc.createTable( COLUMN_TABLE ,  column , dataDF.schema, True, buckets= 11 )\n\n#append dataDF into the table\ndataDF.write.insertInto( COLUMN_TABLE )\nresults1 = snc.sql( SELECT * FROM COLUMN_TABLE )\n\nprint( contents of column table are: )\nresults1.select( col1 ,  col2 ,  col3 ). show()  The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster was expanded) a bucket is the smallest unit that can be moved around. For more details about the properties ('props1' map in above example) and createTable API refer to documentation for  row and column tables  Create row tables using API, update the contents of row table    // create a row format table called ROW_TABLE\n  snc.dropTable( ROW_TABLE , ifExists = true)\n  //  row  is the table format \n  // dataDF.schema provides the schema for table\n  val props2 = Map.empty[String, String]\n  snc.createTable( ROW_TABLE ,  row , dataDF.schema, props2)\n\n  // append dataDF into the data\n  dataDF.write.insertInto( ROW_TABLE )\n\n  val results2 = snc.sql( select * from ROW_TABLE )\n  println( contents of row table are: )\n  results2.foreach(println)\n\n  // row tables can be mutated\n  // for example update  ROW_TABLE  and set col3 to 99 where\n  // criteria  col3 = 3  is true using update API\n  snc.update( ROW_TABLE ,  COL3 = 3 , org.apache.spark.sql.Row(99),  COL3  )\n\n  val results3 = snc.sql( SELECT * FROM ROW_TABLE )\n  println( contents of row table are after setting col3 = 99 are: )\n  results3.foreach(println)\n\n  // update rows using sql update statement\n  snc.sql( UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99 )\n  val results4 = snc.sql( SELECT * FROM ROW_TABLE )\n  println( contents of row table are after setting col1 = 100 are: )\n  results4.foreach(println)", 
            "title": "Python"
        }, 
        {
            "location": "/jobs/#snappystreamingcontext", 
            "text": "SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.  Below example shows how to use the SnappyStreamingContext to apply a schema to existing DStream and then query the SchemaDStream with simple SQL. It also shows the SnappyStreamingContext ability to deal with sql queries.", 
            "title": "SnappyStreamingContext"
        }, 
        {
            "location": "/jobs/#scala_2", 
            "text": "import org.apache.spark.sql._\nimport org.apache.spark.streaming._\nimport scala.collection.mutable\nimport org.apache.spark.rdd._\nimport org.apache.spark.sql.types._\nimport scala.collection.immutable.Map\n\n  val snsc = new SnappyStreamingContext(sc, Duration(1))\n  val schema = StructType(List(StructField( id , IntegerType) ,StructField( text , StringType)))\n\n  case class ShowCaseSchemaStream (loc:Int, text:String)\n\n  snsc.snappyContext.dropTable( streamingExample , ifExists = true)\n  snsc.snappyContext.createTable( streamingExample ,  column ,  schema, Map.empty[String, String] , false)\n\n  def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =  ShowCaseSchemaStream( i, s Text$i ))\n\n  val dstream = snsc.queueStream[ShowCaseSchemaStream](\n                mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))\n\n  val schemaDStream = snsc.createSchemaDStream(dstream )\n\n  schemaDStream.foreachDataFrame(df =  { \n      df.write.format( column ).\n      mode(SaveMode.Append).\n      options(Map.empty[String, String]).\n      saveAsTable( streamingExample )    })\n\n  snsc.start()   \n  snsc.sql( select count(*) from streamingExample ).show", 
            "title": "Scala"
        }, 
        {
            "location": "/jobs/#python_2", 
            "text": "from pyspark.streaming.snappy.context import SnappyStreamingContext\nfrom pyspark.sql.types import *\n\ndef  rddList(start, end): \n  return sc.parallelize(range(start,  end)).map(lambda i : ( i,  Text  + str(i)))\n\ndef saveFunction(df):\n   df.write.format( column ).mode( append ).saveAsTable( streamingExample )\n\nschema=StructType([StructField( loc , IntegerType()), \n                   StructField( text , StringType())])\n\nsnsc = SnappyStreamingContext(sc, 1)\n\nsnsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])\n\nsnsc._snappycontext.dropTable( streamingExample  , True)\nsnsc._snappycontext.createTable( streamingExample ,  column , schema)\n\ndstream = snsc.queueStream(getQueueOfRDDs())\nschemadstream = snsc.createSchemaDStream(dstream, schema)\nschemadstream.foreachDataFrame(lambda df: saveFunction(df))\nsnsc.start()\n\nsnsc.sql( select count(*) from streamingExample ).show()   Note - Currently Snappy dont have Python API's added for continuous queries and SDE/Sampling.", 
            "title": "Python"
        }, 
        {
            "location": "/jobs/#running-spark-programs-inside-the-database", 
            "text": "Note: Above simple example uses local mode (i.e. development mode) to create tables and update data. In the production environment, users will want to deploy the SnappyData system as a unified cluster (default cluster model that consists of servers that embed colocated Spark executors and Snappy stores, locators, and a job server enabled lead node) or as a split cluster (where Spark executors and Snappy stores form independent clusters). Refer to the   deployment  chapter for all the supported deployment modes and the  configuration  chapter for configuring the cluster. This mode is supported in both Java and Scala. Support for Python is yet not added.   To create a job that can be submitted through the job server, the job must implement the  SnappySQLJob or SnappyStreamingJob  trait. Your job will look like:", 
            "title": "Running Spark programs inside the database"
        }, 
        {
            "location": "/jobs/#scala_3", 
            "text": "class SnappySampleJob implements SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappyContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation\n}", 
            "title": "Scala"
        }, 
        {
            "location": "/jobs/#java_2", 
            "text": "class SnappySampleJob extends SnappySQLJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  public Object runSnappyJob(SnappyContext snc, Config jobConfig) {//Implementation}\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(SnappyContext snc, Config config) {//validate}\n}", 
            "title": "Java"
        }, 
        {
            "location": "/jobs/#scala_4", 
            "text": "class SnappyStreamingSampleJob implements SnappyStreamingJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation\n}", 
            "title": "Scala"
        }, 
        {
            "location": "/jobs/#java_3", 
            "text": "class SnappyStreamingSampleJob extends JavaSnappyStreamingJob {\n  /** Snappy uses this as an entry point to execute Snappy jobs. **/\n  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }\n\n  /** SnappyData calls this function to validate the job input and reject invalid job requests **/\n  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)\n  {//validate}\n}   The  Job  traits are simply extensions of the  SparkJob  implemented by  Spark JobServer .    \u2022  runSnappyJob  contains the implementation of the Job.\nThe  SnappyContext / SnappyStreamingContext  is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.  \u2022  isValidJob  allows for an initial validation of the context and any provided configuration.\n If the context and configuration are OK to run the job, returning spark.jobserver.SnappyJobValid\n  will let the job execute, otherwise returning spark.jobserver.SnappyJobInvalid(reason) prevents\n   the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an HTTP/1.1 400 Bad Request status code.\u2028validate helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources.  See  examples  for Spark and spark streaming jobs.   SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs", 
            "title": "Java"
        }, 
        {
            "location": "/jobs/#submitting-jobs", 
            "text": "Following command submits  CreateAndLoadAirlineDataJob  from the  examples  directory.   This job creates dataframes from parquet files, loads the data from dataframe into column tables and row tables and creates sample table on column table in its runJob method. The program is compiled into a jar file (quickstart-0.6.jar) and submitted to jobs server as shown below.  $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart-0.6.jar  The utility snappy-job.sh submits the job and returns a JSON that has a jobId of this job.   --lead option specifies the host name of the lead node along with the port on which it accepts jobs (8090)  --app-name option specifies the name given to the submitted app  --class specifies the name of the class that contains implementation of the Spark job to be run  --app-jar specifies the jar file that packages the code for Spark job   The status returned by the utility is shown below:  {\n   status :  STARTED ,\n   result : {\n     jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 ,\n     context :  snappyContext1452598154529305363 \n  }\n}  This job ID can be used to query the status of the running job.   $ bin/snappy-job.sh status  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48\n\n{\n   duration :  17.53 secs ,\n   classPath :  io.snappydata.examples.CreateAndLoadAirlineDataJob ,\n   startTime :  2016-01-12T16:59:14.746+05:30 ,\n   context :  snappyContext1452598154529305363 ,\n   result :  See /home/hemant/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out ,\n   status :  FINISHED ,\n   jobId :  321e5136-4a18-4c4f-b8ab-f3c8f04f0b48 \n}  Once the tables are created, they can be queried by firing another job. Please refer to  AirlineDataJob  from  examples  for the implementation of the job.   $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.AirlineDataJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart-0.6.jar  The status of this job can be queried in the same manner as shown above. The result of the this job will return a file path that has the query results.   Python users can also submit the python script using spark-submit in split cluster mode. For example below script can be used to read the data loaded by the CreateAndLoadAirlineDataJob. \"snappydata.store.locators\" property denotes the locator url of the snappy cluster and it is used to connect to the snappy cluster.  $ bin/spark-submit \\\n  --master spark://pnq-nthanvi02:7077 \\\n  --conf snappydata.store.locators=localhost:10334 \\\n  --conf spark.ui.port=4042  \n  python/examples/AirlineDataPythonApp.py", 
            "title": "Submitting jobs"
        }, 
        {
            "location": "/jobs/#streaming-jobs", 
            "text": "An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying  --stream  as an option to the submit command. This option will cause creation of a new SnappyStreamingContext before the job is submitted. Alternatively, user may specify the name of an existing/pre-created streaming context as  --context  context-name  with the submit command.  For example,  TwitterPopularTagsJob  from the  examples  directory can be submitted as follows. This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, top 10 popular tweets.  $ bin/snappy-job.sh submit  \\\n    --lead hostNameOfLead:8090  \\\n    --app-name airlineApp \\\n    --class  io.snappydata.examples.TwitterPopularTagsJob \\\n    --app-jar $SNAPPY_HOME/examples/jars/quickstart-0.6.jar \\\n    --stream\n\n{\n   status :  STARTED ,\n   result : {\n     jobId :  982ac142-3550-41e1-aace-6987cb39fec8 ,\n     context :  snappyStreamingContext1463987084945028747 \n  }\n}  User needs to stop the currently running streaming job followed by its streaming context if the user intends to submit another streaming job with a new streaming context.  $ bin/snappy-job.sh stop  \\\n    --lead hostNameOfLead:8090  \\\n    --job-id 982ac142-3550-41e1-aace-6987cb39fec8\n\n$ bin/snappy-job.sh listcontexts  \\\n    --lead hostNameOfLead:8090\n[ snappyContext1452598154529305363 ,  snappyStreamingContext1463987084945028747 ,  snappyStreamingContext ]\n\n$ bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \\\n    --lead hostNameOfLead:8090", 
            "title": "Streaming jobs"
        }, 
        {
            "location": "/rowAndColumnTables/", 
            "text": "Row and column tables\n\n\nColumn tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.\n\n\nRow tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.\n\n\nCreate table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.\n\n\nDDL and DML Syntax for tables\n\n\nCREATE TABLE [IF NOT EXISTS] table_name\n   (\n  COLUMN_DEFININTION\n   )\nUSING 'row | column'\nOPTIONS (\nCOLOCATE_WITH 'table_name',  // Default none\nPARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table.\nBUCKETS  'NumPartitions', // Default 113\nREDUNDANCY        '1' ,\nRECOVER_DELAY     '-1',\nMAX_PART_SIZE      '50',\nEVICTION_BY \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT,\nPERSISTENT  \u2018DISKSTORE_NAME ASYNCHRONOUS | SYNCHRONOUS\u2019, //empty string will map to default diskstore\nOFFHEAP \u2018true | false\u2019 ,\nEXPIRE \u2018TIMETOLIVE in seconds',\n)\n[AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name\n\n\n\nFor row format tables column definition can take underlying GemFire XD syntax to create a table.e.g.note the PRIMARY KEY clause below.\n\n\nsnc.sql(\"CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT)\n         USING row options(BUCKETS '5')\" )\n\n\n\nBut for column table it's restricted to Spark syntax for column definition e.g.\n\n\nsnc.sql(\"CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '5')\" )\n\n\n\nClauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition. \n\n\nSpark API for managing tables\n\n\nGet a reference to \nSnappyContext\n\n\nval snc: SnappyContext = SnappyContext.getOrCreate(sparkContext)\n\n\n\nCreate a SnappyStore table using Spark APIs\n\n\nval props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section\ncase class Data(col1: Int, col2: Int, col3: Int)\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sc.parallelize(data, data.length).map(s =\n new Data(s(0), s(1), s(2)))\nval dataDF = snc.createDataFrame(rdd)\nsnc.createTable(\"column_table\", \"column\", dataDF.schema, props)\n//or create a row format table\nsnc.createTable(\"row_table\", \"row\", dataDF.schema, props)\n\n\n\nDrop a SnappyStore table using Spark APIs\n\n\nsnc.dropTable(tableName, ifExists = true)\n\n\n\nDDL extensions to SnappyStore tables\n\n\nThe below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified, default values are attached. See next section for various restrictions. \n\n\n\n\nCOLOCATE_WITH  : The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist.\n\n\nPARTITION_BY  : Use the PARTITION_BY {COLUMN} clause to provide a set of column names that will determine the partitioning. As a shortcut you can use PARTITION BY PRIMARY KEY to refer to the primary key columns defined for the table . If not specified, it will be a replicated table.\n\n\nBUCKETS  : The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 113.\n\n\nREDUNDANCY : Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail.\n\n\nRECOVER_DELAY : Use the RECOVERY_DELAY clause to specify the default time in milliseconds that existing members will wait before satisfying redundancy after a member crashes. The default is -1, which indicates that redundancy is not recovered after a member fails.\n\n\nMAX_PART_SIZE : The MAXPARTSIZE attribute specifies the maximum memory for any partition on a member in megabytes. Use it to load-balance partitions among available members. If you omit MAXPARTSIZE, then GemFire XD calculates a default value for the table based on available heap memory. You can view the MAXPARTSIZE setting by querying the EVICTIONATTRS column in SYSTABLES.\n\n\nEVICTION_BY : Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store\n\n\nPERSISTENT :  When you specify the PERSISTENT keyword, GemFire XD persists the in-memory table data to a local GemFire XD disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.\n\n\nOFFHEAP : SnappyStore enables you to store the data for selected tables outside of the JVM heap. Storing a table in off-heap memory can improve performance for the table by reducing the CPU resources required to manage the table's data in the heap (garbage collection)\n\n\nEXPIRE: You can use the EXPIRE clause with tables to control SnappyStore memory usage. It will expire the rows after configured TTL.\n\n\n\n\nRestrictions on column tables in the 0.6 release\n\n\n\n\nColumn tables can not specify any primary key, unique key constraints.\n\n\nIndex on column table is not supported.\n\n\nOption EXPIRE is not applicable for column tables.\n\n\nOption EVICTION_BY with value LRUCOUNT is not applicable for column tables. \n\n\n\n\nDML operations on tables\n\n\nINSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\nUPDATE tablename SET column = value [, column = value ...] [WHERE expression]\nPUT INTO tableName (column, ...) VALUES (value, ...)\nDELETE FROM tablename1 [WHERE expression]\nTRUNCATE TABLE tablename1;\n\n\n\nAPI extensions provided in SnappyContext\n\n\nWe have added several APIs in \nSnappyContext\n to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables.\n\n\n//  Applicable for both row \n column tables\ndef insert(tableName: String, rows: Row*): Int .\n\n// Only for row tables\ndef put(tableName: String, rows: Row*): Int\ndef update(tableName: String, filterExpr: String, newColumnValues: Row, \n           updateColumns: String*): Int\ndef delete(tableName: String, filterExpr: String): Int\n\n\n\nUsage SnappyConytext.insert(): Insert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r =\n\n  snappyContext.insert(\"tableName\", Row.fromSeq(r))\n}\n\n\n\nUsage SnappyConytext.put(): Upsert one or more [[org.apache.spark.sql.Row]] into an existing table\n\n\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r =\n\n  snc.put(tableName, Row.fromSeq(r))\n}\n\n\n\nUsage SnappyConytext.update(): Update all rows in table that match passed filter expression\n\n\nsnc.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" )\n\n\n\nUsage SnappyConytext.delete(): Delete all rows in table that match passed filter expression\n\n\nsnc.delete(tableName, \"ITEMREF = 3\")\n\n\n\nRow Buffers for column tables\n\n\nGenerally, the Column table is used for analytical purpose. To this end, most of the\noperations (read or write) on it are bulk operations. Taking advantage of this fact\nthe rows are compressed column wise and stored.\n\n\nIn SnappyData, the column table consists of two components, delta row buffer and\ncolumn store. We try to support individual insert of single row, we store them in\na delta row buffer which is write optimized and highly available.\nOnce the size of buffer reaches the COLUMN_BATCH_SIZE set by user, the delta row\nbuffer is compressed column wise and stored in the column store.\n\n\nAny query on column table, also takes into account the row cached buffer. By doing\nthis, we ensure that the query doesn't miss any data.\n\n\nCatalog in SnappyStore\n\n\nWe use a persistent Hive catalog for all our metadata storage. All table, schema definition are stored here in a reliable manner. As we intend be able to quickly recover from driver failover, we chose GemFireXd itself to store meta information. This gives us ability to query underlying GemFireXD to reconstruct the metastore incase of a driver failover. \n\n\nThere are pending work towards unifying DRDA \n Spark layer catalog, which will part of future releases. \n\n\nSQL Reference to the Syntax\n\n\nFor detailed syntax for GemFire XD check\nhttp://gemfirexd.docs.pivotal.io/docs-gemfirexd/reference/sql-language-reference.html", 
            "title": "Row and Column tables"
        }, 
        {
            "location": "/rowAndColumnTables/#row-and-column-tables", 
            "text": "Column tables organize and manage data in memory in compressed columnar form such that modern day CPUs can traverse and run computations like a sum or an average really fast (as the values are available in contiguous memory). Column table follows the Spark DataSource access model.  Row tables, unlike column tables, are laid out one row at a time in contiguous memory. Rows are typically accessed using keys and its location determined by a hash function and hence very fast for point lookups or updates.  Create table DDL for Row and Column tables allows tables to be partitioned on primary keys, custom partitioned, replicated, carry indexes in memory, persist to disk, overflow to disk, be replicated for HA, etc.", 
            "title": "Row and column tables"
        }, 
        {
            "location": "/rowAndColumnTables/#ddl-and-dml-syntax-for-tables", 
            "text": "CREATE TABLE [IF NOT EXISTS] table_name\n   (\n  COLUMN_DEFININTION\n   )\nUSING 'row | column'\nOPTIONS (\nCOLOCATE_WITH 'table_name',  // Default none\nPARTITION_BY 'PRIMARY KEY | column name', // If not specified it will be a replicated table.\nBUCKETS  'NumPartitions', // Default 113\nREDUNDANCY        '1' ,\nRECOVER_DELAY     '-1',\nMAX_PART_SIZE      '50',\nEVICTION_BY \u2018LRUMEMSIZE 200 | LRUCOUNT 200 | LRUHEAPPERCENT,\nPERSISTENT  \u2018DISKSTORE_NAME ASYNCHRONOUS | SYNCHRONOUS\u2019, //empty string will map to default diskstore\nOFFHEAP \u2018true | false\u2019 ,\nEXPIRE \u2018TIMETOLIVE in seconds',\n)\n[AS select_statement];\n\nDROP TABLE [IF EXISTS] table_name  For row format tables column definition can take underlying GemFire XD syntax to create a table.e.g.note the PRIMARY KEY clause below.  snc.sql(\"CREATE TABLE tableName (Col1 INT NOT NULL PRIMARY KEY, Col2 INT, Col3 INT)\n         USING row options(BUCKETS '5')\" )  But for column table it's restricted to Spark syntax for column definition e.g.  snc.sql(\"CREATE TABLE tableName (Col1 INT ,Col2 INT, Col3 INT) USING column options(BUCKETS '5')\" )  Clauses like PRIMARY KEY, NOT NULL etc. are not supported for column definition.", 
            "title": "DDL and DML Syntax for tables"
        }, 
        {
            "location": "/rowAndColumnTables/#spark-api-for-managing-tables", 
            "text": "Get a reference to  SnappyContext  val snc: SnappyContext = SnappyContext.getOrCreate(sparkContext)  Create a SnappyStore table using Spark APIs  val props = Map('BUCKETS','5') //This map should contain required DDL extensions, see next section\ncase class Data(col1: Int, col2: Int, col3: Int)\nval data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))\nval rdd = sc.parallelize(data, data.length).map(s =  new Data(s(0), s(1), s(2)))\nval dataDF = snc.createDataFrame(rdd)\nsnc.createTable(\"column_table\", \"column\", dataDF.schema, props)\n//or create a row format table\nsnc.createTable(\"row_table\", \"row\", dataDF.schema, props)  Drop a SnappyStore table using Spark APIs  snc.dropTable(tableName, ifExists = true)", 
            "title": "Spark API for managing tables"
        }, 
        {
            "location": "/rowAndColumnTables/#ddl-extensions-to-snappystore-tables", 
            "text": "The below mentioned DDL extensions are required to configure a table based on user requirements. One can specify one or more options to create the kind of table one wants. If no option is specified, default values are attached. See next section for various restrictions.    COLOCATE_WITH  : The COLOCATE_WITH clause specifies a partitioned table with which the new partitioned table must be colocated. The referenced table must already exist.  PARTITION_BY  : Use the PARTITION_BY {COLUMN} clause to provide a set of column names that will determine the partitioning. As a shortcut you can use PARTITION BY PRIMARY KEY to refer to the primary key columns defined for the table . If not specified, it will be a replicated table.  BUCKETS  : The optional BUCKETS attribute specifies the fixed number of \"buckets,\" the smallest unit of data containment for the table that can be moved around. Data in a single bucket resides and moves together. If not specified, the number of buckets defaults to 113.  REDUNDANCY : Use the REDUNDANCY clause to specify the number of redundant copies that should be maintained for each partition, to ensure that the partitioned table is highly available even if members fail.  RECOVER_DELAY : Use the RECOVERY_DELAY clause to specify the default time in milliseconds that existing members will wait before satisfying redundancy after a member crashes. The default is -1, which indicates that redundancy is not recovered after a member fails.  MAX_PART_SIZE : The MAXPARTSIZE attribute specifies the maximum memory for any partition on a member in megabytes. Use it to load-balance partitions among available members. If you omit MAXPARTSIZE, then GemFire XD calculates a default value for the table based on available heap memory. You can view the MAXPARTSIZE setting by querying the EVICTIONATTRS column in SYSTABLES.  EVICTION_BY : Use the EVICTION_BY clause to evict rows automatically from the in-memory table based on different criteria. You can use this clause to create an overflow table where evicted rows are written to a local SnappyStore disk store  PERSISTENT :  When you specify the PERSISTENT keyword, GemFire XD persists the in-memory table data to a local GemFire XD disk store configuration. SnappyStore automatically restores the persisted table data to memory when you restart the member.  OFFHEAP : SnappyStore enables you to store the data for selected tables outside of the JVM heap. Storing a table in off-heap memory can improve performance for the table by reducing the CPU resources required to manage the table's data in the heap (garbage collection)  EXPIRE: You can use the EXPIRE clause with tables to control SnappyStore memory usage. It will expire the rows after configured TTL.", 
            "title": "DDL extensions to SnappyStore tables"
        }, 
        {
            "location": "/rowAndColumnTables/#restrictions-on-column-tables-in-the-06-release", 
            "text": "Column tables can not specify any primary key, unique key constraints.  Index on column table is not supported.  Option EXPIRE is not applicable for column tables.  Option EVICTION_BY with value LRUCOUNT is not applicable for column tables.", 
            "title": "Restrictions on column tables in the 0.6 release"
        }, 
        {
            "location": "/rowAndColumnTables/#dml-operations-on-tables", 
            "text": "INSERT OVERWRITE TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 select_statement1 FROM from_statement;\nINSERT INTO TABLE tablename1 VALUES (value1, value2 ..) ;\nUPDATE tablename SET column = value [, column = value ...] [WHERE expression]\nPUT INTO tableName (column, ...) VALUES (value, ...)\nDELETE FROM tablename1 [WHERE expression]\nTRUNCATE TABLE tablename1;", 
            "title": "DML operations on tables"
        }, 
        {
            "location": "/rowAndColumnTables/#api-extensions-provided-in-snappycontext", 
            "text": "We have added several APIs in  SnappyContext  to manipulate data stored in row and column format. Apart from SQL these APIs can be used to manipulate tables.  //  Applicable for both row   column tables\ndef insert(tableName: String, rows: Row*): Int .\n\n// Only for row tables\ndef put(tableName: String, rows: Row*): Int\ndef update(tableName: String, filterExpr: String, newColumnValues: Row, \n           updateColumns: String*): Int\ndef delete(tableName: String, filterExpr: String): Int  Usage SnappyConytext.insert(): Insert one or more [[org.apache.spark.sql.Row]] into an existing table  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r = \n  snappyContext.insert(\"tableName\", Row.fromSeq(r))\n}  Usage SnappyConytext.put(): Upsert one or more [[org.apache.spark.sql.Row]] into an existing table  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3),\n               Seq(5, 6, 7), Seq(1,100,200))\ndata.map { r = \n  snc.put(tableName, Row.fromSeq(r))\n}  Usage SnappyConytext.update(): Update all rows in table that match passed filter expression  snc.update(tableName, \"ITEMREF = 3\" , Row(99) , \"ITEMREF\" )  Usage SnappyConytext.delete(): Delete all rows in table that match passed filter expression  snc.delete(tableName, \"ITEMREF = 3\")", 
            "title": "API extensions provided in SnappyContext"
        }, 
        {
            "location": "/rowAndColumnTables/#row-buffers-for-column-tables", 
            "text": "Generally, the Column table is used for analytical purpose. To this end, most of the\noperations (read or write) on it are bulk operations. Taking advantage of this fact\nthe rows are compressed column wise and stored.  In SnappyData, the column table consists of two components, delta row buffer and\ncolumn store. We try to support individual insert of single row, we store them in\na delta row buffer which is write optimized and highly available.\nOnce the size of buffer reaches the COLUMN_BATCH_SIZE set by user, the delta row\nbuffer is compressed column wise and stored in the column store.  Any query on column table, also takes into account the row cached buffer. By doing\nthis, we ensure that the query doesn't miss any data.", 
            "title": "Row Buffers for column tables"
        }, 
        {
            "location": "/rowAndColumnTables/#catalog-in-snappystore", 
            "text": "We use a persistent Hive catalog for all our metadata storage. All table, schema definition are stored here in a reliable manner. As we intend be able to quickly recover from driver failover, we chose GemFireXd itself to store meta information. This gives us ability to query underlying GemFireXD to reconstruct the metastore incase of a driver failover.   There are pending work towards unifying DRDA   Spark layer catalog, which will part of future releases.", 
            "title": "Catalog in SnappyStore"
        }, 
        {
            "location": "/rowAndColumnTables/#sql-reference-to-the-syntax", 
            "text": "For detailed syntax for GemFire XD check\nhttp://gemfirexd.docs.pivotal.io/docs-gemfirexd/reference/sql-language-reference.html", 
            "title": "SQL Reference to the Syntax"
        }, 
        {
            "location": "/aqp/", 
            "text": "Overview of Synopsis Data Engine (SDE)\n\n\nThe SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time. \n\n\nFor instance, in exploratory analytics, a data analyst might be slicing and dicing large data sets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering) , while the engineer continues to slice and dice the data sets without any interruptions. \n\n\nWhen accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer, or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters.\n\n\nWhile in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records. \n\n\nUnlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a priori known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query.\n\n\nHow does it work?\n\n\nThe following diagram provides a simplified view into how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can however be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample. \n\n\n\n\nKey Concepts\n\n\nSnappyData SDE relies on two methods for approximations - \nStratified Sampling\n and \nSketching\n. A brief introduction to these concepts is provided below.\n\n\nStratified Sampling\n\n\nSampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying.\n\n\nTake this simple example table that manages AdImpressions. If we create a random sample that is a third of the original size we pick two records in random. \nThis is depicted in the following figure:\n\n\n\n\nIf we run a query like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample. \n\n\n\n\nBut, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries. \n\n\nStratified sampling on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata has enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer. \n\n\n\n\nTo understand these concepts in further detail, refer to the \nhandbook\n. It explains different sampling strategies, error estimation mechanisms, and various types of data synopses.\n\n\nOnline Sampling\n\n\nSDE also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark dataframe APIs to create a uniform random sample over static RDDs. For online sampling, SDE first does \nreservoir sampling\n for each strata in a write-optimized store before flushing it into a read-optimized store for stratified samples. \nThere is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, we can ensure that we have enough samples over each 5 minute time window, while still ensuring that all GEOs have good representation in the sample. \n\n\nSketching\n\n\nWhile stratified sampling ensures that data dimensions with low representation is captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, we use rely on other data structures like a Count-min-sketch to capture data frequencies in a stream. This is a data structure that requires that it captures how often we see an element in a stream for the top-N such elements. \nWhile a \nCount-min-sketch\n is well described, SDE extends this with support for providing top-K estimates over time series data. \n\n\nWorking with Stratified Samples\n\n\nCreate Sample Tables\n\n\nYou can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark dataframes, or sourced from a external data source such as S3 or HDFS. \n\n\nHere is an SQL based example to create sample on tables locally available in the SnappyData cluster. \n\n\nCREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI \n  OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01') \n  AS (SELECT * FROM NYCTAXI);\n\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  OPTIONS (qcs 'hack_license', fraction '0.01') \n  AS (SELECT * FROM TAXIFARE);\n\n\n\n\nOften your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. \nIn this example below, a sample table is created for an S3 (external) dataset:\n\n\nCREATE EXTERNAL TABLE TAXIFARE USING parquet \n  OPTIONS(path 's3a://\nAWS_SECRET_ACCESS_KEY\nAWS_ACCESS_KEY_ID\n@zeppelindemo/nyctaxifaredata_cleaned');\n//Next, create the sample sourced from this table ..\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  options  (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE);\n\n\n\n\n\nQCS (Query Column Set) and Sample selection\n\n\nFor stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction). \n\n\nQCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like \nhour (pickup_datetime)\n.\n\n\nThe parameter \nfraction\n represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes you can get pretty accurate answers with a very small fraction. With most data sets that follow normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. SDE always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. \nFor instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers(lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement. \n\n\nOne can create multiple sample tables using different sample QCS and sample fraction for a given base table. \n\n\nHere are some general guidelines to use when creating samples:\n* Note that samples are only applicable when running aggregation queries. For point lookups or selective queries the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store.\n\n\n\n\n\n\nStart by identifying the most common columns used in GroupBy/Where and Having clauses. \n\n\n\n\n\n\nThen, identify a subset of these columns where the cardinality is not too large. For instance, in the example above we picked 'hack_license' (one license per driver) as the strata and we sample 1% of the records associated with each driver. \n\n\n\n\n\n\nAvoid using unique columns or timestamps for your QCS. For instance, in the example above 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is possibility that each record in the data set has a different times tamp. Instead, when dealing with time series we use the 'hour' function to capture data for each hour. \n\n\n\n\n\n\nWhen accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample. \n\n\n\n\n\n\n\n\nNote: The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed.\n\n\n\n\nRunning queries\n\n\nQueries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause. \n\n\nHere is the syntax:\n\n\nSELECT ... FROM .. WHERE .. GROUP BY ...\n\n\nWITH ERROR \nfraction\n[CONFIDENCE\nfraction\n] [BEHAVIOR \nstring\n]\n\n\n\n\nWITH ERROR\n - this is a mandatory clause. The values are  0 \n value(double) \n 1 . \n\n\nCONFIDENCE\n - this is optional clause. The values are confidence 0 \n value(double) \n 1 . The default value is 0.95\n\n\nBEHAVIOR\n - this is an optional clause. The values are \ndo_nothing\n, \nlocal_omit\n, \nstrict\n,  \nrun_on_full_table\n, \npartial_run_on_base_table\n. The default value is \nrun_on_full_table\n   \n\n\n\n\nThese 'behavior' options are fully described in the section below. \n\n\nHere are some examples:\n\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error 0.10 \n// tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error \n// tolerate any error in answer. Just give me a quick response.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019\n// tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95.\n// If the error for any row is greater than 10% omit the answer. i.e. the row is omitted. \n\n\n\n\nUsing the Spark DataFrame API\n\n\nWe extend the Spark DataFrame API with support for approximate queries. Here is 'withError' API on DataFrames.\n\n\ndef withError(error: Double,\nconfidence: Double = Constant.DEFAULT_CONFIDENCE,\nbehavior: String = \nDO_NOTHING\n): DataFrame\n\n\n\n\nQuery examples using the DataFrame API ..\n\n\nsnc.table(baseTable).agg(Map(\nArrDelay\n -\n \nsum\n)).orderBy( desc(\nMonth_\n)).withError(0.10) \nsnc.table(baseTable).agg(Map(\nArrDelay\n -\n \nsum\n)).orderBy( desc(\nMonth_\n)).withError(0.10, 0.95, 'local_omit\u2019) \n\n\n\n\nSupporting BI tools or existing Apps\n\n\nTo allow BI tools and existing Apps that say might be generating SQL, SDE also supports specifying these options through your SQL connection or using the Snappy SQLContext. \n\n\nsnContext.sql(s\nspark.sql.aqp.error=$error\n)\nsnContext.sql(s\nspark.sql.aqp.confidence=$confidence\n)\nsnContext.sql(s\nset spark.sql.aqp.behavior=$behavior\n)\n\n\n\n\nThese settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above.\n\n\nApplications or tools using JDBC/ODBC can set the following properties. \nFor example, when using Apache Zeppelin JDBC interpreter or the snappy-shell you can set the values as below:\n\n\nset spark.sql.aqp.error=$error;\nset spark.sql.aqp.confidence=$confidence;\nset spark.sql.aqp.behavior=$behavior;\n\n\n\n\nMore Examples\n\n\nExample 1:\n\n\ncreate a sample table with qcs 'medallion'\n\n\nCREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI \n  OPTIONS (buckets '7', qcs 'medallion', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);\n\n\n\n\nSQL Query:\n\n\nselect medallion,avg(trip_distance) as avgTripDist,\n  absolute_error(avgTripDist),relative_error(avgTripDist),\n  lower_bound(avgTripDist),upper_bound(avgTripDist) \n  from nyctaxi group by medallion order by medallion desc limit 100\n  with error;\n  // We explain these built-in error functions in a section below.\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nmedallion\n).agg( avg(\ntrip_distance\n).alias(\navgTripDist\n),\n  absolute_error(\navgTripDist\n),  relative_error(\navgTripDist\n), lower_bound(\navgTripDist\n),\n  upper_bound(\navgTripDist\n)).withError(.6, .90, \ndo_nothing\n).sort(col(\nmedallion\n).desc).limit(100)\n\n\n\n\nExample 2:\n\n\ncreate an additional sample table with qcs 'hack_license'\n\n\nCREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS\n(buckets '7', qcs 'hack_license', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);\n\n\n\n\nSQL Query:\n\n\nselect  hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error\n// the engine will automically use the HackLicense sample for more accurate answer to this query.\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nhack_license\n).count().withError(.6,.90,\ndo_nothing\n).sort(col(\ncount\n).desc).limit(10)\n\n\n\n\nExample 3:\n\n\nCreate a sample table using function \"hour(pickup_datetime) as QCS.\n\n\nSample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);\n\n\n\n\nSQL Query:\n\n\nselect sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime)\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(hour(col(\npickup_datetime\n))).agg(Map(\ntrip_time_in_secs\n -\n \nsum\n)).withError(0.6,0.90,\ndo_nothing\n).limit(10)\n\n\n\n\nExample 4:\n\n\nIf you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns.\n\n\nSample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);\n\n\n\n\nSQL Query:\n\n\nSelect hack_license, sum(trip_distance) as daily_trips from nyctaxi  where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license  order by daily_trips desc\n\n\n\n\nDataFrame API Query:\n\n\nsnc.table(basetable).groupBy(\nhack_license\n,\npickup_datetime\n).agg(Map(\ntrip_distance\n -\n \nsum\n)).alias(\ndaily_trips\n).       filter(year(col(\npickup_datetime\n)).equalTo(2013) and month(col(\npickup_datetime\n)).equalTo(9)).withError(0.6,0.90,\ndo_nothing\n).sort(col(\nsum(trip_distance)\n).desc).limit(10)\n\n\n\n\nSample Selection:\n\n\nSample selection logic selects most appropriate sample, based on this relatively simple logic in the current version:\n\n\n\n\nIf the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else,\n\n\nIf query QCS (columns involved in Where/GroupBy/Having matches the sample QCS, then, select that sample\n\n\nIf exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used\n\n\nIf superset of sample QCS is not available, a sample where the sample QCS is subset of query QCS is used\n\n\nWhen multiple stratified samples with subset of QCSs match, sample with most matching columns is used. Largest size of sample gets selected if multiple such samples are available. \n\n\n\n\nHigh-level Accuracy Contracts (HAC)\n\n\nSnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts.\n\n\nWhen an error constraint is not met, the action to be taken is defined in the behavior clause. \n\n\nBehavior Clause\n\n\nSynopsis Data Engine has HAC support using the following behavior clause. \n\n\ndo_nothing\n\n\nThe SDE engine returns the estimate as is. \n\n\n\n\n\nlocal_omit\n\n\nFor aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\". \n\n\n\n\n\nstrict\n\n\nIf any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception. \n\n\n\n\n\nrun_on_full_table\n\n\nIf any of the single output row exceeds the specified error, then the full query is re-executed on the base table.\n\n\n\n\n\npartial_run_on_base_table\n\n\nIf the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups.  This result is then merged (without any duplicates) with the result derived from the sample table. \n\n\n\n\n\nIn the following example, any one of the above behavior clause can be applied. \n\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_  with error \nfraction\n [CONFIDENCE \nfraction\n] [BEHAVIOR \nbehavior\n]\n\n\n\n\nError Functions\n\n\nIn addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection. \n\n\nThe following four methods are available to be used in query projection when running approximate queries:\n\n\n\n\n\n\nabsolute_error(column alias\n) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap) \n\n\n\n\n\n\nrelative_error(column alias)\n : Indicates ratio of absolute error to estimate.\n\n\n\n\n\n\nlower_bound(column alias)\n : Lower value of a estimate interval for a given confidence.\n\n\n\n\n\n\nupper_bound(column alias)\n: Upper value of a estimate interval for a given confidence.\n\n\n\n\n\n\nConfidence is the probability that the value of a parameter falls within a specified range of values.\n\n\nFor example:\n\n\nSELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr),\nUniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9\n\n\n\n\n\n\nThe \nabsolute_error\n and \nrelative_error\n function values returns 0 if query is executed on the base table. \n\n\nlower_bound\n and \nupper_bound\n values returns null if query is executed on the base table. \n\n\nThe values are seen in case behavior is set to \nrun_on_full_table\n or\npartial_run_on_base_table\n\n\n\n\nIn addition to using SQL syntax in the queries, you can use data frame API as well. \nFor example, if you have a data frame for the airline table, then the below query can equivalently also be written as :\n\n\nselect AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95\n\n\n\n\nsnc.table(basetable).groupBy(\nYear_\n).agg( avg(\nArrDelay\n).alias(\narrivalDelay), relative_error(\narrivalDelay\n), absolute_error(\narrivalDelay\n), col(\nYear_\n)).withError(0.10, .95).sort(col(\nYear_\n).asc) \n\n\n\n\nReserved Keywords\n\n\nKeywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword \nsample_\n is reserved for SnappyData.\n\n\nIf the aggregate function is aliased in the query as \nsample_\nany string\n, then what you get is true answers on the sample table, and not the estimates of the base table.\n\n\nselect count() rowCount, count() as sample_count from airline with error 0.1\n\n\nrowCount returns estimate of number of rows in airline table.\nsample_count returns number of rows (true answer) in sample table of airline table.\n\n\nSketching\n\n\nSynopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A \nBloomFilter\n is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a \nCount-Min-Sketch\n which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API.\n\n\nCreating TopK tables\n\n\nTopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query. \nTopK\n queries aim to retrieve, from a potentially very large resultset, only the \nk (k \n= 1)\n best answers.\n\n\nSQL API for creating a TopK table in SnappyData\n \n\n\nsnsc.sql(\ncreate topK table MostPopularTweets on tweetStreamTable \n +\n        \noptions(key 'hashtag', frequencyCol 'retweets')\n)\n\n\n\n\nThe example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables.\n\n\nScala API for creating a TopK table\n \n\n\nval topKOptionMap = Map(\n    \"epoch\" -\n System.currentTimeMillis().toString,\n    \"timeInterval\" -\n \"1000ms\",\n    \"size\" -\n \"40\",\n    \"frequencyCol\" -\n \"retweets\"\n  )\n  val schema = StructType(List(StructField(\"HashTag\", StringType)))\n  snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"),\n    \"HashTag\", schema, topKOptionMap)\n\n\n\nThe code above shows how to do the same thing using the SnappyData Scala API.\n\n\nQuerying the TopK table\n \n\n\nselect * from topkTweets order by EstimatedValue desc\n\n\n\nThe example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most retweets.\n\n\nApproximate TopK analytics for time series data\n\n\nTime is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the timestamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most retweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system.\n\n\nHere is an example of a time based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData SDE module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals.\n\n\nselect hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where \n    startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' \n    order by EstimatedValue desc\n\n\n\nIf time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp.\n\n\nSQL API for creating a TopK table in SnappyData specifying timestampColumn\n \n\n\nIn the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet.\n\n\nsnsc.sql(\ncreate topK table MostPopularTweets on tweetStreamTable \n +\n        \noptions(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' )\n)\n\n\n\n\nThe example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables\n\n\nScala API for creating a TopK table\n \n\n\n    val topKOptionMap = Map(\n        \nepoch\n -\n System.currentTimeMillis().toString,\n        \ntimeInterval\n -\n \n1000ms\n,\n        \nsize\n -\n \n40\n,\n        \nfrequencyCol\n -\n \nretweets\n,\n        \ntimeSeriesColumn\n -\n \ntweetTime\n\n      )\n      val schema = StructType(List(StructField(\nHashTag\n, StringType)))\n      snc.createApproxTSTopK(\nMostPopularTweets\n, Some(\ntweetStreamTable\n),\n        \nHashTag\n, schema, topKOptionMap)\n\n\n\n\nThe code above shows how to do the same thing using the SnappyData Scala API.\n\n\nIt is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the \ntimeInterval\n attribute when creating the TopK table.\n\n\nUsing SDE\n\n\nIn the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.", 
            "title": "Synopsis Data Engine (SDE)"
        }, 
        {
            "location": "/aqp/#overview-of-synopsis-data-engine-sde", 
            "text": "The SnappyData Synopsis Data Engine (SDE) offers a novel and scalable system to analyze large data sets. SDE uses statistical sampling techniques and probabilistic data structures to answer analytic queries with sub-second latency. There is no need to store or process the entire data set. The approach trades off query accuracy for fast response time.   For instance, in exploratory analytics, a data analyst might be slicing and dicing large data sets to understand patterns, trends or to introduce new features. Often the results are rendered in a visualization tool through bar charts, map plots and bubble charts. It would increase the productivity of the engineer by providing a near perfect answer that can be rendered in seconds instead of minutes (visually, it is identical to the 100% correct rendering) , while the engineer continues to slice and dice the data sets without any interruptions.   When accessed using a visualization tool (Apache Zeppelin), users immediately get their almost-perfect answer to analytical queries within a couple of seconds, while the full answer can be computed in the background. Depending on the immediate answer, users can choose to cancel the full execution early, if they are either satisfied with the almost-perfect initial answer, or if after viewing the initial results they are no longer interested in viewing the final results. This can lead to dramatically higher productivity and significantly less resource consumption in multi-tenant and concurrent workloads on shared clusters.  While in-memory analytics can be fast, it is still expensive and cumbersome to provision large clusters. Instead, SDE allows you to retain data in existing databases and disparate sources, and only caches a fraction of the data using stratified sampling and other techniques. In many cases, data explorers can use their laptops and run high-speed interactive analytics over billions of records.   Unlike existing optimization techniques based on OLAP cubes or in-memory extracts that can consume a lot of resources and work for a priori known queries, the SnappyData Synopses data structures are designed to work for any ad-hoc query.", 
            "title": "Overview of Synopsis Data Engine (SDE)"
        }, 
        {
            "location": "/aqp/#how-does-it-work", 
            "text": "The following diagram provides a simplified view into how the SDE works. The SDE is deeply integrated with the SnappyData store and its general purpose SQL query engine. Incoming rows (could come from static or streaming sources) are continuously sampled into one or more \"sample\" tables. These samples can be considered much like how a database utilizes indexes - for optimization. There can however be one difference, that is, the \"exact\" table may or may not be managed by SnappyData (for instance, this may be a set of folders in S3 or Hadoop). When queries are executed, the user can optionally specify their tolerance for error through simple SQL extensions. SDE transparently goes through a sample selection process to evaluate if the query can be satisfied within the error constraint. If so, the response is generated directly from the sample.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/aqp/#key-concepts", 
            "text": "SnappyData SDE relies on two methods for approximations -  Stratified Sampling  and  Sketching . A brief introduction to these concepts is provided below.", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/aqp/#stratified-sampling", 
            "text": "Sampling is quite intuitive and commonly used by data scientists and explorers. The most common algorithm in use is 'uniform random sampling'. As the term implies, the algorithm is designed to randomly pick a small fraction of the population (the full data set). The algorithm is not biased on any characteristics in the data set. It is totally random and the probability of any element being selected in the sample is the same (or uniform). But, uniform random sampling does not work well for general purpose querying.  Take this simple example table that manages AdImpressions. If we create a random sample that is a third of the original size we pick two records in random. \nThis is depicted in the following figure:   If we run a query like 'SELECT avg(bid) FROM AdImpresssions where geo = 'VT'', the answer is a 100% wrong. The common solution to this problem could be to increase the size of the sample.    But, if the data distribution along this 'GEO' dimension is skewed, you could still keep picking any records or have too few records to produce a good answer to queries.   Stratified sampling on the other hand, allows the user to specify the common dimensions used for querying and ensures that each dimension or strata has enough representation in the sampled data set. For instance, as shown in the following figure, a sample stratified on 'Geo' would provide a much better answer.    To understand these concepts in further detail, refer to the  handbook . It explains different sampling strategies, error estimation mechanisms, and various types of data synopses.", 
            "title": "Stratified Sampling"
        }, 
        {
            "location": "/aqp/#online-sampling", 
            "text": "SDE also supports continuous sampling over streaming data and not just static data sets. For instance, you can use the Spark dataframe APIs to create a uniform random sample over static RDDs. For online sampling, SDE first does  reservoir sampling  for each strata in a write-optimized store before flushing it into a read-optimized store for stratified samples. \nThere is also explicit support for time series. For instance, if AdImpressions are continuously streaming in, we can ensure that we have enough samples over each 5 minute time window, while still ensuring that all GEOs have good representation in the sample.", 
            "title": "Online Sampling"
        }, 
        {
            "location": "/aqp/#sketching", 
            "text": "While stratified sampling ensures that data dimensions with low representation is captured, it still does not work well when you want to capture outliers. For instance, queries like 'Find the top-10 users with the most re-tweets in the last 5 minutes may not result in good answers. Instead, we use rely on other data structures like a Count-min-sketch to capture data frequencies in a stream. This is a data structure that requires that it captures how often we see an element in a stream for the top-N such elements. \nWhile a  Count-min-sketch  is well described, SDE extends this with support for providing top-K estimates over time series data.", 
            "title": "Sketching"
        }, 
        {
            "location": "/aqp/#working-with-stratified-samples", 
            "text": "", 
            "title": "Working with Stratified Samples"
        }, 
        {
            "location": "/aqp/#create-sample-tables", 
            "text": "You can create sample tables on datasets that can be sourced from any source supported in Spark/SnappyData. For instance, these can be SnappyData in-memory tables, Spark dataframes, or sourced from a external data source such as S3 or HDFS.   Here is an SQL based example to create sample on tables locally available in the SnappyData cluster.   CREATE SAMPLE TABLE NYCTAXI_PICKUP_SAMPLE ON NYCTAXI \n  OPTIONS (qcs 'hour(pickup_datetime)', fraction '0.01') \n  AS (SELECT * FROM NYCTAXI);\n\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  OPTIONS (qcs 'hack_license', fraction '0.01') \n  AS (SELECT * FROM TAXIFARE);  Often your data set is too large to also fit in available cluster memory. If so, you can create an external table pointing to the source. \nIn this example below, a sample table is created for an S3 (external) dataset:  CREATE EXTERNAL TABLE TAXIFARE USING parquet \n  OPTIONS(path 's3a:// AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID @zeppelindemo/nyctaxifaredata_cleaned');\n//Next, create the sample sourced from this table ..\nCREATE SAMPLE TABLE TAXIFARE_HACK_LICENSE_SAMPLE on TAXIFARE \n  options  (qcs 'hack_license', fraction '0.01') AS (SELECT * FROM TAXIFARE);", 
            "title": "Create Sample Tables"
        }, 
        {
            "location": "/aqp/#qcs-query-column-set-and-sample-selection", 
            "text": "For stratified samples, you are required to specify the columns used for stratification(QCS) and how big the sample needs to be (fraction).   QCS, which stands for Query Column Set is typically the most commonly used dimensions in your query GroupBy/Where and Having clauses. A QCS can also be constructed using SQL expressions - for instance, using a function like  hour (pickup_datetime) .  The parameter  fraction  represents the fraction of the full population that is managed in the sample. Intuition tells us that higher the fraction, more accurate the answers. But, interestingly, with large data volumes you can get pretty accurate answers with a very small fraction. With most data sets that follow normal distribution, the error rate for aggregations exponentially drops with the fraction. So, at some point, doubling the fraction does not drop the error rate. SDE always attempts to adjust its sampling rate for each stratum so that there is enough representation for all sub-groups. \nFor instance, in the above example, taxi drivers that have very few records may actually be sampled at a rate much higher than 1% while very active drivers(lot of records) is automatically sampled at a lower rate. The algorithm always attempts to maintain the overall 1% fraction specified in the 'create sample' statement.   One can create multiple sample tables using different sample QCS and sample fraction for a given base table.   Here are some general guidelines to use when creating samples:\n* Note that samples are only applicable when running aggregation queries. For point lookups or selective queries the engine automatically rejects all samples and runs the query on the base table. These queries typically would execute optimally anyway on the underlying data store.    Start by identifying the most common columns used in GroupBy/Where and Having clauses.     Then, identify a subset of these columns where the cardinality is not too large. For instance, in the example above we picked 'hack_license' (one license per driver) as the strata and we sample 1% of the records associated with each driver.     Avoid using unique columns or timestamps for your QCS. For instance, in the example above 'pickup_datetime' is a time stamp and is not a good candidate given its likely hood of high cardinality. That is, there is possibility that each record in the data set has a different times tamp. Instead, when dealing with time series we use the 'hour' function to capture data for each hour.     When accuracy of queries is not acceptable, add more samples using the common columns used in GroupBy/Where clauses as mentioned above. The system automatically picks the appropriate sample.", 
            "title": "QCS (Query Column Set) and Sample selection"
        }, 
        {
            "location": "/aqp/#note-the-value-of-the-qcs-column-should-not-be-empty-or-set-to-null-for-stratified-sampling-or-an-error-may-be-reported-when-the-query-is-executed", 
            "text": "", 
            "title": "Note: The value of the QCS column should not be empty or set to null for stratified sampling, or an error may be reported when the query is executed."
        }, 
        {
            "location": "/aqp/#running-queries", 
            "text": "Queries can be executed directly on sample tables or on the base table. Any query executed on the sample directly will always result in an approximate answer. When queries are executed on the base table users can specify their error tolerance and additional behavior to permit approximate answers. The Engine will automatically figure out if the query can be executed by any of the available samples. If not, the query can be executed on the base table based on the behavior clause.   Here is the syntax:", 
            "title": "Running queries"
        }, 
        {
            "location": "/aqp/#select-from-where-group-by", 
            "text": "", 
            "title": "SELECT ... FROM .. WHERE .. GROUP BY ..."
        }, 
        {
            "location": "/aqp/#with-error-fractionconfidencefraction-behavior-string", 
            "text": "WITH ERROR  - this is a mandatory clause. The values are  0   value(double)   1 .   CONFIDENCE  - this is optional clause. The values are confidence 0   value(double)   1 . The default value is 0.95  BEHAVIOR  - this is an optional clause. The values are  do_nothing ,  local_omit ,  strict ,   run_on_full_table ,  partial_run_on_base_table . The default value is  run_on_full_table       These 'behavior' options are fully described in the section below.   Here are some examples:  SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error 0.10 \n// tolerate a maximum error of 10% in each row in the answer with a default confidence level of 0.95.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc \n  with error \n// tolerate any error in answer. Just give me a quick response.\n\nSELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_ desc with error 0.10 confidence 0.95 behavior \u2018local_omit\u2019\n// tolerate a maximum error of 10% in each row in the answer with a confidence interval of 0.95.\n// If the error for any row is greater than 10% omit the answer. i.e. the row is omitted.", 
            "title": "WITH ERROR &lt;fraction&gt;[CONFIDENCE&lt;fraction&gt;] [BEHAVIOR &lt;string&gt;]"
        }, 
        {
            "location": "/aqp/#using-the-spark-dataframe-api", 
            "text": "We extend the Spark DataFrame API with support for approximate queries. Here is 'withError' API on DataFrames.  def withError(error: Double,\nconfidence: Double = Constant.DEFAULT_CONFIDENCE,\nbehavior: String =  DO_NOTHING ): DataFrame  Query examples using the DataFrame API ..  snc.table(baseTable).agg(Map( ArrDelay  -   sum )).orderBy( desc( Month_ )).withError(0.10) \nsnc.table(baseTable).agg(Map( ArrDelay  -   sum )).orderBy( desc( Month_ )).withError(0.10, 0.95, 'local_omit\u2019)", 
            "title": "Using the Spark DataFrame API"
        }, 
        {
            "location": "/aqp/#supporting-bi-tools-or-existing-apps", 
            "text": "To allow BI tools and existing Apps that say might be generating SQL, SDE also supports specifying these options through your SQL connection or using the Snappy SQLContext.   snContext.sql(s spark.sql.aqp.error=$error )\nsnContext.sql(s spark.sql.aqp.confidence=$confidence )\nsnContext.sql(s set spark.sql.aqp.behavior=$behavior )  These settings will apply to all queries executed via this SQLContext. Application can override this by also using the SQL extensions specified above.  Applications or tools using JDBC/ODBC can set the following properties. \nFor example, when using Apache Zeppelin JDBC interpreter or the snappy-shell you can set the values as below:  set spark.sql.aqp.error=$error;\nset spark.sql.aqp.confidence=$confidence;\nset spark.sql.aqp.behavior=$behavior;", 
            "title": "Supporting BI tools or existing Apps"
        }, 
        {
            "location": "/aqp/#more-examples", 
            "text": "", 
            "title": "More Examples"
        }, 
        {
            "location": "/aqp/#example-1", 
            "text": "create a sample table with qcs 'medallion'  CREATE SAMPLE TABLE NYCTAXI_SAMPLEMEDALLION ON NYCTAXI \n  OPTIONS (buckets '7', qcs 'medallion', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);  SQL Query:  select medallion,avg(trip_distance) as avgTripDist,\n  absolute_error(avgTripDist),relative_error(avgTripDist),\n  lower_bound(avgTripDist),upper_bound(avgTripDist) \n  from nyctaxi group by medallion order by medallion desc limit 100\n  with error;\n  // We explain these built-in error functions in a section below.  DataFrame API Query:  snc.table(basetable).groupBy( medallion ).agg( avg( trip_distance ).alias( avgTripDist ),\n  absolute_error( avgTripDist ),  relative_error( avgTripDist ), lower_bound( avgTripDist ),\n  upper_bound( avgTripDist )).withError(.6, .90,  do_nothing ).sort(col( medallion ).desc).limit(100)", 
            "title": "Example 1:"
        }, 
        {
            "location": "/aqp/#example-2", 
            "text": "create an additional sample table with qcs 'hack_license'  CREATE SAMPLE TABLE NYCTAXI_SAMPLEHACKLICENSE ON NYCTAXI OPTIONS\n(buckets '7', qcs 'hack_license', fraction '0.01', strataReservoirSize '50') AS (SELECT * FROM NYCTAXI);  SQL Query:  select  hack_license, count(*) count from NYCTAXI group by hack_license order by count desc limit 10 with error\n// the engine will automically use the HackLicense sample for more accurate answer to this query.  DataFrame API Query:  snc.table(basetable).groupBy( hack_license ).count().withError(.6,.90, do_nothing ).sort(col( count ).desc).limit(10)", 
            "title": "Example 2:"
        }, 
        {
            "location": "/aqp/#example-3", 
            "text": "Create a sample table using function \"hour(pickup_datetime) as QCS.  Sample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hourOfDay', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);  SQL Query:  select sum(trip_time_in_secs)/60 totalTimeDrivingInHour, hour(pickup_datetime) from nyctaxi group by hour(pickup_datetime)  DataFrame API Query:  snc.table(basetable).groupBy(hour(col( pickup_datetime ))).agg(Map( trip_time_in_secs  -   sum )).withError(0.6,0.90, do_nothing ).limit(10)", 
            "title": "Example 3:"
        }, 
        {
            "location": "/aqp/#example-4", 
            "text": "If you want a higher assurance of accurate answers for your query, match the QCS to \"group by columns\" followed by any filter condition columns. Here is a sample using multiple columns.  Sample Tablecreate sample table nyctaxi_hourly_sample on nyctaxi options (buckets '7', qcs 'hack_license, year(pickup_datetime), month(pickup_datetime)', fraction '0.01', strataReservoirSize '50') AS (select *, hour(pickupdatetime) as hourOfDay from nyctaxi);  SQL Query:  Select hack_license, sum(trip_distance) as daily_trips from nyctaxi  where year(pickup_datetime) = 2013 and month(pickup_datetime) = 9 group by hack_license  order by daily_trips desc  DataFrame API Query:  snc.table(basetable).groupBy( hack_license , pickup_datetime ).agg(Map( trip_distance  -   sum )).alias( daily_trips ).       filter(year(col( pickup_datetime )).equalTo(2013) and month(col( pickup_datetime )).equalTo(9)).withError(0.6,0.90, do_nothing ).sort(col( sum(trip_distance) ).desc).limit(10)", 
            "title": "Example 4:"
        }, 
        {
            "location": "/aqp/#sample-selection", 
            "text": "Sample selection logic selects most appropriate sample, based on this relatively simple logic in the current version:   If the query is not an aggregation query (based on COUNT, AVG, SUM) then reject the use of any samples. The query is executed on the base table. Else,  If query QCS (columns involved in Where/GroupBy/Having matches the sample QCS, then, select that sample  If exact match is not available, then, if the sample QCS is a superset of query QCS, that sample is used  If superset of sample QCS is not available, a sample where the sample QCS is subset of query QCS is used  When multiple stratified samples with subset of QCSs match, sample with most matching columns is used. Largest size of sample gets selected if multiple such samples are available.", 
            "title": "Sample Selection:"
        }, 
        {
            "location": "/aqp/#high-level-accuracy-contracts-hac", 
            "text": "SnappyData combines state-of-the-art approximate query processing techniques and a variety of data synopses to ensure interactive analytics over both, streaming and stored data. Using high-level accuracy contracts (HAC), SnappyData offers end users intuitive means for expressing their accuracy requirements, without overwhelming them with statistical concepts.  When an error constraint is not met, the action to be taken is defined in the behavior clause.", 
            "title": "High-level Accuracy Contracts (HAC)"
        }, 
        {
            "location": "/aqp/#behavior-clause", 
            "text": "Synopsis Data Engine has HAC support using the following behavior clause.", 
            "title": "Behavior Clause"
        }, 
        {
            "location": "/aqp/#do_nothing", 
            "text": "The SDE engine returns the estimate as is.", 
            "title": "&lt;do_nothing&gt;"
        }, 
        {
            "location": "/aqp/#local_omit", 
            "text": "For aggregates that do not satisfy the error criteria, the value is replaced by a special value like \"null\".", 
            "title": "&lt;local_omit&gt;"
        }, 
        {
            "location": "/aqp/#strict", 
            "text": "If any of the aggregate column in any of the rows do not meet the HAC requirement, the system throws an exception.", 
            "title": "&lt;strict&gt;"
        }, 
        {
            "location": "/aqp/#run_on_full_table", 
            "text": "If any of the single output row exceeds the specified error, then the full query is re-executed on the base table.", 
            "title": "&lt;run_on_full_table&gt;"
        }, 
        {
            "location": "/aqp/#partial_run_on_base_table", 
            "text": "If the error is more than what is specified in the query, for any of the output rows (that is sub-groups for a group by query), the query is re-executed on the base table for those sub-groups.  This result is then merged (without any duplicates) with the result derived from the sample table.    In the following example, any one of the above behavior clause can be applied.   SELECT sum(ArrDelay) ArrivalDelay, Month_ from airline group by Month_ order by Month_  with error  fraction  [CONFIDENCE  fraction ] [BEHAVIOR  behavior ]", 
            "title": "&lt;partial_run_on_base_table&gt;"
        }, 
        {
            "location": "/aqp/#error-functions", 
            "text": "In addition to this, SnappyData supports error functions that can be specified in the query projection. These error functions are supported for the SUM, AVG and COUNT aggregates in the projection.   The following four methods are available to be used in query projection when running approximate queries:    absolute_error(column alias ) : Indicates absolute error present in the estimate (approx answer) calculated using error estimation method (ClosedForm or Bootstrap)     relative_error(column alias)  : Indicates ratio of absolute error to estimate.    lower_bound(column alias)  : Lower value of a estimate interval for a given confidence.    upper_bound(column alias) : Upper value of a estimate interval for a given confidence.    Confidence is the probability that the value of a parameter falls within a specified range of values.  For example:  SELECT avg(ArrDelay) as AvgArr ,absolute_error(AvgArr),relative_error(AvgArr),lower_bound(AvgArr), upper_bound(AvgArr),\nUniqueCarrier FROM airline GROUP BY UniqueCarrier order by UniqueCarrier WITH ERROR 0.12 confidence 0.9   The  absolute_error  and  relative_error  function values returns 0 if query is executed on the base table.   lower_bound  and  upper_bound  values returns null if query is executed on the base table.   The values are seen in case behavior is set to  run_on_full_table  or partial_run_on_base_table   In addition to using SQL syntax in the queries, you can use data frame API as well. \nFor example, if you have a data frame for the airline table, then the below query can equivalently also be written as :  select AVG(ArrDelay) arrivalDelay, relative_error(arrivalDelay), absolute_error(arrivalDelay), Year_ from airline group by Year_ order by Year_ with error 0.10 confidence 0.95  snc.table(basetable).groupBy( Year_ ).agg( avg( ArrDelay ).alias( arrivalDelay), relative_error( arrivalDelay ), absolute_error( arrivalDelay ), col( Year_ )).withError(0.10, .95).sort(col( Year_ ).asc)", 
            "title": "Error Functions"
        }, 
        {
            "location": "/aqp/#reserved-keywords", 
            "text": "Keywords are predefined reserved words that have special meanings and cannot be used in a paragraph. Keyword  sample_  is reserved for SnappyData.  If the aggregate function is aliased in the query as  sample_ any string , then what you get is true answers on the sample table, and not the estimates of the base table.  select count() rowCount, count() as sample_count from airline with error 0.1  rowCount returns estimate of number of rows in airline table.\nsample_count returns number of rows (true answer) in sample table of airline table.", 
            "title": "Reserved Keywords"
        }, 
        {
            "location": "/aqp/#sketching_1", 
            "text": "Synopses data structures are typically much smaller than the base data sets that they represent. They use very little space and provide fast, approximate answers to queries. A  BloomFilter  is a commonly used example of a synopsis data structure. Another example of a synopsis structure is a  Count-Min-Sketch  which serves as a frequency table of events in a stream of data. The ability to use Time as a dimension for querying makes synopses structures much more useful. As streams are ingested, all relevant synopses are updated incrementally and can be queried using SQL or the Scala API.", 
            "title": "Sketching"
        }, 
        {
            "location": "/aqp/#creating-topk-tables", 
            "text": "TopK queries are used to rank attributes to answer \"best, most interesting, most important\" class of questions. TopK structures store elements ranking them based on their relevance to the query.  TopK  queries aim to retrieve, from a potentially very large resultset, only the  k (k  = 1)  best answers.  SQL API for creating a TopK table in SnappyData    snsc.sql( create topK table MostPopularTweets on tweetStreamTable   +\n         options(key 'hashtag', frequencyCol 'retweets') )  The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables.  Scala API for creating a TopK table    val topKOptionMap = Map(\n    \"epoch\" -  System.currentTimeMillis().toString,\n    \"timeInterval\" -  \"1000ms\",\n    \"size\" -  \"40\",\n    \"frequencyCol\" -  \"retweets\"\n  )\n  val schema = StructType(List(StructField(\"HashTag\", StringType)))\n  snc.createApproxTSTopK(\"MostPopularTweets\", Some(\"tweetStreamTable\"),\n    \"HashTag\", schema, topKOptionMap)  The code above shows how to do the same thing using the SnappyData Scala API.  Querying the TopK table    select * from topkTweets order by EstimatedValue desc  The example above queries the TopK table which returns the top 40 (the depth of the TopK table was set to 40) hashtags with the most retweets.", 
            "title": "Creating TopK tables"
        }, 
        {
            "location": "/aqp/#approximate-topk-analytics-for-time-series-data", 
            "text": "Time is used as an attribute in creating the TopK structures. Time can be an attribute of the incoming data set (which is frequently the case with streaming data sets) and in the absence of that, the system uses arrival time of the batch as the timestamp for that incoming batch. The TopK structure is populated along the dimension of time. As an example, the most retweeted hashtags in each window are stored in the data structure. This allows us to issue queries like, \"what are the most popular hashtags in a given time interval?\" Queries of this nature are typically difficult to execute and not easy to optimize (due to space considerations) in a traditional system.  Here is an example of a time based query on the TopK structure which returns the most popular hashtags in the time interval queried. The SnappyData SDE module provides two attributes startTime and endTime which can be used to run queries on arbitrary time intervals.  select hashtag, EstimatedValue, ErrorBoundsInfo from MostPopularTweets where \n    startTime='2016-01-26 10:07:26.121' and endTime='2016-01-26 11:14:06.121' \n    order by EstimatedValue desc  If time is an attribute in the incoming data set, it can be used instead of the system generated time. In order to do this, the TopK table creation is provided the name of the column containing the timestamp.  SQL API for creating a TopK table in SnappyData specifying timestampColumn    In the example below tweetTime is a field in the incoming dataset which carries the timestamp of the tweet.  snsc.sql( create topK table MostPopularTweets on tweetStreamTable   +\n         options(key 'hashtag', frequencyCol 'retweets', timeSeriesColumn 'tweetTime' ) )  The example above create a TopK table called MostPopularTweets, the base table for which is tweetStreamTable. It uses the hashtag field of tweetStreamTable as its key field and maintains the TopN hashtags that have the highest retweets value in the base table. This works for both static tables and streaming tables  Scala API for creating a TopK table        val topKOptionMap = Map(\n         epoch  -  System.currentTimeMillis().toString,\n         timeInterval  -   1000ms ,\n         size  -   40 ,\n         frequencyCol  -   retweets ,\n         timeSeriesColumn  -   tweetTime \n      )\n      val schema = StructType(List(StructField( HashTag , StringType)))\n      snc.createApproxTSTopK( MostPopularTweets , Some( tweetStreamTable ),\n         HashTag , schema, topKOptionMap)  The code above shows how to do the same thing using the SnappyData Scala API.  It is worth noting that the user has the ability to disable time as a dimension if desired. This is done by not providing the  timeInterval  attribute when creating the TopK table.", 
            "title": "Approximate TopK analytics for time series data"
        }, 
        {
            "location": "/aqp/#using-sde", 
            "text": "In the current release SDE queries only work for SUM, AVG and COUNT aggregations. Joins are only supported to non-samples in this release. The SnappyData SDE module will gradually expand the scope of queries that can be serviced through it. But the overarching goal here is to dramatically cut down on the load on current systems by diverting at least some queries to the sampling subsystem and increasing productivity through fast response times.", 
            "title": "Using SDE"
        }, 
        {
            "location": "/streamingWithSQL/", 
            "text": "SnappyData\u2019s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. \nHere is a brief overview of \nSpark streaming\n from the Spark Streaming guide. \n\n\nSpark Streaming Overview\n\n\nSpark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex\u2028algorithms expressed with high-level functions like \nmap\n, \nreduce\n, \njoin\n and \nwindow\n.\n\n\nFinally, processed data can be pushed out to filesystems, databases,\u2028and live dashboards. In fact, you can apply Spark's \nmachine learning\n and \ngraph processing\n algorithms on data streams.\u2028\u2028\n\n\n\n  \n\n\n\n\n\nInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n\n\n \n\n \n\n\n\nSpark Streaming provides a high-level abstraction called \ndiscretized stream\n or \nDStream\n,\u2028which represents a continuous stream of data. DStreams can be created either from input data\u2028streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level\u2028operations on other DStreams. Internally, a DStream is represented as a sequence of\u2028\nRDDs\n.\u2028\n\n\nAdditional details on the Spark Streaming concepts and programming is covered \nhere\n.\n\n\nSnappyData Streaming extensions over Spark\n\n\nWe offer the following enhancements over Spark Streaming : \n\n\n\n\n\n\nManage Streams declaratively\n: Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to the any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. \n\n\n\n\n\n\nSQL based stream processing\n: With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. \n\n\n\n\n\n\nContinuous queries and time windows\n: Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query. \n\n\n\n\n\n\nOLAP optimizations\n: By integrating and collocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store.\n\n\n\n\n\n\nApproximate stream analytics\n: When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.\n\n\n\n\n\n\nWorking with stream tables\n\n\nSnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.\n\n\n// DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFINITION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'\nOPTIONS (\n // multiple stream source specific options\nstoragelevel '', \nrowConverter '', \ntopics '',\nkafkaParams '',\nconsumerKey '',\nconsumerSecret '',\naccessToken '',\naccessTokenSecret '',\nhostname '',\nport '',\ndirectory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT \nbatchInterval\n [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE]\n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP\n\n\n\nFor example to create a stream table using kafka source : \n\n\n    val sc = new SparkContext(new SparkConf().setAppName(\nexample\n).setMaster(\nlocal[*]\n))\n    val snc = SnappyContext.getOrCreate(sc)\n    var snsc = SnappyStreamingContext(snc, Seconds(1))\n\n    snsc.sql(\ncreate stream table streamTable (userId string, clickStreamLog string) \n +\n        \nusing kafka_stream options (\n +\n        \nstoragelevel 'MEMORY_AND_DISK_SER_2', \n +\n        \nrowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', \n +\n        \nkafkaParams 'zookeeper.connect-\nlocalhost:2181;auto.offset.reset-\nsmallest;group.id-\nmyGroupId', \n +\n        \ntopics 'streamTopic:01')\n)\n\n    // You can get a handle of underlying DStream of the table\n    val dStream = snsc.getSchemaDStream(\nstreamTable\n)\n\n    // You can also save the DataFrames to an external table\n    dStream.foreachDataFrame(_.write.insertInto(tableName))\n\n\n\n\nThe streamTable created in above example can be accessed from snappy-shell and can be queried using ad-hoc SQL queries.\n\n\nStream SQL through Snappy-Shell\n\n\nStart a SnappyData cluster and connect through snappy-shell : \n\n\n//create a connection\nsnappy\n connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy\n streaming init 2secs;\n\n// Create a stream table\nsnappy\n create stream table streamTable (id long, text string, fullName string, country string,\nretweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\naccessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming \nsnappy\n streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy\n select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy\n drop table streamTable;\n\n// Stop the streaming\nsnappy\n streaming stop;\n\n\n\nSchemaDStream\n\n\nSchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers the similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provide foreachDataFrame API.SchemaDStream can be registered as a table.\nSome of these ideas (especially naming our abstractions) were borrowed from Intel's Streaming SQL project - https://github.com/Intel-bigdata/spark-streamingsql\n\n\nRegistering Continuous queries\n\n\n    //You can join two stream tables and produce a result stream. \n    val resultStream = snsc.registerCQ(\nSELECT s1.id, s1.text FROM stream1 window (duration \n    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id\n)\n\n    // You can also save the DataFrames to an external table\n    dStream.foreachDataFrame(_.write.insertInto(\nyourTableName\n))\n\n\n\n\nDynamic(ad-hoc) Continuous queries\n\n\nUnlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The CQs can be registered even after the \nSnappyStreamingContext\n has started.\n\n\nWhat is currently out-of-scope?\n\n\nContinuous Queries through command line(Snappy-Shell)", 
            "title": "Stream processing using SQL"
        }, 
        {
            "location": "/streamingWithSQL/#spark-streaming-overview", 
            "text": "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex\u2028algorithms expressed with high-level functions like  map ,  reduce ,  join  and  window .  Finally, processed data can be pushed out to filesystems, databases,\u2028and live dashboards. In fact, you can apply Spark's  machine learning  and  graph processing  algorithms on data streams.\u2028\u2028  \n     Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.  \n  \n   Spark Streaming provides a high-level abstraction called  discretized stream  or  DStream ,\u2028which represents a continuous stream of data. DStreams can be created either from input data\u2028streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level\u2028operations on other DStreams. Internally, a DStream is represented as a sequence of\u2028 RDDs .\u2028  Additional details on the Spark Streaming concepts and programming is covered  here .", 
            "title": "Spark Streaming Overview"
        }, 
        {
            "location": "/streamingWithSQL/#snappydata-streaming-extensions-over-spark", 
            "text": "We offer the following enhancements over Spark Streaming :     Manage Streams declaratively : Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to the any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions.     SQL based stream processing : With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams.     Continuous queries and time windows : Similar to popular stream processing products, applications can register \u201ccontinuous\u201d queries on streams. By default, spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query.     OLAP optimizations : By integrating and collocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store.    Approximate stream analytics : When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.", 
            "title": "SnappyData Streaming extensions over Spark"
        }, 
        {
            "location": "/streamingWithSQL/#working-with-stream-tables", 
            "text": "SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.  // DDL for creating a stream table\nCREATE STREAM TABLE [IF NOT EXISTS] table_name\n(COLUMN_DEFINITION)\nUSING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'\nOPTIONS (\n // multiple stream source specific options\nstoragelevel '', \nrowConverter '', \ntopics '',\nkafkaParams '',\nconsumerKey '',\nconsumerSecret '',\naccessToken '',\naccessTokenSecret '',\nhostname '',\nport '',\ndirectory ''\n)\n\n// DDL for dropping a stream table\nDROP TABLE [IF EXISTS] table_name\n\n// Initialize StreamingContext\nSTREAMING INIT  batchInterval  [SECS|SECOND|MILLIS|MILLISECOND|MINS|MINUTE]\n\n// Start streaming\nSTREAMING START\n\n// Stop streaming\nSTREAMING STOP  For example to create a stream table using kafka source :       val sc = new SparkContext(new SparkConf().setAppName( example ).setMaster( local[*] ))\n    val snc = SnappyContext.getOrCreate(sc)\n    var snsc = SnappyStreamingContext(snc, Seconds(1))\n\n    snsc.sql( create stream table streamTable (userId string, clickStreamLog string)   +\n         using kafka_stream options (  +\n         storagelevel 'MEMORY_AND_DISK_SER_2',   +\n         rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter',   +\n         kafkaParams 'zookeeper.connect- localhost:2181;auto.offset.reset- smallest;group.id- myGroupId',   +\n         topics 'streamTopic:01') )\n\n    // You can get a handle of underlying DStream of the table\n    val dStream = snsc.getSchemaDStream( streamTable )\n\n    // You can also save the DataFrames to an external table\n    dStream.foreachDataFrame(_.write.insertInto(tableName))  The streamTable created in above example can be accessed from snappy-shell and can be queried using ad-hoc SQL queries.", 
            "title": "Working with stream tables"
        }, 
        {
            "location": "/streamingWithSQL/#stream-sql-through-snappy-shell", 
            "text": "Start a SnappyData cluster and connect through snappy-shell :   //create a connection\nsnappy  connect client 'localhost:1527';\n\n// Initialize streaming with batchInterval of 2 seconds\nsnappy  streaming init 2secs;\n\n// Create a stream table\nsnappy  create stream table streamTable (id long, text string, fullName string, country string,\nretweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',\naccessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');\n\n// Start the streaming \nsnappy  streaming start;\n\n//Run ad-hoc queries on the streamTable on current batch of data\nsnappy  select id, text, fullName from streamTable where text like '%snappy%'\n\n// Drop the streamTable\nsnappy  drop table streamTable;\n\n// Stop the streaming\nsnappy  streaming stop;", 
            "title": "Stream SQL through Snappy-Shell"
        }, 
        {
            "location": "/streamingWithSQL/#schemadstream", 
            "text": "SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL queries on DStreams. It is similar to SchemaRDD, which offers the similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provide foreachDataFrame API.SchemaDStream can be registered as a table.\nSome of these ideas (especially naming our abstractions) were borrowed from Intel's Streaming SQL project - https://github.com/Intel-bigdata/spark-streamingsql", 
            "title": "SchemaDStream"
        }, 
        {
            "location": "/streamingWithSQL/#registering-continuous-queries", 
            "text": "//You can join two stream tables and produce a result stream. \n    val resultStream = snsc.registerCQ( SELECT s1.id, s1.text FROM stream1 window (duration \n    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id )\n\n    // You can also save the DataFrames to an external table\n    dStream.foreachDataFrame(_.write.insertInto( yourTableName ))", 
            "title": "Registering Continuous queries"
        }, 
        {
            "location": "/streamingWithSQL/#dynamicad-hoc-continuous-queries", 
            "text": "Unlike Spark streaming, you do not need to register all your stream output transformations (which is a continuous query in this case) before the start of StreamingContext. The CQs can be registered even after the  SnappyStreamingContext  has started.", 
            "title": "Dynamic(ad-hoc) Continuous queries"
        }, 
        {
            "location": "/streamingWithSQL/#what-is-currently-out-of-scope", 
            "text": "Continuous Queries through command line(Snappy-Shell)", 
            "title": "What is currently out-of-scope?"
        }, 
        {
            "location": "/deployment/", 
            "text": "Deployment topologies\n\n\nThis section provides a short overview of the different runtime deployment architectures available and recommendations on when to choose one over the other. \nThere are three deployment modes available in snappydata. \n\n\n\n\n\n\n\n\nDeployment Mode\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nUnified Cluster\n\n\nReal time production application. Here the Spark Executor(compute) and Snappy DataStore are collocated\n\n\n\n\n\n\nSplit Cluster\n\n\nSpark executors and SnappyStore form independent clusters. Use for computationally heavy computing and Batch processing\n\n\n\n\n\n\nLocal\n\n\nThis is for development where client application, the executors and data store are all running in the same JVM\n\n\n\n\n\n\n\n\nUnified cluster mode (aka 'Embedded store' mode)\n\n\nThis is the default cluster model where Spark computations and in-memory data store run collocated in the same JVM. This is our ootb configuration and suitable for most SnappyData real time production environments. You launch Snappy Data servers to bootstrap any data from disk, replicas or from external data sources and Spark executors are dynamically launched when the first Spark Job arrives. \n\n\nYou either start SnappyData members using the \nsnappy_start_all\n script or you start them individually. \n\n\n# start members using the ssh scripts \n$ sbin/snappy-start-all.sh\n\n# start members individually\n$ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334\n\n\n\n\nSpark applications are coordinated by a SparkContext instance that runs in the Application's main program called the 'Driver'. The driver coordinates the execution by running parallel tasks on executors and is responsible for delivering results to the application when 'Jobs'(i.e. actions like print() ) are executed. \nWhen executing in this unified cluster mode there can only be a single Spark Context (a single coordinator if you may) for the cluster. To support multiple concurrent Jobs or applications Snappydata manages a singleton SparkContext created and running in the 'Lead' node. i.e. the Spark context is fully managed by Snappydata. Applications simply submit \nJobs\n and don't have to be concerned about HA for the context or the driver program. \nThe rationale for our design is further explored \nhere\n. \n\n\nFully managed Spark driver and context\n\n\nPrograms can connect to the lead node and submit Jobs. The Driver is managed by the Snappy cluster in the lead node and the application doesn\u2019t create or manage the Spark context. Applications implement the \nSnappySQLJob\n or the \nSnappyStreamingJob\n trait as described in the \nBuilding Snappy applications using Spark API\n section.\n\n\nApplication managed Spark driver and context\n\n\nWhile Snappy recommends the use of scala traits mentioned above to implement your application, you could also run your native Spark program on the unified cluster with a slight change to the cluster URL. \n\n\nval conf = new SparkConf().\n              // here the locator url is passed as part of the master url\n              setMasterURL(\nsnappydata://localhost:10334\n).\n              set(\njobserver.enabled\n, \ntrue\n)\nval sc = new SparkContext(conf) \n\n\n\n\n\n\nNote\n\n\nWe currently don't support external cluster managers like YARN when operating in this mode. While, it is easy to expand and redistribute the data by starting new data servers dynamically we expect such dynamic resource allocations to be a planned and seldom exercised option. Re-distributing large quantities of data can be very expensive and can slow down running applications. \nFor computational intensive workloads or batch processing workloads where extensive data shuffling is involved consider using the Split cluster mode described next. \n\n\n\n\nSplit cluster mode\n\n\nIn this mode, Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Apache Spark runs in this mode. \n\n\nSpecifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.\n\n\nThe driver program managing the SparkContext also participate as a peer member in the SnappyData distributed system and gets access to the store catalog information. To enable this, you must set the \nlocator\n host/port in the configuration (see example below). When executors running the spark cluster access these tables the catalog metadata is used to locate the store servers managing data partitions and would be accessed in parallel. \nRead the \nSpark cluster overview\n for more details on the native Spark architecture. \n\n\nval conf = new SparkConf().\n              // Here the spark context connects with Spark's master running on 7077. \n              setMasterURL(\nspark://localhost:7077\n).\n              set(\nsnappydata.store.locators\n, \nlocalhost:10334\n) \nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// The following code connects with the snappy locator to fetch hive metastore. \nval snappyContext = SnappyContext(sc) \n\n\n\n\n\nThe catalog is initialized lazily when SnappyData functionality is accessed. \nThe big benefit even while the clusters for compute and data is split is that the catalog is immediately visible to the Spark executors nodes and applications don\u2019t have to explicitly manage connections and schema related information. This design is quite similar to the Spark\u2019s native support for Hive. \n\n\nWhen accessing partitioned data, the partitions are fetched as compressed blobs that is fully compatible with the columnar compression built into Spark. All access is automatically parallelized. \n\n\nLocal mode\n\n\nAs the name implies, use this mode to execute everything locally in the application JVM. The local vs cluster modes are described in the \nSpark Programming guide\n.\n\n\nval conf = new SparkConf().\n               setMaster(\nlocal[*]\n).\n               // Starting jobserver helps when you would want to test your jobs in a local mode. \n               set(\njobserver.enabled\n, \ntrue\n)\nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// JobServer is started too.", 
            "title": "Deployment topologies"
        }, 
        {
            "location": "/deployment/#deployment-topologies", 
            "text": "This section provides a short overview of the different runtime deployment architectures available and recommendations on when to choose one over the other. \nThere are three deployment modes available in snappydata.      Deployment Mode  Description      Unified Cluster  Real time production application. Here the Spark Executor(compute) and Snappy DataStore are collocated    Split Cluster  Spark executors and SnappyStore form independent clusters. Use for computationally heavy computing and Batch processing    Local  This is for development where client application, the executors and data store are all running in the same JVM", 
            "title": "Deployment topologies"
        }, 
        {
            "location": "/deployment/#unified-cluster-mode-aka-embedded-store-mode", 
            "text": "This is the default cluster model where Spark computations and in-memory data store run collocated in the same JVM. This is our ootb configuration and suitable for most SnappyData real time production environments. You launch Snappy Data servers to bootstrap any data from disk, replicas or from external data sources and Spark executors are dynamically launched when the first Spark Job arrives.   You either start SnappyData members using the  snappy_start_all  script or you start them individually.   # start members using the ssh scripts \n$ sbin/snappy-start-all.sh\n\n# start members individually\n$ bin/snappy-shell locator start  -dir=/node-a/locator1 \n$ bin/snappy-shell server start  -dir=/node-b/server1  -locators:localhost:10334  Spark applications are coordinated by a SparkContext instance that runs in the Application's main program called the 'Driver'. The driver coordinates the execution by running parallel tasks on executors and is responsible for delivering results to the application when 'Jobs'(i.e. actions like print() ) are executed. \nWhen executing in this unified cluster mode there can only be a single Spark Context (a single coordinator if you may) for the cluster. To support multiple concurrent Jobs or applications Snappydata manages a singleton SparkContext created and running in the 'Lead' node. i.e. the Spark context is fully managed by Snappydata. Applications simply submit  Jobs  and don't have to be concerned about HA for the context or the driver program. \nThe rationale for our design is further explored  here .", 
            "title": "Unified cluster mode (aka 'Embedded store' mode)"
        }, 
        {
            "location": "/deployment/#fully-managed-spark-driver-and-context", 
            "text": "Programs can connect to the lead node and submit Jobs. The Driver is managed by the Snappy cluster in the lead node and the application doesn\u2019t create or manage the Spark context. Applications implement the  SnappySQLJob  or the  SnappyStreamingJob  trait as described in the  Building Snappy applications using Spark API  section.", 
            "title": "Fully managed Spark driver and context"
        }, 
        {
            "location": "/deployment/#application-managed-spark-driver-and-context", 
            "text": "While Snappy recommends the use of scala traits mentioned above to implement your application, you could also run your native Spark program on the unified cluster with a slight change to the cluster URL.   val conf = new SparkConf().\n              // here the locator url is passed as part of the master url\n              setMasterURL( snappydata://localhost:10334 ).\n              set( jobserver.enabled ,  true )\nval sc = new SparkContext(conf)", 
            "title": "Application managed Spark driver and context"
        }, 
        {
            "location": "/deployment/#note", 
            "text": "We currently don't support external cluster managers like YARN when operating in this mode. While, it is easy to expand and redistribute the data by starting new data servers dynamically we expect such dynamic resource allocations to be a planned and seldom exercised option. Re-distributing large quantities of data can be very expensive and can slow down running applications. \nFor computational intensive workloads or batch processing workloads where extensive data shuffling is involved consider using the Split cluster mode described next.", 
            "title": "Note"
        }, 
        {
            "location": "/deployment/#split-cluster-mode", 
            "text": "In this mode, Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Apache Spark runs in this mode.   Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark\u2019s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.  The driver program managing the SparkContext also participate as a peer member in the SnappyData distributed system and gets access to the store catalog information. To enable this, you must set the  locator  host/port in the configuration (see example below). When executors running the spark cluster access these tables the catalog metadata is used to locate the store servers managing data partitions and would be accessed in parallel. \nRead the  Spark cluster overview  for more details on the native Spark architecture.   val conf = new SparkConf().\n              // Here the spark context connects with Spark's master running on 7077. \n              setMasterURL( spark://localhost:7077 ).\n              set( snappydata.store.locators ,  localhost:10334 ) \nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// The following code connects with the snappy locator to fetch hive metastore. \nval snappyContext = SnappyContext(sc)   The catalog is initialized lazily when SnappyData functionality is accessed. \nThe big benefit even while the clusters for compute and data is split is that the catalog is immediately visible to the Spark executors nodes and applications don\u2019t have to explicitly manage connections and schema related information. This design is quite similar to the Spark\u2019s native support for Hive.   When accessing partitioned data, the partitions are fetched as compressed blobs that is fully compatible with the columnar compression built into Spark. All access is automatically parallelized.", 
            "title": "Split cluster mode"
        }, 
        {
            "location": "/deployment/#local-mode", 
            "text": "As the name implies, use this mode to execute everything locally in the application JVM. The local vs cluster modes are described in the  Spark Programming guide .  val conf = new SparkConf().\n               setMaster( local[*] ).\n               // Starting jobserver helps when you would want to test your jobs in a local mode. \n               set( jobserver.enabled ,  true )\nval sc = new SparkContext(conf) \n// use sc to use Spark and Snappy features. \n// JobServer is started too.", 
            "title": "Local mode"
        }, 
        {
            "location": "/apidocsintro/", 
            "text": "API Documentation\n\n\nAPI reference for SnappyData can be found \nhere\n.", 
            "title": "API"
        }, 
        {
            "location": "/apidocsintro/#api-documentation", 
            "text": "API reference for SnappyData can be found  here .", 
            "title": "API Documentation"
        }, 
        {
            "location": "/aqp_aws/", 
            "text": "Overview of SnappyData iSight-Cloud\n\n\niSight-Cloud is a cloud-based service that allows for instant visualization of analytic query results on large datasets. Powered by the SnappyData Synopsis Data Engine (\nSDE\n), users interact with iSight-Cloud to populate the synopsis engine with the right data sets and accelerate SQL queries by using the engine to provide latency bounded responses to large complex aggregate queries. \n\n\niSight-Cloud uses Apache Zeppelin as the front end notebook to display results and allows users to build powerful notebooks representing key elements of their business in a matter of minutes. \n\n\nThe service provides a web URL that spins up a cluster instance on AWS or users can download the iSight-Cloud EC2 script to configure a custom sized cluster, to create and render powerful visualizations of their big data sets with the click of a button. \nWith iSight-Cloud, you can speed up the process of understanding what your data is telling you, and move on to the task of organizing your business around those insights rapidly.\n\n\nIn this document, we describe the features provided by SnappyData for analyzing your data. It also provides details for deploying a SnappyData Cloud cluster on AWS CloudFormation or on AWS using the EC2 script. \n\n\nRefer to the the examples and guidelines provided in this document to help you create notebooks using which, you can execute SQL queries or data frame API to analyze your data.\n\n\nKey Components\n\n\nThis section provides a brief description of the key terms used in this document. \n\n\n\n\nAmazon Web Services (AWS\n):  Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that are important for SnappyData are, Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3).\n\n\nSnappyData Cluster\n:  A database cluster which has three main components - Locator, Server and Lead\n\n\nApache Zeppelin\n: Apache Zeppelin is a web-based notebook that enables interactive data analytics. It allows you to make data-driven, interactive and collaborative documents with SQL queries or directly use the Spark API to process data.\n\n\nInterpreters\n: A software module which is loaded into Apache Zeppelin upon startup. Interpreters allow various third party products including SnappyData to interact with Apache Zeppelin. The SnappyData interpreter gives users the ability to execute SQL queries or use the data frame API to visualize data.\n\n\n\n\nQuick Start Steps\n\n\nTo understand the product follow these easy steps that can get you started quickly:\n\n\n\n\nSetting up SnappyData Cloud Cluster\n\n\nDeploying SnappyData Cloud Cluster with iSight CloudBuilder\n\n\nDeploying SnappyData Cloud Cluster on AWS using Scripts\n\n\n\n\n\n\nUsing Apache Zeppelin\n    \n\n\nUsing Predefined Notebook\n\n\nCreating your own Notebook\n\n\n\n\n\n\nLoading Data from AWS S3\n\n\nMonitoring SnappyData Cloud Cluster\n\n\n\n\n\n\nSetting Up SnappyData Cloud Cluster\n\n\nThis section discusses the steps required for setting up and deploying SnappyData Cloud Cluster on AWS using the iSight CloudBuilder and using script.\n\n\n\n\nDeploying SnappyData Cloud Cluster with iSight CloudBuilder\n\n\nWatch the following  video to learn how easy it is to use iSight CloudBuilder, which generates a SnappyData Cloud Cluster.\n\n\n\n\nPrerequisites\n\n\nBefore you begin,:\n\n\n\n\nEnsure that you have an existing AWS account with required permissions to launch EC2 resources\n\n\nSign in to the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.\n\n\nCreate an EC2 key pair in the region where you want to launch the SnappyData Cloud cluster\n\n\n\n\nSnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData Cloud cluster. In this release, the configuration supports launching the cluster on a single EC2 instance.\n\n\nIt is recommended that you select an instance type with higher processing power and more memory for this cluster, as it would be running four processes (locator, lead, a data server and an Apache Zeppelin server) on it.\n\n\nThis method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started! \n\n\nConfiguring and Launching the SnappyData Cloud Cluster\n\n\nLaunch the iSight CloudBuilder from \nhttp://www.snappydata.io/cloudbuilder\n. \n\n\n\n\n\n\nEnter the name for your cluster. Each cluster is identified by it\u2019s unique name. \nThe names and details of the members are automatically derived from the provided cluster name. \n\n\n\n\n\n\n\n\nEnter a name of an existing EC2 KeyPair. This enables SSH access to the cluster. \nRefer to the Amazon documentation for more information on  \ngenerating your own key pair\n.\n \n\n\n\n\n\n\n\nSelect an instance based on the capacity that you require. \n\n\n\n\n\n\n\nEnter the size of the EBS storage volume to be attached to the Amazon EC2 instance in the \nEBS Volume Size(gigabytes)\n field.\n\n\n\n\n\n\nNote: Currently only Amazon Elastic Block Storage (EBS) is supported. \n\n\n\n\n\n\n\n\nEnter your email address.  \n\n\n\n\n\n\n\n\nClick \nGenerate\n. \n\n\n\n\n\n\nOn the next page, select the AWS region, and then click \nLaunch Cluster\n to launch your single-node cluster.\n\n\n\n\nNote: \n\n\n\n\n\n\nThe region you select must match the key pair you created.\n\n\n\n\n\n\nIf you are not already logged into AWS, you are redirected to the AWS sign-in page. \n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nSelect Template page\n, the URL for the Amazon S3 template is pre-populated. Click \nNext\n to continue.   \n\n\n\n\n\n\n\n\nYou can change the stack name or click \nNext\n to use the provided default value.\n\n\n\n\nNote: The stack name must contain only letters, numbers, dashes and should start with an alpha character.\n\n\n\n\n\n\n\n\nSpecify the tags (key-value pairs) for resources in your stack or leave the field empty and click \nNext\n.\n\n\n\n\n\n\nOn the \nReview\n page, verify the details and click \nCreate\n to create a stack. \n\n\n\n\nNote: This operation may take a few minutes to complete. \n\n\n\n\n\n\n\n\nThe next page lists the existing stacks. Click \nRefresh\n to view the updated list and the status of the stack creation. \nWhen the cluster has started, the status of the stack changes to \nCREATE_COMPLETE\n. \n\n\n\n\n\n\n\n\n\n\nClick on the \nOutputs\n tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration. \n\n    \n\n\n\n\nNote: If the status of the stack displays \nROLLBACK_IN_PROGRESS\n or \nDELETE_COMPLETE\n, the stack creation may have failed. Some common problems that might have caused the failure are:\n\n\n\n\nInsufficient Permissions\n: Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.\n\n\nInvalid Keypair\n: Verify that the EC2 keypair exists in the region you selected in the iSight CloudBuilder creation steps.\n\n\nLimit Exceeded\n: Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.\n\n\n\n\n\n\n\n\n\n\nFor more information, refer to the \nApache Zeppelin\n section or refer to the \nApache Zeppelin documentation\n.\n\n\n\n\nNote: To stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it.\n\n\n\n\n\n\nDeploying SnappyData Cloud Cluster on AWS using Scripts\n\n\nPrerequisites\n\n\nBefore you begin, do the following:\n\n\n\n\n\n\nEnsure you have an existing AWS account with required permissions to launch EC2 resources.\n\n\n\n\n\n\nEC2 key pair created in the region where you want to launch the SnappyData cluster.\n\n\n\n\n\n\nUsing the AWS Secret Access Key and the Access Key ID, set the two environment variables, \nAWS_SECRET_ACCESS_KEY\n and \nAWS_ACCESS_KEY_ID\n.\n\n\nIf you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file. You can find this information from the AWS IAM console.\n\n\nFor example:  \n\n\nexport   AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112\n\n\nexport AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10\n\n\n\n\n\n\nEnsure Python v 2.7 or later is installed on your local computer.\n\n\n\n\n\n\nSnappyData provides a script that allows you to launch and manage SnappyData clusters on Amazon Elastic Compute Cloud (EC2). \n\n\nDownload the script from the \nSnappyData Release page\n. \nThe package is available in compressed files (\nsnappydata-ec2-\nversion\n.tar.gz\n). Extract the contents to a location on your computer.\n\n\nLaunching SnappyData Cluster\n\n\nTo execute the script:\n\n\nIn the command prompt, go to the directory where the \nsnappydata-ec2-\nversion\n.tar.gz\n is extracted, and enter the following:\n\n\n./snappy-ec2 -k \nyour-key-name\n -i \nyour-keyfile-path\n \naction\n \nyour-cluster-name\n\n\nHere, \nyour-key-name\n refers to the EC2 key pair, \nyour-keyfile-path\n refers to the path to the key file, \naction\n refers to the action to be performed (for example, launch, start, stop).\n\n\nBy default, the script starts one instance of the locator, lead and server. \nThe script identifies each cluster by it's unique cluster name, and internally ties members (locators, leads and stores/servers) of the cluster with EC2 security groups. \n\n\nThe  names and details of the members are automatically derived from the provided cluster name. \n\n\nFor example, if you launch a cluster named \nmy-cluster\n, the locator is available in security group named \nmy-cluster-locator\n and the store/server are available in \nmy-cluster-store\n.\n\n\nWhen running the script you can also specify properties like number of stores and region.\nFor example, using the following command, you can start a SnappyData cluster named \nsnappydata-cluster\n with 2 stores (or servers) in the default N. Virginia (us-east-1) region on AWS. It also starts an Apache Zeppelin server on the instance where lead is running.\n\n\n./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file --stores=2 --with-zeppelin=embedded launch snappydata-cluster \n\n\n\n\nTo start Apache Zeppelin on a separate instance, use \n--with-zeppelin=non-embedded\n. \n\n\nFor comprehensive list of command options, run \n./snappy-ec2\n in the command prompt.\n\n\n\n\nLoading Data from AWS S3\n\n\nSnappyData provides you with predefined buckets which contain datasets. When data is loaded, the table reads from the files available at the specified external location (AWS S3). \n\n\n\n\nNote:\n\n\n\n\n\n\nThe Amazon S3 buckets and files are private by default. Ensure that you set the permissions required to make the data publicly accessible. Please refer to the \ndocumentation provided by Amazon S3\n for detailed information on creating a bucket, adding files and setting required permissions.\n\n\n\n\n\n\nYou can also find AWS related information on the AWS homepage, from the \nAccount\n \n \nSecurity Credentials\n \n \nAccess Credentials\n option.\n\n\n\n\n\n\nInformation related to the Bucket Name and Folder Location can be found on the AWS S3 site.\n\n\n\n\n\n\n\n\nTo define a table that references the data in AWS S3, create a paragraph in the following format:\n\n\n%sql\nDROP TABLE IF EXISTS \ntable_name\n ;\nCREATE EXTERNAL TABLE \ntable_name\n USING parquet OPTIONS(path '\nAWS_SECRET_ACCESS_KEY\n:\nAWS_ACCESS_KEY_ID\n@\nbucket_Name\n/\nfolder_name\n');\n\n\n\n\nThe values are:\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription/Value\n\n\n\n\n\n\n\n\n\n\ntable_name\n\n\nThe name of the table\n\n\n\n\n\n\nAWS_SECRET_ACCESS_KEY\n:\nAWS_ACCESS_KEY_ID\n\n\nSecurity credentials used to authenticate and authorize calls that you make to AWS.\n\n\n\n\n\n\nbucket_Name\n\n\nThe name of the bucket where the folder is located. Default value: zeppelindemo\n\n\n\n\n\n\nfolder_name\n\n\nThe folder name where the data is stored. Default value: nytaxifaredata\n\n\n\n\n\n\n\n\n\n\nUsing Apache Zeppelin\n\n\nApache Zeppelin provides web-based notebooks for data exploration. A notebook consists of one or more paragraphs, and each paragraph consists of a section each for code and results.\nLaunch Apache Zeppelin from the web browser by accessing the host and port associated with your Apache Zeppelin server. For example,http://\nzeppelin_host\n:\nport_number\n. The welcome page which lists existing notebooks is displayed.\n\nSnappyData provides predefined notebooks which are displayed on the home page after you have logged into Apache Zeppelin. For more information, see \nUsing Predefined Notebooks\n.\n\n\nRefer to the \nApache Zeppelin documentation\n, for more information.\n\n\nUsing the Interpreter\n\n\nSnappyData Interpreter group consists of the interpreters \n%snappydata.spark\n and \n%snappydata.sql\n.\nTo use an interpreter, add the associated interpreter directive with the format, \n%\nInterpreter_name\n at the beginning of a paragraph in your note. In a paragraph, use one of the interpreters, and then enter required commands.\n\n\n\n\nNote:\n\n\n\n\nThe SnappyData Interpreter provides a basic auto-completion functionality. Press (Ctrl+.) on the keyboard to view a list of suggestions.\n\n\nIt is recommend that you use the SQL interpreter to run queries on the SnappyData cluster, as an out of memory error may be reported with running the Scala interpreter.\n\n\n\n\n\n\nSQL Interpreter\n\n\nThe \n%snappydata.sql\n code specifies the default SQL interpreter. This interpreter is used to execute SQL queries on SnappyData cluster.\n\n\nMulti-Line Statements\n\n\nMulti-line statements as well as multiple statements on the same line are also supported as long as they are separated by a semicolon. However, only the result of the last query is displayed.\n\n\nSnappyData provides a list of connection-specific SQL properties that can be applied to the paragraph that is executed. \n\n\nIn the following example, \nspark.sql.shuffle.partitions\n allows you to specify the number of partitions to be used for this query:\n\n\n%sql\nset spark.sql.shuffle.partitions=6; \nselect medallion,avg(trip_distance) as avgTripDist from nyctaxi group by medallion order by medallion desc limit 100 with error\n\n\n\n\nSnappyData Directives in Apache Zeppelin\n\n\nYou can execute approximate queries on SnappyData cluster by using the \n%sql show-instant-results-first\n directive. \nIn this case, the query is first executed on the sample table and the approximate result is displayed, after which the query is run on the base table. Once the query is complete, the approximate result is replaced with the actual result.\n\n\nIn the following example, you can see that the query is first executed on the sample table, and the time required to execute the query is displayed. \nAt the same time, the query is executed on the base table, and the total time required to execute the query on the base table is displayed.\n\n\n%sql show-instant-results-first\nselect avg(trip_time_in_secs/60) tripTime, hour(pickup_datetime), count(*) howManyTrips, absolute_error(tripTime) from nyctaxi where pickup_latitude \n 40.767588 and pickup_latitude \n 40.749775 and pickup_longitude \n -74.001632 and  pickup_longitude \n -73.974595 and dropoff_latitude \n 40.716800 and  dropoff_latitude \n  40.717776 and dropoff_longitude \n  -74.017682 and dropoff_longitude \n -74.000945 group by hour(pickup_datetime);\n\n\n\n\n\n\n\n\nNote: This directive works only for the SQL interpreter and an error may be displayed for the Scala interpreter.\n\n\n\n\nScala Interpreter\n\n\nThe \n%snappydata.spark\n code specifies the default Scala interpreter. This interpreter is used to write Scala code in the paragraph.\nSnappyContext is injected in this interpreter and can be accessed using variable \nsnc\n.\n\n\n\n\nUsing Predefined Notebooks\n\n\nSnappyData provides you predefined notebooks \nNYCTAXI Analytics\n and \nAirline Data Analytics\n which contains definitions that are stored in a single file. \n\n\nWhen you launch Apache Zeppelin in the browser, the welcome page displays the existing notebooks. Open a notebook and run any of the paragraphs to analyze data and view the result. \n\n\n\n\nCreating Notebooks - Try it Yourself!\n\n\n\n\nLog on to Apache Zeppelin, create a notebook and insert a new paragraph.\n\n\nUse \n%snappydata.spark\n for SnappyData interpreter or use \n%snappydata.sql\n for SQL interpreter.\n\n\nDownload a dataset you want to use and create tables as mentioned below\n\n\n\n\nExamples of Queries and Results\n\n\nThis section provides you with examples you can use in a paragraph.\n\n\n\n\nIn this example, you can create tables using external dataset from AWS S3.\n\n\n\n\n\n\n\n\nIn this example, you can execute a query on a base table using the SQL interpreter. It returns the number of rides per week. \n\n\n\n\n\n\n\n\nIn this example, you can execute a query on a sample table using the SQL interpreter. It returns the number of rides per week\n\n\n\n\n\n\n\n\nIn this example, you are processing data using the SnappyData Scala interpreter.\n\n\n\n\n\n\n\n\nApache Zeppelin allows you to dynamically create input fields. To create a text input field, use \n${fieldname}\n.\nIn this example, the input forms are, \n${taxiin=60} or taxiout \n ${taxiout=60}\n\n\n\n\n\n\n\n\nMonitoring the SnappyData Cloud Cluster\n\n\nYou can monitor the SnappyData cluster using SnappyData Pulse Console and the Apache Spark Console. The monitoring tools enable you to observe and record the performance and the activities on the SnappyData cluster.\n\n\nThe SnappyData Pulse Console provides a graphical dashboard which helps you monitor vital, real-time health and performance of SnappyData clusters, members and tables. \nIt provides information on the health, operational and configuration data, system alerts, CPU, disk and memory usage, throughput performance and statistics for system members like locators, leads, stores/servers, connected clients etc.\n\nIt also displays data information for various tables created in the cluster on different nodes/members along with their statistics.\n\n\nThe Apache Spark Console displays useful information about SnappyData. This includes, a list of scheduler stages and tasks, summary of tables and memory usage.\n\n\nAccessing the Console\n\n\nTo access the SnappyData Pulse or Apache Spark console from the Apache Zeppelin notebook: \n\n\n\n\nClick on the \nSpark UI\n or \nPulse\n links provided in the paragraph. \n\n\n\nFor the SnappyData Pulse console, enter the default login credentials \"admin\" as both the user name and password.\n\n\nOnce you have logged in, you can start monitoring SnappyData cluster. \n\n\n\n\nThe Technology Powering iSight Cloud\n\n\niSight Cloud uses the SnappyData Synopsis Engine to deliver blazing fast responses to queries that have long processing times. Analytic queries typically aim to provide aggregate information and involve full table or partial table scans. The cost of these queries is directly proportional to the amount of data that needs to be scanned. Analytics queries also often involve distributed joins of a dimension table with one or more fact tables. The cost of pruning these queries down to the final result is directly proportional to the size of the data involved. Distributed joins involve lots of data movement making such queries extremely expensive in traditional systems that process the entire data set.\n\n\nThe Synopsis Data Engine offers a breakthrough solution to these problems by building out stratified samples of the most common columns used in queries, as well as other probabilistic data structures like count-min-sketch, bloom filters etc. The use of these structures, along with extensions to the querying engine allow users to get almost-perfect answers to complex queries in a fraction of the time it used to take to answer these queries.\n\n\nFor more information on SDE and sampling techniques used by SnappyData, refer to the \nSDE documentation\n.", 
            "title": "Using iSight-Cloud"
        }, 
        {
            "location": "/aqp_aws/#overview-of-snappydata-isight-cloud", 
            "text": "iSight-Cloud is a cloud-based service that allows for instant visualization of analytic query results on large datasets. Powered by the SnappyData Synopsis Data Engine ( SDE ), users interact with iSight-Cloud to populate the synopsis engine with the right data sets and accelerate SQL queries by using the engine to provide latency bounded responses to large complex aggregate queries.   iSight-Cloud uses Apache Zeppelin as the front end notebook to display results and allows users to build powerful notebooks representing key elements of their business in a matter of minutes.   The service provides a web URL that spins up a cluster instance on AWS or users can download the iSight-Cloud EC2 script to configure a custom sized cluster, to create and render powerful visualizations of their big data sets with the click of a button. \nWith iSight-Cloud, you can speed up the process of understanding what your data is telling you, and move on to the task of organizing your business around those insights rapidly.  In this document, we describe the features provided by SnappyData for analyzing your data. It also provides details for deploying a SnappyData Cloud cluster on AWS CloudFormation or on AWS using the EC2 script.   Refer to the the examples and guidelines provided in this document to help you create notebooks using which, you can execute SQL queries or data frame API to analyze your data.", 
            "title": "Overview of SnappyData iSight-Cloud"
        }, 
        {
            "location": "/aqp_aws/#key-components", 
            "text": "This section provides a brief description of the key terms used in this document.    Amazon Web Services (AWS ):  Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform that offers a suite of cloud-computing services. The services provided by this platform that are important for SnappyData are, Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3).  SnappyData Cluster :  A database cluster which has three main components - Locator, Server and Lead  Apache Zeppelin : Apache Zeppelin is a web-based notebook that enables interactive data analytics. It allows you to make data-driven, interactive and collaborative documents with SQL queries or directly use the Spark API to process data.  Interpreters : A software module which is loaded into Apache Zeppelin upon startup. Interpreters allow various third party products including SnappyData to interact with Apache Zeppelin. The SnappyData interpreter gives users the ability to execute SQL queries or use the data frame API to visualize data.", 
            "title": "Key Components"
        }, 
        {
            "location": "/aqp_aws/#quick-start-steps", 
            "text": "To understand the product follow these easy steps that can get you started quickly:   Setting up SnappyData Cloud Cluster  Deploying SnappyData Cloud Cluster with iSight CloudBuilder  Deploying SnappyData Cloud Cluster on AWS using Scripts    Using Apache Zeppelin       Using Predefined Notebook  Creating your own Notebook    Loading Data from AWS S3  Monitoring SnappyData Cloud Cluster", 
            "title": "Quick Start Steps"
        }, 
        {
            "location": "/aqp_aws/#setting-up-snappydata-cloud-cluster", 
            "text": "This section discusses the steps required for setting up and deploying SnappyData Cloud Cluster on AWS using the iSight CloudBuilder and using script.", 
            "title": "Setting Up SnappyData Cloud Cluster"
        }, 
        {
            "location": "/aqp_aws/#deploying-snappydata-cloud-cluster-with-isight-cloudbuilder", 
            "text": "Watch the following  video to learn how easy it is to use iSight CloudBuilder, which generates a SnappyData Cloud Cluster.", 
            "title": "Deploying SnappyData Cloud Cluster with iSight CloudBuilder"
        }, 
        {
            "location": "/aqp_aws/#prerequisites", 
            "text": "Before you begin,:   Ensure that you have an existing AWS account with required permissions to launch EC2 resources  Sign in to the AWS console using your AWS account-specific URL. This ensures that the account-specific URL is stored as a cookie in the browser, which then redirects you to the appropriate AWS URL for subsequent logins.  Create an EC2 key pair in the region where you want to launch the SnappyData Cloud cluster   SnappyData uses the AWS CloudFormation feature to automatically install, configure and start a SnappyData Cloud cluster. In this release, the configuration supports launching the cluster on a single EC2 instance.  It is recommended that you select an instance type with higher processing power and more memory for this cluster, as it would be running four processes (locator, lead, a data server and an Apache Zeppelin server) on it.  This method is recommended as the fastest way to deploy SnappyData. All you need is an existing AWS account and login credentials to get started!", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aqp_aws/#configuring-and-launching-the-snappydata-cloud-cluster", 
            "text": "Launch the iSight CloudBuilder from  http://www.snappydata.io/cloudbuilder .     Enter the name for your cluster. Each cluster is identified by it\u2019s unique name. \nThe names and details of the members are automatically derived from the provided cluster name.      Enter a name of an existing EC2 KeyPair. This enables SSH access to the cluster. \nRefer to the Amazon documentation for more information on   generating your own key pair .      Select an instance based on the capacity that you require.     Enter the size of the EBS storage volume to be attached to the Amazon EC2 instance in the  EBS Volume Size(gigabytes)  field.    Note: Currently only Amazon Elastic Block Storage (EBS) is supported.      Enter your email address.       Click  Generate .     On the next page, select the AWS region, and then click  Launch Cluster  to launch your single-node cluster.   Note:     The region you select must match the key pair you created.    If you are not already logged into AWS, you are redirected to the AWS sign-in page.        On the  Select Template page , the URL for the Amazon S3 template is pre-populated. Click  Next  to continue.        You can change the stack name or click  Next  to use the provided default value.   Note: The stack name must contain only letters, numbers, dashes and should start with an alpha character.     Specify the tags (key-value pairs) for resources in your stack or leave the field empty and click  Next .    On the  Review  page, verify the details and click  Create  to create a stack.    Note: This operation may take a few minutes to complete.      The next page lists the existing stacks. Click  Refresh  to view the updated list and the status of the stack creation. \nWhen the cluster has started, the status of the stack changes to  CREATE_COMPLETE .       Click on the  Outputs  tab, to view the links (URL) required for launching Apache Zeppelin, which provides web-based notebooks for data exploration.  \n       Note: If the status of the stack displays  ROLLBACK_IN_PROGRESS  or  DELETE_COMPLETE , the stack creation may have failed. Some common problems that might have caused the failure are:   Insufficient Permissions : Verify that you have the required permissions for creating a stack (and other AWS resources) on AWS.  Invalid Keypair : Verify that the EC2 keypair exists in the region you selected in the iSight CloudBuilder creation steps.  Limit Exceeded : Verify that you have not exceeded your resource limit. For example, if you exceed the allocated limit of Amazon EC2 instances, the resource creation fails and an error is reported.      For more information, refer to the  Apache Zeppelin  section or refer to the  Apache Zeppelin documentation .   Note: To stop incurring charges for the instance, you can either terminate the instance or delete the stack. You cannot connect to or restart an instance after you have terminated it.", 
            "title": "Configuring and Launching the SnappyData Cloud Cluster"
        }, 
        {
            "location": "/aqp_aws/#deploying-snappydata-cloud-cluster-on-aws-using-scripts", 
            "text": "", 
            "title": "Deploying SnappyData Cloud Cluster on AWS using Scripts"
        }, 
        {
            "location": "/aqp_aws/#prerequisites_1", 
            "text": "Before you begin, do the following:    Ensure you have an existing AWS account with required permissions to launch EC2 resources.    EC2 key pair created in the region where you want to launch the SnappyData cluster.    Using the AWS Secret Access Key and the Access Key ID, set the two environment variables,  AWS_SECRET_ACCESS_KEY  and  AWS_ACCESS_KEY_ID .  If you already have set up the AWS Command Line Interface on your local machine, the script automatically detects and uses the credentials from the AWS credentials file. You can find this information from the AWS IAM console.  For example:    export   AWS_SECRET_ACCESS_KEY=abcD12efGH34ijkL56mnoP78qrsT910uvwXYZ1112  export AWS_ACCESS_KEY_ID=A1B2C3D4E5F6G7H8I9J10    Ensure Python v 2.7 or later is installed on your local computer.    SnappyData provides a script that allows you to launch and manage SnappyData clusters on Amazon Elastic Compute Cloud (EC2).   Download the script from the  SnappyData Release page . \nThe package is available in compressed files ( snappydata-ec2- version .tar.gz ). Extract the contents to a location on your computer.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aqp_aws/#launching-snappydata-cluster", 
            "text": "To execute the script:  In the command prompt, go to the directory where the  snappydata-ec2- version .tar.gz  is extracted, and enter the following:  ./snappy-ec2 -k  your-key-name  -i  your-keyfile-path   action   your-cluster-name  Here,  your-key-name  refers to the EC2 key pair,  your-keyfile-path  refers to the path to the key file,  action  refers to the action to be performed (for example, launch, start, stop).  By default, the script starts one instance of the locator, lead and server. \nThe script identifies each cluster by it's unique cluster name, and internally ties members (locators, leads and stores/servers) of the cluster with EC2 security groups.   The  names and details of the members are automatically derived from the provided cluster name.   For example, if you launch a cluster named  my-cluster , the locator is available in security group named  my-cluster-locator  and the store/server are available in  my-cluster-store .  When running the script you can also specify properties like number of stores and region.\nFor example, using the following command, you can start a SnappyData cluster named  snappydata-cluster  with 2 stores (or servers) in the default N. Virginia (us-east-1) region on AWS. It also starts an Apache Zeppelin server on the instance where lead is running.  ./snappy-ec2 -k ec2-keypair-name -i /path/to/keypair/private/key/file --stores=2 --with-zeppelin=embedded launch snappydata-cluster   To start Apache Zeppelin on a separate instance, use  --with-zeppelin=non-embedded .   For comprehensive list of command options, run  ./snappy-ec2  in the command prompt.", 
            "title": "Launching SnappyData Cluster"
        }, 
        {
            "location": "/aqp_aws/#loading-data-from-aws-s3", 
            "text": "SnappyData provides you with predefined buckets which contain datasets. When data is loaded, the table reads from the files available at the specified external location (AWS S3).    Note:    The Amazon S3 buckets and files are private by default. Ensure that you set the permissions required to make the data publicly accessible. Please refer to the  documentation provided by Amazon S3  for detailed information on creating a bucket, adding files and setting required permissions.    You can also find AWS related information on the AWS homepage, from the  Account     Security Credentials     Access Credentials  option.    Information related to the Bucket Name and Folder Location can be found on the AWS S3 site.     To define a table that references the data in AWS S3, create a paragraph in the following format:  %sql\nDROP TABLE IF EXISTS  table_name  ;\nCREATE EXTERNAL TABLE  table_name  USING parquet OPTIONS(path ' AWS_SECRET_ACCESS_KEY : AWS_ACCESS_KEY_ID @ bucket_Name / folder_name ');  The values are:     Property  Description/Value      table_name  The name of the table    AWS_SECRET_ACCESS_KEY : AWS_ACCESS_KEY_ID  Security credentials used to authenticate and authorize calls that you make to AWS.    bucket_Name  The name of the bucket where the folder is located. Default value: zeppelindemo    folder_name  The folder name where the data is stored. Default value: nytaxifaredata", 
            "title": "Loading Data from AWS S3"
        }, 
        {
            "location": "/aqp_aws/#using-apache-zeppelin", 
            "text": "Apache Zeppelin provides web-based notebooks for data exploration. A notebook consists of one or more paragraphs, and each paragraph consists of a section each for code and results.\nLaunch Apache Zeppelin from the web browser by accessing the host and port associated with your Apache Zeppelin server. For example,http:// zeppelin_host : port_number . The welcome page which lists existing notebooks is displayed. \nSnappyData provides predefined notebooks which are displayed on the home page after you have logged into Apache Zeppelin. For more information, see  Using Predefined Notebooks .  Refer to the  Apache Zeppelin documentation , for more information.", 
            "title": "Using Apache Zeppelin"
        }, 
        {
            "location": "/aqp_aws/#using-the-interpreter", 
            "text": "SnappyData Interpreter group consists of the interpreters  %snappydata.spark  and  %snappydata.sql .\nTo use an interpreter, add the associated interpreter directive with the format,  % Interpreter_name  at the beginning of a paragraph in your note. In a paragraph, use one of the interpreters, and then enter required commands.   Note:   The SnappyData Interpreter provides a basic auto-completion functionality. Press (Ctrl+.) on the keyboard to view a list of suggestions.  It is recommend that you use the SQL interpreter to run queries on the SnappyData cluster, as an out of memory error may be reported with running the Scala interpreter.", 
            "title": "Using the Interpreter"
        }, 
        {
            "location": "/aqp_aws/#sql-interpreter", 
            "text": "The  %snappydata.sql  code specifies the default SQL interpreter. This interpreter is used to execute SQL queries on SnappyData cluster.", 
            "title": "SQL Interpreter"
        }, 
        {
            "location": "/aqp_aws/#multi-line-statements", 
            "text": "Multi-line statements as well as multiple statements on the same line are also supported as long as they are separated by a semicolon. However, only the result of the last query is displayed.  SnappyData provides a list of connection-specific SQL properties that can be applied to the paragraph that is executed.   In the following example,  spark.sql.shuffle.partitions  allows you to specify the number of partitions to be used for this query:  %sql\nset spark.sql.shuffle.partitions=6; \nselect medallion,avg(trip_distance) as avgTripDist from nyctaxi group by medallion order by medallion desc limit 100 with error", 
            "title": "Multi-Line Statements"
        }, 
        {
            "location": "/aqp_aws/#snappydata-directives-in-apache-zeppelin", 
            "text": "You can execute approximate queries on SnappyData cluster by using the  %sql show-instant-results-first  directive. \nIn this case, the query is first executed on the sample table and the approximate result is displayed, after which the query is run on the base table. Once the query is complete, the approximate result is replaced with the actual result.  In the following example, you can see that the query is first executed on the sample table, and the time required to execute the query is displayed. \nAt the same time, the query is executed on the base table, and the total time required to execute the query on the base table is displayed.  %sql show-instant-results-first\nselect avg(trip_time_in_secs/60) tripTime, hour(pickup_datetime), count(*) howManyTrips, absolute_error(tripTime) from nyctaxi where pickup_latitude   40.767588 and pickup_latitude   40.749775 and pickup_longitude   -74.001632 and  pickup_longitude   -73.974595 and dropoff_latitude   40.716800 and  dropoff_latitude    40.717776 and dropoff_longitude    -74.017682 and dropoff_longitude   -74.000945 group by hour(pickup_datetime);    Note: This directive works only for the SQL interpreter and an error may be displayed for the Scala interpreter.", 
            "title": "SnappyData Directives in Apache Zeppelin"
        }, 
        {
            "location": "/aqp_aws/#scala-interpreter", 
            "text": "The  %snappydata.spark  code specifies the default Scala interpreter. This interpreter is used to write Scala code in the paragraph.\nSnappyContext is injected in this interpreter and can be accessed using variable  snc .", 
            "title": "Scala Interpreter"
        }, 
        {
            "location": "/aqp_aws/#using-predefined-notebooks", 
            "text": "SnappyData provides you predefined notebooks  NYCTAXI Analytics  and  Airline Data Analytics  which contains definitions that are stored in a single file.   When you launch Apache Zeppelin in the browser, the welcome page displays the existing notebooks. Open a notebook and run any of the paragraphs to analyze data and view the result.", 
            "title": "Using Predefined Notebooks"
        }, 
        {
            "location": "/aqp_aws/#creating-notebooks-try-it-yourself", 
            "text": "Log on to Apache Zeppelin, create a notebook and insert a new paragraph.  Use  %snappydata.spark  for SnappyData interpreter or use  %snappydata.sql  for SQL interpreter.  Download a dataset you want to use and create tables as mentioned below", 
            "title": "Creating Notebooks - Try it Yourself!"
        }, 
        {
            "location": "/aqp_aws/#examples-of-queries-and-results", 
            "text": "This section provides you with examples you can use in a paragraph.   In this example, you can create tables using external dataset from AWS S3.     In this example, you can execute a query on a base table using the SQL interpreter. It returns the number of rides per week.      In this example, you can execute a query on a sample table using the SQL interpreter. It returns the number of rides per week     In this example, you are processing data using the SnappyData Scala interpreter.     Apache Zeppelin allows you to dynamically create input fields. To create a text input field, use  ${fieldname} .\nIn this example, the input forms are,  ${taxiin=60} or taxiout   ${taxiout=60}", 
            "title": "Examples of Queries and Results"
        }, 
        {
            "location": "/aqp_aws/#monitoring-the-snappydata-cloud-cluster", 
            "text": "You can monitor the SnappyData cluster using SnappyData Pulse Console and the Apache Spark Console. The monitoring tools enable you to observe and record the performance and the activities on the SnappyData cluster.  The SnappyData Pulse Console provides a graphical dashboard which helps you monitor vital, real-time health and performance of SnappyData clusters, members and tables. \nIt provides information on the health, operational and configuration data, system alerts, CPU, disk and memory usage, throughput performance and statistics for system members like locators, leads, stores/servers, connected clients etc. \nIt also displays data information for various tables created in the cluster on different nodes/members along with their statistics.  The Apache Spark Console displays useful information about SnappyData. This includes, a list of scheduler stages and tasks, summary of tables and memory usage.", 
            "title": "Monitoring the SnappyData Cloud Cluster"
        }, 
        {
            "location": "/aqp_aws/#accessing-the-console", 
            "text": "To access the SnappyData Pulse or Apache Spark console from the Apache Zeppelin notebook:    Click on the  Spark UI  or  Pulse  links provided in the paragraph.   For the SnappyData Pulse console, enter the default login credentials \"admin\" as both the user name and password.  Once you have logged in, you can start monitoring SnappyData cluster.", 
            "title": "Accessing the Console"
        }, 
        {
            "location": "/aqp_aws/#the-technology-powering-isight-cloud", 
            "text": "iSight Cloud uses the SnappyData Synopsis Engine to deliver blazing fast responses to queries that have long processing times. Analytic queries typically aim to provide aggregate information and involve full table or partial table scans. The cost of these queries is directly proportional to the amount of data that needs to be scanned. Analytics queries also often involve distributed joins of a dimension table with one or more fact tables. The cost of pruning these queries down to the final result is directly proportional to the size of the data involved. Distributed joins involve lots of data movement making such queries extremely expensive in traditional systems that process the entire data set.  The Synopsis Data Engine offers a breakthrough solution to these problems by building out stratified samples of the most common columns used in queries, as well as other probabilistic data structures like count-min-sketch, bloom filters etc. The use of these structures, along with extensions to the querying engine allow users to get almost-perfect answers to complex queries in a fraction of the time it used to take to answer these queries.  For more information on SDE and sampling techniques used by SnappyData, refer to the  SDE documentation .", 
            "title": "The Technology Powering iSight Cloud"
        }, 
        {
            "location": "/LICENSE/", 
            "text": "LICENSE\n\n\n                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n\n\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n\n\n\n\n\nDefinitions.\n\n\n\"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n\n\"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n\n\"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n\n\"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n\n\"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n\n\"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n\n\n\n\n\nGrant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n\n\n\n\n\nGrant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n\n\n\n\n\nRedistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n\n(a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n\n(b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n\n(c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n\nYou may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n\n\n\n\n\nSubmission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n\n\n\n\n\nTrademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n\n\n\n\n\nDisclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n\n\n\n\n\nLimitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n\n\n\n\n\nAccepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n\n\n\n\n\nEND OF TERMS AND CONDITIONS\n\n\nAPPENDIX: How to apply the Apache License to your work.\n\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n\n\n\nCopyright 2016 SnappyData Inc.\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "License"
        }, 
        {
            "location": "/LICENSE/#license", 
            "text": "Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/  TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION    Definitions.  \"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.  \"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.  \"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.  \"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.  \"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.  \"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.  \"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).  \"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.  \"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"  \"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.    Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.    Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.    Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:  (a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and  (b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and  (c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and  (d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.  You may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.    Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.    Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.    Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.    Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.    Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.    END OF TERMS AND CONDITIONS  APPENDIX: How to apply the Apache License to your work.    To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.  Copyright 2016 SnappyData Inc.  Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at     http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", 
            "title": "LICENSE"
        }
    ]
}