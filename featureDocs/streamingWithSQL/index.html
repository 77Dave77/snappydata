<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="SnappyData development team">
  
  <title>Stream processing using SQL - SnappyData Docs - Preview release</title>
  

  <link rel="shortcut icon" href="../../favicon.ico">
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../extra.css" rel="stylesheet">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Stream processing using SQL";
    var mkdocs_page_input_path = "featureDocs/streamingWithSQL.md";
    var mkdocs_page_url = "/featureDocs/streamingWithSQL/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> SnappyData Docs - Preview release</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../build-instructions/">Building from source, project layout</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../quickstart/">Quickstart</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../snappyIntroduction/">Overview</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../features/">Key features</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../architecture/">Architecture</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../configuration/">Configuring the cluster</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../connectingToCluster/">Connecting using JDBC, Spark</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../jobs/">Developing Apps using the Spark API</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../rowAndColumnTables/">Row and Column tables</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../aqp/">Approximate query processing</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Stream processing using SQL</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#spark-streaming-overview">Spark Streaming Overview</a></li>
                
            
                <li class="toctree-l3"><a href="#snappydata-streaming-extensions-over-spark">SnappyData Streaming extensions over Spark</a></li>
                
            
                <li class="toctree-l3"><a href="#working-with-stream-tables">Working with stream tables</a></li>
                
            
                <li class="toctree-l3"><a href="#stream-sql-through-snappy-shell">Stream SQL through Snappy-Shell</a></li>
                
            
                <li class="toctree-l3"><a href="#schemadstream">SchemaDStream</a></li>
                
            
                <li class="toctree-l3"><a href="#registering-continuous-queries">Registering Continuous queries</a></li>
                
            
                <li class="toctree-l3"><a href="#dynamicad-hoc-conitnous-queries">Dynamic(ad-hoc) Conitnous queries</a></li>
                
            
                <li class="toctree-l3"><a href="#what-is-currently-out-of-scope">What is currently out-of-scope?</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../deployment/">Deployment topologies</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>About</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../LICENSE/">License</a>
        
    </li>

        
    </ul>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">SnappyData Docs - Preview release</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
    
    <li>Stream processing using SQL</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/SnappyDataInc/snappydata" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>SnappyData’s streaming functionality builds on top of Spark Streaming and primarily is aimed at making it simpler to build streaming applications and integration with the built-in store. 
Here is a brief overview of <a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark streaming</a> from the Spark Streaming guide. </p>
<h2 id="spark-streaming-overview">Spark Streaming Overview</h2>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>.</p>
<p>Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark's <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> and <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">graph processing</a> algorithms on data streams.  </p>
<p style="text-align: center;">
  <img
    src="http://spark.apache.org/docs/latest/img/streaming-arch.png"
    title="Spark Streaming architecture"     alt="Spark Streaming"
    width="70%"
  />
</p>

<p>Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.</p>
<p style="text-align: center;">
 <img src="http://spark.apache.org/docs/latest/img/streaming-flow.png"
 title="Spark Streaming data flow"
 alt="Spark Streaming"
 width="70%" />
 </p>

<p>Spark Streaming provides a high-level abstraction called <em>discretized stream</em> or <em>DStream</em>, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of <a href="../api/scala/index.html#org.apache.spark.rdd.RDD">RDDs</a>. </p>
<p>Additional details on the Spark Streaming concepts and programming is covered <a href="../Spark Streaming">here</a>.</p>
<h2 id="snappydata-streaming-extensions-over-spark">SnappyData Streaming extensions over Spark</h2>
<p>We offer the following enhancements over Spark Streaming : </p>
<ol>
<li>
<p><strong>Manage Streams declaratively</strong>: Similar to SQL Tables, Streams can be defined declaratively from any SQL client and managed as Tables in the persistent system catalog of SnappyStore. The declarative language follows the SQL language and provides access to the any of the Spark Streaming streaming adapters such as Kafka or file input streams. Raw tuples arriving can be transformed into a proper structure through pluggable transformers providing the desired flexibility for custom filtering or type conversions. </p>
</li>
<li>
<p><strong>SQL based stream processing</strong>: With streams visible as Tables they can be joined with other streams or resident tables (reference data, history, etc). Essentially, the entire SQL language can be used to analyze distributed streams. </p>
</li>
<li>
<p><strong>Continuous queries and time windows</strong>: Similar to popular stream processing products, applications can register “continuous” queries on streams. By default, spark streaming emits batches once every second and any registered queries would be executed each time a batch is emitted. To support arbitrary time ranges, we extend the standard SQL to be able to specify the time window for the query. </p>
</li>
<li>
<p><strong>OLAP optimizations</strong>: By integrating and collocating stream processing with our hybrid in-memory storage engine, we leverage our optimizer and column store for expensive scans and aggregations, while providing fast key-based operations with our row store.</p>
</li>
<li>
<p><strong>Reduced shuffling through co-partitioning</strong>: With SnappyData, the partitioning key used by the input queue (e.g., for Kafka sources), the stream processor and the underlying store can all be the same. This dramatically reduces the need to shuffle records.</p>
</li>
<li>
<p><strong>Approximate stream analytics</strong>: When the volumes are too high, a stream can be summarized using various forms of samples and sketches to enable fast time series analytics. This is particularly useful when applications are interested in trending patterns, for instance, rendering a set of trend lines in real time on user displays.</p>
</li>
</ol>
<h2 id="working-with-stream-tables">Working with stream tables</h2>
<p>SnappyData supports creation of stream tables from Twitter, Kafka, Files, Sockets sources.</p>
<pre><code>// DDL for creating a stream table
CREATE STREAM TABLE [IF NOT EXISTS] table_name
(COLUMN_DEFININTION)
USING 'kafka_stream | file_stream | twitter_stream | socket_stream | directkafka_stream'
OPTIONS (
 // multiple stream source specific options
storagelevel '', 
rowConverter '', 
zkQuorum '',
groupId '',
topics '', 
kafkaParams '',
consumerKey '',
consumerSecret '',
accessToken '',
accessTokenSecret '',
hostname '',
port '',
directory ''
)

// DDL for dropping a stream table
DROP TABLE [IF EXISTS] table_name

// Initialize StreamingContext
STREAMING INIT &lt;batchIntervalSeconds&gt;

// Start streaming
STREAMING START

// Stop streaming
STREAMING STOP
</code></pre>
<p>For example to create a stream table using kafka source : </p>
<pre><code class="scala">    val sc = new SparkContext(new SparkConf().setAppName(&quot;example&quot;).setMaster(&quot;local[*]&quot;))
    val snc = SnappyContext.getOrCreate(sc)
    var snsc = SnappyStreamingContext(snc, Seconds(1))

    snsc.sql(&quot;create stream table streamTable (userId string, clickStreamLog string) &quot; +
        &quot;using kafka_stream options (&quot; +
        &quot;storagelevel 'MEMORY_AND_DISK_SER_2', &quot; +
        &quot;rowConverter 'io.snappydata.app.streaming.KafkaStreamToRowsConverter', &quot; +
        &quot;zkQuorum 'localhost:2181', &quot; +
        &quot;groupId 'streamConsumer', &quot; +
        &quot;topics 'streamTopic:01')&quot;)

    // You can get a handle of underlying DStream of the table
    val dStream = snsc.getSchemaDStream(&quot;streamTable&quot;)

    // You can also save the DataFrames to an external table
    dStream.foreachDataFrame(_.write.insertInto(tableName))
</code></pre>

<p>The streamTable created in above example can be accessed from snappy-shell and can be queried using ad-hoc SQL queries.</p>
<h2 id="stream-sql-through-snappy-shell">Stream SQL through Snappy-Shell</h2>
<p>Start a SnappyData cluster and connect through snappy-shell : </p>
<pre><code>//create a connection
snappy&gt; connect client 'localhost:1527';

// Initialize streaming with batchInterval of 2 seconds
snappy&gt; streaming init 2;

// Create a stream table
snappy&gt; create stream table streamTable (id long, text string, fullName string, country string,
retweets int, hashtag  string) using twitter_stream options (consumerKey '', consumerSecret '',
accessToken '', accessTokenSecret '', rowConverter 'org.apache.spark.sql.streaming.TweetToRowsConverter');

// Start the streaming 
snappy&gt; streaming start;

//Run ad-hoc queries on the streamTable on current batch of data
snappy&gt; select id, text, fullName from streamTable where text like '%snappy%'

// Drop the streamTable
snappy&gt; drop table streamTable;

// Stop the streaming
snappy&gt; streaming stop;
</code></pre>
<h2 id="schemadstream">SchemaDStream</h2>
<p>SchemaDStream is SQL based DStream with support for schema/Product. It offers the ability to manipulate SQL query on DStreams. It is similar to SchemaRDD, which offers the similar functions. Internally, RDD of each batch duration is treated as a small table and CQs are evaluated on those small tables. Similar to foreachRDD in DStream, SchemaDStream provide foreachDataFrame API.SchemaDStream can be registered as table.</p>
<h2 id="registering-continuous-queries">Registering Continuous queries</h2>
<pre><code class="scala">    //You can join two stream tables and produce a result stream. 
    val resultStream = snsc.registerCQ(&quot;SELECT s1.id, s1.text FROM stream1 window (duration 
    '2' seconds, slide '2' seconds) s1 JOIN stream2 s2 ON s1.id = s2.id&quot;)

    // You can also save the DataFrames to an external table
    dStream.foreachDataFrame(_.write.insertInto(&quot;yourTableName&quot;))
</code></pre>

<h2 id="dynamicad-hoc-conitnous-queries">Dynamic(ad-hoc) Conitnous queries</h2>
<p>Unlike Spark streaming, you do not need to register all your stream output transformations (which is continous query in this case) before the start of StreamingContext. The CQs can be registered even after the SnappyStreamingContext has started.</p>
<h2 id="what-is-currently-out-of-scope">What is currently out-of-scope?</h2>
<p>Continous Queries through command line(Snappy-Shell) </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../deployment/" class="btn btn-neutral float-right" title="Deployment topologies">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../aqp/" class="btn btn-neutral" title="Approximate query processing"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright &copy; 2016 <a href="http://snappydata.io</a>.</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/SnappyDataInc/snappydata" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../aqp/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../deployment/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
