apply plugin: 'wrapper'

buildscript {
  repositories {
    maven { url "https://plugins.gradle.org/m2" }
    jcenter()
  }
  dependencies {
    classpath "com.github.maiflai:gradle-scalatest:0.9"
    classpath "org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.10:0.7.2"
    classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.2'
  }
}

allprojects {
  // We want to see all test results.  This is equivalatent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  repositories {
    maven { url "file://" + rootDir.getAbsolutePath() + "/local-repo" }
    mavenLocal()
    maven { url "http://dl.bintray.com/spark-jobserver/maven" }
    jcenter()
    maven { url "https://repository.apache.org/content/repositories/releases" }
    maven { url "https://repository.jboss.org/nexus/content/repositories/releases" }
    maven { url "https://repo.eclipse.org/content/repositories/paho-releases" }
    maven { url "https://repository.cloudera.com/artifactory/cloudera-repos" }
    maven { url "https://oss.sonatype.org/content/repositories/orgspark-project-1113" }
    maven { url "http://repository.mapr.com/maven" }
    maven { url "https://repo.spring.io/libs-release" }
    maven { url "http://maven.twttr.com" }
    maven { url "http://repository.apache.org/snapshots" }
  }

  apply plugin: 'java'
  apply plugin: 'maven'
  apply plugin: 'scalaStyle'
  apply plugin: 'idea'
  apply plugin: 'eclipse'
  //apply plugin: 'com.github.maiflai.scalatest'

  group = 'io.snappydata'
  version = '0.1.0-SNAPSHOT'

  // apply compiler options
  sourceCompatibility = 1.7
  targetCompatibility = 1.7

  compileJava.options.encoding = 'UTF-8'
  compileJava.options.compilerArgs << '-Xlint:all,-serial,-path'
  javadoc.options.charSet = 'UTF-8'

  ext {
    scalaBinaryVersion = '2.10'
    scalaVersion = scalaBinaryVersion + '.4'
    sparkVersion = '1.5.0-SNAPSHOT.1'
    log4jVersion = '1.2.17'
    slf4jVersion = '1.7.12'
    junitVersion = '4.11'
    hadoopVersion = '2.4.1'
    gemfireXDVersion = '2.0-Beta'
    buildFlags = ''
  }

  if (!buildRoot.isEmpty()) {
    buildDir = new File(buildRoot, 'scala-' + scalaBinaryVersion + '/' +  project.path.replace(':', '/'))
  } else {
    // default output directory like in sbt/maven
    buildDir = 'build-artifacts/scala-' + scalaBinaryVersion
  }

  dependencies {
    compile 'log4j:log4j:' + log4jVersion
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion

    testCompile "junit:junit:${junitVersion}"
    testCompile 'org.scalatest:scalatest_' + scalaBinaryVersion + ':2.2.1'

    testRuntime 'org.pegdown:pegdown:1.1.0'
  }
}

// Configure scalaStyle for only non spark related modules
configure(subprojects.findAll {!(it.name ==~ /snappy-spark.*/)}) {
  scalaStyle {
    configLocation = "scalastyle-config.xml"
    source = "src/main/scala"
  }
}

subprojects {
  // the run task for a selected sub-project
  task run(type:JavaExec) {
    if (!project.hasProperty('mainClass')) {
      main = 'io.snappydata.app.SparkSQLTest'
    } else {
      main = mainClass
    }
    if (project.hasProperty('params')) {
      args = params.split(",") as List
    }
    classpath = sourceSets.main.runtimeClasspath + sourceSets.test.runtimeClasspath
    jvmArgs = [ '-Xmx2g', '-XX:MaxPermSize=512m' ]
  }

  // apply default manifest
  jar {
    manifest {
      attributes(
        "Manifest-Version"  : "1.0",
        "Created-By"        : System.getProperty("user.name"),
        "Title"             : rootProject.name,
        "Version"           : version,
        "Vendor"            : "Snappy Data, Inc."
      )
    }
  }

  task packageSources(type: Jar, dependsOn: classes) {
    classifier = 'sources'
    from sourceSets.main.allSource
  }
  task packageDocs(type: Jar, dependsOn: javadoc) {
    classifier = 'sources'
    from javadoc.destinationDir
  }
  /*
  artifacts {
    archives packageSources
    archives packageDocs
  }
  */

  configurations {
    testOutput {
      extendsFrom testCompile
      description 'a dependency that exposes test artifacts'
    }
  }

  /*
  configurations.all {
    resolutionStrategy {
      // fail eagerly on version conflict (includes transitive dependencies)
      // e.g. multiple different versions of the same dependency (group and name are equal)
      failOnVersionConflict()
    }
  }
  */

  task packageTests(type: Jar) {
    from sourceSets.test.output
    classifier = 'tests'
  }
  artifacts {
    testOutput packageTests
  }
}

task generateSources {
  dependsOn ':snappy-spark:snappy-spark-streaming-flume-sink_' + scalaBinaryVersion + ':generateAvroJava'
  dependsOn ':snappy-store:generateSources'
}

task assembly {
  dependsOn ":snappy-tools_${scalaBinaryVersion}:shadowJar"

  doFirst {
    delete buildDir
    buildDir.mkdirs()
    file("${buildDir}/lib").mkdirs()
  }
  doLast {
    // copy datanucleus jars specifically since they don't work as part of fat jar
    def datanucleusJars = project(":snappy-spark:snappy-spark-hive_${scalaBinaryVersion}").configurations.runtime.filter {
      it.getName().contains('datanucleus')
    }
    copy {
      from datanucleusJars
      into "${buildDir}/lib"
    }

    // create the RELEASE file
    def release = file("${buildDir}/RELEASE")
    def gitCommitId = "git rev-parse HEAD".execute().text.trim()
    release << "Snappy Spark ${project.version} ${gitCommitId} built for Hadoop $hadoopVersion\n"
    release << "Build flags: ${buildFlags}\n"

    def toolsProject = project(":snappy-tools_${scalaBinaryVersion}")
    def baseName = 'snappy-spark-assembly'
    def archiveName = "snappy-spark-assembly_${scalaBinaryVersion}-${version}-hadoop${hadoopVersion}.jar"
    println "SW: renaming ${toolsProject.buildDir}/libs/${toolsProject.shadowJar.archiveName} to ${buildDir}/lib/${archiveName}"
    file("${toolsProject.buildDir}/libs/${toolsProject.shadowJar.archiveName}").renameTo("${buildDir}/lib/${archiveName}")
    copy {
      from "${project(':snappy-spark').projectDir}/bin"
      into "${buildDir}/bin"
      filter { line ->
        line.replaceAll('assembly/target/scala-2.10', 'lib')
      }
    }
    copy {
      from "${project(':snappy-spark').projectDir}/sbin"
      into "${buildDir}/sbin"
    }
    copy {
      from "${project(':snappy-spark').projectDir}/conf"
      into "${buildDir}/conf"
    }
    copy {
      from "${project(':snappy-spark').projectDir}/python"
      into "${buildDir}/python"
    }
    copy {
      from "${project(':snappy-spark').projectDir}/data"
      into "${buildDir}/data"
    }

    def sparkR = "${project(':snappy-spark').projectDir}/R/lib/SparkR"
    if (file(sparkR).exists()) {
      copy {
        from sparkR
        into "${buildDir}/R/lib"
      }
    }
  }
}

