<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="SnappyData Development Team">
  
  <title>Developing Apps using the Spark API - SnappyData Documentation</title>
  

  <link rel="shortcut icon" href="../favicon.ico">
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../extra.css" rel="stylesheet">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Developing Apps using the Spark API";
    var mkdocs_page_input_path = "jobs.md";
    var mkdocs_page_url = "/jobs/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> SnappyData Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../build-instructions/">Building from source, project layout</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../snappyIntroduction/">Overview</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../features/">Key features</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../architecture/">Architecture</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../configuration/">Configuring the cluster</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../connectingToCluster/">Connecting using JDBC, Spark</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Developing Apps using the Spark API</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#building-snappy-applications-using-spark-api">Building Snappy applications using Spark API</a></li>
                
                    <li><a class="toctree-l4" href="#snappycontext">SnappyContext</a></li>
                
                    <li><a class="toctree-l4" href="#snappystreamingcontext">SnappyStreamingContext</a></li>
                
                    <li><a class="toctree-l4" href="#running-spark-programs-inside-the-database">Running Spark programs inside the database</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../rowAndColumnTables/">Row and Column tables</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../aqp/">Synopsis Data Engine (SDE)</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../streamingWithSQL/">Stream processing using SQL</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../deployment/">Deployment topologies</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../apidocsintro/">API</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../aqp_aws/">Using iSight-Cloud</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>About</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../LICENSE/">License</a>
        
    </li>

        
    </ul>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">SnappyData Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Developing Apps using the Spark API</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/SnappyDataInc/snappydata" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="building-snappy-applications-using-spark-api">Building Snappy applications using Spark API</h2>
<p>SnappyData bundles Spark and supports all the Spark APIs. You can create Object based RDDs and run transformations or use the higher level APIs (like Spark ML). All SnappyData managed tables are also accessible as DataFrame and the API extends Spark classes like SQLContext and DataFrames.  So, we recommend getting to know the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#overview">concepts in SparkSQL</a> and the <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes">DataFrame API</a>. And, you can store and manage arbitrary RDDs (or even Spark DataSets) through implicit or explicit transformation to a DataFrame. While, the complete SQL support is still evolving, the supported SQL is much richer than SparkSQL. The extension SQL supported by the SnappyStore can be referenced <a href="../rowAndColumnTables/">here</a>.</p>
<p>In Spark SQL, all tables are temporary and cannot be shared across different applications. While you can manage such temporary tables, SnappyData tables are automatically registered to a built-in persistent catalog. This is similar to how Spark SQL uses the Hive catalog to natively work with Hive clusters. Data in tables is primarily managed in-memory with one or more consistent copies across machines or racks, but, can also be reliably managed on disk. </p>
<h3 id="snappycontext">SnappyContext</h3>
<p>A <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.SnappyContext">SnappyContext</a> is the main entry point for SnappyData extensions to Spark. A SnappyContext extends Spark's <a href="http://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.sql.SQLContext">SQLContext</a> to work with Row and Column tables. Any DataFrame can be managed as SnappyData tables and any table can be accessed as a DataFrame. This is similar to <a href="http://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.sql.hive.HiveContext">HiveContext</a> - integrates the SQLContext functionality with the Snappy store.</p>
<h5 id="using-snappycontext-to-create-table-and-query-data">Using SnappyContext to create table and query data</h5>
<p>Below are examples to create a SnappyContext from SparkContext.</p>
<h6 id="scala">Scala</h6>
<pre><code class="scala">  val conf = new org.apache.spark.SparkConf()
               .setAppName(&quot;ExampleTest&quot;)
               .setMaster(&quot;local[*]&quot;)

  val sc = new org.apache.spark.SparkContext(conf)
  // get the SnappyContext
  val snc = org.apache.spark.sql.SnappyContext(sc)
</code></pre>

<h6 id="java">Java</h6>
<pre><code class="java">  SparkConf conf = new org.apache.spark.SparkConf()
               .setAppName(&quot;ExampleTest&quot;)
               .setMaster(&quot;local[*]&quot;);

  JavaSparkContext sc = new JavaSparkContext(conf);
  // get the SnappyContext
  SnappyContext snc = SnappyContext.getOrCreate(sc);
</code></pre>

<h6 id="python">Python</h6>
<pre><code class="python">from pyspark.sql.snappy import SnappyContext
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName(&quot;ExampleTest&quot;).setMaster(&quot;local[*]&quot;)
sc = SparkContext(conf=conf)
# get the SnappyContext
snc = SnappyContext(sc)
</code></pre>

<p>Create columnar tables using API. Other than <code>create</code> and <code>drop</code> table, rest are all based on the Spark SQL Data Source APIs. </p>
<h6 id="scala_1">Scala</h6>
<pre><code class="scala">  val props1 = Map(&quot;BUCKETS&quot; -&gt; &quot;2&quot;)  // Number of partitions to use in the SnappyStore
  case class Data(COL1: Int, COL2: Int, COL3: Int)
  val data = Seq(Seq(1, 2, 3), Seq(7, 8, 9), Seq(9, 2, 3), Seq(4, 2, 3), Seq(5, 6, 7))
  val rdd = sc.parallelize(data, data.length).map(s =&gt; new Data(s(0), s(1), s(2)))

  val dataDF = snc.createDataFrame(rdd)

  // create a column table
  snc.dropTable(&quot;COLUMN_TABLE&quot;, ifExists = true)

  // &quot;column&quot; is the table format (that is row or column)
  // dataDF.schema provides the schema for table
  snc.createTable(&quot;COLUMN_TABLE&quot;, &quot;column&quot;, dataDF.schema, props1)
  // append dataDF into the table
  dataDF.write.insertInto(&quot;COLUMN_TABLE&quot;)

  val results1 = snc.sql(&quot;SELECT * FROM COLUMN_TABLE&quot;)
  println(&quot;contents of column table are:&quot;)
  results1.foreach(println)

</code></pre>

<h6 id="java_1">Java</h6>
<pre><code class="java">
    Map&lt;String, String&gt; props1 = new HashMap&lt;&gt;();
    props1.put(&quot;buckets&quot;, &quot;11&quot;);

    JavaRDD&lt;Row&gt; jrdd = jsc.parallelize(Arrays.asList(
        RowFactory.create(1,2,3),
        RowFactory.create(7,8,9),
        RowFactory.create(9,2,3),
        RowFactory.create(4,2,3),
        RowFactory.create(5,6,7)
    ));

    StructType schema = new StructType(new StructField[]{
        new StructField(&quot;col1&quot;, DataTypes.IntegerType, false, Metadata.empty()),
        new StructField(&quot;col2&quot;, DataTypes.IntegerType, false, Metadata.empty()),
        new StructField(&quot;col3&quot;, DataTypes.IntegerType, false, Metadata.empty()),
    });

    DataFrame dataDF = snc.createDataFrame(jrdd, schema);

    // create a column table
    snc.dropTable(&quot;COLUMN_TABLE&quot;, true);

    // &quot;column&quot; is the table format (that is row or column)
    // dataDF.schema provides the schema for table
    snc.createTable(&quot;COLUMN_TABLE&quot;, &quot;column&quot;, dataDF.schema(), props1, false);
    // append dataDF into the table
    dataDF.write().insertInto(&quot;COLUMN_TABLE&quot;);

    DataFrame results1 = snc.sql(&quot;SELECT * FROM COLUMN_TABLE&quot;);
    System.out.println(&quot;contents of column table are:&quot;);
    for (Row r : results1.select(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;). collect()) {
        System.out.println(r);
    }

</code></pre>

<h6 id="python_1">Python</h6>
<pre><code class="python">from pyspark.sql.types import *

data = [(1,2,3),(7,8,9),(9,2,3),(4,2,3),(5,6,7)]
rdd = sc.parallelize(data)
schema=StructType([StructField(&quot;col1&quot;, IntegerType()), 
                   StructField(&quot;col2&quot;, IntegerType()), 
                   StructField(&quot;col3&quot;, IntegerType())])

dataDF = snc.createDataFrame(rdd, schema)

# create a column table
snc.dropTable(&quot;COLUMN_TABLE&quot;, True)
#&quot;column&quot; is the table format (that is row or column)
#dataDF.schema provides the schema for table
snc.createTable(&quot;COLUMN_TABLE&quot;, &quot;column&quot;, dataDF.schema, True, buckets=&quot;11&quot;)

#append dataDF into the table
dataDF.write.insertInto(&quot;COLUMN_TABLE&quot;)
results1 = snc.sql(&quot;SELECT * FROM COLUMN_TABLE&quot;)

print(&quot;contents of column table are:&quot;)
results1.select(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;). show()
</code></pre>

<p>The optional BUCKETS attribute specifies the number of partitions or buckets to use. In SnappyStore, when data migrates between nodes (say if the cluster was expanded) a bucket is the smallest unit that can be moved around. For more details about the properties ('props1' map in above example) and createTable API refer to documentation for <a href="../rowAndColumnTables/">row and column tables</a></p>
<p>Create row tables using API, update the contents of row table</p>
<pre><code class="scala">  // create a row format table called ROW_TABLE
  snc.dropTable(&quot;ROW_TABLE&quot;, ifExists = true)
  // &quot;row&quot; is the table format 
  // dataDF.schema provides the schema for table
  val props2 = Map.empty[String, String]
  snc.createTable(&quot;ROW_TABLE&quot;, &quot;row&quot;, dataDF.schema, props2)

  // append dataDF into the data
  dataDF.write.insertInto(&quot;ROW_TABLE&quot;)

  val results2 = snc.sql(&quot;select * from ROW_TABLE&quot;)
  println(&quot;contents of row table are:&quot;)
  results2.foreach(println)

  // row tables can be mutated
  // for example update &quot;ROW_TABLE&quot; and set col3 to 99 where
  // criteria &quot;col3 = 3&quot; is true using update API
  snc.update(&quot;ROW_TABLE&quot;, &quot;COL3 = 3&quot;, org.apache.spark.sql.Row(99), &quot;COL3&quot; )

  val results3 = snc.sql(&quot;SELECT * FROM ROW_TABLE&quot;)
  println(&quot;contents of row table are after setting col3 = 99 are:&quot;)
  results3.foreach(println)

  // update rows using sql update statement
  snc.sql(&quot;UPDATE ROW_TABLE SET COL1 = 100 WHERE COL3 = 99&quot;)
  val results4 = snc.sql(&quot;SELECT * FROM ROW_TABLE&quot;)
  println(&quot;contents of row table are after setting col1 = 100 are:&quot;)
  results4.foreach(println)
</code></pre>

<h3 id="snappystreamingcontext">SnappyStreamingContext</h3>
<p>SnappyData extends Spark streaming so stream definitions can be declaratively written using SQL and these streams can be analyzed using static and dynamic SQL.</p>
<p>Below example shows how to use the SnappyStreamingContext to apply a schema to existing DStream and then query the SchemaDStream with simple SQL. It also shows the SnappyStreamingContext ability to deal with sql queries.</p>
<h6 id="scala_2">Scala</h6>
<pre><code class="scala">import org.apache.spark.sql._
import org.apache.spark.streaming._
import scala.collection.mutable
import org.apache.spark.rdd._
import org.apache.spark.sql.types._
import scala.collection.immutable.Map

  val snsc = new SnappyStreamingContext(sc, Duration(1))
  val schema = StructType(List(StructField(&quot;id&quot;, IntegerType) ,StructField(&quot;text&quot;, StringType)))

  case class ShowCaseSchemaStream (loc:Int, text:String)

  snsc.snappyContext.dropTable(&quot;streamingExample&quot;, ifExists = true)
  snsc.snappyContext.createTable(&quot;streamingExample&quot;, &quot;column&quot;,  schema, Map.empty[String, String] , false)

  def rddList(start:Int, end:Int) = sc.parallelize(start to end).map(i =&gt; ShowCaseSchemaStream( i, s&quot;Text$i&quot;))

  val dstream = snsc.queueStream[ShowCaseSchemaStream](
                mutable.Queue(rddList(1, 10), rddList(10, 20), rddList(20, 30)))

  val schemaDStream = snsc.createSchemaDStream(dstream )

  schemaDStream.foreachDataFrame(df =&gt; { 
      df.write.format(&quot;column&quot;).
      mode(SaveMode.Append).
      options(Map.empty[String, String]).
      saveAsTable(&quot;streamingExample&quot;)    })

  snsc.start()   
  snsc.sql(&quot;select count(*) from streamingExample&quot;).show
</code></pre>

<h6 id="python_2">Python</h6>
<pre><code class="python">from pyspark.streaming.snappy.context import SnappyStreamingContext
from pyspark.sql.types import *

def  rddList(start, end): 
  return sc.parallelize(range(start,  end)).map(lambda i : ( i, &quot;Text&quot; + str(i)))

def saveFunction(df):
   df.write.format(&quot;column&quot;).mode(&quot;append&quot;).saveAsTable(&quot;streamingExample&quot;)

schema=StructType([StructField(&quot;loc&quot;, IntegerType()), 
                   StructField(&quot;text&quot;, StringType())])

snsc = SnappyStreamingContext(sc, 1)

snsc.queueStream([rddList(1,10) , rddList(10,20), rddList(20,30)])

snsc._snappycontext.dropTable(&quot;streamingExample&quot; , True)
snsc._snappycontext.createTable(&quot;streamingExample&quot;, &quot;column&quot;, schema)

dstream = snsc.queueStream(getQueueOfRDDs())
schemadstream = snsc.createSchemaDStream(dstream, schema)
schemadstream.foreachDataFrame(lambda df: saveFunction(df))
snsc.start()

snsc.sql(&quot;select count(*) from streamingExample&quot;).show()
</code></pre>

<blockquote>
<p>Note - Currently Snappy dont have Python API's added for continuous queries and SDE/Sampling.</p>
</blockquote>
<h3 id="running-spark-programs-inside-the-database">Running Spark programs inside the database</h3>
<blockquote>
<p>Note: Above simple example uses local mode (i.e. development mode) to create tables and update data. In the production environment, users will want to deploy the SnappyData system as a unified cluster (default cluster model that consists of servers that embed colocated Spark executors and Snappy stores, locators, and a job server enabled lead node) or as a split cluster (where Spark executors and Snappy stores form independent clusters). Refer to the  <a href="../deployment/">deployment</a> chapter for all the supported deployment modes and the <a href="../configuration/">configuration</a> chapter for configuring the cluster. This mode is supported in both Java and Scala. Support for Python is yet not added.</p>
</blockquote>
<p>To create a job that can be submitted through the job server, the job must implement the <em>SnappySQLJob or SnappyStreamingJob</em> trait. Your job will look like:</p>
<h6 id="scala_3">Scala</h6>
<pre><code class="scala">class SnappySampleJob implements SnappySQLJob {
  /** Snappy uses this as an entry point to execute Snappy jobs. **/
  def runSnappyJob(sc: SnappyContext, jobConfig: Config): Any

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation
}
</code></pre>

<h6 id="java_2">Java</h6>
<pre><code class="java">class SnappySampleJob extends SnappySQLJob {
  /** Snappy uses this as an entry point to execute Snappy jobs. **/
  public Object runSnappyJob(SnappyContext snc, Config jobConfig) {//Implementation}

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  public SnappyJobValidation isValidJob(SnappyContext snc, Config config) {//validate}
}

</code></pre>

<h6 id="scala_4">Scala</h6>
<pre><code class="scala">class SnappyStreamingSampleJob implements SnappyStreamingJob {
  /** Snappy uses this as an entry point to execute Snappy jobs. **/
  def runSnappyJob(sc: SnappyStreamingContext, jobConfig: Config): Any

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  def isValidJob(sc: SnappyContext, config: Config): SnappyJobValidation
}
</code></pre>

<h6 id="java_3">Java</h6>
<pre><code class="java">class SnappyStreamingSampleJob extends JavaSnappyStreamingJob {
  /** Snappy uses this as an entry point to execute Snappy jobs. **/
  public Object runSnappyJob(JavaSnappyStreamingContext snsc, Config jobConfig) {//implementation }

  /** SnappyData calls this function to validate the job input and reject invalid job requests **/
  public SnappyJobValidation isValidJob(JavaSnappyStreamingContext snc, Config jobConfig)
  {//validate}
}
</code></pre>

<blockquote>
<p>The <em>Job</em> traits are simply extensions of the <em>SparkJob</em> implemented by <a href="https://github.com/spark-jobserver/spark-jobserver">Spark JobServer</a>. </p>
</blockquote>
<p>• <code>runSnappyJob</code> contains the implementation of the Job.
The <a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.SnappyContext">SnappyContext</a>/<a href="http://snappydatainc.github.io/snappydata/apidocs/#org.apache.spark.sql.streaming.SnappyStreamingContext">SnappyStreamingContext</a> is managed by the SnappyData Leader (which runs an instance of Spark JobServer) and will be provided to the job through this method. This relieves the developer from the boiler-plate configuration management that comes with the creation of a Spark job and allows the Job Server to manage and re-use contexts.</p>
<p>• <code>isValidJob</code> allows for an initial validation of the context and any provided configuration.
 If the context and configuration are OK to run the job, returning spark.jobserver.SnappyJobValid
  will let the job execute, otherwise returning spark.jobserver.SnappyJobInvalid(reason) prevents
   the job from running and provides means to convey the reason of failure. In this case, the call immediately returns an HTTP/1.1 400 Bad Request status code. validate helps you preventing running jobs that will eventually fail due to missing or wrong configuration and save both time and resources.</p>
<p>See <a href="https://github.com/SnappyDataInc/snappydata/tree/master/examples/src/main/scala/io/snappydata/examples">examples</a> for Spark and spark streaming jobs. </p>
<p>SnappySQLJob trait extends the SparkJobBase trait. It provides users the singleton SnappyContext object that may be reused across jobs. SnappyContext singleton object creates one SQLContext per incoming SQL connection. Similarly SnappyStreamingJob provides users access to SnappyStreamingContext object that can be reused across jobs</p>
<h4 id="submitting-jobs">Submitting jobs</h4>
<p>Following command submits <a href="https://github.com/SnappyDataInc/snappydata/blob/master/examples/src/main/scala/io/snappydata/examples/CreateAndLoadAirlineDataJob.scala">CreateAndLoadAirlineDataJob</a> from the <a href="https://github.com/SnappyDataInc/snappydata/tree/master/examples/src/main/scala/io/snappydata/examples">examples</a> directory.   This job creates dataframes from parquet files, loads the data from dataframe into column tables and row tables and creates sample table on column table in its runJob method. The program is compiled into a jar file (quickstart-0.6.jar) and submitted to jobs server as shown below.</p>
<pre><code>$ bin/snappy-job.sh submit  \
    --lead hostNameOfLead:8090  \
    --app-name airlineApp \
    --class  io.snappydata.examples.CreateAndLoadAirlineDataJob \
    --app-jar $SNAPPY_HOME/lib/quickstart-0.6.jar
</code></pre>

<p>The utility snappy-job.sh submits the job and returns a JSON that has a jobId of this job.</p>
<ul>
<li>--lead option specifies the host name of the lead node along with the port on which it accepts jobs (8090)</li>
<li>--app-name option specifies the name given to the submitted app</li>
<li>--class specifies the name of the class that contains implementation of the Spark job to be run</li>
<li>--app-jar specifies the jar file that packages the code for Spark job</li>
</ul>
<p>The status returned by the utility is shown below:</p>
<pre><code class="json">{
  &quot;status&quot;: &quot;STARTED&quot;,
  &quot;result&quot;: {
    &quot;jobId&quot;: &quot;321e5136-4a18-4c4f-b8ab-f3c8f04f0b48&quot;,
    &quot;context&quot;: &quot;snappyContext1452598154529305363&quot;
  }
}
</code></pre>

<p>This job ID can be used to query the status of the running job. </p>
<pre><code>$ bin/snappy-job.sh status  \
    --lead hostNameOfLead:8090  \
    --job-id 321e5136-4a18-4c4f-b8ab-f3c8f04f0b48

{
  &quot;duration&quot;: &quot;17.53 secs&quot;,
  &quot;classPath&quot;: &quot;io.snappydata.examples.CreateAndLoadAirlineDataJob&quot;,
  &quot;startTime&quot;: &quot;2016-01-12T16:59:14.746+05:30&quot;,
  &quot;context&quot;: &quot;snappyContext1452598154529305363&quot;,
  &quot;result&quot;: &quot;See /home/hemant/snappyhome/work/localhost-lead-1/CreateAndLoadAirlineDataJob.out&quot;,
  &quot;status&quot;: &quot;FINISHED&quot;,
  &quot;jobId&quot;: &quot;321e5136-4a18-4c4f-b8ab-f3c8f04f0b48&quot;
}
</code></pre>

<p>Once the tables are created, they can be queried by firing another job. Please refer to <a href="https://github.com/SnappyDataInc/snappydata/blob/master/examples/src/main/scala/io/snappydata/examples/AirlineDataJob.scala">AirlineDataJob</a> from <a href="https://github.com/SnappyDataInc/snappydata/tree/master/examples/src/main/scala/io/snappydata/examples">examples</a> for the implementation of the job. </p>
<pre><code>$ bin/snappy-job.sh submit  \
    --lead hostNameOfLead:8090  \
    --app-name airlineApp \
    --class  io.snappydata.examples.AirlineDataJob \
    --app-jar $SNAPPY_HOME/lib/quickstart-0.6.jar
</code></pre>

<p>The status of this job can be queried in the same manner as shown above. The result of the this job will return a file path that has the query results. </p>
<p>Python users can also submit the python script using spark-submit in split cluster mode. For example below script can be used to read the data loaded by the CreateAndLoadAirlineDataJob. "snappydata.store.locators" property denotes the locator url of the snappy cluster and it is used to connect to the snappy cluster.</p>
<pre><code>$ bin/spark-submit \
  --master spark://pnq-nthanvi02:7077 \
  --conf snappydata.store.locators=localhost:10334 \
  --conf spark.ui.port=4042  
  python/examples/AirlineDataPythonApp.py
</code></pre>

<h4 id="streaming-jobs">Streaming jobs</h4>
<p>An implementation of SnappyStreamingJob can be submitted to the lead node of SnappyData cluster by specifying <code>--stream</code> as an option to the submit command. This option will cause creation of a new SnappyStreamingContext before the job is submitted. Alternatively, user may specify the name of an existing/pre-created streaming context as <code>--context &lt;context-name&gt;</code> with the submit command.</p>
<p>For example, <a href="https://github.com/SnappyDataInc/snappydata/blob/master/examples/src/main/scala/io/snappydata/examples/TwitterPopularTagsJob.scala">TwitterPopularTagsJob</a> from the <a href="https://github.com/SnappyDataInc/snappydata/tree/master/examples/src/main/scala/io/snappydata/examples">examples</a> directory can be submitted as follows. This job creates stream tables on tweet streams, registers continuous queries and prints results of queries such as top 10 hash tags of last two second, top 10 hash tags until now, top 10 popular tweets.</p>
<pre><code>$ bin/snappy-job.sh submit  \
    --lead hostNameOfLead:8090  \
    --app-name airlineApp \
    --class  io.snappydata.examples.TwitterPopularTagsJob \
    --app-jar $SNAPPY_HOME/lib/quickstart-0.6.jar \
    --stream

{
  &quot;status&quot;: &quot;STARTED&quot;,
  &quot;result&quot;: {
    &quot;jobId&quot;: &quot;982ac142-3550-41e1-aace-6987cb39fec8&quot;,
    &quot;context&quot;: &quot;snappyStreamingContext1463987084945028747&quot;
  }
}
</code></pre>

<p>User needs to stop the currently running streaming job followed by its streaming context if the user intends to submit another streaming job with a new streaming context.</p>
<pre><code>$ bin/snappy-job.sh stop  \
    --lead hostNameOfLead:8090  \
    --job-id 982ac142-3550-41e1-aace-6987cb39fec8

$ bin/snappy-job.sh listcontexts  \
    --lead hostNameOfLead:8090
[&quot;snappyContext1452598154529305363&quot;, &quot;snappyStreamingContext1463987084945028747&quot;, &quot;snappyStreamingContext&quot;]

$ bin/snappy-job.sh stopcontext snappyStreamingContext1463987084945028747  \
    --lead hostNameOfLead:8090
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../rowAndColumnTables/" class="btn btn-neutral float-right" title="Row and Column tables">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../connectingToCluster/" class="btn btn-neutral" title="Connecting using JDBC, Spark"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright &copy; 2016 SnappyData Inc.</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/SnappyDataInc/snappydata" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../connectingToCluster/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../rowAndColumnTables/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
